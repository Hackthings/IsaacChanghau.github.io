<!doctype html>



  


<html class="theme-next mist use-motion" lang="en">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="java,deep learning,deeplearning4j,natural language processing,word embeddings,skip-gram,logistic regression,bag-of-words," />





  <link rel="alternate" href="/atom.xml" title="Isaac Changhau" type="application/atom+xml" />




  <link rel="shortcut icon" type="image/x-icon" href="/tltimg.jpg?v=5.1.1" />






<meta name="description" content="I began to get to know the natural language process area when I started to work as a research engineer around one year">
<meta name="keywords" content="java,deep learning,deeplearning4j,natural language processing,word embeddings,skip-gram,logistic regression,bag-of-words">
<meta property="og:type" content="article">
<meta property="og:title" content="Word2Vec Summary -- Mathematical Principles and Java Implementation">
<meta property="og:url" content="https://isaacchanghau.github.io/2017/05/13/Word2Vec/index.html">
<meta property="og:site_name" content="Isaac Changhau">
<meta property="og:description" content="I began to get to know the natural language process area when I started to work as a research engineer around one year">
<meta property="og:locale" content="en">
<meta property="og:image" content="https://isaacchanghau.github.io/images/nlp/word2vec/sigmoid.png">
<meta property="og:image" content="https://isaacchanghau.github.io/images/nlp/word2vec/partition.png">
<meta property="og:image" content="https://isaacchanghau.github.io/images/nlp/word2vec/huffmantree.png">
<meta property="og:image" content="https://isaacchanghau.github.io/images/nlp/word2vec/huffmancoding.png">
<meta property="og:image" content="https://isaacchanghau.github.io/images/nlp/word2vec/ngramparams.png">
<meta property="og:image" content="https://isaacchanghau.github.io/images/nlp/word2vec/bengio.png">
<meta property="og:image" content="https://isaacchanghau.github.io/images/nlp/word2vec/word2vecmodels.png">
<meta property="og:image" content="https://isaacchanghau.github.io/images/nlp/word2vec/cbow.png">
<meta property="og:image" content="https://isaacchanghau.github.io/images/nlp/word2vec/cbow-hierarchicalsoftmax.png">
<meta property="og:image" content="https://isaacchanghau.github.io/images/nlp/word2vec/subdivision.png">
<meta property="og:image" content="https://isaacchanghau.github.io/images/nlp/word2vec/skipgram.png">
<meta property="og:updated_time" content="2018-02-20T04:05:06.536Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Word2Vec Summary -- Mathematical Principles and Java Implementation">
<meta name="twitter:description" content="I began to get to know the natural language process area when I started to work as a research engineer around one year">
<meta name="twitter:image" content="https://isaacchanghau.github.io/images/nlp/word2vec/sigmoid.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    sidebar: {"position":"right","display":"post","offset":12,"offset_float":0,"b2t":true,"scrollpercent":true},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://isaacchanghau.github.io/2017/05/13/Word2Vec/"/>





  <title>Word2Vec Summary -- Mathematical Principles and Java Implementation | Isaac Changhau</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  














</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-right page-post-detail ">
    <div class="headband"></div>
    
    <!--<a href="https://github.com/IsaacChanghau"><img style="position: absolute; top: 0; right: 0; border: 0;" src="https://camo.githubusercontent.com/38ef81f8aca64bb9a64448d0d70f1308ef5341ab/68747470733a2f2f73332e616d617a6f6e6177732e636f6d2f6769746875622f726962626f6e732f666f726b6d655f72696768745f6461726b626c75655f3132313632312e706e67" alt="Fork me on GitHub" data-canonical-src="https://s3.amazonaws.com/github/ribbons/forkme_right_darkblue_121621.png"></a>-->

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Isaac Changhau</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">Stay Hungry, Stay Foolish</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th-list"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-vcard"></i> <br />
            
            About
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            Search
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="Searching..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://isaacchanghau.github.io/2017/05/13/Word2Vec/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Isaac Changhau">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Isaac Changhau">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Word2Vec Summary -- Mathematical Principles and Java Implementation</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-05-13T15:54:47+08:00">
                2017-05-13
              </time>
            

            
              <span class="post-updated">
                &nbsp; | &nbsp; <i class="fa fa-calendar-check-o"></i> Updated on
                <time itemprop="dateUpdated" datetime="2018-02-20T12:05:06+08:00" content="2018-02-20">
                  2018-02-20
                </time>
              </span>
            

            

            
          </span>

          
            <span class="post-letters-count">
              &nbsp; | &nbsp;
              <i class="fa fa-file-word-o"></i>
            <span>Count 11,562 words</span>
              <!--&nbsp; | &nbsp;
              <i class="fa fa-clock-o"></i>
            <span>Reading 72 min</span>-->
            </span>
          

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Natural-Language-Processing/" itemprop="url" rel="index">
                    <span itemprop="name">Natural Language Processing</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>I began to get to know the natural language process area when I started to work as a research engineer around one year<a id="more"></a> ago, and the first technique I was asked to learn is the Word2Vec. Since the Word2Vec was proposed amost four years ago, there are already amount of research reports, papers, tutorials and even applications are available online. In order to understand this technique well, I explored and studied many of those online resources at that time. Now, one year later, I find that the details of Word2Vec have disappeared from my mind when I reviewed it a few days ago, and consider that the knowledge I gained from Word2Vec helps me a lot in my futher work since I studied it, thus, it is a good choice for me to write something about Word2Vec down and cement the impression of it. Note that Word2Vec does not belong to deep learning methods strictly, it is only a two-layer, shallow neural networks, I also tag this article to <em>Deep Learning</em> for convenience.</p>
<h1 id="introduction">Introduction</h1>
<p>Word2Vec was created by a team of researchers led by Tomas Mikolov at Google. It is a shallow, two-layer neural networks that are trained to reconstruct linguistic contexts of words. Word2Vec takes as its input a large corpus of text and produces a vector space, typically of several hundred dimensions, with each unique word in the corpus being assigned a corresponding vector in the space.</p>
<h1 id="prerequisites">Prerequisites</h1>
<p>Here I will introduce the knowledge used in Word2Vec, including <em>Sigmoid</em>, <em>Softmax</em>, <em>Logistic Regression</em>, <em>Huffman Coding</em>, and so forth.</p>
<h2 id="sigmoid">Sigmoid</h2>
<p>A <a href="https://en.wikipedia.org/wiki/Sigmoid_function" target="_blank" rel="noopener">Sigmoid function</a> is a special case of the <a href="https://en.wikipedia.org/wiki/Logistic_function" target="_blank" rel="noopener">logistic function</a> having a characteristic “S”-shaped curve. It is defined by the formula<span class="math display">\[\sigma (x)=\frac{1}{1+e^{-x}}\]</span>And its derivaitve is <span class="math inline">\(\sigma&#39;(x)=\sigma (x)\centerdot (1-\sigma (x))\)</span>, the graph of Sigmoid and its derivative are shown below <img src="/images/nlp/word2vec/sigmoid.png" alt="Sigmoid"> Sigmoid function takes a real-valued number and “squashes” it into range between 0 and 1, i.e., <span class="math inline">\(\sigma (x)\in (0,1)\)</span>. Approximately, we can derive from the graph that large negative numbers (e.g. <span class="math inline">\(x&lt;-6, \sigma (-6)\approx 0.0025\)</span>) become 0 and large positive numbers (e.g. <span class="math inline">\(x&gt;6, \sigma (6)\approx 0.9975\)</span>) become 1. Thus, in practice, considering that we need to compute a mass of <span class="math inline">\(\sigma (x)\)</span> of various <span class="math inline">\(x\)</span> and the required precision is not strict, thus we can apply an approximate computation method (as Mikolov did). Here we assume that <span class="math inline">\(\sigma (x)=0, x&lt;-6\)</span> and <span class="math inline">\(\sigma (x)=1, x&gt;6\)</span>, then, for <span class="math inline">\(x\in [-6,6]\)</span>, we isometrically cut into 1000 parts, as shown in the graph, <img src="/images/nlp/word2vec/partition.png" alt="Partition"> compute <span class="math inline">\(\sigma (x)\)</span> for each <span class="math inline">\(x_{i}\)</span>, and store them into an array, which can be expediently used for further process. Below is the Java implementation. <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/** Boundary for maximum exponent allowed */</span></span><br><span class="line"><span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> MAX_EXP = <span class="number">6</span>;</span><br><span class="line"><span class="comment">/** Size of the pre-cached exponent table */</span></span><br><span class="line"><span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> EXP_TABLE_SIZE = <span class="number">1_000</span>;</span><br><span class="line"><span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">double</span>[] EXP_TABLE = <span class="keyword">new</span> <span class="keyword">double</span>[EXP_TABLE_SIZE];</span><br><span class="line"><span class="keyword">static</span> &#123;</span><br><span class="line">	<span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; EXP_TABLE_SIZE; i++) &#123;</span><br><span class="line">		<span class="comment">// Precompute the exp() table</span></span><br><span class="line">		EXP_TABLE[i] = Math.exp((i / (<span class="keyword">double</span>) EXP_TABLE_SIZE * <span class="number">2</span> - <span class="number">1</span>) * MAX_EXP);</span><br><span class="line">		<span class="comment">// Precompute f(x) = x / (x + 1)</span></span><br><span class="line">		EXP_TABLE[i] /= EXP_TABLE[i] + <span class="number">1</span>;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h2 id="softmax">Softmax</h2>
<p>The Softmax function is a generalization of the logistic function that “squashes” a K-dimensional vector <span class="math inline">\(\mathbf{z}\)</span> from arbitrary real values to a K-dimensional vector <span class="math inline">\(\sigma (\mathbf{z})\)</span> of real values in the range <span class="math inline">\([0,1]\)</span> that add up to 1. For more details about Softmax, you can read my another blog: <a href="https://isaacchanghau.github.io/2017/05/12/Activation-Functions-in-Artificial-Neural-Networks/">Activation Functions in Artificial Neural Networks</a>.</p>
<h2 id="logistic-regression">Logistic Regression</h2>
<p><a href="https://en.wikipedia.org/wiki/Logistic_regression" target="_blank" rel="noopener">Logistic regression</a> is the appropriate regression analysis to conduct when the dependent variable is dichotomous. In logistic regression, the dependent variable is binary or dichotomous, i.e. it only contains data coded as 1 (true, success, etc.) or 0 (false, failure, etc.). The goal of logistic regression is to find the best fitting model to describe the relationship between the dichotomous characteristic of interest and a set of independent variables. Note that there is a related method named <a href="https://en.wikipedia.org/wiki/Multinomial_logistic_regression" target="_blank" rel="noopener">Multinomial Logistic Regression</a>, which is a classification method that generalizes logistic regression to multiclass problems, i.e. with more than two possible discrete outcomes. Here we only discuss the binary logistic regression. For instance, considering a binary classification problem, like spam classification, suppose that <span class="math inline">\(\{\mathbf{x}_{i},y_{i}\}_{i=1}^{m}\)</span> is the sample data of a binary classification problem, where <span class="math inline">\(\mathbf{x}_{i}\in \mathbb{R}^{n}\)</span>, <span class="math inline">\(y_{i}\in \{0,1\}\)</span>. Using Sigmoid function, for an arbitrary sample <span class="math inline">\(\mathbf{x}=(x_{1},x_{2},\dots ,x_{n})^{T}\)</span>, we can write the hypothesis function of such binary classification problem as<span class="math display">\[h_{\theta}(\mathbf{x})=\sigma (\theta_{0}+\theta_{1}x_{1}+\theta_{2}x_{2}+\dots +\theta_{n}x_{n})\]</span>where <span class="math inline">\(\theta=(\theta_{0},\theta_{1},\dots ,\theta_{n})^{T}\)</span> is the coefficient. Briefly, we introduce <span class="math inline">\(x_{0}=1\)</span> and extend <span class="math inline">\(\mathbf{x}\)</span> to <span class="math inline">\((x_{0},x_{1},x_{2},\dots ,x_{n})^{T}\)</span>, thus, <span class="math inline">\(h_{\theta}\)</span> can be simplified as<span class="math display">\[h_{\theta}(\mathbf{x})=\sigma (\theta^{T}\mathbf{x})=\frac{1}{1+e^{-\theta^{T}\mathbf{x}}}\]</span>The Sigmoid function maps any values of a real number to a value from 0 to 1, therefore, the output can be regarded as the posterior probability for each class. Assume the sample data follows <a href="https://en.wikipedia.org/wiki/Bernoulli_distribution" target="_blank" rel="noopener">Bernoulli distribution</a>, then we have<span class="math display">\[P(Y=y_{i}|\mathbf{x}_{i})=\begin{cases} h_{\theta}(\mathbf{x}_{i}),&amp;y_{i}=1; \\ 1-h_{\theta}(\mathbf{x}_{i}),&amp;y_{i}=0. \end{cases}\]</span>Combining the two equations, it can written as <span class="math display">\[P(Y=y_{i}|\mathbf{x}_{i})=\big(h_{\theta}(\mathbf{x}_{i}) \big)^{y_{i}}+\big( 1-h_{\theta}(\mathbf{x}_{i}) \big)^{1-y_{i}}\]</span>Next step is to compute <span class="math inline">\(\theta\)</span>, and the key point of this task is focusing on measuring the performance of model prediction, thus, here we introduce the likelihood function, which estimates the maximum likelihood of the coefficients, can be expressed as<span class="math display">\[L(\theta)=\prod_{i=1}^{m}\bigg(\big( h_{\theta}(\mathbf{x}_{i}) \big)^{y_{i}}+\big( 1-h_{\theta}(\mathbf{x}_{i}) \big)^{1-y_{i}}\bigg)\]</span>In order to derive the optimized coefficients <span class="math inline">\(\theta\)</span>, we need to maximize this likelihood function, but the calculation is complex because of the mathematical product. To make it easier, we take the logarithm of the likelihood function. Additionally, we insert a minus sign to turn the object to minimize the negative log-likelihood function. The equation is defined as<span class="math display">\[J(\theta)=-\log \big(L(\theta)\big)=-\frac{1}{m}\sum_{i=1}^{m}\bigg(y_{i}\centerdot\log(h_{\theta}(\mathbf{x}_{i}))+(1-y_{i})\centerdot\log(1-h_{\theta}(\mathbf{x}_{i}))\bigg)\]</span>which is so called logarithmic loss function or log-likelihood loss function, and <span class="math inline">\(\theta\)</span> is computed by optimized this function. For more datails about Logistic Regression: <a href="https://en.wikipedia.org/wiki/Logistic_regression" target="_blank" rel="noopener">Wikipedia</a>, <a href="http://ufldl.stanford.edu/tutorial/supervised/LogisticRegression/" target="_blank" rel="noopener">UFLDL Tutorial</a>. To help you have a better understanding of how to implement logistic regression. You can get codes of logistic regression from my GitHub repository: <a href="https://github.com/IsaacChanghau/NeuralNetworksBasics/blob/master/src/main/java/com/isaac/neuralnetworks/LogisticRegression.java" target="_blank" rel="noopener">[Full Java Implementation]</a>, <a href="https://github.com/IsaacChanghau/NeuralNetworksBasics/blob/master/src/main/java/com/isaac/neuralnetworksND4J/LogisticRegressionND4J.java" target="_blank" rel="noopener">[Java Implementation with ND4J]</a>. Since the codes are fairly long, I do not show them here in order to save space.</p>
<h2 id="bayes-formula">Bayes’ Formula</h2>
<p>Bayes’ formula is an important method for computing conditional probabilities. It is often used to compute posterior probabilities (as opposed to priorior probabilities) given observations. Suppose that <span class="math inline">\(P(A)\)</span> and <span class="math inline">\(P(B)\)</span> represent the probability of event <span class="math inline">\(A\)</span> happens and the probability of event <span class="math inline">\(B\)</span> happens respectively, <span class="math inline">\(P(A|B)\)</span> denotes the probability of event <span class="math inline">\(A\)</span> happens given event <span class="math inline">\(B\)</span>, <span class="math inline">\(P(A,B)\)</span> denotes the probability that event <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> happen simultaneously, so we have<span class="math display">\[P(A|B)=\frac{P(A,B)}{P(B)},\quad P(B|A)=\frac{P(A,B)}{P(A)},\\ P(A,B)=P(B)\centerdot P(A|B)=P(A)\centerdot P(B|A)\quad and\quad P(A|B)=P(A)\centerdot\frac{P(B|A)}{P(B)}\]</span>For four variables, <span class="math inline">\(A_{1}\)</span>, <span class="math inline">\(A_{2}\)</span>, <span class="math inline">\(A_{3}\)</span>, <span class="math inline">\(A_{4}\)</span>, using the knowledge above, it is able to compute that<span class="math display">\[P(A_{4},A_{3},A_{2},A_{1})=P(A_{4}|A_{3},A_{2},A_{1})\centerdot P(A_{3}|A_{2},A_{1})\centerdot P(A_{2}|A_{1})\centerdot P(A_{1})\]</span>Generally, consider an indexed set of sets <span class="math inline">\(A_{1},\dots,A_{n}\)</span>, we can derive that<span class="math display">\[P\bigg( \bigcap_{k=1}^{n}A_{k}\bigg) = \prod_{k=1}^{n}P\bigg( A_{k}|\bigcap_{j=1}^{k-1}A_{j}\bigg)\]</span>This is so called Bayes’ <a href="https://en.wikipedia.org/wiki/Chain_rule_%28probability%29" target="_blank" rel="noopener">Chain Rule</a>.</p>
<h2 id="huffman-coding">Huffman Coding</h2>
<p>In computer science and information theory, a Huffman code is a particular type of optimal prefix code that is commonly used for lossless data compression. The process of finding and/or using such a code proceeds by means of <a href="https://en.wikipedia.org/wiki/Huffman_coding" target="_blank" rel="noopener">Huffman coding</a>, a lossless data encoding algorithm, developed by <a href="https://en.wikipedia.org/wiki/David_A._Huffman" target="_blank" rel="noopener">David A. Huffman</a> and published in the 1952 paper – <a href="http://compression.ru/download/articles/huff/huffman_1952_minimum-redundancy-codes.pdf" target="_blank" rel="noopener">A Method for the Construction of Minimum-Redundancy Codes</a>.</p>
<h3 id="huffman-tree">Huffman Tree</h3>
<p><a href="https://en.wikipedia.org/wiki/Tree_%28data_structure%29" target="_blank" rel="noopener">Tree</a> is an important non-linear data structure that simulates a hierarchical tree structure, with a root value and subtrees of children with a parent node, represented as a set of linked nodes. A tree data structure can be defined recursively (locally) as a collection of nodes (starting at a root node), where each node is a data structure consisting of a value, together with a list of references to nodes (the “children”), with the constraints that no reference is duplicated, and none points to the root. Alternatively, a tree can be defined abstractly as a whole (globally) as an ordered tree, with a value assigned to each node. Both these perspectives are useful: while a tree can be analyzed mathematically as a whole, when actually represented as a data structure it is usually represented and worked with separately by node. And here is some common-used concepts in tree: - <em>Path</em> and <em>Path Length</em>: the path is a sequence of nodes and edges connecting a node with a descendant, while the number of edges is path length. For example, denote that the level number of root node as 1, then the path length from root node to the node at <span class="math inline">\(L\)</span> level is <span class="math inline">\(L-1\)</span>. - <em>Node Weight</em> and <em>Weight Path Length</em> (WPL): give a (non-negative) value with a certain meaning to the nodes in the tree, this value is called weight. The WPL is defined as the product of the node weight and the path length from root node to this node. - <em>WPL of Tree</em>: the WPL of Tree is defined as the sum of WPL of all the leaf nodes.</p>
<p><a href="https://en.wikipedia.org/wiki/Binary_tree" target="_blank" rel="noopener">Binary tree</a> is a typical tree data structure in which each node has at most two children, which are referred to as the left child and the right child. Particularly, <a href="https://en.wikipedia.org/wiki/Binary_search_tree" target="_blank" rel="noopener">binary search tree</a> (BST), also called ordered or sorted binary tree, is a particular type of binary tree that keep its nodes in sorted order, where the left subtree and right subtree can not interchange of position. Given <span class="math inline">\(n\)</span> weighted nodes as <span class="math inline">\(n\)</span> leaf nodes to construct a binary tree, if WPL of the built binary tree reaches the minimum, such built binary tree is an optimal binary tree, also named as <strong>Huffman Tree</strong>.</p>
<h3 id="construction-of-huffman-tree">Construction of Huffman Tree</h3>
<p>Given <span class="math inline">\(n\)</span> weighted nodes <span class="math inline">\(\{w_{1},w_{2},\dots ,w_{n}\}\)</span> as the leaf nodes of a binary tree, the Huffman tree can be constructed as follow: 1. Regard <span class="math inline">\(\{w_{1},w_{2},\dots ,w_{n}\}\)</span> as a forest of <span class="math inline">\(n\)</span> trees (each tree has only one node); 2. Combine two trees with minimal root node weight in the forest to build a new tree, each of these two tree becomes a subtree, and the root node weight of the new tree is the sum of the root nodes weight of its left and right subtrees; 3. Remove the two selected trees in step (2), and add the new tree into the forest; 4. Repeat step (2) and (3), till only one tree left in the forest, and this tree is the computed Huffman tree.</p>
<p>For instance, suppose that we have a list of weighted nodes <span class="math inline">\(\{15,8,6,5,3,1\}\)</span>, the Huffman tree can be built via this way <img src="/images/nlp/word2vec/huffmantree.png" alt="Construction of Huffman Tree"> From the graph, we can derive that the node with larger weight is more closer to root node. In the construction process, the additional node generated through merge process is marked as red. Since every two nodes need to be merged one time, thus, if the number of leaf nodes is <span class="math inline">\(n\)</span>, then <span class="math inline">\(n-1\)</span> additional nodes will be generated while constructing Huffman tree. In this case, <span class="math inline">\(n=6\)</span>, so 5 additional nodes are generated. Note that in the above case, we stipulate that the node with higher weight is the left child node, while the node with lower weight is the right child node. Certainly, if you reverse the stipulation, it is also true.</p>
<h3 id="coding-process">Coding Process</h3>
<p>The binary prefix code built by Huffman tree is called Huffman Code, which satisfy the condition of prefix code as well as guarantee the length of message codes is shortest. Given a message “AFTER DATA EAR ARE ART AREA” to transmit, the character set used here is “<span class="math inline">\(A\)</span>, <span class="math inline">\(E\)</span>, <span class="math inline">\(R\)</span>, <span class="math inline">\(T\)</span>, <span class="math inline">\(F\)</span>, <span class="math inline">\(D\)</span>”, their frequency are 8, 4, 5, 3, 1, 1, respectively. To transmit the message, we always want to keep the length of such message as short as possible, since the frequency of each character is different, so we try to use short codes for character with high frequency and long codes for character with low frequency in order to optimize whole message codes, then the Huffman coding is used. To construct the Huffman tree, we can derive <img src="/images/nlp/word2vec/huffmancoding.png" alt="Huffman Coding"> then each character can be represent by a binary code, e.g., <span class="math inline">\(A\)</span> is represented by “11”, <span class="math inline">\(F\)</span> is represented by “0101”, etc. The Word2Vec toolkit also use the technology of Huffman coding, it treats the word in the corpus as leaf node, and the frequency of the word in corpus as weight, by constructing Huffman tree to coding each word. There are two stipulations in Word2Vec, one is that the node with large weight is treated as left child node, and another is the left child node is coded as 1. To keep the same, we also follow these stipulations. Here is the Java Implementation of Huffman Coding in Word2Vec: <a href="https://github.com/IsaacChanghau/Word2VecJava/blob/master/src/main/java/com/medallia/word2vec/huffman/HuffmanCoding.java" target="_blank" rel="noopener">[link]</a>.</p>
<h1 id="natural-language-model">Natural Language Model</h1>
<p>A statistical <a href="https://en.wikipedia.org/wiki/Language_model" target="_blank" rel="noopener">language model</a> is a probability distribution over sequences of words. Given such a sequence, say of length <span class="math inline">\(m\)</span>, it assigns a probability <span class="math inline">\(P(w_{1},\dots ,w_{m})\)</span> to the whole sequence. Having a way to estimate the relative likelihood of different phrases is useful in many natural language processing applications, especially ones that generate text as an output. Language modeling is used in speech recognition, machine translation, part-of-speech tagging, parsing, hand-writing recognition, information retrieval and other applications. For example, in speech recognition, the computer tries to match sounds with word sequences. The language model provides context to distinguish between words and phrases that sound similar. For example, in American English, the phrases “recognize speech” and “wreck a nice beach” are pronounced almost the same but mean very different things. These ambiguities are easier to resolve when evidence from the language model is incorporated with the pronunciation model and the acoustic model. Although there are many language models, like N-gram model, decision tree, maximum entropy model, conditional random field, neural network and etc. Here we only discuss N-gram model and neural network based model.</p>
<h2 id="n-gram-model">N-gram Model</h2>
<p>Given a corpus <span class="math inline">\(\boldsymbol{\mathcal{C}}\)</span> and a sentence consist of <span class="math inline">\(N\)</span> words, where <span class="math inline">\(\mathbf{w}=(w_{1},w_{2},\dots ,w_{N})\)</span>, then the joint probability of <span class="math inline">\(w_{1},w_{2},\dots ,w_{N}\)</span> is<span class="math display">\[P(\mathbf{w})=P(w_{1},w_{2},\dots ,w_{N})\]</span>Using <strong>Bayes’ Formula</strong>, it can be decomposed to<span class="math display">\[P(\mathbf{w})=P(w_{1})\centerdot P(w_{2}|w_{1})\centerdot P(w_{3}|w_{1},w_{2})\centerdot\centerdot\centerdot P(w_{N}|w_{1},w_{2},\dots ,w_{N-1})\]</span>where the conditional probabilities <span class="math inline">\(P(w_{1})\)</span>, <span class="math inline">\(P(w_{2}|w_{1})\)</span>, <span class="math inline">\(\dots\)</span>, <span class="math inline">\(P(w_{N}|w_{1},w_{2},\dots ,w_{N-1})\)</span> is the parameters of language model. Once these parameters are determined, then we can derive the probability of <span class="math inline">\(P(\mathbf{w})\)</span>. Take the approximate computation of <span class="math inline">\(P(w_{k}|w_{1}^{k-1})\)</span> first, where <span class="math inline">\(w_{1}^{k-1}=(w_{1},\dots ,w_{k-1})\)</span>. By Bayes’ formula, we have<span class="math display">\[P(w_{k}|w_{1}^{k-1})=\frac{P(w_{1}^{k})}{P(w_{1}^{k-1})}\]</span>According to the <a href="https://en.wikipedia.org/wiki/Law_of_large_numbers" target="_blank" rel="noopener">law of large numbers</a>, when the corpus <span class="math inline">\(\boldsymbol{\mathcal{C}}\)</span> is large enough, then <span class="math inline">\(P(w_{k}|w_{1}^{k-1})\)</span> can be approximately written as<span class="math display">\[P(w_{k}|w_{1}^{k-1})\approx\frac{counter(w_{1}^{k})}{counter(w_{1}^{k-1})}\]</span>where <span class="math inline">\(counter(w_{1}^{k})\)</span> and <span class="math inline">\(counter(w_{1}^{k-1})\)</span> represent the appearance frequency of <span class="math inline">\(w_{1}^{k}\)</span> and <span class="math inline">\(w_{1}^{k-1}\)</span> in corpus respectively. One can imagine, once <span class="math inline">\(k\)</span> goes larger, the statistics of <span class="math inline">\(counter(w_{1}^{k})\)</span> and <span class="math inline">\(counter(w_{1}^{k-1})\)</span> will become extremely time consuming. The N-gram model is used to solve this problem, in the previous case, we can see that the appearance probability of a word is related to all the words in front of it. How about assuming that such probability is only related to a certain number of words in front of it. This is the basic idea of N-gram model, who makes a <span class="math inline">\(n-1\)</span> order <a href="https://en.wikipedia.org/wiki/Markov_chain" target="_blank" rel="noopener">Markov Hypothesis</a> to restrict that the appearance probability of a word is only related to <span class="math inline">\(n-1\)</span> words in front of it, say<span class="math display">\[P(w_{k}|w_{1}^{k-1})\approx P(w_{k}|w_{k-n+1}^{k-1})\approx\frac{counter(w_{k-n+1}^{k})}{counter(w_{k-n+1}^{k-1})}\]</span>For example, by setting <span class="math inline">\(n=2\)</span>, we will have<span class="math display">\[P(w_{k}|w_{1}^{k-1})\approx\frac{counter(w_{k-1},w_{k})}{counter(w_{k-1})}\]</span>After simplification, it bacomes easier to compute the single parameter and reduce the total amount of parameters. Below gives the relationship between <span class="math inline">\(n\)</span> and parameter number (suppose that the size of dictionary <span class="math inline">\(\boldsymbol{\mathcal{D}}\)</span> is 200000): <img src="/images/nlp/word2vec/ngramparams.png" alt="N-gram parameters"> From the table, we can derive that the computational complexity increase with the growth of <span class="math inline">\(n\)</span>, say, <span class="math inline">\(\big(\boldsymbol{\mathcal{O}}(|\boldsymbol{\mathcal{D}}|^{n})\big)\)</span>, it is an exponentially incremental magnitute, so <span class="math inline">\(n\)</span> can not set too large, normally, <span class="math inline">\(n=3\)</span> (trigram) is enough for most cases. For the model performance, theoretically, <span class="math inline">\(n\)</span> is bigger, the performance will be better. However, when <span class="math inline">\(n\)</span> goes up to some extent, the hoist scope of model performance will decrease. Another thing is that when the number of parameters goes up, the distinguishability of model bacomes better, the reliability goes down, becasue of the instances of the single parameter decrease. Thus, it is needed to make a compromise between distinguishability and reliability. Moreover, smoothing is needed in N-gram model, because of these reasons: 1. if <span class="math inline">\(counter(w_{k-n+1}^{k})=0\)</span>, we can not say that <span class="math inline">\(P(w_{k}|w_{1}^{k-1})\)</span> is equal to 0; 2. if <span class="math inline">\(counter(w_{k-n+1}^{k})=counter(w_{k-n+1}^{k-1})\)</span>, we can not say that <span class="math inline">\(P(w_{k}|w_{1}^{k-1})\)</span> is equal to 1.</p>
<p>Thus, the <a href="https://en.wikipedia.org/wiki/N-gram#Smoothing_techniques" target="_blank" rel="noopener">smoothing techniques</a> is used to deal with this issue. In practice it is necessary to smooth the probability distributions by also assigning non-zero probabilities to unseen words or n-grams. The reason is that models derived directly from the n-gram frequency counts have severe problems when confronted with any n-grams that have not explicitly been seen before – the zero-frequency problem. Various smoothing methods are used, from simple “add-one” (Laplace) smoothing (assign a count of 1 to unseen n-grams) to more sophisticated models, such as <a href="https://en.wikipedia.org/wiki/Good%E2%80%93Turing_discounting" target="_blank" rel="noopener">Good–Turing discounting</a> or <a href="https://en.wikipedia.org/wiki/Katz%27s_back-off_model" target="_blank" rel="noopener">back-off models</a>. Some of these methods are equivalent to assigning a prior distribution to the probabilities of the n-grams and using Bayesian inference to compute the resulting posterior n-gram probabilities. However, the more sophisticated smoothing models were typically not derived in this fashion, but instead through independent considerations. However, in machine learning field, there is a commonly used method: after constructing a model of the task, then building a objective function for it, trying to optimize this function to got an optimized parameters, and using the model with this optimized parameters to do the further predict task. For statistical language model, using maximal likelihood, the objective function can be set as<span class="math display">\[\prod_{w\in\boldsymbol{\mathcal{C}}}P(w|context(w))\]</span>where <span class="math inline">\(\boldsymbol{\mathcal{C}}\)</span> represents the corpus, <span class="math inline">\(context(w)\)</span> denotes the context of word <span class="math inline">\(w\)</span>, i.e., the set of circumjacent words of <span class="math inline">\(w\)</span>. And when <span class="math inline">\(context(w)=\emptyset\)</span>, we have <span class="math inline">\(P(w|context(w))=P(w)\)</span>. Particularly, for N-gram model, <span class="math inline">\(context(w_{i})=w_{i-n+1}^{i-1}\)</span>. In practice, the maximal logarithm likelihood is often used, i.e., setting the objective function as<span class="math display">\[\mathcal{L}=\sum_{w\in\boldsymbol{\mathcal{C}}}\log P(w|context(w))=\sum_{w\in\boldsymbol{\mathcal{C}}}\log F(w,context(w),\boldsymbol{\theta})\]</span>and maximizing this function, where <span class="math inline">\(\boldsymbol{\theta}\)</span> is the parameter set to be determined and <span class="math inline">\(P(w|context(w))\)</span> here is treated as the function of <span class="math inline">\(w\)</span> and <span class="math inline">\(context(w)\)</span>. Once we obtain the optimized parameter set <span class="math inline">\(\boldsymbol{\theta}^{*}\)</span> by optimizing the objective function above, <span class="math inline">\(F\)</span> is well-determined too, and further any probability <span class="math inline">\(P(w|context(w))\)</span> is able to be computed by <span class="math inline">\(F(w,context(w),\boldsymbol{\theta}^{*})\)</span>. Compare to N-gram model, this method do not need to compute and store all the probability values in advance, it computes the probability directly, and by choosing a suitable model will make the number of parameters in <span class="math inline">\(\boldsymbol{\theta}\)</span> much less than that in N-gram model. Actually, this method is the base of Word2Vec algorithm framework, and we will intruduce it in details in the following part.</p>
<h2 id="neural-probabilistic-based-model">Neural Probabilistic based Model</h2>
<p>In natural language processing task, we need to use machine learning algorithms to deal with natural language, however, machines can not understand human language directly, thus, we have to make the laguage mathematicization. In neural probabilistic language model, there is an important concept named <strong>distributed representation</strong> (word vector). It is a good way to digitalize natural language, unlike the traditional <a href="https://en.wikipedia.org/wiki/One-hot" target="_blank" rel="noopener">on-hot representation</a>, which just signify the words, do not contain any semantic information and easily suffer dimensionality curse in deep learning task. The <a href="https://www.iro.umontreal.ca/~vincentp/ift3395/lectures/backprop_old.pdf" target="_blank" rel="noopener">distributed representation</a> method overcomes those drawbacks of on-hot representation, its basic idea is mapping each word of a certain language to a short vector with fixed length through training, all of those vectors consist a word vector space, and each vector can be treated as point in this space, then introducing “distance” concept, and the “distance” among words can be used to determine their (grammatical, semantic, and etc.) similarities. Mathematically, for an arbitrary word <span class="math inline">\(w\)</span> the dictionary <span class="math inline">\(\boldsymbol{\mathcal{D}}\)</span>, assign a real number vector with fixed length <span class="math inline">\(e(w)\in\mathbb{R}^{m}\)</span>, <span class="math inline">\(e(w)\)</span> is called the word vector of <span class="math inline">\(w\)</span>, and <span class="math inline">\(m\)</span> is the length of word vector. In practice, there are many different methods can be used to estimate the word vectors, like Latent Semantic Analysis (<a href="https://en.wikipedia.org/wiki/Latent_semantic_analysis" target="_blank" rel="noopener">LSA</a>), Latent Dirichlet Allocation (<a href="https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation" target="_blank" rel="noopener">LDA</a>), Vector Space Models (<a href="https://en.wikipedia.org/wiki/Vector_space_model" target="_blank" rel="noopener">VSMs</a>), Neural Probabilistic based Model and etc. Since the Word2Vec is kind of the neural probabilistic based language model to obtain the word vectors with distributed representation, so here we only introduce the neural probabilistic model. Here we use the paper, “<a href="http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf" target="_blank" rel="noopener">A Neural Probabilistic Language Model</a>”, proposed by <a href="http://www.iro.umontreal.ca/~bengioy/yoshua_en/" target="_blank" rel="noopener">Bengio</a> in 2003 as an example to introduce the neural probabilistical based language model. Below gives the architecture graph of this model, it is a four layer neural networks, with input layer, projection layer, hidden layer and output layer. <img src="/images/nlp/word2vec/bengio.png" alt="Neural Probabilistic Language Model"> In this graph, <span class="math inline">\(tanh\)</span> is the <a href="https://en.wikipedia.org/wiki/Hyperbolic_function" target="_blank" rel="noopener">hyperbolic tangent function</a>, which acts as the activation function of hidden layer; <span class="math inline">\(\mathbf{W}\)</span> and <span class="math inline">\(\mathbf{p}\)</span> are the weight matrix and bias vector between projection layer and hidden layer respectively; while <span class="math inline">\(\mathbf{U}\)</span> and <span class="math inline">\(\mathbf{q}\)</span> are the weight matrix and bias vector between hidden layer and output layer respectively. Note that the dimension of projection layer is <span class="math inline">\((n-1)\centerdot m\)</span>, the dimension of output layer is <span class="math inline">\(N=|\boldsymbol{\mathcal{D}}|\)</span>, where the <span class="math inline">\(|\boldsymbol{\mathcal{D}}|\)</span> denotes the number of words in the dictionary, and the dimension of hidden layer is an adaptive parameter, which can be modified by user. The <span class="math inline">\(w_{t-n+1},\dots ,w_{t-1}\)</span> are the <span class="math inline">\(n-1\)</span> words in front of word <span class="math inline">\(w_{t}\)</span> in corpus <span class="math inline">\(\boldsymbol{\mathcal{C}}\)</span>, i.e., <span class="math inline">\(context(w_{t})=\big(w_{t-n+1},\dots ,w_{t-1}\big)\)</span>. Here, we simply denotes the <span class="math inline">\(w_{t}\)</span> as <span class="math inline">\(w\)</span>, so <span class="math inline">\(\big(w, context(w)\big)\)</span> is a training sample. According to the architecture in the graph<span class="math display">\[\begin{cases}\mathbf{z}_{w}=tanh\big(\mathbf{W}\mathbf{X}_{w}+\mathbf{p}\big),\\\mathbf{y}_{w}=\mathbf{U}\mathbf{z}_{w}+\mathbf{q}.\end{cases}\]</span>Here, <span class="math inline">\(\mathbf{y}_{w}=\big(y_{w,1},y_{w,2},\dots ,y_{w,N}\big)^{T}\)</span> is a vector with length <span class="math inline">\(N\)</span>, and its element can not represent the probability in this case. In order to make <span class="math inline">\(y_{w,i}\)</span> represents the probability of the target word is <span class="math inline">\(i^{th}\)</span> word in dictionary <span class="math inline">\(\boldsymbol{\mathcal{D}}\)</span> for the given <span class="math inline">\(context(w)\)</span>, we need to do a Softmax normalization, after normalizing, <span class="math inline">\(P(w|context(w))\)</span> can be expressed as<span class="math display">\[P(w|context(w))=\frac{exp(y_{w,i_{w}})}{\sum_{i=1}^{N}exp(y_{w,i})}\]</span>where <span class="math inline">\(i_{w}\)</span> represents the index of word <span class="math inline">\(w\)</span> in dictionary <span class="math inline">\(\boldsymbol{\mathcal{D}}\)</span>. Note that, in the last section, we said that <span class="math inline">\(P(w|context(w))\)</span> is treated as the function of <span class="math inline">\(w\)</span> and <span class="math inline">\(context(w)\)</span>, i.e., <span class="math inline">\(P(w|context(w))=F(w,context(w),\boldsymbol{\theta})\)</span>, so what are the parameters to be determined, i.e, what does <span class="math inline">\(\boldsymbol{\theta}\)</span> contain? In summary, it includes two parts: 1. word vector: <span class="math inline">\(e(w)\in\mathbb{R}^{m}, w\in\boldsymbol{\mathcal{D}}\)</span> and filled vector. 2. neural network parameters: <span class="math inline">\(\mathbf{W}\in\mathbb{R}^{n_{h}\centerdot (n-1)m}\)</span>, <span class="math inline">\(\mathbf{p}\in\mathbb{R}^{n_{h}}\)</span>, <span class="math inline">\(\mathbf{U}\in\mathbb{R}^{N\centerdot n_{h}}\)</span>, and <span class="math inline">\(\mathbf{q}\in\mathbb{R}^{N}\)</span>.</p>
<p>All of those parameters can be obtained through training. And compare to N-gram model, neural probabilistic based model has two major advantages: 1. the similarities among words can be expressed by word vectors. 2. it does not need additional smoothing process.</p>
<h1 id="word2vec">Word2Vec</h1>
<p>In Word2Vec, Mikolov et al. proposed two models, one is Continuous Bag-of-Words (CBOW) Model and another is Skip-gram (SG) Model. The graph of models are shown below, both of two models have three layers: <strong>Input layer</strong>, <strong>Projection layer</strong> and <strong>Output layer</strong>. From the graph, for CBOW model, it is given the contexts <span class="math inline">\(\{w_{t-2},w_{t-1},w_{t+1},w_{t+2}\}\)</span> to predict the word <span class="math inline">\(w_{t}\)</span>, while SG model is given the word <span class="math inline">\(w_{t}\)</span> to predict the contexts <span class="math inline">\(\{w_{t-2},w_{t-1},w_{t+1},w_{t+2}\}\)</span>. <img src="/images/nlp/word2vec/word2vecmodels.png" alt="Word2Vec Models"> Different from other neural network language models, CBOW and SG models do not have hidden layer, by taking out hidden layer, the models convert from neural network structure to logarithmic-linear structure directly, which in accordance with Logistic Regression. Compare to three layer neural networks, the logarithmic-linear structure decreases one layer’s matrix manipulation, which significantly improve model’s training speed. The second thing is that CBOW and Skip-gram ignore the word order information of context, so they do not concatenate the word vector of each word in context as those neural network language models did, however, they directly sum the word vector of each word in context (in the later version of Word2Vec, it changes to average of word vectors). Another difference is that the output layer of the models are tree structure. Here we define some notations and symbols: <span class="math inline">\(\boldsymbol{\mathcal{C}}\)</span> represents corpus; <span class="math inline">\(\boldsymbol{\mathcal{D}}\)</span> represents dictionary; <span class="math inline">\(e(w)\)</span> represents word vector of word <span class="math inline">\(w\)</span>; <span class="math inline">\(context(w)\)</span> represents a set of words is constituted by before and after <span class="math inline">\(n\)</span> words of word <span class="math inline">\(w\)</span>, i.e. <span class="math inline">\(context(w)\)</span> contains a set of words <span class="math inline">\(\{w_{-n},\dots ,w_{-1},w_{1},\dots ,w_{n}\}\)</span>; <span class="math inline">\(m\)</span> represents the dimension of word vector.</p>
<h2 id="continuous-bag-of-words-model">Continuous Bag-of-Words Model</h2>
<p>As aforesaid, CBOW model contains three layer: input layer, projection layer and output layer. Given a training sample <span class="math inline">\((w,context(w))\)</span>, 1. <strong>Input layer</strong>: it includes <span class="math inline">\(2n\)</span> word vectors in <span class="math inline">\(context(w)\)</span>, i.e., <span class="math inline">\(e(w_{-n}),\dots ,e(w_{n})\in\mathbb{R}^{m}\)</span>, where <span class="math inline">\(\mathbb{R}\)</span> is the set of real numbers. 2. <strong>Projection layer</strong>: accumulate the <span class="math inline">\(2n\)</span> vectors, i.e. <span class="math inline">\(\mathbf{x}_{w}=\sum_{i=-n,i\neq 0}^{n}\big(e(w_{i})\big)\)</span>. 3. <strong>Output layer</strong>: the output corresponds to a Huffman tree, it uses the words appear in the corpus <span class="math inline">\(\mathcal{C}\)</span> as leaf nodes, uses the appearance frequency of word in the corpus as weight. In this Huffman tree, there are <span class="math inline">\(N (=|\boldsymbol{\mathcal{D}}|)\)</span> leaf nodes, each of them is correspond to a word in dictionary <span class="math inline">\(\boldsymbol{\mathcal{D}}\)</span>, and there are <span class="math inline">\(N-1\)</span> nonleaf nodes as well.</p>
<p><img src="/images/nlp/word2vec/cbow.png" alt="CBOW"> For neural network based language model, most of the calculations are concentrated on the matrix manipulation between hidden layer and output layer, as well as the softmax normalized operation at output layer. From the graph above, CBOW makes specially change on these high computational conplexity field, it takes out hidden layer, and uses Huffman tree at output layer, which lay the foundation of the utilization of Hierarchical Softmax technique. Since the CBOW does not have hidden layer, so its input layer is the representation of context, and it can predict target word directly. And similar to other neural network based language model, the optimization objective of CBOW is to maximize the following log-likelihood function<span class="math display">\[\mathcal{L}=\sum_{w\in\boldsymbol{\mathcal{C}}}\log P\big(w|context(w)\big)\]</span>and the key point is to construct and calculate the conditional probability function <span class="math inline">\(P\big(w|context(w)\big)\)</span>.</p>
<h3 id="hierarchical-softmax">Hierarchical Softmax</h3>
<p>Hierarchical softmax is an efficient way of computing softmax, a key technique to improve performance in Word2Vec. It uses a binary tree to represent all words in the vocabulary. If there are <span class="math inline">\(V\)</span> words, all of them must be leaf units of the tree. It can be proved that there are <span class="math inline">\(V-1\)</span> inner units. For each leaf unit, there exists a unique path from the root to the unit; and this path is used to estimate the probability of the word represented by the leaf unit. Before talking about this technique, we introduce some related symbols. Considering a leaf node in the Huffman tree, assume this leaf node corresponding to the word <span class="math inline">\(w\)</span> in dictionary <span class="math inline">\(\boldsymbol{\mathcal{D}}\)</span>, note that 1. <span class="math inline">\(\mathbf{p}\)</span>: the path from root node to the leaf node corresponding to <span class="math inline">\(w\)</span>; 2. <span class="math inline">\(l\)</span>: the number of nodes within the path <span class="math inline">\(\mathbf{p}\)</span>; 3. <span class="math inline">\(p_{1},p_{2},\dots ,p_{l}\)</span>: <span class="math inline">\(l\)</span> nodes in the path <span class="math inline">\(\mathbf{p}\)</span>, where <span class="math inline">\(p_{1}\)</span> represents root node, <span class="math inline">\(p_{l}\)</span> represents the leaf node corresponding to <span class="math inline">\(w\)</span>. 4. <span class="math inline">\(d_{2},d_{3},\dots ,d_{l}\in\{0,1\}\)</span>: the Huffman codes of word <span class="math inline">\(w\)</span>, which is consist of <span class="math inline">\(l-1\)</span> codes, where <span class="math inline">\(d_{j}\)</span> represents the code of <span class="math inline">\(j^{th}\)</span> node in the path <span class="math inline">\(\mathbf{p}\)</span> (root node does not have corresponding code); 5. <span class="math inline">\(\boldsymbol{\theta}_{1}^{w},\boldsymbol{\theta}_{2}^{w},\dots, \boldsymbol{\theta}_{l-1}^{w}\in\mathbb{R}^{m}\)</span>: the corresponding vectors (auxiliary vectors) of nonleaf nodes in path <span class="math inline">\(\mathbf{p}\)</span>, where <span class="math inline">\(\boldsymbol{\theta}_{j}^{w}\)</span> represents the corresponding vector of <span class="math inline">\(j^{th}\)</span> nonleaf node in path <span class="math inline">\(\mathbf{p}\)</span>.</p>
<p>Then give a training sample “I like this girl very much”, and each word here has a certain frequency, it is coded into Huffman tree as shown in the graph below, and consider the situation that <span class="math inline">\(w=girl\)</span>. <img src="/images/nlp/word2vec/cbow-hierarchicalsoftmax.png" alt="CBOW – Hierarchical Softmax"> In the graph, the five nodes are linked by four red thick lines consist the path <span class="math inline">\(\mathbf{p}\)</span>, and its length <span class="math inline">\(l=5\)</span>. <span class="math inline">\(p_{1},p_{2},p_{3},p_{4},p_{5}\)</span> are the five nodes in the path, while <span class="math inline">\(p_{1}\)</span> corresponding to root node. <span class="math inline">\(d_{2},d_{3},d_{4},d_{5}\)</span> are 1, 0, 0, 1, respectively, which means that the Huffman codes of word “girl” is 1001. Besides, <span class="math inline">\(\boldsymbol{\theta}_{1}^{w},\boldsymbol{\theta}_{2}^{w},\boldsymbol{\theta}_{3}^{w},\boldsymbol{\theta}_{4}^{w}\)</span> denotes the corresponding vectors of four nonleaf nodes in path <span class="math inline">\(\mathbf{p}\)</span>. Next step is to use vector <span class="math inline">\(\mathbf{x}_{w}\in\mathbb{R}^{m}\)</span> and Huffman tree to define the conditional probability function <span class="math inline">\(P\big(w|context(w)\big)\)</span>. Using <span class="math inline">\(w=girl\)</span> as example, from root node to the leaf node “girl”, it goes through four branches, each branch can be treated as a <strong>dichotomy</strong>. Here we stipulate that the node with Huffman code “0” is defined as <strong>positive class</strong>, while the node with Huffman code “1” is defined as <strong>negative class</strong> (same as Word2Vec does), i.e., <span class="math inline">\(Label(p_{i})=1-d_{i}\)</span>, where <span class="math inline">\(i=2,3,\dots ,l\)</span>. In brief, when classifying a node, if it is classified to left, it is a negative class, otherwise, it is a positive class. According to logistic regression, the probability of a node to be classified as positive class is<span class="math display">\[\sigma\big(\mathbf{x}_{w}^{T}\boldsymbol{\theta}\big)=\frac{1}{1+e^{-\mathbf{x}_{w}^{T}\boldsymbol{\theta}}}\]</span>Otherwise, the probability to be classified as negative class is <span class="math inline">\(1-\sigma\big(\mathbf{x}_{w}^{T}\boldsymbol{\theta}\big)\)</span>, note that the auxiliary vectors <span class="math inline">\(\boldsymbol{\theta}_{j}^{w}\)</span> we introduce above performs the same role as the undetermined parameter vector <span class="math inline">\(\boldsymbol{\theta}\)</span> here. Since four dichotomies happen from root node to the leaf node “girl”, the probabbility of each calssification is - <span class="math inline">\(1^{st}\)</span> time: <span class="math inline">\(P(d_{2}|\mathbf{x}_{w},\boldsymbol{\theta}_{1}^{w})=1-\sigma\big(\mathbf{x}_{w}^{T}\boldsymbol{\theta}_{1}^{w}\big)\)</span> - <span class="math inline">\(2^{nd}\)</span> time: <span class="math inline">\(P(d_{3}|\mathbf{x}_{w},\boldsymbol{\theta}_{2}^{w})=\sigma\big(\mathbf{x}_{w}^{T}\boldsymbol{\theta}_{2}^{w}\big)\)</span> - <span class="math inline">\(3^{rd}\)</span> time: <span class="math inline">\(P(d_{4}|\mathbf{x}_{w},\boldsymbol{\theta}_{3}^{w})=\sigma\big(\mathbf{x}_{w}^{T}\boldsymbol{\theta}_{3}^{w}\big)\)</span> - <span class="math inline">\(4^{th}\)</span> time: <span class="math inline">\(P(d_{5}|\mathbf{x}_{w},\boldsymbol{\theta}_{4}^{w})=1-\sigma\big(\mathbf{x}_{w}^{T}\boldsymbol{\theta}_{4}^{w}\big)\)</span></p>
<p>For <span class="math inline">\(w=girl\)</span>, the probability of <span class="math inline">\(P(w|context(w))\)</span> can be written as<span class="math display">\[P(w|context(w))=\prod_{j=2}^{5}P\big(d_{j}|\mathbf{x}_{w},\boldsymbol{\theta}_{j-1}^{w}\big)\]</span>So this is the basic idea of <strong>Hierarchical Softmax</strong>. Generally, for an arbbitrarily word <span class="math inline">\(w\)</span> in dictionary <span class="math inline">\(\boldsymbol{\mathcal{D}}\)</span>, there must be one, and only one, path <span class="math inline">\(\mathbf{p}\)</span> from root node to the leaf node corresponding to the word <span class="math inline">\(w\)</span>. <span class="math inline">\(l-1\)</span> branches exist on this path, each breanch is treated as a dichotomy, and each dichotomy generates a probability, then the <span class="math inline">\(P(w|context(w))\)</span> is defined as the product of those probabilities.<span class="math display">\[p(w|context(w))=\prod_{j=2}^{l}P\big(d_{j}|\mathbf{x}_{w},\boldsymbol{\theta}_{j-1}^{w}\big)\]</span>where,<span class="math display">\[P\big(d_{j}|\mathbf{x}_{w},\boldsymbol{\theta}_{j-1}^{w}\big)=\begin{cases} \sigma\big(\mathbf{x}_{w}^{T}\boldsymbol{\theta}_{j-1}^{w}\big),&amp;d_{j}=0; \\ 1-\sigma\big(\mathbf{x}_{w}^{T}\boldsymbol{\theta}_{j-1}^{w}\big),&amp;d_{j}=1. \end{cases}\]</span>or it can be wirtten as<span class="math display">\[P\big(d_{j}|\mathbf{x}_{w},\boldsymbol{\theta}_{j-1}^{w}\big)=\bigg(\sigma\big(\mathbf{x}_{w}^{T}\boldsymbol{\theta}_{j-1}^{w}\big)\bigg)^{1-d_{j}}\centerdot\bigg(1-\sigma\big(\mathbf{x}_{w}^{T}\boldsymbol{\theta}_{j-1}^{w}\big)\bigg)^{d_{j}}\]</span>Then substituting the formula above to the log-likelihood, i.e., the optimization objective of CBOW, then we have<span class="math display">\[\mathcal{L}=\sum_{w\in\boldsymbol{\mathcal{C}}}\log\prod_{j=2}^{l}\bigg\{\bigg(\sigma\big(\mathbf{x}_{w}^{T}\boldsymbol{\theta}_{j-1}^{w}\big)\bigg)^{1-d_{j}}\centerdot\bigg(1-\sigma\big(\mathbf{x}_{w}^{T}\boldsymbol{\theta}_{j-1}^{w}\big)\bigg)^{d_{j}}\bigg\}\\=\sum_{w\in\boldsymbol{\mathcal{C}}}\sum_{j=2}^{l}\bigg\{(1-d_{j})\centerdot\log\big(\sigma\big(\mathbf{x}_{w}^{T}\boldsymbol{\theta}_{j-1}^{w}\big)+d_{j}\centerdot\log\big(1-\sigma\big(\mathbf{x}_{w}^{T}\boldsymbol{\theta}_{j-1}^{w}\big)\bigg\}\]</span>Hereto, the function above is the target function of CBOW, then our task is to optimize it, i.e., maximize this function. In Word2Vec, the author use the <strong>Stochastic Gradient Ascent</strong> method. And the key point of gradient descent/ascent methods are to find the gradient computing formula. To make further process simple, we denote that<span class="math display">\[\mathcal{L}(w,j)=(1-d_{j})\centerdot\log\bigg(\sigma\big(\mathbf{x}_{w}^{T}\boldsymbol{\theta}_{j-1}^{w}\big)\bigg)+d_{j}\centerdot\log\bigg(1-\sigma\big(\mathbf{x}_{w}^{T}\boldsymbol{\theta}_{j-1}^{w}\big)\bigg)\]</span>The procedure of stochastic gradient ascent is: for every sample <span class="math inline">\((w, context(w))\)</span>, update all the related parameters in objective function. Observe the target function <span class="math inline">\(\mathcal{L}\)</span>, we can derive that the parameters in this objective function are <span class="math inline">\(\mathbf{x}_{w}\)</span> and <span class="math inline">\(\boldsymbol{\theta}_{j-1}^{w}\)</span>. Here we give the gradients of <span class="math inline">\(\mathcal{L}(w,j)\)</span> with regrad to these parameter vectors. First, consider the gradient computation of <span class="math inline">\(\mathcal{L}(w,j)\)</span> with regard to <span class="math inline">\(\boldsymbol{\theta}_{j-1}^{w}\)</span>:<span class="math display">\[\frac{\partial\mathcal{L}(w,j)}{\partial\boldsymbol{\theta}_{j-1}^{w}}=\frac{\partial}{\partial\boldsymbol{\theta}_{j-1}^{w}}\bigg\{(1-d_{j})\centerdot\log\bigg(\sigma\big(\mathbf{x}_{w}^{T}\boldsymbol{\theta}_{j-1}^{w}\big)\bigg)+d_{j}\centerdot\log\bigg(1-\sigma\big(\mathbf{x}_{w}^{T}\boldsymbol{\theta}_{j-1}^{w}\big)\bigg)\bigg\}\\=(1-d_{j})\bigg(1-\sigma\big(\mathbf{x}_{w}^{T}\boldsymbol{\theta}_{j-1}^{w}\big)\bigg)\mathbf{x}_{w}-d_{j}\sigma\big(\mathbf{x}_{w}^{T}\boldsymbol{\theta}_{j-1}^{w}\big)\mathbf{x}_{w}\\=\bigg\{(1-d_{j})\bigg(1-\sigma\big(\mathbf{x}_{w}^{T}\boldsymbol{\theta}_{j-1}^{w}\big)\bigg)-d_{j}\sigma\big(\mathbf{x}_{w}^{T}\boldsymbol{\theta}_{j-1}^{w}\big)\bigg\}\mathbf{x}_{w}\\=\big[1-d_{j}-\sigma\big(\mathbf{x}_{w}^{T}\boldsymbol{\theta}_{j-1}^{w}\big)\big]\mathbf{x}_{w}\]</span> Thus the update formula of <span class="math inline">\(\boldsymbol{\theta}_{j-1}^{w}\)</span> can be written as (<span class="math inline">\(\eta\)</span> is learning rate)<span class="math display">\[\boldsymbol{\theta}_{j-1}^{w}:=\boldsymbol{\theta}_{j-1}^{w}+\eta\big[1-d_{j}-\sigma\big(\mathbf{x}_{w}^{T}\boldsymbol{\theta}_{j-1}^{w}\big)\big]\mathbf{x}_{w}\]</span>Then consider the gradient of <span class="math inline">\(\mathcal{L}(w,j)\)</span> with regard to <span class="math inline">\(\mathbf{x}_{w}\)</span>, since the variable <span class="math inline">\(\mathbf{x}_{w}\)</span> and <span class="math inline">\(\boldsymbol{\theta}_{j-1}^{w}\)</span> are symmetrical, which means that they can change position, so we can derive that<span class="math display">\[\frac{\partial\mathcal{L}(w,j)}{\partial\mathbf{x}_{w}}=\big[1-d_{j}-\sigma\big(\mathbf{x}_{w}^{T}\boldsymbol{\theta}_{j-1}^{w}\big)\big]\boldsymbol{\theta}_{j-1}^{w}\]</span>So far we have got the gradients of <span class="math inline">\(\mathcal{L}(w,j)\)</span> with regard to <span class="math inline">\(\boldsymbol{\theta}_{j-1}^{w}\)</span> and <span class="math inline">\(\mathbf{x}_{w}\)</span>, but our ultimate goal is to compute the word vector for every word in the dictionary <span class="math inline">\(\boldsymbol{\mathcal{D}}\)</span>. In order to use <span class="math inline">\(\frac{\partial\mathcal{L}(w,j)}{\partial\mathbf{x}_{w}}\)</span> to update <span class="math inline">\(e(\tilde{w}),\tilde{w}\in context(w)\)</span>, in Word2Vec, the author directly use<span class="math display">\[e(\tilde{w}):=e(\tilde{w})+\eta\sum_{j=2}^{l}\frac{\partial\mathcal{L}(w,j)}{\partial\mathbf{x}_{w}},\tilde{w}\in context(w)\]</span>it contributes <span class="math inline">\(\sum_{j=2}^{l}\frac{\partial\mathcal{L}(w,j)}{\partial\mathbf{x}_{w}}\)</span> to every word vector of word in <span class="math inline">\(context(w)\)</span> (think about if it is more reasonable that change <span class="math inline">\(\eta\)</span> to <span class="math inline">\(\frac{\eta}{|context(w)|}\)</span>, where <span class="math inline">\(|context(w)|\)</span> denotes the number of words in <span class="math inline">\(context(w)\)</span>, i.e., use average contribution). It is easy to understand, since <span class="math inline">\(\mathbf{x}_{w}\)</span> is the summation of all the word vectors in <span class="math inline">\(context(w)\)</span>, it should be contributed to every word after gradient calculation. Here is the pseudocode about using stochastic gradient ascent method to update the parameters in CBOW: &gt; 1. <span class="math inline">\(\mathbf{v}=\boldsymbol{0}\)</span> &gt; 2. <span class="math inline">\(\mathbf{x}_{w}=\sum_{w\in context(w)}e(w)\)</span> &gt; 3. For <span class="math inline">\(j=2:l\)</span> do : &gt; 3.1 <span class="math inline">\(q=\sigma\big(\mathbf{x}_{w}^{T}\boldsymbol{\theta}_{j-1}^{w}\big)\)</span> &gt; 3.2 <span class="math inline">\(g=\eta\centerdot (1-d_{j}-q)\)</span> &gt; 3.3 <span class="math inline">\(\mathbf{v}:=\mathbf{v}+g\centerdot\boldsymbol{\theta}_{j-1}^{w}\)</span> &gt; 3.4 <span class="math inline">\(\boldsymbol{\theta}_{j-1}^{w}:=\boldsymbol{\theta}_{j-1}^{w}+g\centerdot\mathbf{x}_{w}\)</span> &gt; 4. For <span class="math inline">\(w\in context(w)\)</span> do : &gt; <span class="math inline">\(e(w):=e(w)+\mathbf{v}\)</span></p>
<p>Note that <span class="math inline">\(e(w)\)</span> is corresponding to the <em>“syn0”</em> in Word2Vec source codes, <span class="math inline">\(\boldsymbol{\theta}_{j-1}^{w}\)</span> is corresponding to <em>“syn1”</em>, <span class="math inline">\(\mathbf{x}_{w}\)</span> is corresponding to <em>“neu1”</em> and <span class="math inline">\(\mathbf{v}\)</span> is corresponding to <em>“neu1e”</em>. Here shows some java codes of the procedure. For step 1, the vector <span class="math inline">\(\mathbf{v}\)</span> is initialized to zero (“layer1_size” is dimension of vector, i.e., <span class="math inline">\(m\)</span> in this article): <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> c = <span class="number">0</span>; c &lt; layer1_size; c++)</span><br><span class="line">	neu1e[c] = <span class="number">0</span>;</span><br></pre></td></tr></table></figure></p>
<p>For step 2: <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> c = <span class="number">0</span>; c &lt; layer1_size; c++)</span><br><span class="line">	neu1[c] = <span class="number">0</span>;</span><br><span class="line">...</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> a = b; a &lt; window * <span class="number">2</span> + <span class="number">1</span> - b; a++) &#123;</span><br><span class="line">	<span class="keyword">if</span> (a == window) <span class="keyword">continue</span>;</span><br><span class="line">	<span class="keyword">int</span> c = sentencePosition - window + a;</span><br><span class="line">	<span class="keyword">if</span> (c &lt; <span class="number">0</span> || c &gt;= sentenceLength) <span class="keyword">continue</span>;</span><br><span class="line">	<span class="keyword">int</span> idx = huffmanNodes.get(sentence.get(c)).idx;</span><br><span class="line">	<span class="keyword">for</span> (<span class="keyword">int</span> d = <span class="number">0</span>; d &lt; layer1_size; d++) &#123; neu1[d] += syn0[idx][d]; &#125;</span><br><span class="line">	...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>For step 3: <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> d = <span class="number">0</span>; d &lt; huffmanNode.code.length; d++) &#123;</span><br><span class="line">	<span class="keyword">double</span> f = <span class="number">0</span>;</span><br><span class="line">	<span class="keyword">int</span> l2 = huffmanNode.point[d];</span><br><span class="line">	<span class="comment">/**------------------------------3.1--------------------------------------*/</span></span><br><span class="line">	<span class="comment">// Propagate hidden -&gt; output</span></span><br><span class="line">	<span class="keyword">for</span> (<span class="keyword">int</span> c = <span class="number">0</span>; c &lt; layer1_size; c++) f += neu1[c] * syn1[l2][c];</span><br><span class="line">	<span class="keyword">if</span> (f &lt;= -MAX_EXP || f &gt;= MAX_EXP) <span class="keyword">continue</span>;</span><br><span class="line">	<span class="keyword">else</span> f = EXP_TABLE[(<span class="keyword">int</span>)((f + MAX_EXP) * (EXP_TABLE_SIZE / MAX_EXP / <span class="number">2</span>))];</span><br><span class="line">	<span class="comment">/**------------------------------3.2--------------------------------------*/</span></span><br><span class="line">	<span class="comment">// 'g' is the gradient multiplied by the learning rate</span></span><br><span class="line">	<span class="keyword">double</span> g = (<span class="number">1</span> - huffmanNode.code[d] - f) * alpha;</span><br><span class="line">	<span class="comment">/**------------------------------3.3--------------------------------------*/</span></span><br><span class="line">	<span class="comment">// Propagate errors output -&gt; hidden</span></span><br><span class="line">	<span class="keyword">for</span> (<span class="keyword">int</span> c = <span class="number">0</span>; c &lt; layer1_size; c++)</span><br><span class="line">		neu1e[c] += g * syn1[l2][c];</span><br><span class="line">	<span class="comment">/**------------------------------3.4--------------------------------------*/</span></span><br><span class="line">	<span class="comment">// Learn weights hidden -&gt; output</span></span><br><span class="line">	<span class="keyword">for</span> (<span class="keyword">int</span> c = <span class="number">0</span>; c &lt; layer1_size; c++)</span><br><span class="line">		syn1[l2][c] += g * neu1[c];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>For step 4: <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> a = b; a &lt; window * <span class="number">2</span> + <span class="number">1</span> - b; a++) &#123;</span><br><span class="line">	<span class="keyword">if</span> (a == window) <span class="keyword">continue</span>;</span><br><span class="line">	<span class="keyword">int</span> c = sentencePosition - window + a;</span><br><span class="line">	<span class="keyword">if</span> (c &lt; <span class="number">0</span> || c &gt;= sentenceLength) <span class="keyword">continue</span>;</span><br><span class="line">	<span class="keyword">int</span> idx = huffmanNodes.get(sentence.get(c)).idx;</span><br><span class="line">	<span class="keyword">for</span> (<span class="keyword">int</span> d = <span class="number">0</span>; d &lt; layer1_size; d++) syn0[idx][d] += neu1e[d]; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h3 id="negative-sampling">Negative Sampling</h3>
<p>The idea of negative sampling is more straightforward than hierarchical softmax: in order to deal with the difficulty of having too many output vectors that need to be updated per iteration, we only update a sample of them. Apparently the output word (i.e., the ground truth, or positive sample) should be kept in our sample and gets updated, and we need to sample a few words as negative samples (hence “negative sampling”). A probabilistic distribution is needed for the sampling process, and it can be arbitrarily chosen. For instance, the words in dictionary <span class="math inline">\(\boldsymbol{\mathcal{D}}\)</span> have different appearance frequency in corpus <span class="math inline">\(\boldsymbol{\mathcal{C}}\)</span>, some are higher, some are lower. For those words with high frequency, the probability to be chosen as negative sample is high, and vice versa. So the concrete ways in Word2Vec to do negative sampling is: define <span class="math inline">\(len_{0}=0, len_{k}=\sum_{j=1}^{k}ratio(w_{j}), k=1,2,\dots ,N\)</span>, where <span class="math inline">\(w_{j}\)</span> means the <span class="math inline">\(j^{th}\)</span> words in dictionary <span class="math inline">\(\boldsymbol{\mathcal{D}}\)</span>, <span class="math inline">\(ratio(w)=\frac{counter(w)}{\sum_{w&#39;\in\boldsymbol{\mathcal{D}}}counter(w&#39;)}\)</span>, and <span class="math inline">\(counter(\centerdot)\)</span> represents the frequency of a word appears in corpus <span class="math inline">\(\boldsymbol{\mathcal{C}}\)</span>. Then using <span class="math inline">\(\big\{len_{j}\big\}_{j=0}^{N}\)</span> as the subdivision nodes, we can derive an unequidistant subdivision in the <span class="math inline">\([0,1]\)</span> interval. And <span class="math inline">\(I_{i}=(len_{i-1},len_{i}], i=1,2,\dots ,N\)</span> is the <span class="math inline">\(N\)</span> sub-intervals. Further, introducing an equidistant subdivision in this <span class="math inline">\([0,1]\)</span> interval, and the subdivision nodes are <span class="math inline">\(\big\{m_{j}\big\}_{j=0}^{M}\)</span>, where <span class="math inline">\(M\gg N\)</span>, see the graph below <img src="/images/nlp/word2vec/subdivision.png" alt="Subdivision"> Then project the equidistant subdivision nodes into the unequidistant subdivision (as red dot lines), we can derive the mapping relation between <span class="math inline">\(\big\{m_{j}\big\}_{j=0}^{M}\)</span> and <span class="math inline">\(\big\{I_{j}\big\}_{j=1}^{N}\)</span>(or <span class="math inline">\(\big\{w_{j}\big\}_{j=1}^{N}\)</span>):<span class="math display">\[Table(i)=w_{k}, m_{i}\in I_{k}, i=1,2,\dots ,M-1\]</span>Once we have this table, it is easy to do sampling: generating a random integer <span class="math inline">\(r\)</span> within <span class="math inline">\([1,M-1]\)</span>, <span class="math inline">\(Table(r)\)</span> is a sample. One thing to mention here is that when doing negative sampling for <span class="math inline">\(w_{j}\)</span>, if the random integer index to the <span class="math inline">\(w_{j}\)</span> itself, then skip. It is also worth mentioning that, in Word2Vec source codes, the authors did not use <span class="math inline">\(counter(w)\)</span> directly to set the weight for word in dictionary <span class="math inline">\(\boldsymbol{\mathcal{D}}\)</span>, but powered <span class="math inline">\(counter(w)\)</span> with <span class="math inline">\(\alpha =0.75\)</span>. In addition, <span class="math inline">\(M\)</span> (corresponding to “table_size” in codes) is set to <span class="math inline">\(10^{8}\)</span>. See the java codes below: <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> TABLE_SIZE = (<span class="keyword">int</span>) <span class="number">1e8</span>;</span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">initializeUnigramTable</span><span class="params">()</span> </span>&#123;</span><br><span class="line">	<span class="keyword">long</span> trainWordsPow = <span class="number">0</span>;</span><br><span class="line">	<span class="keyword">double</span> power = <span class="number">0.75</span>;</span><br><span class="line">	<span class="keyword">for</span> (HuffmanNode node : huffmanNodes.values()) &#123; trainWordsPow += Math.pow(node.count, power); &#125;</span><br><span class="line">	Iterator&lt;HuffmanNode&gt; nodeIter = huffmanNodes.values().iterator();</span><br><span class="line">	HuffmanNode last = nodeIter.next();</span><br><span class="line">	<span class="keyword">double</span> d1 = Math.pow(last.count, power) / trainWordsPow;</span><br><span class="line">	<span class="keyword">int</span> i = <span class="number">0</span>;</span><br><span class="line">	<span class="keyword">for</span> (<span class="keyword">int</span> a = <span class="number">0</span>; a &lt; TABLE_SIZE; a++) &#123;</span><br><span class="line">		table[a] = i;</span><br><span class="line">		<span class="keyword">if</span> (a / (<span class="keyword">double</span>) TABLE_SIZE &gt; d1) &#123;</span><br><span class="line">			i++;</span><br><span class="line">			HuffmanNode next = nodeIter.hasNext() ? nodeIter.next() : last;</span><br><span class="line">			d1 += Math.pow(next.count, power) / trainWordsPow;</span><br><span class="line">			last = next;</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>In CBOW model, given the <span class="math inline">\(context(w)\)</span>, and we need to predict <span class="math inline">\(w\)</span>, thus, for the given <span class="math inline">\(context(w)\)</span>, <span class="math inline">\(w\)</span> is a positive sample, then other words are negative samples. Suppose that we have selected a negative sample subset <span class="math inline">\(NEG(w)\neq\emptyset\)</span>, and for <span class="math inline">\(\forall\tilde{w}\in\boldsymbol{\mathcal{D}}\)</span>, define<span class="math display">\[L^{w}(\tilde{w})=\begin{cases}1, &amp; \tilde{w}=w;\\ 0, &amp; \tilde{w}\neq w.\end{cases}\]</span>to represents the label of <span class="math inline">\(\tilde{w}\)</span>, i.e., the label of positive sample is 1, negative sample is 0. For a given positive sample <span class="math inline">\(\big(w, context(w)\big)\)</span>, we want to maximize<span class="math display">\[g(w)=\prod_{u\in\{w\}\cup NEG(w)}P(u|context(w))\]</span>where<span class="math display">\[P(u|context(w))=\big[\sigma(\mathbf{x}_{w}^{T}\boldsymbol{\theta}^{u})\big]^{L^{w}(u)}\centerdot\big[1-\sigma(\mathbf{x}_{w}^{T}\boldsymbol{\theta}^{u})\big]^{1-L^{w}(u)}\]</span>here the <span class="math inline">\(\mathbf{x}_{w}\)</span> also denotes the sum of vectors of all words in <span class="math inline">\(context(w)\)</span>, <span class="math inline">\(\boldsymbol{\theta}^{u}\in\mathbb{R}^{m}\)</span> represents an auxiliary vector, which is corresponding to word <span class="math inline">\(u\)</span>. Combine the tow formula above, we have<span class="math display">\[g(w)=\sigma(\mathbf{x}_{w}^{T}\boldsymbol{\theta}^{w})\prod_{u\in NEG(w)}\big[1-\sigma(\mathbf{x}_{w}^{T}\boldsymbol{\theta}^{u})\big]\]</span>the <span class="math inline">\(\sigma(\mathbf{x}_{w}^{T}\boldsymbol{\theta}^{w})\)</span> represents the probability when predicted word is <span class="math inline">\(w\)</span>, while <span class="math inline">\(\sigma(\mathbf{x}_{w}^{T}\boldsymbol{\theta}^{u}), u\in NEG(w)\)</span> represents the probability when predicted word is <span class="math inline">\(u\)</span>. Thus, maximizing <span class="math inline">\(g(w)\)</span> is equal to maximizing <span class="math inline">\(\sigma(\mathbf{x}_{w}^{T}\boldsymbol{\theta}^{w})\)</span> and minimizing <span class="math inline">\(\sigma(\mathbf{x}_{w}^{T}\boldsymbol{\theta}^{u})\)</span> simultaneously. Actually, this is what we want, say, increase the probability of positive sample while decrease the probability of negative samples. So, for a given <span class="math inline">\(\boldsymbol{\mathcal{C}}\)</span>, the overall optimization target function is<span class="math display">\[\mathcal{L}=\log G=\log\prod_{w\in\boldsymbol{\mathcal{C}}}g(w)=\sum_{w\in\boldsymbol{\mathcal{C}}}\log g(w) \\=\sum_{w\in\boldsymbol{\mathcal{C}}}\sum_{u\in\{w\}\cup NEG(w)}\big\{L^{w}(u)\centerdot\log\big[\sigma(\mathbf{x}_{w}^{T}\boldsymbol{\theta}^{u})\big]+\big[1-L^{w}(u)\big]\centerdot\big[1-\sigma(\mathbf{x}_{w}^{T}\boldsymbol{\theta}^{u})\big]\} \\=\sum_{w\in\boldsymbol{\mathcal{C}}}\big\{\log\big[\sigma(\mathbf{x}_{w}^{T}\boldsymbol{\theta}^{u})\big]+\sum_{u\in NEG(w)}\log\big[\sigma(-\mathbf{x}_{w}^{T}\boldsymbol{\theta}^{u})\big]\big\}\]</span>Same as before, we denote<span class="math display">\[\mathcal{L}(w,u)=L^{w}(u)\centerdot\log\big[\sigma(\mathbf{x}_{w}^{T}\boldsymbol{\theta}^{u})\big]+\big[1-L^{w}(u)\big]\centerdot\big[1-\sigma(\mathbf{x}_{w}^{T}\boldsymbol{\theta}^{u})\big]\]</span>and before using stochastic gradient ascent method to optimize this target function, we need to get the gradients of <span class="math inline">\(\mathcal{L}(w,u)\)</span> with regard to <span class="math inline">\(\boldsymbol{\theta}^{u}\)</span> and <span class="math inline">\(\mathbf{x}_{w}\)</span> respectively. Samely, considering the gradient computation of <span class="math inline">\(\mathcal{L}(w,u)\)</span> with regard to <span class="math inline">\(\boldsymbol{\theta}^{u}\)</span>:<span class="math display">\[\frac{\partial\mathcal{L}(w,u)}{\partial\boldsymbol{\theta}^{u}}=\big[L^{w}(u)-\sigma(\mathbf{x}_{w}^{T}\boldsymbol{\theta}^{u})\big]\mathbf{x}_{w}\]</span>Hence, the update formula of <span class="math inline">\(\boldsymbol{\theta}^{u}\)</span> can be written as<span class="math display">\[\boldsymbol{\theta}^{u}:=\boldsymbol{\theta}^{u}+\eta\big[L^{w}(u)-\sigma(\mathbf{x}_{w}^{T}\boldsymbol{\theta}^{u})\big]\mathbf{x}_{w}\]</span>Then, using the symmetry of <span class="math inline">\(\boldsymbol{\theta}^{u}\)</span> and <span class="math inline">\(\mathbf{x}_{w}\)</span> in <span class="math inline">\(\mathcal{L}(w,u)\)</span>, we have<span class="math display">\[\frac{\partial\mathcal{L}(w,u)}{\partial\mathbf{x}_{w}}=\big[L^{w}(u)-\sigma(\mathbf{x}_{w}^{T}\boldsymbol{\theta}^{u})\big]\boldsymbol{\theta}^{u}\]</span>Hence, using <span class="math inline">\(\frac{\partial\mathcal{L}(w,u)}{\partial\mathbf{x}_{w}}\)</span>, we can derive the update formula of <span class="math inline">\(e(\tilde{w}),\tilde{w}\in context(w)\)</span>:<span class="math display">\[e(\tilde{w}):=e(\tilde{w})+\eta\sum_{u\in\{w\}\cup NEG(w)}\frac{\partial\mathcal{L}(w,u)}{\partial\mathbf{x}_{w}}, \tilde{w}\in context(w)\]</span>Below is the pseudocode of negative sampling in CBOW: &gt; 1. <span class="math inline">\(\mathbf{v}=\boldsymbol{0}\)</span> &gt; 2. <span class="math inline">\(\mathbf{x}_{w}=\sum_{u\in context(w)}e(u)\)</span> &gt; 3. For <span class="math inline">\(u=\{w\}\cup NEG(w)\)</span> do : &gt; 3.1 <span class="math inline">\(q=\sigma\big(\mathbf{x}_{w}^{T}\boldsymbol{\theta}^{u}\big)\)</span> &gt; 3.2 <span class="math inline">\(g=\eta\centerdot (L^{w}(u)-q)\)</span> &gt; 3.3 <span class="math inline">\(\mathbf{v}:=\mathbf{v}+g\centerdot\boldsymbol{\theta}^{u}\)</span> &gt; 3.4 <span class="math inline">\(\boldsymbol{\theta}^{u}:=\boldsymbol{\theta}^{u}+g\centerdot\mathbf{x}_{w}\)</span> &gt; 4. For <span class="math inline">\(u\in context(w)\)</span> do : &gt; <span class="math inline">\(e(u):=e(u)+\mathbf{v}\)</span></p>
<p>And here is the java codes of negative sampling (step 1,2,4 is same as hierarchical softmax above): <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**-------------------------------3---------------------------------------*/</span></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> d = <span class="number">0</span>; d &lt;= config.negativeSamples; d++) &#123;</span><br><span class="line">	<span class="keyword">int</span> target;</span><br><span class="line">	<span class="keyword">final</span> <span class="keyword">int</span> label;</span><br><span class="line">	<span class="keyword">if</span> (d == <span class="number">0</span>) &#123;</span><br><span class="line">		target = huffmanNode.idx;</span><br><span class="line">		label = <span class="number">1</span>;</span><br><span class="line">	&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">		nextRandom = incrementRandom(nextRandom);</span><br><span class="line">		target = table[(<span class="keyword">int</span>) (((nextRandom &gt;&gt; <span class="number">16</span>) % TABLE_SIZE) + TABLE_SIZE) % TABLE_SIZE];</span><br><span class="line">		<span class="keyword">if</span> (target == <span class="number">0</span>) target = (<span class="keyword">int</span>) (((nextRandom % (vocabSize - <span class="number">1</span>)) + vocabSize - <span class="number">1</span>) % (vocabSize - <span class="number">1</span>)) + <span class="number">1</span>;</span><br><span class="line">		<span class="keyword">if</span> (target == huffmanNode.idx) <span class="keyword">continue</span>;</span><br><span class="line">		label = <span class="number">0</span>;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">int</span> l2 = target;</span><br><span class="line">	<span class="comment">/**------------------------------3.1--------------------------------------*/</span></span><br><span class="line">	<span class="keyword">double</span> f = <span class="number">0</span>;</span><br><span class="line">	<span class="keyword">for</span> (<span class="keyword">int</span> c = <span class="number">0</span>; c &lt; layer1_size; c++) f += neu1[c] * syn1neg[l2][c];</span><br><span class="line">	<span class="comment">/**------------------------------3.2--------------------------------------*/</span></span><br><span class="line">	<span class="keyword">final</span> <span class="keyword">double</span> g;</span><br><span class="line">	<span class="keyword">if</span> (f &gt; MAX_EXP) g = (label - <span class="number">1</span>) * alpha;</span><br><span class="line">	<span class="keyword">else</span> <span class="keyword">if</span> (f &lt; -MAX_EXP) g = (label - <span class="number">0</span>) * alpha;</span><br><span class="line">	<span class="keyword">else</span> g = (label - EXP_TABLE[(<span class="keyword">int</span>) ((f + MAX_EXP) * (EXP_TABLE_SIZE / MAX_EXP / <span class="number">2</span>))]) * alpha;</span><br><span class="line">	<span class="comment">/**------------------------------3.3--------------------------------------*/</span></span><br><span class="line">	<span class="keyword">for</span> (<span class="keyword">int</span> c = <span class="number">0</span>; c &lt; layer1_size; c++) neu1e[c] += g * syn1neg[l2][c];</span><br><span class="line">	<span class="comment">/**------------------------------3.4--------------------------------------*/</span></span><br><span class="line">	<span class="keyword">for</span> (<span class="keyword">int</span> c = <span class="number">0</span>; c &lt; layer1_size; c++) syn1neg[l2][c] += g * neu1[c];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h2 id="skip-gram-model">Skip-gram Model</h2>
<p>Similar to CBOW model, Skip-gram model contains three layers: input layer, projection layer and output layer. However, given a training sample <span class="math inline">\(\big(w,context(w)\big)\)</span>: 1. <strong>Input Layer</strong>: it only contain to vector <span class="math inline">\(e(w)\in\mathbb{R}^{m}\)</span> of central word <span class="math inline">\(w\)</span> in current sample. 2. <strong>Projection Layer</strong>: it is a identitical projection, which is redundant in Skip-gram model, since it maps <span class="math inline">\(e(w)\)</span> from input layer to this layer directly. 3. <strong>Output Layer</strong>: Same as CBOW model.</p>
<p><img src="/images/nlp/word2vec/skipgram.png" alt="Skip-gram"> Compare to CBOW, the optimization objective of Skip-gram is to maximize the following log-likelihood function<span class="math display">\[\mathcal{L}=\sum_{w\in\boldsymbol{\mathcal{C}}}\log P\big(context(w)|w\big)\]</span>Samely, the key point is to construct and calculate the conditional probability function <span class="math inline">\(P\big(context(w)|w\big)\)</span>.</p>
<h3 id="hierarchical-softmax-1">Hierarchical Softmax</h3>
<p>For Skip-gram model, given the central word <span class="math inline">\(w\)</span>, we need to predict the words in <span class="math inline">\(context(w)\)</span>. Since Skip-gram is similar to CBOW in inference process, here we use the same notations and symbols in hierarchical softmax of CBOW. In Skip-gram, the conditional probability function <span class="math inline">\(P\big(context(w)|w\big)\)</span> is defineed as<span class="math display">\[P\big(context(w)|w\big)=\prod_{u\in context(w)}P(u|w)\]</span>and using the idea of hierarchical softmax introduced in CBOW, we can rewrite the <span class="math inline">\(P(u|w)\)</span> as<span class="math display">\[P(u|w)=\prod_{j=2}^{l}P(d_{j}|e(w),\boldsymbol{\theta}_{j-1}^{u})\]</span>where<span class="math display">\[P(d_{j}|e(w),\boldsymbol{\theta}_{j-1}^{u})=\big[\sigma\big(e(w)^{T}\boldsymbol{\theta}_{j-1}^{u}\big)\big]^{1-d_{j}}\centerdot\big[1-\sigma\big(e(w)^{T}\boldsymbol{\theta}_{j-1}^{u}\big)\big]^{d_{j}}\]</span>So we can derive the complete expression of log-likelihood target function of Skip-gram model:<span class="math display">\[\mathcal{L}=\sum_{w\in\boldsymbol{\mathcal{C}}}\log\prod_{u\in context(w)}\prod_{j=2}^{l}\big\{\big[\sigma\big(e(w)^{T}\boldsymbol{\theta}_{j-1}^{u}\big)\big]^{1-d_{j}}\centerdot\big[1-\sigma\big(e(w)^{T}\boldsymbol{\theta}_{j-1}^{u}\big)\big]^{d_{j}}\big\}\\=\sum_{w\in\boldsymbol{\mathcal{C}}}\sum_{u\in context(w)}\sum_{j=2}^{l}\big\{(1-d_{j})\centerdot\log\big[\sigma\big(e(w)^{T}\boldsymbol{\theta}_{j-1}^{u}\big)\big]+d_{j}\centerdot\log\big[1-\sigma\big(e(w)^{T}\boldsymbol{\theta}_{j-1}^{u}\big)\big]\big\}\]</span>Hereto, the function above is the target function of Skip-gram, then our task is to optimize it, i.e., maximize this function. In Word2Vec, the author use the <strong>Stochastic Gradient Ascent</strong> method. And the key point of gradient descent/ascent methods are to find the gradient computing formula. To make further process simple, we denote that<span class="math display">\[\mathcal{L}(w,u,j)=(1-d_{j})\centerdot\log\big[\sigma\big(e(w)^{T}\boldsymbol{\theta}_{j-1}^{u}\big)\big]+d_{j}\centerdot\log\big[1-\sigma\big(e(w)^{T}\boldsymbol{\theta}_{j-1}^{u}\big)\big]\]</span>First, same as CBOW, consider the gradient computation of <span class="math inline">\(\mathcal{L}(w,u,j)\)</span> with regard to <span class="math inline">\(\boldsymbol{\theta}_{j-1}^{u}\)</span> (the computing process is same as the related part in CBOW):<span class="math display">\[\frac{\partial\mathcal{L}(w,u,j)}{\partial\boldsymbol{\theta}_{j-1}^{u}}=\big[1-d_{j}-\sigma\big(e(w)^{T}\boldsymbol{\theta}_{j-1}^{u}\big)\big]\centerdot e(w)\]</span>So, the update formula of <span class="math inline">\(\boldsymbol{\theta}_{j-1}^{u}\)</span> is<span class="math display">\[\boldsymbol{\theta}_{j-1}^{u}:=\boldsymbol{\theta}_{j-1}^{u}+\eta\big[1-d_{j}-\sigma\big(e(w)^{T}\boldsymbol{\theta}_{j-1}^{u}\big)\big]\centerdot e(w)\]</span>Next, we consider the gradient of <span class="math inline">\(\mathcal{L}(w,u,j)\)</span> with regard to <span class="math inline">\(e(w)\)</span>, using the symmetry of <span class="math inline">\(e(w)\)</span> and <span class="math inline">\(\boldsymbol{\theta}_{j-1}^{u}\)</span> in <span class="math inline">\(\mathcal{L}(w,u,j)\)</span>, we can derive that<span class="math display">\[\frac{\partial\mathcal{L}(w,u,j)}{\partial e(w)}=\big[1-d_{j}-\sigma\big(e(w)^{T}\boldsymbol{\theta}_{j-1}^{u}\big)\big]\centerdot \boldsymbol{\theta}_{j-1}^{u}\]</span>Thus, the update function of <span class="math inline">\(e(w)\)</span> is<span class="math display">\[e(w):=e(w)+\eta\sum_{u\in context(w)}\sum_{j=2}^{l}\frac{\partial\mathcal{L}(w,u,j)}{\partial e(w)}\]</span>Here is the pseudocode about using stochastic gradient ascent method to update the parameters in Skip-gram (note that in Word2Vec source codes, the <span class="math inline">\(e(w)\)</span> will be updated each time when a word <span class="math inline">\(u\)</span> in <span class="math inline">\(context(w)\)</span> is processed, not just updated after all the words in <span class="math inline">\(context(w)\)</span> are processed): &gt;For <span class="math inline">\(u\in context(w)\)</span> do: &gt; 1. <span class="math inline">\(\mathbf{v}=\boldsymbol{0}\)</span> &gt; 2. For <span class="math inline">\(j=2:l\)</span> do: &gt; 2.1 <span class="math inline">\(q=\sigma\big(e(w)^{T}\boldsymbol{\theta}_{j-1}^{u}\big)\)</span> &gt; 2.2 <span class="math inline">\(g=\eta (1-d_{j}-q)\)</span> &gt; 2.3 <span class="math inline">\(\mathbf{v}:=\mathbf{v}+g\centerdot\boldsymbol{\theta}_{j-1}^{u}\)</span> &gt; 2.4 <span class="math inline">\(\boldsymbol{\theta}_{j-1}^{u}:=\boldsymbol{\theta}_{j-1}^{u}+g\centerdot e(w)\)</span> &gt; 3. <span class="math inline">\(e(w):=e(w)+\mathbf{v}\)</span></p>
<p>Here is the corresponding relationship between pseudocode and Word2Vec source codes: <span class="math inline">\(e(w)\)</span> is corresponding to the “syn0” in Word2Vec source codes, <span class="math inline">\(\boldsymbol{\theta}_{j-1}^{u}\)</span> is corresponding to “syn1”, while <span class="math inline">\(\mathbf{v}\)</span> is corresponding to “neu1e”. Below the java codes of this procedure: <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> a = b; a &lt; window * <span class="number">2</span> + <span class="number">1</span> - b; a++) &#123;</span><br><span class="line">	<span class="comment">/**--------------------------------1--------------------------------------*/</span></span><br><span class="line">	<span class="keyword">for</span> (<span class="keyword">int</span> d = <span class="number">0</span>; d &lt; layer1_size; d++) neu1e[d] = <span class="number">0</span>;</span><br><span class="line">	...</span><br><span class="line">	<span class="comment">/**--------------------------------2--------------------------------------*/</span></span><br><span class="line">	<span class="keyword">for</span> (<span class="keyword">int</span> d = <span class="number">0</span>; d &lt; huffmanNode.code.length; d++) &#123;</span><br><span class="line">		<span class="keyword">double</span> f = <span class="number">0</span>;</span><br><span class="line">		<span class="keyword">int</span> l2 = huffmanNode.point[d];</span><br><span class="line">		<span class="comment">/**------------------------------2.1--------------------------------------*/</span></span><br><span class="line">		<span class="comment">// Propagate hidden -&gt; output</span></span><br><span class="line">		<span class="keyword">for</span> (<span class="keyword">int</span> e = <span class="number">0</span>; e &lt; layer1_size; e++) f += syn0[l1][e] * syn1[l2][e];</span><br><span class="line">		<span class="keyword">if</span> (f &lt;= -MAX_EXP || f &gt;= MAX_EXP) <span class="keyword">continue</span>;</span><br><span class="line">		<span class="keyword">else</span> f = EXP_TABLE[(<span class="keyword">int</span>)((f + MAX_EXP) * (EXP_TABLE_SIZE / MAX_EXP / <span class="number">2</span>))];</span><br><span class="line">		<span class="comment">/**------------------------------2.2--------------------------------------*/</span></span><br><span class="line">		<span class="comment">// 'g' is the gradient multiplied by the learning rate</span></span><br><span class="line">		<span class="keyword">double</span> g = (<span class="number">1</span> - huffmanNode.code[d] - f) * alpha;</span><br><span class="line">		<span class="comment">/**------------------------------2.3--------------------------------------*/</span></span><br><span class="line">		<span class="comment">// Propagate errors output -&gt; hidden</span></span><br><span class="line">		<span class="keyword">for</span> (<span class="keyword">int</span> e = <span class="number">0</span>; e &lt; layer1_size; e++) neu1e[e] += g * syn1[l2][e];</span><br><span class="line">		<span class="comment">/**------------------------------2.4--------------------------------------*/</span></span><br><span class="line">		<span class="comment">// Learn weights hidden -&gt; output</span></span><br><span class="line">		<span class="keyword">for</span> (<span class="keyword">int</span> e = <span class="number">0</span>; e &lt; layer1_size; e++) syn1[l2][e] += g * syn0[l1][e];</span><br><span class="line">	&#125;</span><br><span class="line">	...</span><br><span class="line">	<span class="comment">/**--------------------------------3--------------------------------------*/</span></span><br><span class="line">	<span class="comment">// Learn weights input -&gt; hidden</span></span><br><span class="line">	<span class="keyword">for</span> (<span class="keyword">int</span> d = <span class="number">0</span>; d &lt; layer1_size; d++) &#123; syn0[l1][d] += neu1e[d]; &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h3 id="negative-sampling-1">Negative Sampling</h3>
<p>Since we already have the experience in negative sampling of CBOW model, for skip-gram model, the derivation process is similar. First of all, we rewrite the optimization target function <span class="math inline">\(G=\prod_{w\in\boldsymbol{\mathcal{C}}}g(w)\)</span> to<span class="math display">\[G=\prod_{w\in\boldsymbol{\mathcal{C}}}\prod_{u\in context(w)}g(u)\]</span>Here, <span class="math inline">\(\prod_{u\in context(w)}g(u)\)</span> represents a target that we want to maximize for a given sample <span class="math inline">\(\big(w, context(w)\big)\)</span>, and <span class="math inline">\(g(u)\)</span> is similar to the <span class="math inline">\(g(w)\)</span> in the corresponding part of CBOW, and <span class="math inline">\(g(u)\)</span> is defined as<span class="math display">\[g(u) =\prod_{z\in\{u\}\cup NEG(u)}P(z|w)\]</span>where <span class="math inline">\(NEG(u)\)</span> denotes the generated subset of negative sample when deal with word <span class="math inline">\(u\)</span>, the conditional probability <span class="math inline">\(P(z|w)\)</span> is<span class="math display">\[P(z|w)=\begin{cases} \sigma\big(e(w)^{T}\boldsymbol{\theta}^{z}\big), &amp; L^{u}(z)=1;\\1-\sigma\big(e(w)^{T}\boldsymbol{\theta}^{z}\big), &amp; L^{u}(z)=0.\end{cases}\]</span>or simply, we can merge this two expressions:<span class="math display">\[P(z|w)=\big[\sigma\big(e(w)^{T}\boldsymbol{\theta}^{z}\big)\big]^{L^{u}(z)}\centerdot\big[1-\sigma\big(e(w)^{T}\boldsymbol{\theta}^{z}\big)\big]^{1-L^{u}(z)}\]</span>Hence, taking the logarithm of <span class="math inline">\(G\)</span>, our ultimate target function is<span class="math display">\[\mathcal{L}=\log G=\log\prod_{w\in\boldsymbol{\mathcal{C}}}\prod_{u\in context(w)}g(u)=\sum_{w\in\boldsymbol{\mathcal{C}}}\sum_{u\in context(w)}\log g(u)\\=\sum_{w\in\boldsymbol{\mathcal{C}}}\sum_{u\in context(w)}\log\prod_{z\in\{u\}\cup NEG(u)}P(z|w)=\sum_{w\in\boldsymbol{\mathcal{C}}}\sum_{u\in context(w)}\sum_{z\in\{u\}\cup NEG(u)}\log P(z|w)\\=\sum_{w\in\boldsymbol{\mathcal{C}}}\sum_{u\in context(w)}\sum_{z\in\{u\}\cup NEG(u)}\big\{L^{u}(z)\centerdot\log\big[\sigma\big(e(w)^{T}\boldsymbol{\theta}^{z}\big)\big]+\big[1-L^{u}(z)\big]\centerdot\log\big[1-\sigma\big(e(w)^{T}\boldsymbol{\theta}^{z}\big)\big]\big\}\]</span><strong>It is worth to mention that the negative sampling based Skip-gram model in Word2Vec source codes is not coding based on the formula above</strong>. Since, if it is based on the formula above, then for every sample <span class="math inline">\(\big(w, context(w)\big)\)</span>, the Word2Vec needs to do the negative sampling for every word in <span class="math inline">\(context(w)\)</span>, however, it only do <span class="math inline">\(|context(w)|\)</span> times negative sampling for <span class="math inline">\(w\)</span>. Here gives some thinking about this issue: the essence of Skip-gram is still using the CBOW model, it just change the summation of vectors of <span class="math inline">\(context(w)\)</span> to take them into consideration one by one. In this case, for a given sample <span class="math inline">\(\big(w, context(w)\big)\)</span>, we want to maximize<span class="math display">\[g(w)=\prod_{\tilde{w}\in context(w)}\prod_{u\in\{w\}\cup NEG^{\tilde{w}}(w)}P(u|\tilde{w})\]</span>where<span class="math display">\[P(u|\tilde{w})=\begin{cases}\sigma\big(e(\tilde{w})^{T}\boldsymbol{\theta}^{u}\big), &amp; L^{w}(u)=1;\\1-\sigma\big(e(\tilde{w})^{T}\boldsymbol{\theta}^{u}\big), &amp; L^{w}(u)=0.\end{cases}\]</span>or it can be written as<span class="math display">\[P(u|\tilde{w})=\big[\sigma\big(e(\tilde{w})^{T}\boldsymbol{\theta}^{u}\big)\big]^{L^{w}(u)}\centerdot\big[1-\sigma\big(e(\tilde{w})^{T}\boldsymbol{\theta}^{u}\big)\big]^{1-L^{w}(u)}\]</span>here <span class="math inline">\(NEG^{\tilde{w}}(w)\)</span> represents the subset of negative sample when dealing with word <span class="math inline">\(\tilde{w}\)</span>. Thereoupon, for a given corpus <span class="math inline">\(\boldsymbol{\mathcal{C}}\)</span>, function <span class="math inline">\(G=\prod_{w\in\boldsymbol{\mathcal{C}}}g(w)\)</span> can be treated as an overall optimization objective. Similarily, taking the logarithm of <span class="math inline">\(G\)</span>, our ultimate objective function is<span class="math display">\[\mathcal{L}=\log G=\log\prod_{w\in\boldsymbol{\mathcal{C}}}g(w)=\sum_{w\in\boldsymbol{\mathcal{C}}}g(w)\\=\sum_{w\in\boldsymbol{\mathcal{C}}}\sum_{\tilde{w}\in context(w)}\sum_{u\in\{w\}\cup NEG^{\tilde{w}}(w)}\big\{L^{w}(u)\centerdot\log\big[\sigma\big(e(\tilde{w})^{T}\boldsymbol{\theta}^{u}\big)\big]+\big[1-L^{w}(u)\big]\centerdot\log\big[1-\sigma\big(e(\tilde{w})^{T}\boldsymbol{\theta}^{u}\big)\big]\big\}\]</span>To make it simple, we denotes <span class="math inline">\(\mathcal{L}(w,\tilde{w},u)\)</span>:<span class="math display">\[\mathcal{L}(w,\tilde{w},u)=L^{w}(u)\centerdot\log\big[\sigma\big(e(\tilde{w})^{T}\boldsymbol{\theta}^{u}\big)\big]+\big[1-L^{w}(u)\big]\centerdot\log\big[1-\sigma\big(e(\tilde{w})^{T}\boldsymbol{\theta}^{u}\big)\big]\]</span>Then we use Stochastic Gradient Ascent method to optimize this objective function. First, consider the gradient computation of <span class="math inline">\(\mathcal{L}(w,\tilde{w},u)\)</span> with regard to <span class="math inline">\(\boldsymbol{\theta}^{u}\)</span>:<span class="math display">\[\frac{\partial\mathcal{L}(w,\tilde{w},u)}{\partial\boldsymbol{\theta}^{u}}=\big[L^{w}(u)-\sigma\big(e(\tilde{w})^{T}\boldsymbol{\theta}^{u}\big)\big]e(\tilde{w})\]</span>thus, the update formula of <span class="math inline">\(\boldsymbol{\theta}^{u}\)</span> can be written as<span class="math display">\[\boldsymbol{\theta}^{u}:=\boldsymbol{\theta}^{u}+\eta\big[L^{w}(u)-\sigma\big(e(\tilde{w})^{T}\boldsymbol{\theta}^{u}\big)\big]e(\tilde{w})\]</span>Then, consider the gradient of <span class="math inline">\(\mathcal{L}(w,\tilde{w},u)\)</span> with regard to <span class="math inline">\(e(\tilde{w})\)</span>, using the symmetry of <span class="math inline">\(\boldsymbol{\theta}^{u}\)</span> and <span class="math inline">\(e(\tilde{w})\)</span> in <span class="math inline">\(\mathcal{L}(w,\tilde{w},u)\)</span>, then we have<span class="math display">\[\frac{\partial\mathcal{L}(w,\tilde{w},u)}{\partial e(\tilde{w})}=\big[L^{w}(u)-\sigma\big(e(\tilde{w})^{T}\boldsymbol{\theta}^{u}\big)\big]\boldsymbol{\theta}^{u}\]</span>hence, the update formula of <span class="math inline">\(e(\tilde{w})\)</span> can be written as<span class="math display">\[e(\tilde{w}):=e(\tilde{w})+\eta\sum_{u\in\{w\}\cup NEG^{\tilde{w}}(w)}\frac{\partial\mathcal{L}(w,\tilde{w},u)}{\partial e(\tilde{w})}\]</span>Here we give the pseudocode of using stochastic gradient ascent model to update the parameters in negative sampling based Skip-gram model. &gt;For <span class="math inline">\(\tilde{w}\in context(w)\)</span> do: &gt; 1. <span class="math inline">\(\mathbf{v}=\boldsymbol{0}\)</span> &gt; 2. For <span class="math inline">\(u=\{w\}\cup NEG^{\tilde{w}}(w)\)</span> do: &gt; 2.1 <span class="math inline">\(q=\sigma\big(e(\tilde{w})^{T}\boldsymbol{\theta}^{u}\big)\)</span> &gt; 2.2 <span class="math inline">\(g=\eta (L^{w}(u)-q)\)</span> &gt; 2.3 <span class="math inline">\(\mathbf{v}:=\mathbf{v}+g\centerdot\boldsymbol{\theta}^{u}\)</span> &gt; 2.4 <span class="math inline">\(\boldsymbol{\theta}^{u}:=\boldsymbol{\theta}^{u}+g\centerdot e(\tilde{w})\)</span> &gt; 3. <span class="math inline">\(e(\tilde{w}):=e(\tilde{w})+\mathbf{v}\)</span></p>
<p>Below the java codes of this procedure: <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**--------------------------------3--------------------------------------*/</span></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> d = <span class="number">0</span>; d &lt;= config.negativeSamples; d++) &#123;</span><br><span class="line">	<span class="keyword">int</span> target;</span><br><span class="line">	<span class="keyword">final</span> <span class="keyword">int</span> label;</span><br><span class="line">	<span class="keyword">if</span> (d == <span class="number">0</span>) &#123;</span><br><span class="line">		target = huffmanNode.idx;</span><br><span class="line">		label = <span class="number">1</span>;</span><br><span class="line">	&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">		nextRandom = incrementRandom(nextRandom);</span><br><span class="line">		target = table[(<span class="keyword">int</span>) (((nextRandom &gt;&gt; <span class="number">16</span>) % TABLE_SIZE) + TABLE_SIZE) % TABLE_SIZE];</span><br><span class="line">		<span class="keyword">if</span> (target == <span class="number">0</span>) target = (<span class="keyword">int</span>) (((nextRandom % (vocabSize - <span class="number">1</span>)) + vocabSize - <span class="number">1</span>) % (vocabSize - <span class="number">1</span>)) + <span class="number">1</span>;</span><br><span class="line">		<span class="keyword">if</span> (target == huffmanNode.idx) <span class="keyword">continue</span>;</span><br><span class="line">		label = <span class="number">0</span>;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">int</span> l2 = target;</span><br><span class="line">	<span class="comment">/**------------------------------3.1--------------------------------------*/</span></span><br><span class="line">	<span class="keyword">double</span> f = <span class="number">0</span>;</span><br><span class="line">	<span class="keyword">for</span> (<span class="keyword">int</span> c = <span class="number">0</span>; c &lt; layer1_size; c++) f += neu1[c] * syn1neg[l2][c];</span><br><span class="line">	<span class="keyword">final</span> <span class="keyword">double</span> g;</span><br><span class="line">	<span class="comment">/**------------------------------3.2--------------------------------------*/</span></span><br><span class="line">	<span class="keyword">if</span> (f &gt; MAX_EXP) g = (label - <span class="number">1</span>) * alpha;</span><br><span class="line">	<span class="keyword">else</span> <span class="keyword">if</span> (f &lt; -MAX_EXP) g = (label - <span class="number">0</span>) * alpha;</span><br><span class="line">	<span class="keyword">else</span> g = (label - EXP_TABLE[(<span class="keyword">int</span>) ((f + MAX_EXP) * (EXP_TABLE_SIZE / MAX_EXP / <span class="number">2</span>))]) * alpha;</span><br><span class="line">	<span class="comment">/**------------------------------3.3--------------------------------------*/</span></span><br><span class="line">	<span class="keyword">for</span> (<span class="keyword">int</span> c = <span class="number">0</span>; c &lt; layer1_size; c++) neu1e[c] += g * syn1neg[l2][c];</span><br><span class="line">	<span class="comment">/**------------------------------3.4--------------------------------------*/</span></span><br><span class="line">	<span class="keyword">for</span> (<span class="keyword">int</span> c = <span class="number">0</span>; c &lt; layer1_size; c++) syn1neg[l2][c] += g * neu1[c];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h2 id="dl4j-implementation">DL4J Implementation</h2>
<p>Nowadays, there are lots of toolkit can be used to construct Word2Vec framework, like <a href="https://radimrehurek.com/gensim/models/word2vec.html" target="_blank" rel="noopener">gensim</a> for Python, <a href="https://www.tensorflow.org/tutorials/word2vec" target="_blank" rel="noopener">TensorFlow</a> for Python, google Word2Vec open <a href="https://code.google.com/archive/p/word2vec/" target="_blank" rel="noopener">source</a> for C(++), <a href="https://deeplearning4j.org/word2vec" target="_blank" rel="noopener">deeplearning4j</a> for Scala and Java and etc. In this case, you may not need to implement Word2Vec by writting all the codes youself, what you need to do is using these packages to build your Word2Vec framework effectively. Here is an sample codes of using deeplearning4j to implement Word2Vec <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// wikinotitle is dumped from wikipedia, which is around 13GB, here I just put a 66.6MB sample data</span></span><br><span class="line">String filePath = <span class="keyword">new</span> ClassPathResource(<span class="string">"wikinotitle"</span>).getFile().getAbsolutePath();</span><br><span class="line">log.info(<span class="string">"Load &amp; Vectorize Sentences...."</span>);</span><br><span class="line"><span class="comment">// Strip white space before and after for each line</span></span><br><span class="line">SentenceIterator iter = <span class="keyword">new</span> BasicLineIterator(filePath);</span><br><span class="line"><span class="comment">// Split on white spaces in the line to get words</span></span><br><span class="line">TokenizerFactory t = <span class="keyword">new</span> DefaultTokenizerFactory();</span><br><span class="line"><span class="comment">// CommonPreprocessor will apply the following regex to each token: [\d\.:,"'\(\)\[\]|/?!;]+</span></span><br><span class="line"><span class="comment">// So, effectively all numbers, punctuation symbols and some special symbols are stripped off.</span></span><br><span class="line"><span class="comment">// Additionally it forces lower case for all tokens.</span></span><br><span class="line">t.setTokenPreProcessor(<span class="keyword">new</span> CommonPreprocessor());</span><br><span class="line">log.info(<span class="string">"Building model...."</span>);</span><br><span class="line"><span class="comment">// for small corpus, load --&gt; w2vBuilder4SmallCorpus(iter, t);</span></span><br><span class="line">Word2Vec vec = w2vBuilder(iter, t);</span><br><span class="line">log.info(<span class="string">"Fitting Word2Vec model...."</span>);</span><br><span class="line">vec.fit();</span><br><span class="line">log.info(<span class="string">"done."</span>);</span><br><span class="line"><span class="comment">// Write word vectors to file</span></span><br><span class="line">log.info(<span class="string">"Writing word vectors to file...."</span>);</span><br><span class="line">WordVectorSerializer.writeWord2VecModel(vec, <span class="string">"src/main/resources/wiki-model"</span>);</span><br><span class="line">log.info(<span class="string">"done."</span>);</span><br></pre></td></tr></table></figure></p>
<p>The full codes you can get from my GitHub repository: <a href="https://github.com/IsaacChanghau/Word2VecfJava/tree/master/src/main/java/com/isaac/word2vec" target="_blank" rel="noopener">Word2VecExample</a>, or directly go to the Deeplearning4j offical website: <a href="https://deeplearning4j.org/word2vec" target="_blank" rel="noopener">[link]</a></p>
<h1 id="reference">Reference</h1>
<ol type="1">
<li><a href="https://en.wikipedia.org/wiki/Word2vec" target="_blank" rel="noopener">Word2Vec – Wikipedia</a></li>
<li><a href="https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf" target="_blank" rel="noopener">Distributed Representations of Words and Phrases and their Compositionality</a></li>
<li><a href="https://arxiv.org/pdf/1402.3722.pdf" target="_blank" rel="noopener">word2vec Explained: Deriving Mikolov et al.’s Negative-Sampling Word-Embedding Method</a></li>
<li><a href="https://arxiv.org/pdf/1411.2738.pdf" target="_blank" rel="noopener">word2vec Parameter Learning Explained</a></li>
<li><a href="https://arxiv.org/pdf/1301.3781.pdf" target="_blank" rel="noopener">Efficient Estimation of Word Representations in Vector Space</a></li>
<li><a href="http://www.cnblogs.com/peghoty/p/3857839.html" target="_blank" rel="noopener">Explanation of Mathematical Principles in Word2Vec (Chinese)</a></li>
<li><a href="http://licstar.net/archives/687" target="_blank" rel="noopener">Word and Document Embeddings based on Neural Network Approaches (Chinese)</a></li>
<li><a href="https://github.com/IsaacChanghau/Word2VecJava" target="_blank" rel="noopener">Word2VecJava</a></li>
<li><a href="https://code.google.com/archive/p/word2vec/" target="_blank" rel="noopener">Word2Vec Source</a></li>
</ol>

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        
  <ul class="post-copyright">
    <li class="post-copyright-author">
      <strong>Author:</strong>
      Isaac Changhau
    </li>
    <li class="post-copyright-link">
      <strong>Link:</strong>
      <a href="https://isaacchanghau.github.io/2017/05/13/Word2Vec/" title="Word2Vec Summary -- Mathematical Principles and Java Implementation">https://isaacchanghau.github.io/2017/05/13/Word2Vec/</a>
    </li>
    <li class="post-copyright-license">
      <strong>Notice: </strong>
      All articles are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/3.0/" rel="external nofollow" target="_blank">CC BY-NC-SA 3.0</a> unless stating additionally. Contact me via email for questions or discussion.
    </li>
  </ul>


      
    </div>

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/java/" rel="tag"># java</a>
          
            <a href="/tags/deep-learning/" rel="tag"># deep learning</a>
          
            <a href="/tags/deeplearning4j/" rel="tag"># deeplearning4j</a>
          
            <a href="/tags/natural-language-processing/" rel="tag"># natural language processing</a>
          
            <a href="/tags/word-embeddings/" rel="tag"># word embeddings</a>
          
            <a href="/tags/skip-gram/" rel="tag"># skip-gram</a>
          
            <a href="/tags/logistic-regression/" rel="tag"># logistic regression</a>
          
            <a href="/tags/bag-of-words/" rel="tag"># bag-of-words</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2017/05/04/机器学习-周志华-学习笔记-2/" rel="next" title="机器学习(周志华)--学习笔记(二) 线性回归(Linear Regression)">
                <i class="fa fa-chevron-left"></i> 机器学习(周志华)--学习笔记(二) 线性回归(Linear Regression)
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2017/05/18/word2vecf/" rel="prev" title="Word2Vecf -- Dependency-Based Word Embeddings and Lexical Substitute">
                Word2Vecf -- Dependency-Based Word Embeddings and Lexical Substitute <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            Overview
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.gif"
               alt="Isaac Changhau" />
          <p class="site-author-name" itemprop="name">Isaac Changhau</p>
           
              <p class="site-description motion-element" itemprop="description">Stay Hungry, Stay Foolish</p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives/">
                <span class="site-state-item-count">43</span>
                <span class="site-state-item-name">posts</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/categories/index.html">
                <span class="site-state-item-count">8</span>
                <span class="site-state-item-name">categories</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">40</span>
                <span class="site-state-item-name">tags</span>
              </a>
            </div>
          

        </nav>

        
          <div class="feed-link motion-element">
            <a href="/atom.xml" rel="alternate">
              <i class="fa fa-rss"></i>
              RSS
            </a>
          </div>
        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="mailto:isaac.changhau@gmail.com" target="_blank" title="Email">
                  
                    <i class="fa fa-fw fa-envelope"></i>
                  
                  Email
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://www.linkedin.com/in/isaac-changhau" target="_blank" title="Linkedin">
                  
                    <i class="fa fa-fw fa-linkedin"></i>
                  
                  Linkedin
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://github.com/IsaacChanghau" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                  GitHub
                </a>
              </span>
            
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#introduction"><span class="nav-number">1.</span> <span class="nav-text">Introduction</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#prerequisites"><span class="nav-number">2.</span> <span class="nav-text">Prerequisites</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#sigmoid"><span class="nav-number">2.1.</span> <span class="nav-text">Sigmoid</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#softmax"><span class="nav-number">2.2.</span> <span class="nav-text">Softmax</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#logistic-regression"><span class="nav-number">2.3.</span> <span class="nav-text">Logistic Regression</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#bayes-formula"><span class="nav-number">2.4.</span> <span class="nav-text">Bayes’ Formula</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#huffman-coding"><span class="nav-number">2.5.</span> <span class="nav-text">Huffman Coding</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#huffman-tree"><span class="nav-number">2.5.1.</span> <span class="nav-text">Huffman Tree</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#construction-of-huffman-tree"><span class="nav-number">2.5.2.</span> <span class="nav-text">Construction of Huffman Tree</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#coding-process"><span class="nav-number">2.5.3.</span> <span class="nav-text">Coding Process</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#natural-language-model"><span class="nav-number">3.</span> <span class="nav-text">Natural Language Model</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#n-gram-model"><span class="nav-number">3.1.</span> <span class="nav-text">N-gram Model</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#neural-probabilistic-based-model"><span class="nav-number">3.2.</span> <span class="nav-text">Neural Probabilistic based Model</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#word2vec"><span class="nav-number">4.</span> <span class="nav-text">Word2Vec</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#continuous-bag-of-words-model"><span class="nav-number">4.1.</span> <span class="nav-text">Continuous Bag-of-Words Model</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#hierarchical-softmax"><span class="nav-number">4.1.1.</span> <span class="nav-text">Hierarchical Softmax</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#negative-sampling"><span class="nav-number">4.1.2.</span> <span class="nav-text">Negative Sampling</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#skip-gram-model"><span class="nav-number">4.2.</span> <span class="nav-text">Skip-gram Model</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#hierarchical-softmax-1"><span class="nav-number">4.2.1.</span> <span class="nav-text">Hierarchical Softmax</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#negative-sampling-1"><span class="nav-number">4.2.2.</span> <span class="nav-text">Negative Sampling</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#dl4j-implementation"><span class="nav-number">4.3.</span> <span class="nav-text">DL4J Implementation</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#reference"><span class="nav-number">5.</span> <span class="nav-text">Reference</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      
        <div class="back-to-top">
          <i class="fa fa-arrow-up"></i>
          
            <span id="scrollpercent"><span>0</span>%</span>
          
        </div>
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy;  2017 - 
  <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-star"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Isaac Changhau</span>
</div>


<div class="powered-by">
  Powered by <a class="theme-link" href="https://hexo.io">Hexo</a>
</div>

<div class="theme-info">
  Theme -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT
  </a>
</div>


        

        
      </div>
    </footer>

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.1"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.1"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.1"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.1"></script>



  


  




	





  





  





  






  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->





  

  

  

  
  


  

  

</body>
</html>
