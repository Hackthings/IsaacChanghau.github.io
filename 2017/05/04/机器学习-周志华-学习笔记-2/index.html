<!doctype html>



  


<html class="theme-next mist use-motion" lang="en">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="java,machine learning,deeplearning4j,spark,scala,linear regression,logistic regression,linear discriminant analysis," />





  <link rel="alternate" href="/atom.xml" title="Isaac Changhau" type="application/atom+xml" />




  <link rel="shortcut icon" type="image/x-icon" href="/tltimg.jpg?v=5.1.1" />






<meta name="description" content="第三章–线性模型 (linear regression)">
<meta name="keywords" content="java,machine learning,deeplearning4j,spark,scala,linear regression,logistic regression,linear discriminant analysis">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习(周志华)--学习笔记(二) 线性回归(Linear Regression)">
<meta property="og:url" content="https://isaacchanghau.github.io/2017/05/04/机器学习-周志华-学习笔记-2/index.html">
<meta property="og:site_name" content="Isaac Changhau">
<meta property="og:description" content="第三章–线性模型 (linear regression)">
<meta property="og:locale" content="en">
<meta property="og:image" content="https://isaacchanghau.github.io/images/machinelearning/linear/log-linear-regression.png">
<meta property="og:image" content="https://isaacchanghau.github.io/images/machinelearning/linear/lda.png">
<meta property="og:image" content="https://isaacchanghau.github.io/images/machinelearning/linear/ovoovr.png">
<meta property="og:image" content="https://isaacchanghau.github.io/images/machinelearning/linear/ecoc.png">
<meta property="og:updated_time" content="2017-08-06T03:59:02.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="机器学习(周志华)--学习笔记(二) 线性回归(Linear Regression)">
<meta name="twitter:description" content="第三章–线性模型 (linear regression)">
<meta name="twitter:image" content="https://isaacchanghau.github.io/images/machinelearning/linear/log-linear-regression.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    sidebar: {"position":"right","display":"post","offset":12,"offset_float":0,"b2t":true,"scrollpercent":true},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://isaacchanghau.github.io/2017/05/04/机器学习-周志华-学习笔记-2/"/>





  <title>机器学习(周志华)--学习笔记(二) 线性回归(Linear Regression) | Isaac Changhau</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  














</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-right page-post-detail ">
    <div class="headband"></div>
    
    <!--<a href="https://github.com/IsaacChanghau"><img style="position: absolute; top: 0; right: 0; border: 0;" src="https://camo.githubusercontent.com/38ef81f8aca64bb9a64448d0d70f1308ef5341ab/68747470733a2f2f73332e616d617a6f6e6177732e636f6d2f6769746875622f726962626f6e732f666f726b6d655f72696768745f6461726b626c75655f3132313632312e706e67" alt="Fork me on GitHub" data-canonical-src="https://s3.amazonaws.com/github/ribbons/forkme_right_darkblue_121621.png"></a>-->

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Isaac Changhau</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">Stay Hungry, Stay Foolish</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th-list"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-vcard"></i> <br />
            
            About
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            Search
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="Searching..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://isaacchanghau.github.io/2017/05/04/机器学习-周志华-学习笔记-2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Isaac Changhau">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Isaac Changhau">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">机器学习(周志华)--学习笔记(二) 线性回归(Linear Regression)</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-05-04T10:49:58+08:00">
                2017-05-04
              </time>
            

            
              <span class="post-updated">
                &nbsp; | &nbsp; <i class="fa fa-calendar-check-o"></i> Updated on
                <time itemprop="dateUpdated" datetime="2017-08-06T11:59:02+08:00" content="2017-08-06">
                  2017-08-06
                </time>
              </span>
            

            

            
          </span>

          
            <span class="post-letters-count">
              &nbsp; | &nbsp;
              <i class="fa fa-file-word-o"></i>
            <span>Count 8,835 words</span>
              <!--&nbsp; | &nbsp;
              <i class="fa fa-clock-o"></i>
            <span>Reading 40 min</span>-->
            </span>
          

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Machine Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>第三章–线性模型 (linear regression)<a id="more"></a></p>
<h1 id="线性模型"><a href="#线性模型" class="headerlink" title="线性模型"></a>线性模型</h1><p>给定由$d$个属性描述的示例$\mathbf{x}=(x_{1};x_{2};\dots ;x_{d})$，其中$x_{i}$是$\mathbf{x}$在第$i$个属性上的取值，线性模型(linear model)试图学的一个通过属性的线性组合来进行预测的函数，即$$f(\mathbf{x})=\mathbf{w}^{T}\mathbf{x}+b$$其中$\mathbf{w}=(w_{1};w_{2};\dots ;w_{d})$。$\mathbf{w}$和$b$学得之后，模型就可以确定。</p>
<h1 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h1><p>给定数据集$D={(\mathbf{x}_{1},y_{1}),(\mathbf{x}_{2},y_{2}),\dots ,(\mathbf{x}_{m},y_{m})}$，其中$\mathbf{x}_{i}=(x_{i1};x_{i2};\dots ;x_{id}), y_{i}\in \mathbb{R}$。“线性回归” (linear regression)试图学得$$f(\mathbf{x}_{i})=\mathbf{w}^{T}\mathbf{x}_{i}+b, 使得f(\mathbf{x}_{i})\simeq y_{i}$$接下来的任务是确定$\mathbf{w}$和$b$，其关键在于衡量$f(x)$和$y$之间的差别，而均方误差是回归任务中最常用的性能度量，它对应了常用的“欧式距离” (Euclidean distance)，因此可以试图让均方误差最小化。均方误差最小化进行模型求解的方法称为“最小二乘法” (least square method)。在线性回归中，最小二乘法就是试图找到一条直线，使所有样本到直线上的欧式距离之和最小。这里，我们将$\mathbf{w}$和$b$吸收入向量形式$\mathbf{\hat{w}}=(b;\mathbf{w})=(b;w_{1};w_{2};\dots ;w_{d})$，相应的，将数据集$D$中的$\mathbf{x}_{i}$表示为$\mathbf{x}_{i}=(1;x_{i1};x_{i2};\dots ;x_{id})$，因此数据集$D$可以表示为一个$m\times (d+1)$的矩阵$\mathbf{X}=(\mathbf{x}_{1};\mathbf{x}_{2};\dots ;\mathbf{x}_{m})^{T}$，再把标记也写成向量形式$\mathbf{y}=(y_{1};y_{2};\dots ;y_{m})$。于是有$$f(\mathbf{X})=\mathbf{X}\mathbf{\hat{w}}, 使得f(\mathbf{X})\simeq\mathbf{y}$$最小化均方误差，有$$\mathbf{\hat{w}}^{*}=\arg\min_{\mathbf{\hat{w}}}(f(\mathbf{X})-\mathbf{y})^{2}=\arg\min_{\mathbf{\hat{w}}}(\mathbf{y}-\mathbf{X}\mathbf{\hat{w}})^{T}(\mathbf{y}-\mathbf{X}\mathbf{\hat{w}})$$令$E_{\mathbf{\hat{w}}}=(\mathbf{y}-\mathbf{X}\mathbf{\hat{w}})^{T}(\mathbf{y}-\mathbf{X}\mathbf{\hat{w}})$，对$\mathbf{\hat{w}}$求导得到$$\frac{\partial E_{\mathbf{\hat{w}}}}{\partial \mathbf{\hat{w}}}=2\mathbf{X}^{T}(\mathbf{X}\mathbf{\hat{w}}-\mathbf{y})$$令上式为零可得$\mathbf{\hat{w}}$最优解的封闭式。当$\mathbf{X}^{T}\mathbf{X}$为<strong>满秩矩阵</strong> (full-rank matrix)或<strong>正定矩阵</strong> (positive definite matrix)时，可得到$$\mathbf{\hat{w}}^{*}=(\mathbf{X}^{T}\mathbf{X})^{-1}\mathbf{X}^{T}\mathbf{y}$$其中$(\mathbf{X}^{T}\mathbf{X})^{-1}$是$\mathbf{X}^{T}\mathbf{X}$的逆矩阵。然而现实任务中，$\mathbf{X}^{T}\mathbf{X}$往往不是满秩矩阵，此时可解出多个$\mathbf{\hat{w}}$，都能使均方误差最小化，而选择哪一个解作为输出将由算法的归纳偏好决定，常见的做法是引入正则化(regularization)项。<br>线性模型预测值不仅可以逼近$y$，也可以使其逼近$y$的衍生物。比如，可将输出标记的对数作为线性模型逼近的目标，即“对数线性回归” (log-linear regression)$$\ln y=\mathbf{w}^{T}\mathbf{x}+b$$它实际上是让$e^{\mathbf{w}^{T}\mathbf{x}+b}$逼近$y$，形式上它仍是线性回归，但实质上已是在求取输入空间到输出空间的非线性函数映射，如图<br><img src="/images/machinelearning/linear/log-linear-regression.png" alt="log-linear regression"><br>更一般地，考虑单调可微函数$g(\centerdot)$，令$$y=g^{-1}(\mathbf{w}^{T}\mathbf{x}+b)$$这样得到的模型称为“广义线性模型” (generalized linear model)，其中函数$g(\centerdot)$称为“联系函数” (link function)。对数线性回归是广义线性模型在$g(\centerdot)=\ln (\centerdot)$时的特例。</p>
<h1 id="对数几率回归"><a href="#对数几率回归" class="headerlink" title="对数几率回归"></a>对数几率回归</h1><p>将线性回归应用到分类任务的方法: 找到一个单调可微函数将分类任务的真实标记$y$与线性回归模型的预测值联系起来。例如，对于二分类任务，$y\in {0,1}$，使用“<a href="https://en.wikipedia.org/wiki/Heaviside_step_function" target="_blank" rel="noopener">单位阶跃函数</a>” (unit-step/Heaviside function)可将模型预测值转换为0/1值。但是单位阶跃函数不连续，于是使用近似单位阶跃函数的替代函数，如使用“对数机率函数”$$y=\frac{1}{1+e^{-z}}$$它是一种“<a href="https://en.wikipedia.org/wiki/Sigmoid_function" target="_blank" rel="noopener">Sigmoid函数</a>”，用对数机率函数作为$g^{-1}$得到$$y=\frac{1}{1+e^{-(\mathbf{w}^{T}\mathbf{x}+b)}}$$变换得$$\ln \frac{y}{1-y}=\mathbf{w}^{T}\mathbf{x}+b$$这种模型称为“对数机率回归” (<a href="https://en.wikipedia.org/wiki/Logistic_regression" target="_blank" rel="noopener">logistic regression</a>)。虽然它的名字是“回归”但实际上是一种分类学习方法。对数函数是任意阶可导的凸函数，许多数值优化算法均可以求解其最优解。<br>重写上式，有:$$\ln\frac{p(y=1|\mathbf{x})}{p(y=0|\mathbf{x})}=\boldsymbol{\mathcal{w}}^{T}\mathbf{x}+b$$其中:$$p(y=1|\mathbf{x})=\frac{e^{(\mathbf{w}^{T}\mathbf{x}+b)}}{1+e^{(\mathbf{w}^{T}\mathbf{x}+b)}}$$$$p(y=0|\mathbf{x})=\frac{1}{1+e^{(\mathbf{w}^{T}\mathbf{x}+b)}}$$于是，可通过“极大似然法”(maximum likelihood method)来估计$\boldsymbol{\mathcal{w}}$和$b$。给定数据集$\{(\mathbf{x}_{i},y_{i})\}_{i=1}^{m}$，对数回归模型最大化“对数似然”(log-likelihood):$$\mathcal{l}(\boldsymbol{\mathcal{w}},b)=\sum_{i=1}^{m}\ln p(y_{i}|\mathbf{x}_{i};\boldsymbol{\mathcal{w}},b)$$即令每个样本属于其真实标记的概率越大越好。这里令$\boldsymbol{\beta}=(\boldsymbol{\mathcal{w}};b)$，$\hat{\mathbf{x}}=(\mathbf{x};1)$，则$\boldsymbol{\mathcal{w}}^{T}\mathbf{x}+b$简写为$\boldsymbol{\beta}^{T}\hat{\mathbf{x}}$。再令$p_{1}(\hat{\mathbf{x}};\boldsymbol{\beta})=p(y=1|\hat{\mathbf{x}};\boldsymbol{\beta})$，$p_{0}(\hat{\mathbf{x}};\boldsymbol{\beta})=p(y=0|\hat{\mathbf{x}};\boldsymbol{\beta})=1-p_{1}(\hat{\mathbf{x}};\boldsymbol{\beta})$，则:$$p(y_{i}|\mathbf{x}_{i};\boldsymbol{\mathcal{w}},b)=y_{i}p_{1}(\hat{\mathbf{x}}_{i};\boldsymbol{\beta})+(1-y_{i})p_{0}(\hat{\mathbf{x}}_{i};\boldsymbol{\beta})$$该问题可转化为最小化:$$\mathcal{l}(\boldsymbol{\beta})=\sum_{i=1}^{m}\bigg(-y_{i}\boldsymbol{\beta}^{T}\hat{\mathbf{x}}_{i}+\ln\big(1+e^{\boldsymbol{\beta}^{T}\hat{\mathbf{x}}_{i}}\big)\bigg)$$上式为关于$\boldsymbol{\beta}$的高阶可导连续凸函数，根据凸优化理论，经典的数值优化方法如“<a href="https://en.wikipedia.org/wiki/Gradient_descent" target="_blank" rel="noopener">梯度下降法</a>” (gradient descent method)、“<a href="https://en.wikipedia.org/wiki/Newton%27s_method" target="_blank" rel="noopener">牛顿法</a>” (Newton method)等都可以求得其最优解，于是有:$$\boldsymbol{\beta}^{*}=\arg\min_{\boldsymbol{\beta}}\mathcal{l}(\boldsymbol{\beta})$$以牛顿法为例，其第$t+1$轮迭代解的更新公式为:$$\boldsymbol{\beta}^{t+1}=\boldsymbol{\beta}^{t}-\bigg(\frac{\partial^{2}\mathcal{l}(\boldsymbol{\beta})}{\partial\boldsymbol{\beta}\partial\boldsymbol{\beta}^{T}}\bigg)^{-1}\frac{\partial\mathcal{l}(\boldsymbol{\beta})}{\partial\boldsymbol{\beta}}$$其中关于$\boldsymbol{\beta}$的一阶、二阶导数分别为:$$\frac{\partial\mathcal{l}(\boldsymbol{\beta})}{\partial\boldsymbol{\beta}}=-\sum_{i=1}^{m}\hat{\mathbf{x}}_{i}(y_{i}-p_{1}(\hat{\mathbf{x}}_{i};\boldsymbol{\beta}))$$$$\frac{\partial^{2}\mathcal{l}(\boldsymbol{\beta})}{\partial\boldsymbol{\beta}\partial\boldsymbol{\beta}^{T}}=\sum_{i=1}^{m}\hat{\mathbf{x}}_{i}\hat{\mathbf{x}}_{i}^{T}p_{1}(\hat{\mathbf{x}}_{i};\boldsymbol{\beta})(1-p_{1}(\hat{\mathbf{x}}_{i};\boldsymbol{\beta}))$$</p>
<h1 id="线性判别分析-LDA"><a href="#线性判别分析-LDA" class="headerlink" title="线性判别分析 (LDA)"></a>线性判别分析 (LDA)</h1><p>线性判别分析 (<a href="https://en.wikipedia.org/wiki/Linear_discriminant_analysis" target="_blank" rel="noopener">Linear Discriminant Analysis</a>, LDA)是一种经典的线性学习方法，其思想非常朴素: 给定训练样例集，设法将样例投影到一条直线上，使得同类样例的投影点尽可能接近、异类样例的投影点尽可能的远离；在对新样本进行分类时，将其投影到同样的这条直线上，再根据投影点的位置来判断新样本的类别。如图为二分类例子<br><img src="/images/machinelearning/linear/lda.png" alt="linear discriminant analysis"><br>其中“+”、“-”分别表示正例和反例，椭圆表示数据簇的外轮廓，虚线表示投影，红色实心圆盒实心三角形分别表示两类样本投影后的中心点。(略)</p>
<h1 id="多分类学习任务"><a href="#多分类学习任务" class="headerlink" title="多分类学习任务"></a>多分类学习任务</h1><p>对于多分类学习，一些二分类学习方法可以直接应用于多分类任务，但更多情况下，需要将多分类任务进行“拆解”，再使用二分类学习器来解决。考虑$N$个类别$C_{1},C_{2},\dots,C_{N}$，“拆解法”将多分类任务拆为若干个二分类任务， 然后为每个二分类任务训练一个分类器，测试时将这些分类器预测结果进行集成从而获得最终的结果。最经典的拆分策略有三种: “一对一”(One v.s. One, OvO)、“一对其余”(One v.s. Rest, OvR)、“多对多”(Many v.s. Many, MvM)。<br>给定数据集$D=\{(x_{1},y_{1}),(x_{2},y_{2}),\dots,(x_{m},y_{m})\},y_{i}\in\{C_{1},C_{2},\dots,C_{N}\}$:</p>
<ul>
<li>OvO将$N$个类别两两配对产生$\frac{N\centerdot(N-1)}{2}$个二分类任务，例如，OvO中区分$C_{i}$和$C_{j}$的一个分类器，该分类器把$D$中的$C_{i}$类样例作为正例，$C_{j}$类样例作为负例。测试阶段，新样本同时提交给所有分类器，然后得到$\frac{N\centerdot(N-1)}{2}$个分类结果，最终结果投票产生 (多者胜)。</li>
<li>OvR则是每次将一个类的样例作为正例，其余样例作为负例来训练$N$个分类器。测试时，若仅有一个分类器预测为正类，则对应的类别标记作为最终分类结果；若多个分类器预测为正类，则根据分类器的预测置信度，选择置信度最大的类别标记为分类结果。</li>
<li>MvM是每次将若干个类作为正类，其余作为反类。显然<strong>OvO和OvR是MvM的特例</strong>。MvM的正、负类构造需要有特殊设计，不能随意选择，“纠错输出码”(Error Correcting Output Codes, ECOC)是最常用的MvM技术。</li>
</ul>
<p><img src="/images/machinelearning/linear/ovoovr.png" alt="OvO and OvR"><br>比较OvO和OvR，OvR只需要$N$个分类器，而OvO需要$\frac{N\centerdot(N-1)}{2}$个分类器，因此<strong>OvO的存储开销和测试时间开销通常比OvR更大</strong>。但训练时，OvR的每个分类器都需要使用全部的样例，而OvO的每个分类器之需要两个类的样例，因此<strong>OvO的训练时间开销通常小于OvR</strong>。而预测性能， 则取决于具体的数据分布，多数情况下<strong>两者性能相近</strong>。<br>这里介绍ECOC技术，ECOC是将编码的思想引入类别拆分，并尽可能在解码过程中具有容错性。其主要分两步:</p>
<ol>
<li>编码: 对$N$个类别做$M$次划分，每次划分将一部分类别划为正类，一部分划为负类，从而形成一个二分类训练集，这样一共产生$M$个训练集，可训练出$M$个分类器。</li>
<li>解码: $M$个分类器分别对测试样本进行预测，这些预测标记组成一个编码。将这个预测编码与每个列别各自的编码进行比较，返回其中距离最小的类别作为最终预测结果。</li>
</ol>
<p>类别划分通过“编码矩阵”(coding matrix)指定。常见的编码矩阵有二元编码和三元编码。前者将每个类别划分为正类的负类，后者在正、负类之外，还可指定“停用类”。如图所示<br><img src="/images/machinelearning/linear/ecoc.png" alt="ECOC Coding Graph"><br>(a)中分类器$f_{2}$将$C_{1}$类和$C_{3}$类的样例作为正例，$C_{2}$类和$C_{4}$类的样例作为负例。(b)中分类器$f_{4}$将$C_{1}$类和$C_{4}$类作为正例，$C_{3}$类作为负例。在解码阶段，各分类器的预测结果联合起来形成了测试示例的编码，该编码与各类对应的编码进行比较，将距离最小的编码所对应的类别作为预测结果。距离计算一般采用采用欧式距离(Euclidean distance)或海明距离(Hamming distance)。如(a)中，若基于欧式距离，预测结果为$C_{3}$。<br>该方法之所以称为“纠错输出码”，是因为在测试阶段，ECOC编码对分类的错误有一定的容忍和修正能力。例如在(a)中对测试示例的正确预测编码是$(-1,+1,+1,-1,+1)$，假设在预测时某个分类器出错了，例如$f_{2}$出错从而导致了错误编码$(-1,-1,+1,-1,+1)$，但居于这个编码仍能产生正确的最终分类结果$C_{3}$。一般来说，对同一个学习任务，ECOCO编码越长，纠错能力越强。但是，编码越长，意味着需要训练的分类器越多，计算和存储开销都会增大。此外，对有限类别数，可能的组合数目有限，码长超过一定范围后就失去了意义。<br>对同等长度的编码，任意两个类别间的编码距离越远，则纠错能力越强。因此，在码长较小时可根据这个原则计算出理论最优编码。然而，码长稍大一些就难以有效地确定最优编码，事实上这是一个NP难问题。不过，通常我们不需获得理论最优编码，因为非最优编码在实践中往往已能产生足够好的分类器。另一方面，并不是编码的理论性质越好，分类器性能就越好，因为机器学习问题涉及很多因素，例如将多个类拆解为两个“类别子集”，不同拆解方法所形成的连个类别自己的区分难度往往不同，及其导致的二分类问题的难度不同。于是，一个理论纠错性质好，但导致二分类问题较难的编码，与另一个理论纠错性质差一些，但导致的二分类问题较简单的编码，最终产生的模型性能孰强孰弱很难说。</p>
<h1 id="类别不平衡问题"><a href="#类别不平衡问题" class="headerlink" title="类别不平衡问题"></a>类别不平衡问题</h1><p>类别不平衡(class-imbalance)指分类任务中不同类别的训练样例数目差别很大的情况。这样的情况很常见，例如用拆分解决多分类问题时，即使原数据集中不同类别的训练样例数相当，在使用OvR、MvM策略后产生的二分类任务仍有可能出现类别不平衡现象。从线性分类器的角度，用$y=\boldsymbol{\mathcal{w}}^{T}\mathbf{x}+b$对新样本$\mathbf{x}$进行分类时，事实上是在用预测出的$y$值与一个阈值进行比较，通常$y&gt;0.5$判别为正例，否则为负例。$y$实际上表达了正例的可能性，几率$\frac{y}{1-y}$反映了正例和负例可能性的比值，阈值设置$0.5$表明分类器认为正、负；例可能性高相同，即分类器决策规则为：若$\frac{y}{1-y}&gt;1$则预测为正例。<br>但是，当正反例数目不同时，令$m^{+}$表示正例数目，$m^{-}$表示负例数目，则观测几率为$\frac{m^{+}}{m^{-}}$，假设训练集时真实样本总体的无偏采样，因为观测几率代表例真实几率，所以只要分类器预测几率高于观测几率就因该判定为正例：若$\frac{y}{1-y}&gt;\frac{m^{+}}{m^{-}}$则预测为正例。然而，分类器通常基于阈值$0.5$进行决策，因此需要对预测值进行调整，令$\frac{y’}{1-y’}=\frac{y}{1-y}\times\frac{m^{-}}{m^{+}}$，这是类别不平衡学习的一个基本策略——“再缩放”(rescaling, or rebalance)。<br>再缩放思想虽然简单，但实际操作中，之前假设的“训练集是真实样本总体的无偏采样”往往不成立，即我们未必能有效地基于训练集观测几率来推断真实几率。现有技术大体上有三类做法 (假设负例样本数目大于正例样本数目):</p>
<ol>
<li>直接对训练集例的反例样本进行“欠采样”(undersampling)，即去除部分反例，使得正、负例数目接近，然后再进行学习。</li>
<li>对训练集例正例样本进行“过采样”(oversampling)，即增加一些正例，使得正、负例数目接近，然后再进行学习。</li>
<li>直接给予原始训练集进行学习，但在用训练好的分类器进行预测时，将$\frac{y’}{1-y’}=\frac{y}{1-y}\times\frac{m^{-}}{m^{+}}$嵌入到决策过程中，称为“阈值移动”(threshold-moving)。</li>
</ol>
<p>欠采样的时间开销远小于过采样，因为前者丢弃了很多负例，使得分类器徐连击远小于初始训练集，而过采样法增加了很多正例，其训练集大于初始训练集。需要注意的是:</p>
<ul>
<li>过采样法不能简单的对初始正例样本进行重复采样，否则会导致严重的过拟合。过采样法的代表性算法SMOTE是通过对训练集里的正例进行插值来产生额外的正例。</li>
<li>欠采样法若随机丢弃负例，可能会丢失一些重要信息。欠采样法的代表性算法EasyEnsemble则是利用集成学习机制，将负例划分为若干个集合供不同的学习器使用，这样对每个学习器都进行了欠采样，但在全局来看却不会丢失重要信息。</li>
</ul>
<p>值得一提的是，“再缩放”也是“代价敏感学习”(cost-sensitive learning)的基础。在代价敏感学习中将$\frac{y’}{1-y’}=\frac{y}{1-y}\times\frac{m^{-}}{m^{+}}$中的$\frac{m^{-}}{m^{+}}$用$\frac{cost^{+}}{cost^{-}}$代替即可，其中$cost^{+}$是将正例误分为负例的代价，$cost^{-}$是将负例误分为正例的代价。</p>
<h1 id="习题"><a href="#习题" class="headerlink" title="习题"></a>习题</h1><p><strong>3.1</strong> 试分析在什么情况下$y=w^{T}x+b$中不必考虑偏置项$b$。<br>Ans: 对于线性模型$y=w^{T}x+b$，第$i$个实例减去第一个实例可得$y_{i}-y_{0}=w^{T}(x_{i}-x_{0})$，这里偏置项被消除，因此，可以对于每一个样本数据，均和第一个样本相减，然后对新的样本进行线性回归，就只需要使用模型$y=w^{T}x$。</p>
<p><strong>3.2</strong> 试证明，对于参数$\boldsymbol{\mathcal{w}}$，对数几率回归的目标函数$$y=\frac{1}{1+e^{-(w^{T}x+b)}}\tag{1}$$是非凸的，但是对数似然函数$$\mathcal{l}(\beta)=\sum_{i=1}^{m}\bigg(-y_{i}\beta^{T}\hat{x}_{i}+\ln\big(1+e^{\beta^{T}\hat{x}_{i}}\big)\bigg)\tag{2}$$是凸的。<br>Ans: 如果一个多元函数是凸的，那么它的Hessian矩阵是半正定的。对(1)关于$w$求偏导:$$\frac{\partial y}{\partial w}=x(y-y^{2})$$并对上式求关于$w^{T}$的偏导有：$$\frac{\partial}{\partial w^{T}}\big(\frac{\partial y}{\partial w}\big)=x(1-2y)\big(\frac{\partial y}{\partial w}\big)^{T}=xx^{T}y(y-1)(1-2y)$$这里，$xx^{T}$合同为单位矩阵，所以$xx^{T}$是半正定的。而$y\in(0,1)$，当$y\in{0.5,1}$时，$y(y-1)(1-2y)&lt;0$，使得$\frac{\partial}{\partial w^{T}}\big(\frac{\partial y}{\partial w}\big)$为半负定的，因此$y=\frac{1}{1+e^{-(w^{T}x+b)}}$为非凸的。<br>对于(2)，有$$\frac{\partial}{\partial\beta^{T}}\big(\frac{\partial\mathcal{l}(\beta)}{\partial\beta}\big)=\sum_{i=1}^{m}\hat{x}_{i}\hat{x}_{i}^{T}\mathcal{p}_{1}(\hat{x}_{i};\beta)(1-\mathcal{p}_{1}(\hat{x}_{i};\beta))$$显然$\mathcal{p}_{1}\in(0,1)$，且$\mathcal{p}_{1}(\hat{x}_{i};\beta)(1-\mathcal{p}_{1}(\hat{x}_{i};\beta))\geq0$，因此$\mathcal{l}(\beta)$是凸的。</p>
<p><strong>3.3</strong> 编程实现对率回归，并给出西瓜数据集$3.0\alpha$上的数据结果。</p>
<table>
<thead>
<tr>
<th style="text-align:center">Id</th>
<th style="text-align:center">Density</th>
<th style="text-align:center">Sugar Content</th>
<th style="text-align:center">Good Melon</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">0.697</td>
<td style="text-align:center">0.460</td>
<td style="text-align:center">yes          </td>
</tr>
<tr>
<td style="text-align:center">2</td>
<td style="text-align:center">0.774</td>
<td style="text-align:center">0.376</td>
<td style="text-align:center">yes          </td>
</tr>
<tr>
<td style="text-align:center">3</td>
<td style="text-align:center">0.634</td>
<td style="text-align:center">0.264</td>
<td style="text-align:center">yes          </td>
</tr>
<tr>
<td style="text-align:center">4</td>
<td style="text-align:center">0.608</td>
<td style="text-align:center">0.318</td>
<td style="text-align:center">yes          </td>
</tr>
<tr>
<td style="text-align:center">5</td>
<td style="text-align:center">0.556</td>
<td style="text-align:center">0.215</td>
<td style="text-align:center">yes          </td>
</tr>
<tr>
<td style="text-align:center">6</td>
<td style="text-align:center">0.403</td>
<td style="text-align:center">0.237</td>
<td style="text-align:center">yes          </td>
</tr>
<tr>
<td style="text-align:center">7</td>
<td style="text-align:center">0.481</td>
<td style="text-align:center">0.149</td>
<td style="text-align:center">yes          </td>
</tr>
<tr>
<td style="text-align:center">8</td>
<td style="text-align:center">0.437</td>
<td style="text-align:center">0.211</td>
<td style="text-align:center">yes          </td>
</tr>
<tr>
<td style="text-align:center">9</td>
<td style="text-align:center">0.666</td>
<td style="text-align:center">0.091</td>
<td style="text-align:center">no           </td>
</tr>
<tr>
<td style="text-align:center">10</td>
<td style="text-align:center">0.243</td>
<td style="text-align:center">0.267</td>
<td style="text-align:center">no           </td>
</tr>
<tr>
<td style="text-align:center">11</td>
<td style="text-align:center">0.245</td>
<td style="text-align:center">0.057</td>
<td style="text-align:center">no           </td>
</tr>
<tr>
<td style="text-align:center">12</td>
<td style="text-align:center">0.343</td>
<td style="text-align:center">0.099</td>
<td style="text-align:center">no           </td>
</tr>
<tr>
<td style="text-align:center">13</td>
<td style="text-align:center">0.639</td>
<td style="text-align:center">0.161</td>
<td style="text-align:center">no           </td>
</tr>
<tr>
<td style="text-align:center">14</td>
<td style="text-align:center">0.657</td>
<td style="text-align:center">0.198</td>
<td style="text-align:center">no           </td>
</tr>
<tr>
<td style="text-align:center">15</td>
<td style="text-align:center">0.360</td>
<td style="text-align:center">0.370</td>
<td style="text-align:center">no           </td>
</tr>
<tr>
<td style="text-align:center">16</td>
<td style="text-align:center">0.593</td>
<td style="text-align:center">0.042</td>
<td style="text-align:center">no           </td>
</tr>
<tr>
<td style="text-align:center">17</td>
<td style="text-align:center">0.719</td>
<td style="text-align:center">0.103</td>
<td style="text-align:center">no           </td>
</tr>
</tbody>
</table>
<p>Ans: 西瓜数据集$3.0\alpha$的数据量太小，这里我没用采用，而是使用<a href="https://archive.ics.uci.edu/ml/datasets.html" target="_blank" rel="noopener">UCI数据集</a>的<a href="https://archive.ics.uci.edu/ml/datasets/iris" target="_blank" rel="noopener">Iris数据集</a>作为对率回归的训练和测试集。算法包使用Spark MLlib，代码由Scala实现:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">// data preview</span><br><span class="line">5.1,3.5,1.4,0.2,Iris-setosa</span><br><span class="line">4.9,3.0,1.4,0.2,Iris-setosa</span><br><span class="line">4.7,3.2,1.3,0.2,Iris-setosa</span><br><span class="line">4.6,3.1,1.5,0.2,Iris-setosa</span><br><span class="line">5.0,3.6,1.4,0.2,Iris-setosa</span><br></pre></td></tr></table></figure></p>
<p>Spark MLlib实现对率回归:<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">LogisticRegressionIris</span> </span>&#123;</span><br><span class="line">  <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Iris</span> (<span class="params">features: <span class="type">Vector</span>, classes: <span class="type">Double</span></span>)</span></span><br><span class="line"><span class="class">  <span class="title">def</span> <span class="title">parsingIris</span> (<span class="params">str: <span class="type">String</span></span>)</span>: <span class="type">Iris</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> fields = str.split(<span class="string">","</span>)</span><br><span class="line">    assert(fields.size == <span class="number">5</span>)</span><br><span class="line">    <span class="keyword">val</span> label = fields(<span class="number">4</span>) <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> <span class="string">"Iris-setosa"</span> =&gt; <span class="number">0.0</span></span><br><span class="line">      <span class="keyword">case</span> <span class="string">"Iris-versicolor"</span> =&gt; <span class="number">1.0</span></span><br><span class="line">      <span class="keyword">case</span> <span class="string">"Iris-virginica"</span> =&gt; <span class="number">2.0</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="type">Iris</span>(<span class="type">Vectors</span>.dense(fields(<span class="number">0</span>).toDouble, fields(<span class="number">1</span>).toDouble, fields(<span class="number">2</span>).toDouble, fields(<span class="number">3</span>).toDouble), label)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span> </span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="type">Logger</span>.getLogger(<span class="string">"org"</span>).setLevel(<span class="type">Level</span>.<span class="type">OFF</span>) <span class="comment">// close logger</span></span><br><span class="line">    <span class="comment">/** create spark session */</span></span><br><span class="line">    <span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder().master(<span class="string">"local"</span>).appName(<span class="string">"LogisticRegressionIris"</span>).getOrCreate()</span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line">    <span class="keyword">val</span> iris = spark.read.textFile(<span class="keyword">new</span> <span class="type">ClassPathResource</span>(<span class="string">"iris.txt"</span>).getFile.getAbsolutePath) <span class="comment">// load data and transform to dataframe</span></span><br><span class="line">      .map(parsingIris).toDF().select(<span class="string">"features"</span>, <span class="string">"classes"</span>)</span><br><span class="line">    <span class="keyword">val</span> <span class="type">Array</span>(training, test) = iris.randomSplit(<span class="type">Array</span>(<span class="number">0.8</span>, <span class="number">0.2</span>)) <span class="comment">// 0.8 for training, 0.2 for testing</span></span><br><span class="line">    <span class="keyword">val</span> lr = <span class="keyword">new</span> <span class="type">LogisticRegression</span>() <span class="comment">// logistic regression model</span></span><br><span class="line">      .setMaxIter(<span class="number">100</span>)</span><br><span class="line">      .setRegParam(<span class="number">0.01</span>)</span><br><span class="line">      .setFamily(<span class="string">"multinomial"</span>)</span><br><span class="line">      .setFeaturesCol(<span class="string">"features"</span>)</span><br><span class="line">      .setLabelCol(<span class="string">"classes"</span>)</span><br><span class="line">    <span class="keyword">val</span> model = lr.fit(training) <span class="comment">// fitting dataset</span></span><br><span class="line">    <span class="keyword">val</span> predictions = model.transform(test) <span class="comment">// predict</span></span><br><span class="line">    predictions.select(<span class="string">"classes"</span>, <span class="string">"prediction"</span>).collect().foreach(e =&gt; println(e.get(<span class="number">0</span>) + <span class="string">"\t"</span> + e.get(<span class="number">1</span>)))</span><br><span class="line">    <span class="keyword">val</span> evaluator = <span class="keyword">new</span> <span class="type">RegressionEvaluator</span>().setMetricName(<span class="string">"rmse"</span>).setLabelCol(<span class="string">"classes"</span>).setPredictionCol(<span class="string">"prediction"</span>)</span><br><span class="line">    <span class="keyword">val</span> rmse = evaluator.evaluate(predictions)</span><br><span class="line">    println(<span class="string">s"Root-mean-square error = <span class="subst">$rmse</span>"</span>)</span><br><span class="line">    spark.stop() <span class="comment">// stop spark</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p><strong>3.4</strong> 选择两个UCI数据集，比较10折交叉验证法和留一法所估计出的对率回归的错误率。<br>Ans: 这里只选用一个UCI数据集Iris进行实验。留一法思想在3.3题中已经体现(80% for training, 20% for testing)，这里只做10折交叉验证:<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">object LogisticRegressionIrisCV &#123;</span><br><span class="line">  <span class="function"><span class="keyword">case</span> class <span class="title">Iris</span> <span class="params">(features: Vector, classes: Double)</span></span></span><br><span class="line"><span class="function">  def <span class="title">parsingIris</span> <span class="params">(str: String)</span>: Iris </span>= &#123;</span><br><span class="line">    val fields = str.split(<span class="string">","</span>)</span><br><span class="line">    <span class="keyword">assert</span>(fields.size == <span class="number">5</span>)</span><br><span class="line">    val label = fields(<span class="number">4</span>) match &#123;</span><br><span class="line">      <span class="keyword">case</span> <span class="string">"Iris-setosa"</span> =&gt; <span class="number">0.0</span></span><br><span class="line">      <span class="keyword">case</span> <span class="string">"Iris-versicolor"</span> =&gt; <span class="number">1.0</span></span><br><span class="line">      <span class="keyword">case</span> <span class="string">"Iris-virginica"</span> =&gt; <span class="number">2.0</span></span><br><span class="line">    &#125;</span><br><span class="line">    Iris(Vectors.dense(fields(<span class="number">0</span>).toDouble, fields(<span class="number">1</span>).toDouble, fields(<span class="number">2</span>).toDouble, fields(<span class="number">3</span>).toDouble), label)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function">def <span class="title">main</span> <span class="params">(args: Array[String])</span>: Unit </span>= &#123;</span><br><span class="line">    Logger.getLogger(<span class="string">"org"</span>).setLevel(Level.OFF) <span class="comment">// close logger</span></span><br><span class="line">    <span class="comment">/** create spark session */</span></span><br><span class="line">    val spark = SparkSession.builder().master(<span class="string">"local"</span>).appName(<span class="string">"LogisticRegressionIris"</span>).getOrCreate()</span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line">    <span class="comment">/** load data and transform to dataframe */</span></span><br><span class="line">    val iris = spark.read.textFile(<span class="keyword">new</span> ClassPathResource(<span class="string">"iris.txt"</span>).getFile.getAbsolutePath)</span><br><span class="line">      .map(parsingIris).toDF().select(<span class="string">"features"</span>, <span class="string">"classes"</span>)</span><br><span class="line">    <span class="function">val <span class="title">Array</span><span class="params">(training, test)</span> </span>= iris.randomSplit(Array(<span class="number">0.9</span>, <span class="number">0.1</span>)) <span class="comment">// 90% for training, 10% for testing</span></span><br><span class="line">    val lr = <span class="keyword">new</span> LogisticRegression() <span class="comment">// build logistic regression model</span></span><br><span class="line">      .setMaxIter(<span class="number">20</span>)</span><br><span class="line">      .setFamily(<span class="string">"multinomial"</span>)</span><br><span class="line">      .setFeaturesCol(<span class="string">"features"</span>)</span><br><span class="line">      .setLabelCol(<span class="string">"classes"</span>)</span><br><span class="line">    val pipeline = <span class="keyword">new</span> Pipeline().setStages(Array(lr)) <span class="comment">// build pipeline</span></span><br><span class="line">    val paramGrid = <span class="keyword">new</span> ParamGridBuilder() <span class="comment">// set parameters map</span></span><br><span class="line">      .addGrid(lr.regParam, Array(<span class="number">0.01</span>, <span class="number">0.02</span>, <span class="number">0.05</span>, <span class="number">0.1</span>, <span class="number">0.2</span>))</span><br><span class="line">      .build()</span><br><span class="line">    val cv = <span class="keyword">new</span> CrossValidator() <span class="comment">// build cross validator for logistic regression</span></span><br><span class="line">      .setEstimator(pipeline)</span><br><span class="line">      .setEvaluator(<span class="keyword">new</span> RegressionEvaluator()</span><br><span class="line">        .setLabelCol(<span class="string">"classes"</span>)</span><br><span class="line">        .setPredictionCol(<span class="string">"prediction"</span>)</span><br><span class="line">        .setMetricName(<span class="string">"rmse"</span>))</span><br><span class="line">      .setEstimatorParamMaps(paramGrid)</span><br><span class="line">      .setNumFolds(<span class="number">10</span>)</span><br><span class="line">    val cvModel = cv.fit(training) <span class="comment">// training</span></span><br><span class="line">    val predictions = cvModel.transform(test) <span class="comment">// prediction</span></span><br><span class="line">    val evaluator = <span class="keyword">new</span> RegressionEvaluator() <span class="comment">// evaluating</span></span><br><span class="line">      .setMetricName(<span class="string">"rmse"</span>)</span><br><span class="line">      .setLabelCol(<span class="string">"classes"</span>)</span><br><span class="line">      .setPredictionCol(<span class="string">"prediction"</span>)</span><br><span class="line">    val rmse = evaluator.evaluate(predictions)</span><br><span class="line">    println(s<span class="string">"Root-mean-square error = $rmse"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p><strong>3.5</strong> 编程实现线性判别分析，并给出西瓜数据集$3.0\alpha$上的结果。<br>略</p>
<p><strong>3.6</strong> 线性判别分析仅在线性可分的数据上能获得理想结果，试设计一个改进的方法能使其较好的应用于非线性可分数据。<br>Ans: 在当前维度线性不可分，可以使用适当的映射方法，使其在更高一维上可分，典型的方法有$KLDA$，可以很好的划分数据。</p>
<p><strong>3.7</strong> 令码长为9，类别数位4，试给出海明距离意义下理论最优的ECOC二元码并证明之。<br>Ans: 对于ECOC二元码，当码长为$2^{n}$时，至少可以使$2n$个类别达到最优间隔，他们的海明距离为$2^{n-1}$，比如长度为8时，可以的序列为</p>
<table>
<thead>
<tr>
<th style="text-align:center">Column 1</th>
<th style="text-align:center">Column 2</th>
<th style="text-align:center">Column 3</th>
<th style="text-align:center">Column 4</th>
<th style="text-align:center">Column 5</th>
<th style="text-align:center">Column 6</th>
<th style="text-align:center">Column 7</th>
<th style="text-align:center">Column 8</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">1</td>
<td style="text-align:center">1</td>
<td style="text-align:center">1</td>
<td style="text-align:center">-1</td>
<td style="text-align:center">-1</td>
<td style="text-align:center">-1</td>
<td style="text-align:center">-1</td>
</tr>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">1</td>
<td style="text-align:center">-1</td>
<td style="text-align:center">-1</td>
<td style="text-align:center">1</td>
<td style="text-align:center">1</td>
<td style="text-align:center">-1</td>
<td style="text-align:center">-1</td>
</tr>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">-1</td>
<td style="text-align:center">1</td>
<td style="text-align:center">-1</td>
<td style="text-align:center">1</td>
<td style="text-align:center">-1</td>
<td style="text-align:center">1</td>
<td style="text-align:center">-1</td>
</tr>
<tr>
<td style="text-align:center">-1</td>
<td style="text-align:center">-1</td>
<td style="text-align:center">-1</td>
<td style="text-align:center">-1</td>
<td style="text-align:center">1</td>
<td style="text-align:center">1</td>
<td style="text-align:center">1</td>
<td style="text-align:center">1</td>
</tr>
<tr>
<td style="text-align:center">-1</td>
<td style="text-align:center">-1</td>
<td style="text-align:center">1</td>
<td style="text-align:center">1</td>
<td style="text-align:center">-1</td>
<td style="text-align:center">-1</td>
<td style="text-align:center">1</td>
<td style="text-align:center">1</td>
</tr>
<tr>
<td style="text-align:center">-1</td>
<td style="text-align:center">1</td>
<td style="text-align:center">-1</td>
<td style="text-align:center">1</td>
<td style="text-align:center">-1</td>
<td style="text-align:center">1</td>
<td style="text-align:center">-1</td>
<td style="text-align:center">1</td>
</tr>
</tbody>
</table>
<p>其中4,5,6行是对1,2,3行的取反。若分类数为4，一共可能的分类器共有$2^{4}−2$种(排除了全1和全0)，在码长为8的最优分类器后添加一列没有出现过的分类器，就是码长为9的最优分类器。</p>
<p><strong>3.8</strong> ECOC编码能起到理想纠错作用的重要条件是：在每一位编码上出错的概率相当且独立。试分析多分类任务经ECOC编码后产生的二分类器满足该条件的可能性及由此产生的影响。<br>Ans: 理论上的ECOC码能理想纠错的重要条件是每个码位出错的概率相当，因为如果某个码位的错误率很高，会导致这位始终保持相同的结果，不再有分类作用，这就相当于全0或者全 1的分类器，这点和NFL的前提很像。但由于事实的样本并不一定满足这些条件，所以书中提到了有多种问题依赖的ECOC被提出。</p>
<p><strong>3.9</strong> 使用OvR和MvM将多分类任务分解为二分类任务求解时，试描述为何无需专门针对类别不平衡性进行处理。<br>Ans: 对于OvR，MvM来说，由于对每个类进行了相同的处理，其拆解出的二分类任务中类别不平衡的影响会相互抵消，因此通常不需要专门处理。以ECOC编码为例，每个生成的二分类器会将所有样本分成较为均衡的二类，使类别不平衡的影响减小。当然拆解后仍然可能出现明显的类别不平衡现象，比如一个超级大类和一群小类。</p>
<p><strong>3.10</strong> 试推到出多分类代价敏感学习(仅考虑基于类别的误分类代价)使用“再缩放”能获得理论最优解的条件。<br>Ans: 仅考虑类别分类的误分类代价，那么就默认正确分类的代价为0。于是得到分类表，(假设为3类)</p>
<table>
<thead>
<tr>
<th style="text-align:center">Column 1</th>
<th style="text-align:center">Column 2</th>
<th style="text-align:center">Column 3</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">0</td>
<td style="text-align:center">$c_{12}$</td>
<td style="text-align:center">$c_{13}$</td>
</tr>
<tr>
<td style="text-align:center">$c_{21}$</td>
<td style="text-align:center">0</td>
<td style="text-align:center">$c_{23}$</td>
</tr>
<tr>
<td style="text-align:center">$c_{31}$</td>
<td style="text-align:center">$c_{32}$</td>
<td style="text-align:center">0        </td>
</tr>
</tbody>
</table>
<p>对于二分类而言，将样本为正例的后验概率设为是p,那么预测为正的代价是$(1-p)\centerdot\mathcal{c}_{12}$。预测为负的代价是$p\centerdot\mathcal{c}_{21}$。当$(1-p)\centerdot\mathcal{c}_{12}\leq p\centerdot\mathcal{c}_{21}$样本就会被预测成正例，因为他的代价更小。当不等式取等号时，得到了最优划分，这个阀值$p_{r}=\frac{\mathcal{c}_{12}}{\mathcal{c}_{12}+\mathcal{c}_{21}}$，这表示正例与反例的划分比例应该是初始的$\frac{\mathcal{c}_{12}}{\mathcal{c}_{21}}$倍。假设分类器预设的阀值是$p_{0}$，不考虑代价敏感时，当$\frac{y}{1-y}&gt;\frac{p_{0}}{1-p_{0}}$时取正例。当考虑代价敏感，则应该是$\frac{y}{1-y}&gt;\frac{p_{r}}{1-p_{r}}\centerdot\frac{p_{0}}{1-p_{0}}=\frac{\mathcal{c}_{12}}{\mathcal{c}_{21}}\centerdot\frac{p_{0}}{1-p_{0}}$。<br>推广到对于多分类，任意两类的最优再缩放系数$t_{ij}=\frac{c_{ij}}{c_{ji}}$，然而所有类别的最优缩放系数并不一定能同时满足。当代价表满足下面条件时，能通过再缩放得到最优解。设$t_{ij}=\frac{w_{i}}{w_{j}}$，则$\frac{w_{i}}{w_{j}}=\frac{c_{ij}}{c_{ji}}$对所有$i,j$成立，假设有k类，共$C_{2}^{k}$个等式，此时代价表中$k(k−1)$个数，最少只要知道$2(k−1)$就能推出整张表。</p>
<h1 id="基于神经网络的逻辑回归分类"><a href="#基于神经网络的逻辑回归分类" class="headerlink" title="基于神经网络的逻辑回归分类"></a>基于神经网络的逻辑回归分类</h1><h2 id="DL4J实现"><a href="#DL4J实现" class="headerlink" title="DL4J实现"></a>DL4J实现</h2><p>以下是使用DeepLearning4J实现的基于神经网络的逻辑回归分类算法。<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> ZHANG HAO</span></span><br><span class="line"><span class="comment"> * link: https://www.kaggle.com/uciml/iris</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">IrisClassification</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> Logger log = LoggerFactory.getLogger(IrisClassification.class);</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> numLinesToSkip = <span class="number">0</span>;</span><br><span class="line">        String delimiter = <span class="string">","</span>;</span><br><span class="line">        RecordReader recordReader = <span class="keyword">new</span> CSVRecordReader(numLinesToSkip,delimiter);</span><br><span class="line">        recordReader.initialize(<span class="keyword">new</span> FileSplit(<span class="keyword">new</span> ClassPathResource(<span class="string">"iris.txt"</span>).getFile()));</span><br><span class="line">        <span class="keyword">int</span> labelIndex = <span class="number">4</span>;</span><br><span class="line">        <span class="keyword">int</span> numClasses = <span class="number">3</span>;</span><br><span class="line">        <span class="keyword">int</span> batchSize = <span class="number">150</span>;</span><br><span class="line">        DataSetIterator iterator = <span class="keyword">new</span> RecordReaderDataSetIterator(recordReader,batchSize,labelIndex,numClasses);</span><br><span class="line">        DataSet allData = iterator.next();</span><br><span class="line">        allData.shuffle();</span><br><span class="line">        SplitTestAndTrain testAndTrain = allData.splitTestAndTrain(<span class="number">0.65</span>);</span><br><span class="line">        DataSet trainingData = testAndTrain.getTrain();</span><br><span class="line">        DataSet testData = testAndTrain.getTest();</span><br><span class="line">        <span class="comment">//We need to normalize our data. We'll use NormalizeStandardize (which gives us mean 0, unit variance):</span></span><br><span class="line">        DataNormalization normalizer = <span class="keyword">new</span> NormalizerStandardize();</span><br><span class="line">        normalizer.fit(trainingData);<span class="comment">//Collect the statistics (mean/stdev) from the training data. This does not modify the input data</span></span><br><span class="line">        normalizer.transform(trainingData);     <span class="comment">//Apply normalization to the training data</span></span><br><span class="line">        normalizer.transform(testData);         <span class="comment">//Apply normalization to the test data. This is using statistics calculated from the *training* set</span></span><br><span class="line">        <span class="keyword">final</span> <span class="keyword">int</span> numInputs = <span class="number">4</span>;</span><br><span class="line">        <span class="keyword">int</span> outputNum = <span class="number">3</span>;</span><br><span class="line">        <span class="keyword">int</span> iterations = <span class="number">1000</span>;</span><br><span class="line">        <span class="keyword">long</span> seed = <span class="number">6</span>;</span><br><span class="line">        <span class="keyword">int</span> epochs = <span class="number">100</span>;</span><br><span class="line">        log.info(<span class="string">"Build model...."</span>);</span><br><span class="line">        MultiLayerConfiguration conf = <span class="keyword">new</span> NeuralNetConfiguration.Builder()</span><br><span class="line">                .seed(seed)</span><br><span class="line">                .iterations(iterations)</span><br><span class="line">                .weightInit(WeightInit.XAVIER)</span><br><span class="line">                .learningRate(<span class="number">0.1</span>)</span><br><span class="line">                .regularization(<span class="keyword">true</span>)</span><br><span class="line">                .l2(<span class="number">1e-4</span>)</span><br><span class="line">                .list()</span><br><span class="line">                .layer(<span class="number">0</span>, <span class="keyword">new</span> DenseLayer.Builder().nIn(numInputs).nOut(<span class="number">20</span>).activation(Activation.TANH).build())</span><br><span class="line">                .layer(<span class="number">1</span>, <span class="keyword">new</span> DenseLayer.Builder().nOut(<span class="number">10</span>).activation(Activation.TANH).build())</span><br><span class="line">                .layer(<span class="number">2</span>, <span class="keyword">new</span> OutputLayer.Builder().lossFunction(LossFunctions.LossFunction.NEGATIVELOGLIKELIHOOD).activation(Activation.SOFTMAX).nOut(outputNum).build())</span><br><span class="line">                .backprop(<span class="keyword">true</span>)</span><br><span class="line">                .pretrain(<span class="keyword">false</span>)</span><br><span class="line">                .build();</span><br><span class="line">        <span class="comment">//run the model</span></span><br><span class="line">        MultiLayerNetwork model = <span class="keyword">new</span> MultiLayerNetwork(conf);</span><br><span class="line">        model.init();</span><br><span class="line">        model.setListeners(<span class="keyword">new</span> ScoreIterationListener(<span class="number">100</span>));</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> epoch = <span class="number">0</span>; epoch &lt; epochs; epoch++) &#123; model.fit(trainingData); &#125;</span><br><span class="line">        <span class="comment">//evaluate the model on the test set</span></span><br><span class="line">        Evaluation eval = <span class="keyword">new</span> Evaluation(numClasses);</span><br><span class="line">        INDArray output = model.output(testData.getFeatureMatrix());</span><br><span class="line">        eval.eval(testData.getLabels(), output);</span><br><span class="line">        log.info(eval.stats());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">==========================Scores========================================</span></span><br><span class="line"><span class="comment"> Accuracy:        0.9811</span></span><br><span class="line"><span class="comment"> Precision:       0.9815</span></span><br><span class="line"><span class="comment"> Recall:          0.9792</span></span><br><span class="line"><span class="comment"> F1 Score:        0.9803</span></span><br><span class="line"><span class="comment">========================================================================</span></span><br><span class="line"><span class="comment"> */</span></span><br></pre></td></tr></table></figure></p>
<p>详细代码请浏览我的GitHub repository: <a href="https://github.com/IsaacChanghau/NeuralNetworksLite/blob/master/src/main/java/com/isaac/dl4j/IrisClassification.java" target="_blank" rel="noopener">[link]</a>。</p>
<h2 id="Pure-Java实现"><a href="#Pure-Java实现" class="headerlink" title="Pure-Java实现"></a>Pure-Java实现</h2><p>以下是一个Java demo实现利用逻辑机率回归(logistic regression)处理分类问题(3个类别)。这3个类别的数据是我随机生成的，每个类别的数据均服从特定的uu和σσ的高斯分布。首先定义三组数据，每组数组400个用于训练，60个用于测试，每个输入数据均为2维的向量。输出结果为3维向量: $(1,0,0)$表示class-1，$(0,1,0)$表示class-2，$(0,0,1)$表示class-3。每类数据均服从一个特定的高斯分布:<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">final</span> <span class="keyword">int</span> patterns = <span class="number">3</span>; <span class="comment">// number of classes</span></span><br><span class="line"><span class="keyword">final</span> <span class="keyword">int</span> train_N = <span class="number">400</span> * patterns;</span><br><span class="line"><span class="keyword">final</span> <span class="keyword">int</span> test_N = <span class="number">60</span> * patterns;</span><br><span class="line"><span class="keyword">final</span> <span class="keyword">int</span> nIn = <span class="number">2</span>; <span class="comment">// input dim.</span></span><br><span class="line"><span class="keyword">final</span> <span class="keyword">int</span> nOut = patterns; <span class="comment">// output dim</span></span><br><span class="line"><span class="keyword">double</span>[][] train_X = <span class="keyword">new</span> <span class="keyword">double</span>[train_N][nIn]; <span class="comment">// training matrix --&gt; X</span></span><br><span class="line"><span class="keyword">int</span>[][] train_T = <span class="keyword">new</span> <span class="keyword">int</span>[train_N][nOut]; <span class="comment">// training label --&gt; y</span></span><br><span class="line"><span class="keyword">double</span>[][] test_X = <span class="keyword">new</span> <span class="keyword">double</span>[test_N][nIn];</span><br><span class="line">Integer[][] test_T = <span class="keyword">new</span> Integer[test_N][nOut];</span><br><span class="line">Integer[][] predicted_T = <span class="keyword">new</span> Integer[test_N][nOut];</span><br><span class="line">...</span><br><span class="line"><span class="comment">// Training data for demo</span></span><br><span class="line"><span class="comment">// class 1 : x1 ~ N(-2.0, 1.0), y1 ~ N(+2.0, 1.0)</span></span><br><span class="line"><span class="comment">// class 2 : x2 ~ N(+2.0, 1.0), y2 ~ N(-2.0, 1.0)</span></span><br><span class="line"><span class="comment">// class 3 : x3 ~ N( 0.0, 1.0), y3 ~ N( 0.0, 1.0)</span></span><br><span class="line">GaussianDistribution g1 = <span class="keyword">new</span> GaussianDistribution(-<span class="number">2.0</span>, <span class="number">1.0</span>, rng);</span><br><span class="line">GaussianDistribution g2 = <span class="keyword">new</span> GaussianDistribution(<span class="number">2.0</span>, <span class="number">1.0</span>, rng);</span><br><span class="line">GaussianDistribution g3 = <span class="keyword">new</span> GaussianDistribution(<span class="number">0.0</span>, <span class="number">1.0</span>, rng);</span><br><span class="line"><span class="comment">// data set in class 1</span></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; train_N / patterns - <span class="number">1</span>; i++) &#123;</span><br><span class="line">    train_X[i][<span class="number">0</span>] = g1.random();</span><br><span class="line">    train_X[i][<span class="number">1</span>] = g2.random();</span><br><span class="line">    train_T[i] = <span class="keyword">new</span> <span class="keyword">int</span>[] &#123; <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span> &#125;;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; test_N / patterns - <span class="number">1</span>; i++) &#123;</span><br><span class="line">    test_X[i][<span class="number">0</span>] = g1.random();</span><br><span class="line">    test_X[i][<span class="number">1</span>] = g2.random();</span><br><span class="line">    test_T[i] = <span class="keyword">new</span> Integer[] &#123; <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span> &#125;;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// data set in class 2</span></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = train_N / patterns - <span class="number">1</span>; i &lt; train_N / patterns * <span class="number">2</span> - <span class="number">1</span>; i++) &#123; ... &#125;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = test_N / patterns - <span class="number">1</span>; i &lt; test_N / patterns * <span class="number">2</span> - <span class="number">1</span>; i++) &#123; ... &#125;</span><br><span class="line">...</span><br></pre></td></tr></table></figure></p>
<p>其中上面代码使用的GaussianDistribution定义如下:<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">GaussianDistribution</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">double</span> mean;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">double</span> var;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> Random rng;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">GaussianDistribution</span><span class="params">(<span class="keyword">double</span> mean, <span class="keyword">double</span> var, Random rng)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (var &lt; <span class="number">0.0</span>) &#123; <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(<span class="string">"Variance must be non-negative value."</span>); &#125;</span><br><span class="line">        <span class="keyword">this</span>.mean = mean;</span><br><span class="line">        <span class="keyword">this</span>.var = var;</span><br><span class="line">        <span class="keyword">if</span> (rng == <span class="keyword">null</span>) &#123; rng = <span class="keyword">new</span> Random(); &#125;</span><br><span class="line">        <span class="keyword">this</span>.rng = rng;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">double</span> <span class="title">random</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">double</span> r = <span class="number">0.0</span>;</span><br><span class="line">        <span class="keyword">while</span> (r == <span class="number">0.0</span>) &#123; r = rng.nextDouble(); &#125;</span><br><span class="line">        <span class="keyword">double</span> c = Math.sqrt(-<span class="number">2.0</span> * Math.log(r));   </span><br><span class="line">        <span class="keyword">if</span> (rng.nextDouble() &lt; <span class="number">0.5</span>) &#123; <span class="keyword">return</span> c * Math.sin(<span class="number">2.0</span> * Math.PI * rng.nextDouble()) * var + mean; &#125; </span><br><span class="line">        <span class="keyword">return</span> c * Math.cos(<span class="number">2.0</span> * Math.PI * rng.nextDouble()) * var + mean;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>回归的数值优化使用了“随机梯度下降法” (<a href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent" target="_blank" rel="noopener">Stochastic Gradient Descent</a>)，为了降低每次循环的梯度的计算复杂度(特别是在数据集很大的时候)，这里将数据拆分成若干batch，每个batch的大小为50。关于mini-batch的trade-off请参照:<a href="https://www.quora.com/Intuitively-how-does-mini-batch-size-affect-the-performance-of-stochastic-gradient-descent" target="_blank" rel="noopener">[link]</a>。代码中的具体做法是:<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> minibatchSize = <span class="number">50</span>; <span class="comment">// number of data in each minibatch</span></span><br><span class="line"><span class="keyword">int</span> minibatch_N = train_N / minibatchSize; <span class="comment">// number of minibatches</span></span><br><span class="line"><span class="keyword">double</span>[][][] train_X_minibatch = <span class="keyword">new</span> <span class="keyword">double</span>[minibatch_N][minibatchSize][nIn]; <span class="comment">// minibatches of train data</span></span><br><span class="line"><span class="keyword">int</span>[][][] train_T_minibatch = <span class="keyword">new</span> <span class="keyword">int</span>[minibatch_N][minibatchSize][nOut]; <span class="comment">// minibatches of output data for training</span></span><br><span class="line">List&lt;Integer&gt; minibatchIndex = <span class="keyword">new</span> ArrayList&lt;Integer&gt;(); <span class="comment">// data index for minibatch to apply SGD</span></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; train_N; i++) &#123; minibatchIndex.add(i); &#125;</span><br><span class="line">Collections.shuffle(minibatchIndex, rng); <span class="comment">// shuffle data index for SGD</span></span><br><span class="line"><span class="comment">// create minibatches with training data</span></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; minibatch_N; i++) &#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; minibatchSize; j++) &#123;</span><br><span class="line">        train_X_minibatch[i][j] = train_X[minibatchIndex.get(i * minibatchSize + j)];</span><br><span class="line">        train_T_minibatch[i][j] = train_T[minibatchIndex.get(i * minibatchSize + j)];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>前面介绍的逻辑机率回归使用的是Sigmoid函数，对于多分类任务，我们使用Softmax函数，两者区别和关系请参照: <a href="https://stats.stackexchange.com/questions/233658/softmax-vs-sigmoid-function-in-logistic-classifier" target="_blank" rel="noopener">[link]</a>。使用Sigmoid能得到每个类别的一个值，而使用softmax能得到每个类的概率，其Java实现如下<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">ActivationFunction</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">double</span> <span class="title">sigmoid</span><span class="params">(<span class="keyword">double</span> x)</span> </span>&#123; <span class="keyword">return</span> <span class="number">1.0</span> / (<span class="number">1.0</span> + Math.pow(Math.E, -x)); &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">double</span> <span class="title">dsigmoid</span><span class="params">(<span class="keyword">double</span> y)</span> </span>&#123; <span class="keyword">return</span> y * (<span class="number">1.0</span> - y); &#125;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">double</span>[] softmax(<span class="keyword">double</span>[] x, <span class="keyword">int</span> n) &#123;</span><br><span class="line">        <span class="keyword">double</span>[] y = <span class="keyword">new</span> <span class="keyword">double</span>[n];</span><br><span class="line">        <span class="keyword">double</span> max = <span class="number">0.0</span>;</span><br><span class="line">        <span class="keyword">double</span> sum = <span class="number">0.0</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n; i++) &#123; <span class="keyword">if</span> (max &lt; x[i]) &#123; max = x[i]; &#125; &#125; <span class="comment">// prevent overflow</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n; i++) &#123; y[i] = Math.exp(x[i] - max);    sum += y[i]; &#125;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n; i++) &#123; y[i] /= sum; &#125;</span><br><span class="line">        <span class="keyword">return</span> y;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>接下来，逻辑机率回归函数用于学习特征向量，数值优化使用基于Mini-batch的随机梯度下降法。<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LogisticRegression</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">int</span> nIn;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">int</span> nOut;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">double</span>[][] W;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">double</span>[] b;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">LogisticRegression</span><span class="params">(<span class="keyword">int</span> nIn, <span class="keyword">int</span> nOut)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.nIn = nIn;</span><br><span class="line">        <span class="keyword">this</span>.nOut = nOut;</span><br><span class="line">        W = <span class="keyword">new</span> <span class="keyword">double</span>[nOut][nIn];</span><br><span class="line">        b = <span class="keyword">new</span> <span class="keyword">double</span>[nOut];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">double</span>[][] train(<span class="keyword">double</span>[][] X, <span class="keyword">int</span> T[][], <span class="keyword">int</span> minibatchSize, <span class="keyword">double</span> learningRate) &#123;</span><br><span class="line">        <span class="keyword">double</span>[][] grad_W = <span class="keyword">new</span> <span class="keyword">double</span>[nOut][nIn];</span><br><span class="line">        <span class="keyword">double</span>[] grad_b = <span class="keyword">new</span> <span class="keyword">double</span>[nOut];</span><br><span class="line">        <span class="keyword">double</span>[][] dY = <span class="keyword">new</span> <span class="keyword">double</span>[minibatchSize][nOut];</span><br><span class="line">        <span class="comment">// train with SGD</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> n = <span class="number">0</span>; n &lt; minibatchSize; n++) &#123; <span class="comment">// 1. calculate gradient of W, b</span></span><br><span class="line">            <span class="keyword">double</span>[] predicted_Y_ = output(X[n]);</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; nOut; j++) &#123;</span><br><span class="line">                dY[n][j] = predicted_Y_[j] - T[n][j];</span><br><span class="line">                <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; nIn; i++) &#123; grad_W[j][i] += dY[n][j] * X[n][i]; &#125;</span><br><span class="line">                grad_b[j] += dY[n][j];</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; nOut; j++) &#123; <span class="comment">// 2. update params</span></span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; nIn; i++) &#123; W[j][i] -= learningRate * grad_W[j][i] / minibatchSize; &#125;</span><br><span class="line">            b[j] -= learningRate * grad_b[j] / minibatchSize;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> dY;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">double</span>[] output(<span class="keyword">double</span>[] x) &#123;</span><br><span class="line">        <span class="keyword">double</span>[] preActivation = <span class="keyword">new</span> <span class="keyword">double</span>[nOut];</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; nOut; j++) &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; nIn; i++) &#123; preActivation[j] += W[j][i] * x[i]; &#125;</span><br><span class="line">            preActivation[j] += b[j]; <span class="comment">// linear output</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> ActivationFunction.softmax(preActivation, nOut);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">public</span> Integer[] predict(<span class="keyword">double</span>[] x) &#123;</span><br><span class="line">        <span class="keyword">double</span>[] y = output(x); <span class="comment">// activate input data through learned networks</span></span><br><span class="line">        Integer[] t = <span class="keyword">new</span> Integer[nOut]; <span class="comment">// output is the probability, so cast it to label</span></span><br><span class="line">        <span class="keyword">int</span> argmax = -<span class="number">1</span>;</span><br><span class="line">        <span class="keyword">double</span> max = <span class="number">0.0</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; nOut; i++) &#123;</span><br><span class="line">            <span class="keyword">if</span> (max &lt; y[i]) &#123; max = y[i]; argmax = i; &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; nOut; i++) &#123;</span><br><span class="line">            <span class="keyword">if</span> (i == argmax) &#123; t[i] = <span class="number">1</span>; &#125; </span><br><span class="line">            <span class="keyword">else</span> &#123; t[i] = <span class="number">0</span>; &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> t;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>构建完logistic regression后就可将数据读入进行训练:<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> epochs = <span class="number">2000</span>; <span class="comment">// number of epoches</span></span><br><span class="line"><span class="keyword">double</span> learningRate = <span class="number">0.2</span>; <span class="comment">// initial learning rate</span></span><br><span class="line"><span class="comment">// Build Logistic Regression model</span></span><br><span class="line">LogisticRegression classifier = <span class="keyword">new</span> LogisticRegression(nIn, nOut); <span class="comment">// construct logistic regression</span></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> epoch = <span class="number">0</span>; epoch &lt; epochs; epoch++) &#123; <span class="comment">// train</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> batch = <span class="number">0</span>; batch &lt; minibatch_N; batch++) classifier.train(train_X_minibatch[batch], train_T_minibatch[batch], minibatchSize, learningRate);</span><br><span class="line">    learningRate *= <span class="number">0.95</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>训练结束后进行测试和评估:<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; test_N; i++) &#123; predicted_T[i] = classifier.predict(test_X[i]); &#125;</span><br><span class="line"><span class="keyword">int</span>[][] confusionMatrix = <span class="keyword">new</span> <span class="keyword">int</span>[patterns][patterns]; <span class="comment">// define confusion matrix</span></span><br><span class="line"><span class="keyword">double</span> accuracy = <span class="number">0.0</span>;</span><br><span class="line"><span class="keyword">double</span>[] precision = <span class="keyword">new</span> <span class="keyword">double</span>[patterns];</span><br><span class="line"><span class="keyword">double</span>[] recall = <span class="keyword">new</span> <span class="keyword">double</span>[patterns];</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; test_N; i++) &#123;</span><br><span class="line">    <span class="keyword">int</span> predicted_ = Arrays.asList(predicted_T[i]).indexOf(<span class="number">1</span>);</span><br><span class="line">    <span class="keyword">int</span> actual_ = Arrays.asList(test_T[i]).indexOf(<span class="number">1</span>);</span><br><span class="line">    confusionMatrix[actual_][predicted_] += <span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; patterns; i++) &#123;</span><br><span class="line">    <span class="keyword">double</span> col_ = <span class="number">0</span>.;</span><br><span class="line">    <span class="keyword">double</span> row_ = <span class="number">0</span>.;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; patterns; j++) &#123;</span><br><span class="line">        <span class="keyword">if</span> (i == j) &#123;</span><br><span class="line">            accuracy += confusionMatrix[i][j];</span><br><span class="line">            precision[i] += confusionMatrix[j][i];</span><br><span class="line">            recall[i] += confusionMatrix[i][j];</span><br><span class="line">        &#125;</span><br><span class="line">        col_ += confusionMatrix[j][i];</span><br><span class="line">        row_ += confusionMatrix[i][j];</span><br><span class="line">    &#125;</span><br><span class="line">    precision[i] /= col_;</span><br><span class="line">    recall[i] /= row_;</span><br><span class="line">&#125;</span><br><span class="line">accuracy /= test_N;</span><br></pre></td></tr></table></figure></p>
<p>最终得到评估结果为<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Logistic Regression model evaluation</span><br><span class="line">------------------------------------</span><br><span class="line">Accuracy: 91.1</span><br><span class="line">Precision:</span><br><span class="line">class 1: 95.0</span><br><span class="line">class 2: 91.5</span><br><span class="line">class 3: 86.9</span><br><span class="line">Recall:</span><br><span class="line">class 1: 96.6</span><br><span class="line">class 2: 90.0</span><br><span class="line">class 3: 86.9</span><br></pre></td></tr></table></figure></p>
<p>以上是Logistic Regression的Java实现，该代码仅仅实现了其功能，并没有过多的考虑优化使其准确度达到更高，由于测试数据相对简单，稍作修改很容易就达到更高的准确度。详细代码请参照我的GitHub Repository: <a href="https://github.com/IsaacChanghau/NeuralNetworksLite/blob/master/src/main/java/com/isaac/nns/examples/basic/LogisticRegressionExample.java" target="_blank" rel="noopener">Logistic Regression</a></p>

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        
  <ul class="post-copyright">
    <li class="post-copyright-author">
      <strong>Author:</strong>
      Isaac Changhau
    </li>
    <li class="post-copyright-link">
      <strong>Link:</strong>
      <a href="https://isaacchanghau.github.io/2017/05/04/机器学习-周志华-学习笔记-2/" title="机器学习(周志华)--学习笔记(二) 线性回归(Linear Regression)">https://isaacchanghau.github.io/2017/05/04/机器学习-周志华-学习笔记-2/</a>
    </li>
    <li class="post-copyright-license">
      <strong>Notice: </strong>
      All articles are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/3.0/" rel="external nofollow" target="_blank">CC BY-NC-SA 3.0</a> unless stating additionally. Contact me via email for questions or discussion.
    </li>
  </ul>


      
    </div>

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/java/" rel="tag"># java</a>
          
            <a href="/tags/machine-learning/" rel="tag"># machine learning</a>
          
            <a href="/tags/deeplearning4j/" rel="tag"># deeplearning4j</a>
          
            <a href="/tags/spark/" rel="tag"># spark</a>
          
            <a href="/tags/scala/" rel="tag"># scala</a>
          
            <a href="/tags/linear-regression/" rel="tag"># linear regression</a>
          
            <a href="/tags/logistic-regression/" rel="tag"># logistic regression</a>
          
            <a href="/tags/linear-discriminant-analysis/" rel="tag"># linear discriminant analysis</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2017/04/28/机器学习-周志华-学习笔记-1/" rel="next" title="机器学习(周志华)--学习笔记(一) 模型评估与选择(Model Evaluation and Selection)">
                <i class="fa fa-chevron-left"></i> 机器学习(周志华)--学习笔记(一) 模型评估与选择(Model Evaluation and Selection)
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2017/05/13/Word2Vec/" rel="prev" title="Word2Vec Summary -- Mathematical Principles and Java Implementation">
                Word2Vec Summary -- Mathematical Principles and Java Implementation <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            Overview
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.gif"
               alt="Isaac Changhau" />
          <p class="site-author-name" itemprop="name">Isaac Changhau</p>
           
              <p class="site-description motion-element" itemprop="description">Stay Hungry, Stay Foolish</p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives/">
                <span class="site-state-item-count">43</span>
                <span class="site-state-item-name">posts</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/categories/index.html">
                <span class="site-state-item-count">8</span>
                <span class="site-state-item-name">categories</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">40</span>
                <span class="site-state-item-name">tags</span>
              </a>
            </div>
          

        </nav>

        
          <div class="feed-link motion-element">
            <a href="/atom.xml" rel="alternate">
              <i class="fa fa-rss"></i>
              RSS
            </a>
          </div>
        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="mailto:isaac.changhau@gmail.com" target="_blank" title="Email">
                  
                    <i class="fa fa-fw fa-envelope"></i>
                  
                  Email
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://www.linkedin.com/in/isaac-changhau" target="_blank" title="Linkedin">
                  
                    <i class="fa fa-fw fa-linkedin"></i>
                  
                  Linkedin
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://github.com/IsaacChanghau" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                  GitHub
                </a>
              </span>
            
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#线性模型"><span class="nav-number">1.</span> <span class="nav-text">线性模型</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#线性回归"><span class="nav-number">2.</span> <span class="nav-text">线性回归</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#对数几率回归"><span class="nav-number">3.</span> <span class="nav-text">对数几率回归</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#线性判别分析-LDA"><span class="nav-number">4.</span> <span class="nav-text">线性判别分析 (LDA)</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#多分类学习任务"><span class="nav-number">5.</span> <span class="nav-text">多分类学习任务</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#类别不平衡问题"><span class="nav-number">6.</span> <span class="nav-text">类别不平衡问题</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#习题"><span class="nav-number">7.</span> <span class="nav-text">习题</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#基于神经网络的逻辑回归分类"><span class="nav-number">8.</span> <span class="nav-text">基于神经网络的逻辑回归分类</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#DL4J实现"><span class="nav-number">8.1.</span> <span class="nav-text">DL4J实现</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Pure-Java实现"><span class="nav-number">8.2.</span> <span class="nav-text">Pure-Java实现</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      
        <div class="back-to-top">
          <i class="fa fa-arrow-up"></i>
          
            <span id="scrollpercent"><span>0</span>%</span>
          
        </div>
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy;  2017 - 
  <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-star"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Isaac Changhau</span>
</div>


<div class="powered-by">
  Powered by <a class="theme-link" href="https://hexo.io">Hexo</a>
</div>

<div class="theme-info">
  Theme -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT
  </a>
</div>


        

        
      </div>
    </footer>

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.1"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.1"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.1"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.1"></script>



  


  




	





  





  





  






  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->





  

  

  

  
  


  

  

</body>
</html>
