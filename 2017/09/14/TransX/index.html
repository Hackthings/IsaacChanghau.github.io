<!doctype html>



  


<html class="theme-next mist use-motion" lang="en">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="deep learning,python,natural language processing,c plus plus,word embeddings," />





  <link rel="alternate" href="/atom.xml" title="Isaac Changhau" type="application/atom+xml" />




  <link rel="shortcut icon" type="image/x-icon" href="/tltimg.jpg?v=5.1.1" />






<meta name="description" content="It is a summary of several papers about embedding entities and relationships of multi-relational knowledge database, while each paper contains one kind of specific method. Generally, the set of method">
<meta name="keywords" content="deep learning,python,natural language processing,c plus plus,word embeddings">
<meta property="og:type" content="article">
<meta property="og:title" content="TransX -- Embedding Entities and Relationships of Multi-relational Data">
<meta property="og:url" content="https://isaacchanghau.github.io/2017/09/14/TransX/index.html">
<meta property="og:site_name" content="Isaac Changhau">
<meta property="og:description" content="It is a summary of several papers about embedding entities and relationships of multi-relational knowledge database, while each paper contains one kind of specific method. Generally, the set of method">
<meta property="og:locale" content="en">
<meta property="og:image" content="https://isaacchanghau.github.io/images/nlp/transx/conceptnet-wordnet.png">
<meta property="og:image" content="https://isaacchanghau.github.io/images/nlp/transx/relation-forms.png">
<meta property="og:image" content="https://isaacchanghau.github.io/images/nlp/transx/basic-component.png">
<meta property="og:image" content="https://isaacchanghau.github.io/images/nlp/transx/transe-translation-graph.png">
<meta property="og:image" content="https://isaacchanghau.github.io/images/nlp/transx/process.png">
<meta property="og:image" content="https://isaacchanghau.github.io/images/nlp/transx/transh-graph.png">
<meta property="og:image" content="https://isaacchanghau.github.io/images/nlp/transx/transh-solve-problem.png">
<meta property="og:image" content="https://isaacchanghau.github.io/images/nlp/transx/limitation-transh.png">
<meta property="og:image" content="https://isaacchanghau.github.io/images/nlp/transx/transr.png">
<meta property="og:image" content="https://isaacchanghau.github.io/images/nlp/transx/transd-graph.png">
<meta property="og:updated_time" content="2018-02-20T04:11:46.218Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="TransX -- Embedding Entities and Relationships of Multi-relational Data">
<meta name="twitter:description" content="It is a summary of several papers about embedding entities and relationships of multi-relational knowledge database, while each paper contains one kind of specific method. Generally, the set of method">
<meta name="twitter:image" content="https://isaacchanghau.github.io/images/nlp/transx/conceptnet-wordnet.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    sidebar: {"position":"right","display":"post","offset":12,"offset_float":0,"b2t":true,"scrollpercent":true},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://isaacchanghau.github.io/2017/09/14/TransX/"/>





  <title>TransX -- Embedding Entities and Relationships of Multi-relational Data | Isaac Changhau</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  














</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-right page-post-detail ">
    <div class="headband"></div>
    
    <!--<a href="https://github.com/IsaacChanghau"><img style="position: absolute; top: 0; right: 0; border: 0;" src="https://camo.githubusercontent.com/38ef81f8aca64bb9a64448d0d70f1308ef5341ab/68747470733a2f2f73332e616d617a6f6e6177732e636f6d2f6769746875622f726962626f6e732f666f726b6d655f72696768745f6461726b626c75655f3132313632312e706e67" alt="Fork me on GitHub" data-canonical-src="https://s3.amazonaws.com/github/ribbons/forkme_right_darkblue_121621.png"></a>-->

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Isaac Changhau</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">Stay Hungry, Stay Foolish</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th-list"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-vcard"></i> <br />
            
            About
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            Search
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="Searching..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://isaacchanghau.github.io/2017/09/14/TransX/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Isaac Changhau">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Isaac Changhau">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">TransX -- Embedding Entities and Relationships of Multi-relational Data</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-09-14T22:46:18+08:00">
                2017-09-14
              </time>
            

            
              <span class="post-updated">
                &nbsp; | &nbsp; <i class="fa fa-calendar-check-o"></i> Updated on
                <time itemprop="dateUpdated" datetime="2018-02-20T12:11:46+08:00" content="2018-02-20">
                  2018-02-20
                </time>
              </span>
            

            

            
          </span>

          
            <span class="post-letters-count">
              &nbsp; | &nbsp;
              <i class="fa fa-file-word-o"></i>
            <span>Count 3,730 words</span>
              <!--&nbsp; | &nbsp;
              <i class="fa fa-clock-o"></i>
            <span>Reading 23 min</span>-->
            </span>
          

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Natural-Language-Processing/" itemprop="url" rel="index">
                    <span itemprop="name">Natural Language Processing</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>It is a summary of several papers about embedding entities and relationships of multi-relational knowledge database, while each paper contains one kind of specific method. Generally, the set of methods are called TransX.<a id="more"></a> Here, I only describe the general idea and mathematical process of each method, for more information about those methods in parameters setting, experiment results and discussion, you can directly read the original papers through the links I provide.</p>
<h1 id="multi-relational-data-overview">Multi-relational Data Overview</h1>
<p>There are a lot of relational knowledge database available nowadays, like: - <a href="http://conceptnet.io" target="_blank" rel="noopener">ConceptNet</a>, which is a freely-available semantic network, designed to help computers understand the meanings of words that people use. - <a href="https://wordnet.princeton.edu" target="_blank" rel="noopener">WordNet</a>, which is a large lexical database of English. Nouns, verbs, adjectives and adverbs are grouped into sets of cognitive synonyms (synsets), each expressing a distinct concept. WordNet’s structure makes it a useful tool for computational linguistics and natural language processing. - <a href="https://verbs.colorado.edu/verbnet/" target="_blank" rel="noopener">VerbNet</a>, which is the largest on-line verb lexicon currently available for English. It is a hierarchical domain-independent, broad-coverage verb lexicon with mappings to other lexical resources such as WordNet, <a href="http://www.cis.upenn.edu/~xtag/" target="_blank" rel="noopener">Xtag</a> and <a href="https://framenet.icsi.berkeley.edu/fndrupal/" target="_blank" rel="noopener">FrameNet</a>. - <a href="https://en.wikipedia.org/wiki/Freebase" target="_blank" rel="noopener">FreeBase</a>, which is a large collaborative knowledge base consisting of data composed mainly by its community members. It was an online collection of structured data harvested from many sources, including individual, user-submitted wiki contributions. Freebase aimed to create a global resource that allowed people (and machines) to access common information more effectively. - <a href="https://www.mpi-inf.mpg.de/departments/databases-and-information-systems/research/yago-naga/yago/" target="_blank" rel="noopener">YaGo</a>, which is a huge semantic knowledge base, derived from Wikipedia, WordNet and GeoNames. It has knowledge of more than 10 million entities and contains more than 120 million facts about these entities.</p>
<p>Actually, there are still a lot of different relational knowledge database exist, here we only show some representative databases above. Before we discuss those algorithms, let’s see some examples of multi-relational knowledge database to get the idea about how the database looks like. <img src="/images/nlp/transx/conceptnet-wordnet.png" alt="conceptnet and wordnet"> In the graph above, I show an example of two knowledge DB. For ConceptNet, each entity (or, node) is a concept, and entities are connected to each other through some specific relations, like, <code>oven--UsedFor--&gt;cook</code>, <code>cake--IsA--&gt;dessert</code>, here <code>oven</code>, <code>cook</code>, <code>cake</code> and <code>dessert</code> are entities, while <code>UsedFor</code> and <code>IsA</code> are relations. For WordNet, there are two different entities, one is called Synset (blue node), another is Word (green one). Each synset in WordNet is connected with other synsets through <strong>hypernym</strong> and <strong>hyponym</strong> relations, while Word only connect to its own synset and has some links with other words in the same synset, but never connected to words outside.</p>
<p>Although there are amount of relations and connections within a knowledge database, generally, all of the relations are in the several certain forms, like <strong>one-to-many relation</strong>, <strong>many-to-one relation</strong>, <strong>one-to-one relation</strong>, <strong>co-relation</strong>, <strong>reflexive relation</strong> and so on. <img src="/images/nlp/transx/relation-forms.png" alt="relation-forms"> For example, one-to-many relation, which means one entity with one relation, links to several different entities, say, <code>apple--is_a--&gt;fruit</code>, <code>apple--is_a--&gt;computer_brand</code>, <code>apple--is_a--&gt;computer_manufacturer</code> and so on. For many-to-one relation, it is similar. However, the reflexive relation is unique, since two entities connect to each other with same relation, like <code>plate--is_a-&gt;dish</code>, while <code>dish--is_a--&gt;plate</code> too.</p>
<p>Dispite the scale of relational knowledge databases and how many different relation forms they have, all of them are able to be decomposed to the <strong>basic component</strong> (triple), i.e., an entity connect to another entity with a certain relation, as shown below <img src="/images/nlp/transx/basic-component.png" alt="basic-component"> In order to convert those relational data into embeddings, which are convenient and easy to access via statistical approach, researchers proposed several methods to handle this issue, like <a href="http://machinelearning.wustl.edu/mlpapers/paper_files/ICML2011Nickel_438.pdf" target="_blank" rel="noopener">RESCAL</a>, <a href="https://pdfs.semanticscholar.org/057a/c29c84084a576da56247bdfd63bf17b5a891.pdf" target="_blank" rel="noopener">SE</a>, <a href="http://www.thespermwhale.com/jaseweston/papers/ebrm_mlj.pdf" target="_blank" rel="noopener">SME(linear)</a>, <a href="http://www.thespermwhale.com/jaseweston/papers/ebrm_mlj.pdf" target="_blank" rel="noopener">SME(bilinear)</a>, <a href="https://hal.inria.fr/hal-00776335/document" target="_blank" rel="noopener">LFM</a>, <a href="https://papers.nips.cc/paper/5071-translating-embeddings-for-modeling-multi-relational-data.pdf" target="_blank" rel="noopener">TransE</a>, <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.486.2800&amp;rep=rep1&amp;type=pdf" target="_blank" rel="noopener">TransH</a>, <a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/view/9571/9523" target="_blank" rel="noopener">TransR</a>, <a href="http://www.aclweb.org/anthology/P15-1067" target="_blank" rel="noopener">TransD</a> and so on, while the TransE, TransH, TransR, TransD are a group of similar methods, thus, we put them together and named as <strong>TransX</strong>. So, TransX is a set of methods to create embeddings for entities and relations while remembering their connection information.</p>
<h1 id="transe">TransE</h1>
<p>TransE – <a href="https://papers.nips.cc/paper/5071-translating-embeddings-for-modeling-multi-relational-data.pdf" target="_blank" rel="noopener">Translating Embeddings for Modeling Multi-relational Data</a>.</p>
<p>This paper considers the problem of embedding entities and relationships of multi-relational data in low-dimensional vector spaces, its objective is to propose a canonical model which is easy to train, contains a reduced number of parameters and can scale up to very large databases. So the TransE is proposed, which models relationships by interpreting them as translations operating on the low-dimensional embeddings of the entities.</p>
<p>TransE, an energy-based model for learning low-dimensional embeddings of entities. In TransE, relationships are represented as translations in the embedding space:</p>
<blockquote>
<p>if <span class="math inline">\((h,r,t)\)</span> holds, then the embedding of the tail entity <span class="math inline">\(t\)</span> should be <strong>close to</strong> the embedding of the head entity <span class="math inline">\(h\)</span> <strong>plus</strong> some vector that depends on the relationship <span class="math inline">\(r\)</span>, while it is also the general idea of the training process of TransE.</p>
</blockquote>
<p>The main motivation behind this translation-based parameterization is that hierarchical relationships are extremely common in KBs and translations are the natural transformations for representing them. Another motivation comes from <a href="https://arxiv.org/abs/1310.4546" target="_blank" rel="noopener">Distributed Representations of Words and Phrases and Their Compositionality</a>, in which the authors learn word embeddings from free text, and some <em>1-to-1</em> relationships between entities of different types are (coincidentally rather than willingly) represented by the model as translations in the embedding space. This suggests that there may exist embedding spaces in which <em>1-to-1</em> relationships between entities of different types may, as well, be represented by translations.</p>
<p><strong>Translation-based model</strong>: Given a training set <span class="math inline">\(S\)</span> of triplets <span class="math inline">\((h,r,t)\)</span> composed of two entities <span class="math inline">\(h,t\in E\)</span> (the set of entities) and a relationship <span class="math inline">\(r\in R\)</span> (the set of relationships), TransE learns vector embeddings of the entities and the relationships. The embeddings take values in <span class="math inline">\(\mathbb{R}^{k}\)</span> (<span class="math inline">\(k\)</span> is a model hyperparameter) and are denoted with the same letters, in boldface characters.</p>
<p>The basic idea behind the model is that the functional relation induced by the <span class="math inline">\(r\)</span>-labeled edges corresponds to a translation of the embeddings, i.e. <strong><span class="math inline">\(h+r\approx t\)</span> when <span class="math inline">\((h,r,t)\)</span> holds (<span class="math inline">\(t\)</span> should be a nearest neighbor of <span class="math inline">\(h+r\)</span>), while <span class="math inline">\(h+r\)</span> should be far away from <span class="math inline">\(t\)</span> otherwise</strong>, as the graph below shows. Following an energy-based framework, the energy of a triplet is equal to <span class="math inline">\(d(h+r,t)\)</span> for some dissimilarity measure <span class="math inline">\(d\)</span>, which we take to be either the <span class="math inline">\(L_{1}\)</span> or the <span class="math inline">\(L_{2}\)</span>-norm. <img src="/images/nlp/transx/transe-translation-graph.png" alt="TransE Translation"> To learn such embeddings, we minimize a margin-based ranking criterion over the training set:<span class="math display">\[
\mathcal{L}=\sum_{(h,r,t)\in S}\sum_{(h&#39;,r,t&#39;)\in S&#39;_{(h,r,t)}}\big[\gamma+d(h+r,t)-d(h&#39;+r,t&#39;)\big]_{+}\tag{1}
\]</span>where <span class="math inline">\([x]_{+}\)</span> denotes the positive part of <span class="math inline">\(x\)</span>, <span class="math inline">\(\gamma&gt;0\)</span> is a margin hyperparameter, and the dissimilarity measure <span class="math inline">\(d\)</span> is the squared euclidean distance, which computed by<span class="math display">\[
d(h+r,t)=\lVert h\rVert^{2}_{2}+\lVert r\rVert^{2}_{2}+\lVert t\rVert^{2}_{2}-2\big(h^{T}t+r^{T}(t-h)\big)\tag{2}
\]</span>with the norm constraints that <span class="math inline">\(\lVert h\rVert^{2}_{2}=\lVert t\rVert^{2}_{2}=1\)</span>, and the set of corrupted triplets, constructed according to the equation (3) below, is composed of training triplets with either the head or tail replaced by a random entity (but not both at the same time)<span class="math display">\[
S&#39;_{(h,r,t)}=\big\{(h&#39;,r,t)\vert h&#39;\in E\big\}\bigcup\big\{(h,r,t&#39;)\vert t&#39;\in E\big\}\tag{3}
\]</span>The loss function (1) favors lower values of the energy for training triplets than for corrupted triplets, and is thus a natural implementation of the intended criterion.</p>
<p>The optimization is carried out by stochastic gradient descent (in minibatch mode), over the possible <span class="math inline">\(h\)</span>, <span class="math inline">\(r\)</span>, and <span class="math inline">\(t\)</span>, with the additional constraints that the <span class="math inline">\(L_{2}\)</span>-norm of the embeddings of the entities is 1 (no regularization or norm constraints are given to the label embeddings <span class="math inline">\(r\)</span>). This constraint is important, because it prevents the training process to trivially minimize <span class="math inline">\(\mathcal{L}\)</span> by artificially increasing entity embeddings norms. The detailed optimization procedure is described in the graph below. <img src="/images/nlp/transx/process.png" alt="Process Graph"></p>
<h1 id="transh">TransH</h1>
<p>TransH – <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.486.2800&amp;rep=rep1&amp;type=pdf" target="_blank" rel="noopener">Knowledge Graph Embedding by Translating on Hyperplanes</a>.</p>
<p>Before talking about TransH, let’s see some problems that TransE holds. TransE models a relation <span class="math inline">\(r\)</span> as a translation vector <span class="math inline">\(r\in\mathbb{R}^{k}\)</span>, and assumes the error <span class="math inline">\(\Vert h+r-t\Vert_{l_{1}/l_{2}}\)</span> is low if <span class="math inline">\((h,r,t)\)</span> is a golden triple. It applies well to irreflexive and one-to-one relations but has problems when dealing with reflexive or many-to-one, one-to-many, many-to-many relations. Considering the ideal case of no-error embedding where <span class="math inline">\(h+r-t=0\)</span>, if <span class="math inline">\((h,r,t)\in\Delta\)</span>, we can get the following consequences directly from TransE model. - If <span class="math inline">\((h,r,t)\in\Delta\)</span> and <span class="math inline">\((t,r,h)\in\Delta\)</span>, i.e., <span class="math inline">\(r\)</span> is a reflexive map, then <span class="math inline">\(r=0\)</span> and <span class="math inline">\(h=t\)</span>. - If <span class="math inline">\(\forall i\in\{0,1,\dots,m\}\)</span>, <span class="math inline">\((h_{i},r,t)\in\Delta\)</span>, i.e., <span class="math inline">\(r\)</span> is a many-to-one map, then <span class="math inline">\(h_{0}=\dots=h_{m}\)</span>. Similarly, if <span class="math inline">\((h,r,t_{i})\in\Delta\)</span>, i.e., <span class="math inline">\(r\)</span> is a one-to-many map, then <span class="math inline">\(t_{0}=\dots=t_{m}\)</span>.</p>
<p>The reason leading to the above consequences is, in TransE, the representation of an entity is the same when involved in any relations, ignoring <em>distributed representations of entities when involved in different relations</em>. Hence, TransH is proposed to handle the problems of TransE in modeling reflexive, one-to-many, many-to-one and many-to-many relations. The general idea of TransH is shown below, which introduces a <strong>relation-specific hyperplane</strong> to project the entities to the hyperplane, and the translation process is done in such hyperplane too. For a relation <span class="math inline">\(r\)</span>, the authors position the relation-specific translation vector <span class="math inline">\(d_{r}\)</span> in the relation-specific hyperplane <span class="math inline">\(w_{r}\)</span> (the normal vector) rather than in the same space of entity embeddings. <img src="/images/nlp/transx/transh-graph.png" alt="TransH Graph"> Specifically, for a triplet <span class="math inline">\((h,r,t)\)</span>, the embedding <span class="math inline">\(h\)</span> and <span class="math inline">\(t\)</span> are first projected to the hyperplane <span class="math inline">\(w_{r}\)</span>. The projections are denoted as <span class="math inline">\(h_{\bot}\)</span> and <span class="math inline">\(t_{\bot}\)</span>, respectively, while<span class="math display">\[\begin{aligned}
h_{\bot} &amp;=h-w_{r}^{T}hw_{r}\\
t_{\bot} &amp;=t-w_{r}^{T}tw_{r}
\end{aligned}\]</span>The authors expect <span class="math inline">\(h_{\bot}\)</span> and <span class="math inline">\(t_{\bot}\)</span> can be connected by a translation vector <span class="math inline">\(d_{r}\)</span> on the hyperplane with low error if <span class="math inline">\((h,r,t)\)</span> is a golden triplet. And the graph below shows how the TransH sloves the problems in TransE. <img src="/images/nlp/transx/transh-solve-problem.png" alt="TransH Solve Problems"> Thus, in TransH, by introducing the mechanism of projecting to the relation-specific hyperplane, it enables different roles of an entity in different relations/triplets. Generally, the training process of TransH is similar to TransE, its cost function is also a margin-based ranking loss<span class="math display">\[
\mathcal{L}=\sum_{(h,r,t)\in S}\sum_{(h&#39;,r,t&#39;)\in S&#39;_{(h,r,t)}}\big[f_{r}(h,t)+\gamma-f_{r&#39;}(h&#39;,t&#39;)\big]_{+}\tag{4}
\]</span>where <span class="math inline">\([x]_{+}=\max(0,x)\)</span>, <span class="math inline">\(S\)</span> is a set of golden triples, <span class="math inline">\(S&#39;_{(h,r,t)}\)</span> denotes the set of negative triplets constructed by corrupting <span class="math inline">\((h,r,t)\)</span>, <span class="math inline">\(\gamma\)</span> is the margin separating positive and negative triplets, and <span class="math inline">\(f_{r}(h,t)\)</span> is a score function and derived by<span class="math display">\[
f_{r}(h,t)=\Vert h_{\bot}+d_{r}-t_{\bot}\Vert_{2}^{2}=\Vert(h-w_{r}^{T}hw_{r})+d_{r}-(t-w_{r}^{T}tw_{r})\Vert_{2}^{2}\tag{5}
\]</span>When minimizing the loss <span class="math inline">\(\mathcal{L}\)</span>, the following constraints are considered: - <span class="math inline">\(\forall e\in E\)</span>, <span class="math inline">\(\Vert e\Vert_{2}\leq 1\)</span> - <span class="math inline">\(\forall r\in R\)</span>, <span class="math inline">\(\vert w_{r}^{T}d_{r}\vert/\Vert d_{r}\Vert_{2}\leq\epsilon\)</span> - <span class="math inline">\(\forall r\in R\)</span>, <span class="math inline">\(\Vert w_{r}\Vert_{2}=1\)</span></p>
<p><strong>Reducing False Negative Labels</strong>: Here shows a new method to sample the corrupted triples to reduce the false. 1. Give more chance to replacing the head entity if the relation is one-to-many and give more chance to replacing the tail entity if the relation is many-to-one. 2. Among all the triplets of a relation r, we first get the following two statistics: (1) the average number of tail entities per head entity, denoted as <span class="math inline">\(tph\)</span> (2) the average number of head entities per tail entity, denoted as <span class="math inline">\(hpt\)</span></p>
<p>Then defining a Bernoulli distribution with parameter <span class="math inline">\(\frac{thp}{thp+hpt}\)</span> for sampling: given a golden triplet <span class="math inline">\((h,r,t)\)</span> of the relation <span class="math inline">\(r\)</span>, with probability <span class="math inline">\(\frac{thp}{thp+hpt}\)</span> to corrupt the triplet by replacing the head, and with probability <span class="math inline">\(\frac{pht}{thp+hpt}\)</span> to corrupt the triplet by replacing the tail.</p>
<h1 id="transr-and-ctransr">TransR and CTransR</h1>
<p>TransR – <a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/view/9571/9523" target="_blank" rel="noopener">Learning Entity and Relation Embeddings for Knowledge Graph Completion</a>.</p>
<p>TransR is proposed since the authors realize that TransH still have some limitations, since both TransE and TransH assume embeddings of entities and relations being in the same space. However, an entity may have multiple aspects, and various relations focus on different aspects of entities. Hence, it is intuitive that some entities are similar and thus close to each other in the entity space, but are <strong>comparably different in some specific aspects</strong> and thus <strong>far away from each other in the corresponding relation spaces</strong>. <img src="/images/nlp/transx/limitation-transh.png" alt="TransH Limitations"> Thus, TransR models entities and relations in distinct spaces, i.e., entity space and multiple relation spaces (i.e., relation-specific entity spaces), and performs translation in the corresponding relation space. In TransR, for each triple <span class="math inline">\((h,r,t)\)</span>, entities embeddings are set as <span class="math inline">\(h,t\in\mathbb{R}^{k}\)</span>, and relation embedding is set as <span class="math inline">\(r\in\mathbb{R}^{d}\)</span>, while the dimensions of entity embeddings and relation embeddings are not necessarily identical. For each relation <span class="math inline">\(r\)</span>, set a projection matrix <span class="math inline">\(M_{r}\in\mathbb{R}^{k\times d}\)</span> maps entities from entity space to relation space. <img src="/images/nlp/transx/transr.png" alt="TransR Limitations"> With the mapping matrix, the projected vectors of entities are computed by<span class="math display">\[
h_{r}=hM_{r},\quad t_{r}=tM_{r}
\]</span>And the score function is correspondingly defined as<span class="math display">\[
f_{r}(h,t)=\Vert h_{r}+r-t_{r}\Vert_{2}^{2}\tag{6}
\]</span>with the constraints that <span class="math inline">\(\forall h,r,t\)</span>, <span class="math inline">\(\Vert h\Vert_{2}\leq 1\)</span>, <span class="math inline">\(\Vert r\Vert_{2}\leq 1\)</span>, <span class="math inline">\(\Vert t\Vert_{2}\leq 1\)</span>, <span class="math inline">\(\Vert hM_{r}\Vert_{2}\leq 1\)</span>, <span class="math inline">\(\Vert tM_{r}\Vert_{2}\leq 1\)</span>. And the following margin-based score function as objective for training:<span class="math display">\[
\mathcal{L}=\sum_{(h,r,t)\in S}\sum_{(h&#39;,r,t&#39;)\in S&#39;_{(h,r,t)}}\max\big[0, f_{r}(h,t)+\gamma-f_{r}(h&#39;,t&#39;)\big]\tag{7}
\]</span>where <span class="math inline">\(\max(x, y)\)</span> aims to get the maximum between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>, <span class="math inline">\(\gamma\)</span> is the margin, <span class="math inline">\(S\)</span> is the set of correct triples and <span class="math inline">\(S&#39;\)</span> is the set of incorrect triples.</p>
<p><strong>Cluster-based TransR (CTransR)</strong>: The above mentioned models, including TransE, TransH and TransR, learn a unique vector for each relation, which may be under-representative to fit all entity pairs under this relation, because these relations are usually rather diverse. In order to better model these relations, the authors incorporate the idea of piecewise linear regression to extend TransR.</p>
<p>The basic idea is that first segment input instances into several groups. Formally, for a specific relation <span class="math inline">\(r\)</span>, all entity pairs <span class="math inline">\((h,t)\)</span> in the training data are clustered into multiple groups, and entity pairs in each group are expected to exhibit similar <span class="math inline">\(r\)</span> relation. All entity pairs <span class="math inline">\((h,t)\)</span> are represented with their vector offsets <span class="math inline">\((h−t)\)</span> for clustering, where <span class="math inline">\(h\)</span> and <span class="math inline">\(t\)</span> are obtained with TransE. Afterwards, learn a separate relation vector <span class="math inline">\(r_{c}\)</span> for each cluster and matrix <span class="math inline">\(M_{r}\)</span> for each relation, respectively. The authors define the projected vectors of entities as <span class="math inline">\(h_{r,c}=hM_{r}\)</span>, <span class="math inline">\(t_{r,c}=tM_{r}\)</span> and the score function is defined as<span class="math display">\[
f_{r}(h,t)=\Vert h_{r,c}+r_{c}-t_{r,c}\Vert_{2}^{2}+\alpha\Vert r_{c}-r\Vert_{2}^{2}\tag{8}
\]</span>where <span class="math inline">\(\Vert r_{c}-r\Vert_{2}^{2}\)</span> aims to ensure cluster-specific relation vector <span class="math inline">\(r_{c}\)</span> not too far away from the original relation vector <span class="math inline">\(r\)</span>, and <span class="math inline">\(\alpha\)</span> controls the effect of this constraint. Besides, same to TransR, CTransR also enforce constraints on norm of embeddings <span class="math inline">\(h\)</span>, <span class="math inline">\(r\)</span>, <span class="math inline">\(t\)</span> and mapping matrices.</p>
<h1 id="transd">TransD</h1>
<p>TransD – <a href="http://www.aclweb.org/anthology/P15-1067" target="_blank" rel="noopener">Knowledge Graph Embedding via Dynamic Mapping Matrix</a></p>
<p>Despite that TransR/CTransR has significant improvements compared with previous state-of-the-art models. However, it also has several flaws: 1. For a typical relation <span class="math inline">\(r\)</span>, all entities share the same mapping matrix <span class="math inline">\(M_{r}\)</span>. However, the entities linked by a relation always contains various types and attributes. 2. The projection operation is an interactive process between an entity and a relation, it is unreasonable that the mapping matrices are determined only by relations. 3. Matrix-vector multiplication makes it has large amount of calculation, and when relation number is large, it also has much more parameters than TransE and TransH.</p>
<p>To solve these flaws, TransD is proposed and its basic idea is shown in the graph below. In TransD, <strong>two vectors</strong> are defined for each entity and relation. The first vector represents the meaning of an entity or a relation, the other one (called <strong>projection vector</strong>) represents the way that how to project a entity embedding into a relation vector space and it will be used to construct mapping matrices. Therefore, every entity-relation pair has an unique mapping matrix. In addition, TransD has no matrix-by-vector operations which can be replaced by vectors operations. <img src="/images/nlp/transx/transd-graph.png" alt="TransD Graph"> In the graph above, each shape represents an entity pair appearing in a triplet of relation <span class="math inline">\(r\)</span>. <span class="math inline">\(M_{rh}\)</span> and <span class="math inline">\(M_{rt}\)</span> are mapping matrices of <span class="math inline">\(h\)</span> and <span class="math inline">\(t\)</span>, respectively. <span class="math inline">\(h_{ip}\)</span>, <span class="math inline">\(t_{ip}\)</span> (<span class="math inline">\(i=1,2,3\)</span>) and <span class="math inline">\(r_{p}\)</span> are projection vectors. <span class="math inline">\(h_{i\bot}\)</span> and <span class="math inline">\(t_{i\bot}\)</span> (<span class="math inline">\(i=1,2,3\)</span>) are projected vectors of entities. The projected vectors satisfy <span class="math inline">\(h_{i\bot}+r\approx t_{i\bot}\)</span> (<span class="math inline">\(i=1,2,3\)</span>).</p>
<p><strong>TransD Model</strong>: each named symbol object (entities and relations) is represented by <em>two</em> vectors. The first one captures the meaning of entity (relation), the other one is used to construct mapping matrices. For example, given a triplet <span class="math inline">\((h,r,t)\)</span>, its vectors are <span class="math inline">\(h\)</span>, <span class="math inline">\(h_{p}\)</span>, <span class="math inline">\(r\)</span>, <span class="math inline">\(r_{p}\)</span>, <span class="math inline">\(t\)</span>, <span class="math inline">\(t_{p}\)</span>, where subscript <span class="math inline">\(p\)</span> marks the projection vectors, <span class="math inline">\(h,h_{p},t,t_{p}\in\mathbb{R}^{n}\)</span> and <span class="math inline">\(r,r_{p}\in\mathbb{R}^{m}\)</span>. For each triplet <span class="math inline">\((h,r,t)\)</span> the authors set two mapping matrices <span class="math inline">\(M_{rh},M_{rt}\in\mathbb{R}^{m\times n}\)</span> to project entities from entity space to relation space:<span class="math display">\[\begin{aligned}
M_{rh}&amp;=r_{p}h_{p}^{T}+I^{m\times n}\\
M_{rt}&amp;=r_{p}t_{p}^{T}+I^{m\times n}
\end{aligned}\tag{9}\]</span>Therefore, the mapping matrices are determined by both entities and relations, and this kind of operation makes the two projection vectors interact sufficiently because each element of them can meet every entry comes from another vector. As the authors initialize each mapping matrix with an identity matrix, thus, the <span class="math inline">\(I^{m\times n}\)</span> is added to <span class="math inline">\(M_{rh}\)</span> and <span class="math inline">\(M_{rt}\)</span>. With the mapping matrices, the projected vectors are defined as follows:<span class="math display">\[\begin{aligned}
h_{\bot}&amp;=M_{rh}h\\
t_{\bot}&amp;=M_{rt}t
\end{aligned}\tag{10}\]</span>and the score function is set as<span class="math display">\[
f_{r}(h,t)=-\Vert h_{\bot}+r-t_{\bot}\Vert_{2}^{2}\tag{11}
\]</span>with the constraints that <span class="math inline">\(\Vert h\Vert_{2}\leq 1\)</span>, <span class="math inline">\(\Vert t\Vert_{2}\leq 1\)</span>, <span class="math inline">\(\Vert r\Vert_{2}\leq 1\)</span>, <span class="math inline">\(\Vert h_{\bot}\Vert_{2}\leq 1\)</span> and <span class="math inline">\(\Vert t_{\bot}\Vert_{2}\leq 1\)</span>.</p>
<p>For training process, the authors first denote that <span class="math inline">\(S=\{(h_{i},r_{i},t_{i})\vert y_{i}=1\}\)</span> as the golden triples, and <span class="math inline">\(S&#39;=\{(h_{i},r_{i},t_{i})\vert y_{i}=0\}\)</span> as the negative triples, and the negative triples is derived by<span class="math display">\[
S&#39;=\{(h_{l},r_{k},t_{k})\vert h_{l}\neq h_{k}\land y_{k}=1\}\cup\{(h_{k},r_{k},t_{l})\vert t_{l}\neq t_{k}\land y_{k}=1\}
\]</span>while the authors also use two strategies “unif” and “bern” described in TransH to replace the head or tail entity. using <span class="math inline">\(\xi\)</span> and <span class="math inline">\(\xi&#39;\)</span> to denote a golden triplet and a corresponding negative triplet, respectively. The training objective is<span class="math display">\[
\mathcal{L}=\sum_{\xi\in S}\sum_{\xi&#39;\in S&#39;}\big[\gamma+f_{r}(\xi&#39;)-f_{r}(\xi)\big]_{+}\tag{12}
\]</span><strong>Connections with TransE/H/R and CTransR</strong>: - TransE is a special case of TransD when the dimension of vectors satisfies <span class="math inline">\(m=n\)</span> and all projection vectors are set zero. - TransH is related to TransD when we set <span class="math inline">\(m=n\)</span>. Under the setting, projected vectors of entities can be rewritten as follows:<span class="math display">\[\begin{aligned}
h_{\bot}&amp;=M_{rh}h=h+h_{p}^{T}hr_{p}\\
t_{\bot}&amp;=M_{rt}t=t+t_{p}^{T}tr_{p}
\end{aligned}\]</span>Hence, when <span class="math inline">\(m = n\)</span>, the difference between TransD and TransH is that projection vectors are determinded only by relations in TransH, but TransD’s projection vectors are determinded by both entities and relations. - As to TransR/CTransR, TransD is an improvement of it. TransR/CTransR directly defines a mapping matrix for each relation, TransD consturcts two mapping matrices dynamically for each triplet by setting a projection vector for each entity and relation. In addition, TransD has no matrix-vector multiplication operation which can be replaced by vector operations. Without loss of generality, assuming <span class="math inline">\(m\geq n\)</span>, the projected vectors can be computed as follows:<span class="math display">\[\begin{aligned}
h_{\bot}&amp;=M_{rh}h=h_{p}^{T}hr_{p}+\big[h^{T},0^{T}\big]^{T}\\
t_{\bot}&amp;=M_{rt}t=t_{p}^{T}tr_{p}+\big[t^{T},0^{T}\big]^{T}
\end{aligned}\]</span>Therefore, TransD has less calculation than TransR/CTransR, which makes it train faster and can be applied on large-scale knowledge graphs.</p>
<h1 id="python-and-c-codes">Python and C++ Codes</h1>
<p>The Python and C++ codes of the methods above are available in the GitHub page of Natural Language Processing Lab at Tsinghua University (<a href="https://github.com/thunlp" target="_blank" rel="noopener">THUNLP</a>). Python version: <a href="https://github.com/thunlp/TensorFlow-TransX" target="_blank" rel="noopener">TensorFlow-TransX</a>. Converted TensorFlow-TransX for Python 2 and Tensorflow 0.12.0: <a href="https://github.com/IsaacChanghau/AmusingPythonCodes/tree/master/transx" target="_blank" rel="noopener">link</a> C++ version: <a href="https://github.com/thunlp/Fast-TransX" target="_blank" rel="noopener">Fast-TransX</a>, <a href="https://github.com/thunlp/KB2E" target="_blank" rel="noopener">KB2E</a>.</p>

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        
  <ul class="post-copyright">
    <li class="post-copyright-author">
      <strong>Author:</strong>
      Isaac Changhau
    </li>
    <li class="post-copyright-link">
      <strong>Link:</strong>
      <a href="https://isaacchanghau.github.io/2017/09/14/TransX/" title="TransX -- Embedding Entities and Relationships of Multi-relational Data">https://isaacchanghau.github.io/2017/09/14/TransX/</a>
    </li>
    <li class="post-copyright-license">
      <strong>Notice: </strong>
      All articles are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/3.0/" rel="external nofollow" target="_blank">CC BY-NC-SA 3.0</a> unless stating additionally. Contact me via email for questions or discussion.
    </li>
  </ul>


      
    </div>

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/deep-learning/" rel="tag"># deep learning</a>
          
            <a href="/tags/python/" rel="tag"># python</a>
          
            <a href="/tags/natural-language-processing/" rel="tag"># natural language processing</a>
          
            <a href="/tags/c-plus-plus/" rel="tag"># c plus plus</a>
          
            <a href="/tags/word-embeddings/" rel="tag"># word embeddings</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2017/09/10/Python-浅析-战狼2-170000-影评数据/" rel="next" title="Python 浅析 "战狼II" 170000+影评数据">
                <i class="fa fa-chevron-left"></i> Python 浅析 "战狼II" 170000+影评数据
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2017/09/19/PTransE/" rel="prev" title="PTransE -- Modeling Relation Paths for Representation Learning of Knowledge Bases">
                PTransE -- Modeling Relation Paths for Representation Learning of Knowledge Bases <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            Overview
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.gif"
               alt="Isaac Changhau" />
          <p class="site-author-name" itemprop="name">Isaac Changhau</p>
           
              <p class="site-description motion-element" itemprop="description">Stay Hungry, Stay Foolish</p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives/">
                <span class="site-state-item-count">44</span>
                <span class="site-state-item-name">posts</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/categories/index.html">
                <span class="site-state-item-count">8</span>
                <span class="site-state-item-name">categories</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">41</span>
                <span class="site-state-item-name">tags</span>
              </a>
            </div>
          

        </nav>

        
          <div class="feed-link motion-element">
            <a href="/atom.xml" rel="alternate">
              <i class="fa fa-rss"></i>
              RSS
            </a>
          </div>
        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="mailto:isaac.changhau@gmail.com" target="_blank" title="Email">
                  
                    <i class="fa fa-fw fa-envelope"></i>
                  
                  Email
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://www.linkedin.com/in/isaac-changhau" target="_blank" title="Linkedin">
                  
                    <i class="fa fa-fw fa-linkedin"></i>
                  
                  Linkedin
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://github.com/IsaacChanghau" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                  GitHub
                </a>
              </span>
            
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#multi-relational-data-overview"><span class="nav-number">1.</span> <span class="nav-text">Multi-relational Data Overview</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#transe"><span class="nav-number">2.</span> <span class="nav-text">TransE</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#transh"><span class="nav-number">3.</span> <span class="nav-text">TransH</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#transr-and-ctransr"><span class="nav-number">4.</span> <span class="nav-text">TransR and CTransR</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#transd"><span class="nav-number">5.</span> <span class="nav-text">TransD</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#python-and-c-codes"><span class="nav-number">6.</span> <span class="nav-text">Python and C++ Codes</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      
        <div class="back-to-top">
          <i class="fa fa-arrow-up"></i>
          
            <span id="scrollpercent"><span>0</span>%</span>
          
        </div>
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy;  2017 - 
  <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-star"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Isaac Changhau</span>
</div>


<div class="powered-by">
  Powered by <a class="theme-link" href="https://hexo.io">Hexo</a>
</div>

<div class="theme-info">
  Theme -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT
  </a>
</div>


        

        
      </div>
    </footer>

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.1"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.1"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.1"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.1"></script>



  


  




	





  





  





  






  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->





  

  

  

  
  


  

  

</body>
</html>
