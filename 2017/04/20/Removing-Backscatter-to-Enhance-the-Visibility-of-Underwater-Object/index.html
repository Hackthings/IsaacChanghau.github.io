<!doctype html>



  


<html class="theme-next mist use-motion" lang="en">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="image processing,java,opencv," />





  <link rel="alternate" href="/atom.xml" title="Isaac Changhau" type="application/atom+xml" />




  <link rel="shortcut icon" type="image/x-icon" href="/tltimg.jpg?v=5.1.1" />






<meta name="description" content="This is a novel method based on underwater optical model for underwater object visibility enhancement based on single image.">
<meta name="keywords" content="image processing,java,opencv">
<meta property="og:type" content="article">
<meta property="og:title" content="Removing Backscatter to Enhance the Visibility of Underwater Object">
<meta property="og:url" content="https://isaacchanghau.github.io/2017/04/20/Removing-Backscatter-to-Enhance-the-Visibility-of-Underwater-Object/index.html">
<meta property="og:site_name" content="Isaac Changhau">
<meta property="og:description" content="This is a novel method based on underwater optical model for underwater object visibility enhancement based on single image.">
<meta property="og:locale" content="en">
<meta property="og:image" content="https://isaacchanghau.github.io/images/imageprocessing/backscatter/schema.png">
<meta property="og:image" content="https://isaacchanghau.github.io/images/imageprocessing/backscatter/image-decomposition.png">
<meta property="og:image" content="https://isaacchanghau.github.io/images/imageprocessing/backscatter/background-light.png">
<meta property="og:image" content="https://isaacchanghau.github.io/images/imageprocessing/backscatter/truncate.png">
<meta property="og:image" content="https://isaacchanghau.github.io/images/imageprocessing/backscatter/coarse-transmission.png">
<meta property="og:image" content="https://isaacchanghau.github.io/images/imageprocessing/backscatter/refinement-transmission.png">
<meta property="og:image" content="https://isaacchanghau.github.io/images/imageprocessing/backscatter/enhanced-transmission.png">
<meta property="og:image" content="https://isaacchanghau.github.io/images/imageprocessing/backscatter/channels.png">
<meta property="og:image" content="https://isaacchanghau.github.io/images/imageprocessing/backscatter/channels-histogram.png">
<meta property="og:image" content="https://isaacchanghau.github.io/images/imageprocessing/backscatter/transmissions.png">
<meta property="og:image" content="https://isaacchanghau.github.io/images/imageprocessing/backscatter/illuminance-component.png">
<meta property="og:image" content="https://isaacchanghau.github.io/images/imageprocessing/backscatter/reflectance-component.png">
<meta property="og:image" content="https://isaacchanghau.github.io/images/imageprocessing/backscatter/final-result.png">
<meta property="og:updated_time" content="2018-02-20T03:57:56.715Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Removing Backscatter to Enhance the Visibility of Underwater Object">
<meta name="twitter:description" content="This is a novel method based on underwater optical model for underwater object visibility enhancement based on single image.">
<meta name="twitter:image" content="https://isaacchanghau.github.io/images/imageprocessing/backscatter/schema.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    sidebar: {"position":"right","display":"post","offset":12,"offset_float":0,"b2t":true,"scrollpercent":true},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://isaacchanghau.github.io/2017/04/20/Removing-Backscatter-to-Enhance-the-Visibility-of-Underwater-Object/"/>





  <title>Removing Backscatter to Enhance the Visibility of Underwater Object | Isaac Changhau</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  














</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-right page-post-detail ">
    <div class="headband"></div>
    
    <!--<a href="https://github.com/IsaacChanghau"><img style="position: absolute; top: 0; right: 0; border: 0;" src="https://camo.githubusercontent.com/38ef81f8aca64bb9a64448d0d70f1308ef5341ab/68747470733a2f2f73332e616d617a6f6e6177732e636f6d2f6769746875622f726962626f6e732f666f726b6d655f72696768745f6461726b626c75655f3132313632312e706e67" alt="Fork me on GitHub" data-canonical-src="https://s3.amazonaws.com/github/ribbons/forkme_right_darkblue_121621.png"></a>-->

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Isaac Changhau</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">Stay Hungry, Stay Foolish</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th-list"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-vcard"></i> <br />
            
            About
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            Search
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="Searching..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://isaacchanghau.github.io/2017/04/20/Removing-Backscatter-to-Enhance-the-Visibility-of-Underwater-Object/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Isaac Changhau">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Isaac Changhau">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Removing Backscatter to Enhance the Visibility of Underwater Object</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-04-20T18:40:43+08:00">
                2017-04-20
              </time>
            

            
              <span class="post-updated">
                &nbsp; | &nbsp; <i class="fa fa-calendar-check-o"></i> Updated on
                <time itemprop="dateUpdated" datetime="2018-02-20T11:57:56+08:00" content="2018-02-20">
                  2018-02-20
                </time>
              </span>
            

            

            
          </span>

          
            <span class="post-letters-count">
              &nbsp; | &nbsp;
              <i class="fa fa-file-word-o"></i>
            <span>Count 4,258 words</span>
              <!--&nbsp; | &nbsp;
              <i class="fa fa-clock-o"></i>
            <span>Reading 27 min</span>-->
            </span>
          

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Image-Processing/" itemprop="url" rel="index">
                    <span itemprop="name">Image Processing</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>This is a novel method based on underwater optical model for underwater object visibility enhancement based on single image.<a id="more"></a></p>
<h1 id="general-schema">General Schema</h1>
<p>For an underwater distorted image, it is firstly decomposed to reflectance and illuminance. Then color correction and dehazing methods are utilized to process each component separately, according to their specific features. Finally, we calculate the weights of two components and apply an efficient image fusion method to obtain the enhanced image. <img src="/images/imageprocessing/backscatter/schema.png" alt="The general procedures of objects visibility enhancement process"></p>
<h1 id="image-decomposition">Image Decomposition</h1>
<p>Here we introduce a weighted image decomposition method, which separates original image into two parts, one expresses the illumination component of image while another expresses the reflectance component of image. Since the process of camera captures scenes is based on the luminance of light which is reflected from objects or scenes to camera. For a clear environment, the radiance intensity of scenes spread widely in the display range, which makes captured image shows good contrast information and mean value is closed to middle gray according to gray world assumption. However, for the situations with hazy case, the reflectance would be distorted. In order to obtain accurate luminance and reflectance, we should derive the reflectance component and illumination component by decomposing original image. Firstly, an image can be expressed as:<span class="math display">\[I_{\lambda}(x)=I_{\lambda}^{R}(x)+I_{\lambda}^{I}(x) \tag{1}\]</span>where <span class="math inline">\(\lambda\in\{r,g,b\}\)</span> represents each color component of image, <span class="math inline">\(I_{\lambda}^{R}(x)\)</span> is the reflectance component and <span class="math inline">\(I_{\lambda}^{I}(x)\)</span> is the illumination component:<span class="math display">\[I_{\lambda}^{R}(x)=\gamma\centerdot I_{\lambda}(x) \tag{2}\]</span><span class="math display">\[I_{\lambda}^{I}(x)=(1-\gamma)\centerdot I_{\lambda}(x) \tag{3}\]</span>where <span class="math inline">\(\gamma\)</span> is a weighted parameter, which maintains bright areas remains brighter than dark areas and enhances the contrast of reflectance component to remove the backscatter effect from it as much as possible. So <span class="math inline">\(\gamma\)</span> can be derived by<span class="math display">\[\gamma=\zeta\centerdot\frac{I_{\lambda}(x)}{I_{\lambda}^{\max}} \tag{4}\]</span>where <span class="math inline">\(I_{\lambda}^{\max}\)</span> is the maximal pixel value of <span class="math inline">\(\lambda\)</span> color channel, and <span class="math inline">\(\zeta\)</span> is a control parameter to determine the weight of reflectance component, and <span class="math inline">\(\zeta\in[0,1]\)</span>. If <span class="math inline">\(\zeta=0\)</span>, the whole image is treated as illuminance component, while <span class="math inline">\(\zeta=1\)</span> , the whole image is otherwise treated as reflectance component. For further image process, we consider that the backscatter effect only exists in the illuminance component, while reflectance component only suffers from color distortion. <img src="/images/imageprocessing/backscatter/image-decomposition.png" alt="Image Decomposition"> Above is an example of image decomposition, from left to right: original image and its corresponding histogram, illuminance component and its corresponding histogram, reflectance component and its corresponding histogram.</p>
<h1 id="dehazing-and-color-correction-for-illuminance-component">Dehazing and Color Correction for Illuminance Component</h1>
<h2 id="global-underwater-background-light-estimation">Global Underwater Background Light Estimation</h2>
<p>In order to estimate the background light, an ideal way is to pick up a pixel or an area lies as the maximum depth with regard to the camera, since color distortion and contrast degradation are distance dependent. With distance increases, the haze is denser due to the scattering of turbid medium, which causes relatively brighter color. However, in this scheme, objects or scenes, which are brighter than the background light, may lead to an undesirable selection result. In order to obtain accurate result, this scheme should be eliminated. Since the variance of objects and scenes pixel values are lower with denser haze, we utilize a hierarchical searching method based on the quad-tree subdivision to execute this process. Firstly, the image is separated into four equal rectangular regions, then for each region, we compute the average value subtract the standard deviation values as shown below:<span class="math display">\[Score_{l}=\frac{1}{3N}\sum_{\lambda\in\{r,g,b\}}\sum_{x=1}^{N}I_{l}^{\lambda}(x)-\frac{1}{3}\sum_{\lambda\in\{r,g,b\}}\sqrt{\frac{\sum_{x=1}^{N}(I_{l}^{\lambda}(x)-\bar{I}_{l}^{\lambda})^{2}}{N}} \tag{5}\]</span>where <span class="math inline">\(l=1,2,3,4\)</span> represents to the four image regions, <span class="math inline">\(N\)</span> is the pixel number within the region, <span class="math inline">\(I_{l}^{\lambda}(x)\)</span> is the pixel value of <span class="math inline">\(x\)</span> point of <span class="math inline">\(\lambda\)</span> component of <span class="math inline">\(l\)</span> region, <span class="math inline">\(\bar{I}_{l}^{\lambda}\)</span> is the average pixel value of <span class="math inline">\(\lambda\)</span> component of <span class="math inline">\(l\)</span> region. After that, we select the region with the lowest variance, and divide it into four regions as done before. These processes are repeated till the size is less than the threshold, and normally we set this threshold to 100. Within the determined region, we calculate mean value vector as the final obtained background light and this vector can be considered as the approximately brightest value with the full image. <img src="/images/imageprocessing/backscatter/background-light.png" alt="Background Light"> Above image is choosing background light from proper image block.</p>
<h2 id="transmission-map-estimation">Transmission Map Estimation</h2>
<h3 id="coarse-estimation">Coarse Estimation</h3>
<p>Assuming the background light is given, according to the underwater optical model formation<span class="math display">\[I_{\lambda}(x)=\big(J_{\lambda}(x)\centerdot T_{\lambda}(x)+L_{\lambda}(x)\centerdot t_{\lambda}(x)\big)\centerdot t_{\lambda}(x)+A(\lambda)\centerdot(1-t_{\lambda}(x))\tag{6}\]</span>where <span class="math inline">\(I_{\lambda}(x)\)</span> represent each color component of captured image, <span class="math inline">\(J_{\lambda}(x)\)</span> is the scene radiance, <span class="math inline">\(L_{\lambda}(x)\)</span> is the possible existence of artificial light, <span class="math inline">\(A(\lambda)\)</span> is the background light, <span class="math inline">\(T_{\lambda}(x)=e^{-c(\lambda)\centerdot D(x)}\)</span> expresses the light attenuation in the vertical direction, i.e., vertical transmission and <span class="math inline">\(t_{\lambda}(x)=e^{-c(\lambda)\centerdot d(x)}\)</span> is transmission map in the horizontal direction <span class="math inline">\(c(\lambda)\)</span> is attenuation coefficient for <span class="math inline">\(\lambda\)</span> component, <span class="math inline">\(D(x)\)</span> is vertical depth and <span class="math inline">\(d(x)\)</span> is horizontal depth. In order to recover scenes or objects radiance from the captured image, we still need to estimate the <span class="math inline">\(T_{\lambda}(x)\)</span>, <span class="math inline">\(t_{\lambda}(x)\)</span> and the effect of artificial light. However, for most case, especially in the swallow underwater environment, there is little effect of artificial light. Simply, we eliminate the impact of artificial light and derive the simplified model as:<span class="math display">\[I_{\lambda}(x)=\big(J_{\lambda}(x)\centerdot T_{\lambda}(x)\big)\centerdot t_{\lambda}(x)+A(\lambda)\centerdot(1-t_{\lambda}(x))\tag{7}\]</span>where <span class="math inline">\(A(\lambda)\)</span> is already known after background light estimation, we firstly consider <span class="math inline">\(J_{\lambda}(x)\centerdot T_{\lambda}(x)\)</span> to be one part, denotes as <span class="math inline">\(J_{\lambda}^{T}(x)\)</span>, then the formation can be written as:<span class="math display">\[I_{\lambda}(x)=J_{\lambda}^{T}(x)\centerdot t_{\lambda}(x)+A(\lambda)\centerdot(1-t_{\lambda}(x))\tag{8}\]</span>It has the similar format as hazy formation model and we can derive <span class="math inline">\(J_{\lambda}^{T}(x)\)</span> by estimating the transmission map in the horizontal direction, <span class="math inline">\(t_{\lambda}(x)\)</span>. In such scheme, the transmission map estimation for dehazing in the atmospheric environment can be utilized to compute the transmission map of underwater case after some modifications, since both of these two environments are similar to each other. To estimate the transmission map of each color channel more accurate, we choose an optimal transmission estimation (OTS) method to prevent the over-enhancement and obtain optimized estimation. This method is a generalized dark channel prior (DCP), and in the DCP, scene depth is considered to be local similar, and some pixels within the local area of at least one color channel is nearly to be zero. Since the backscatters due to dust-like particles tend to reduce the contrast of local area, inversely, the contrast information of a degraded area also seems to implicit the effect of backscatters. So the mean square error (MSE) is utilized to measure the contrast of local scene area. The MSE contrast represents the variance of pixel values, which is given by:<span class="math display">\[\mathcal{C}_{MSE}=\sum_{x=1}^{N}\frac{\big(J_{\lambda}^{T}(x)-\bar{J}_{\lambda}^{T}\big)^{2}}{N} \tag{9}\]</span>where <span class="math inline">\(\bar{J}_{\lambda}^{T}\)</span> is the average pixel value of <span class="math inline">\(J_{\lambda}^{T}(x)\)</span>, <span class="math inline">\(N\)</span> is the pixel amount within the local area. By transforming the <span class="math inline">\((8)\)</span>, we can derive the expression of <span class="math inline">\(J_{\lambda}^{T}(x)\)</span>:<span class="math display">\[J_{\lambda}^{T}(x)=\frac{1}{t_{\lambda}(x)}\centerdot\big(I_{\lambda}(x)-A(\lambda)\big)+A(\lambda) \tag{10}\]</span>then, <span class="math inline">\(\mathcal{C}_{MSE}\)</span> can be written as<span class="math display">\[\mathcal{C}_{MSE}=\sum_{x=1}^{N}\frac{\big(I_{\lambda}(x)-\bar{I}_{\lambda}\big)^{2}}{N\centerdot t_{\lambda}(x)^{2}} \tag{11}\]</span>Considering the transmission value is locally same and MSE contrast is inversely proportional to the transmission value <span class="math inline">\(t_{\lambda}(x)\)</span>, which means the contrast of local area is greater with smaller <span class="math inline">\(t_{\lambda}(x)\)</span>. However, <span class="math inline">\(t_{\lambda}(x)\)</span> can not be arbitrarily small because it may cause some pixel values of restored image out of the full dynamic range, and further lead information loss, as shown in the figure below, only pixel values within <span class="math inline">\([\alpha,\beta]\)</span> can be enhanced after mapping process, other pixels will be truncated. In general, choosing a larger transmission value is able to reduce the information loss, but contrast is enhanced by choose smaller transmission value. <img src="/images/imageprocessing/backscatter/truncate.png" alt="Truncated"> The red regions represent the information loss due to the truncation of output pixel values, and input pixel values are mapped to output pixel values according to a transformation function. Thus, the transmission value of <span class="math inline">\(t_{\lambda}(x)\)</span> can not be chosen arbitrarily, contrast enhancement and information loss reduction should be taken into consideration at the same time. First, the contrast enhancement cost function, <span class="math inline">\(E_{c}\)</span> and information loss cost function, <span class="math inline">\(E_{i}\)</span> are designed and then minimize the two functions simultaneously, where contrast enhancement cost is defined as the negative sum of <span class="math inline">\(\mathcal{C}_{MSE}\)</span> of all color channels and information loss cost is defined as the sum of square value of truncated pixel values.<span class="math display">\[E_{c}=-\sum_{\lambda\in\{r,g,b\}}\sum_{x=1}^{N}\frac{\big(J_{\lambda}^{T}(x)-\bar{J}_{\lambda}^{T}\big)^{2}}{N}=-\sum_{\lambda\in\{r,g,b\}}\sum_{x=1}^{N}\frac{\big(I_{\lambda}(x)-\bar{I}_{\lambda}\big)^{2}}{N\centerdot t_{\lambda}(x)^{2}} \tag{12}\]</span><span class="math display">\[E_{i}=-\sum_{\lambda\in\{r,g,b\}}\sum_{x=1}^{N}\bigg\{\big(\min\{0,J_{\lambda}^{T}(x)\}\big)^{2}+\big(\max\{0,J_{\lambda}^{T}(x)-255\}\big)^{2}\bigg\} \tag{13}\]</span>Finally, for each local area, the optimal transmission value <span class="math inline">\(t_{\lambda}(x)\)</span> is estimated by minimizing the following function<span class="math display">\[E=E_{c}+\kappa\centerdot E_{i} \tag{14}\]</span>where <span class="math inline">\(\kappa\)</span> is a weighted parameter to control the influence of information loss cost. <img src="/images/imageprocessing/backscatter/coarse-transmission.png" alt="Coarse Transmission"> The graph above is coarse transmission map generated by general dark channel prior, left: illuminance component, right: corresponding coarse transmission map.</p>
<h3 id="transmission-map-refinement">Transmission Map Refinement</h3>
<p>In the previous sub-section, the transmission value is treated to be local constant of a block. However, the scene depths within each local area are vary spatially and block-based local constant transmission value is likely to yield block artifact, further to weaken the contrast of restoring image. In order to solve this problem, different methods have been proposed to refine the transmission map. Here, image guided filtering is used to refine the transmission map, which also uses an input image as a guidance. The transmission map refinement can be executed by solving a sparse linear system<span class="math display">\[(\mathbf{L}+\lambda\centerdot\mathbf{U})\centerdot t=\lambda\centerdot\tilde{t} \tag{15}\]</span>where <span class="math inline">\(\mathbf{L}\)</span> is matting Laplacian matrix, and <span class="math inline">\(\mathbf{U}\)</span> is the identity matrix with the same size as <span class="math inline">\(\mathbf{L}\)</span>, and <span class="math inline">\(\lambda\)</span> is a regularization parameter. Since in image guided filter, it kernel has the similar form of the elements of matting Laplacian matrix, thus its elements can be represented by:<span class="math display">\[L_{ij}=|\omega|\centerdot(\delta_{ij}-W_{ij}) \tag{16}\]</span>where <span class="math inline">\(|\omega|\)</span> is the number of pixels in a block, <span class="math inline">\(\delta_{ij}\)</span> is Kronecker delta, and <span class="math inline">\(W_{ij}\)</span> is the guided filter kernel weight, which is defined as:<span class="math display">\[W_{ij}=\frac{1}{|\omega|^{2}}\centerdot\sum_{k:(i,j)\in\omega_{k}}\bigg(1+\frac{(I_{i}-\mu_{k})\centerdot(I_{j}-\mu_{k})}{\delta_{k}^{2}+\tau}\bigg) \tag{17}\]</span>where <span class="math inline">\(\mu_{k}\)</span> is mean of <span class="math inline">\(I\)</span>, <span class="math inline">\(\delta_{k}^{2}\)</span> is variance of <span class="math inline">\(I\)</span>, <span class="math inline">\(\tau\)</span> is the regularization parameter and <span class="math inline">\(I\)</span> the guidance image. Thus, the transmission map is refined by computing <span class="math inline">\((15)\)</span>. <img src="/images/imageprocessing/backscatter/refinement-transmission.png" alt="Refinement Transmission"> The graph above is the transmission map refinement via image guided filtering, left: coarse transmission map, right: refined transmission map.</p>
<h3 id="transmission-map-enhancement">Transmission Map Enhancement</h3>
<p>After refinement, the transmission is relatively smoothed. In order to obtain a more accurate transmission map and extract more details from it, the refined transmission map should further be enhanced to improve its texture and details. Since an image can be separated into two parts, one is smooth component, another is detailed component, the transmission map also can be rewritten as this type.<span class="math display">\[t=t_{smooth}+t_{detail} \tag{18}\]</span>where <span class="math inline">\(t_{smooth}\)</span> is the smooth component of transmission map while <span class="math inline">\(t_{detail}\)</span> is the detailed component. For smooth component, we can derive it by using a blur filter. Gaussian low-pass filter (GLPF) is an effective smoothing filter, the idea of Gaussian blur is computing the mean value of center pixel and its surround pixels by utilizing a convolution kernel, and larger the kernel is, smoother the image is. The Gaussian convolution kernel can be represented as:<span class="math display">\[G(x,y)=\frac{1}{2\pi\sigma^{2}}\centerdot e^{-\frac{(x^{2}+y^{2})}{2\sigma^{2}}} \tag{19}\]</span>where <span class="math inline">\(\sigma\)</span> is the scale parameter of Gaussian blur. Therefore, the smooth component can be obtained by smoothing the refined transmission map with Gaussian low-pass filter.<span class="math display">\[t_{smooth}=t*G \tag{20}\]</span>Then the detailed component can be derived as the difference between <span class="math inline">\(t\)</span> and <span class="math inline">\(t_{smooth}\)</span>,<span class="math display">\[t_{detail}=t-t_{smooth} \tag{21}\]</span>After that, the enhanced transmission map is calculated by:<span class="math display">\[t_{enhanced}=t_{smooth}+\alpha\centerdot t_{detail} \tag{22}\]</span>where <span class="math inline">\(\alpha\)</span> is the enhancement parameter to control the amplified degree of detailed component. <img src="/images/imageprocessing/backscatter/enhanced-transmission.png" alt="Enhanced Transmission"> The graph above is the enhanced transmission map, left: refined transmission map, right: enhanced transmission map.</p>
<h3 id="enhanced-transmission-map-for-each-color-component">Enhanced Transmission Map for Each Color Component</h3>
<p>The transmission maps are various among different color components due to different light absorption abilities for light beams with different wavelength. After obtaining the enhanced transmission map, each color component’s transmission map can be derived by exploring the intrinsic relationship and difference among color components. Since underwater images always dominated by one color, i.e., greenish or bluish, and other color components are attenuated, the average pixel value of each color component, although not very accurate, can reflect the attenuation ratio of each color component in water. And it is known that the vertical depth of a given underwater image is hard to estimate due to insufficient prior knowledge and information provided by the image, so the average pixel value can further be utilized to estimate the transmission map for each color component. <img src="/images/imageprocessing/backscatter/channels.png" alt="Channels"> <img src="/images/imageprocessing/backscatter/channels-histogram.png" alt="Channels Histogram"> As in the graph above, using a reference line, we can derive the intensity of each color component, since underwater images often show bluish or greenish, the average intensity of the image, green channel is highest, following by blue channel and red channel is lowest. We are able to get the intrinsic information that the average pixel intensities also contain the attenuation coefficient of each color component. In the underwater optical model, the formation of transmission map can be simplified as <span class="math inline">\(t(x)=e^{-c\centerdot d(x)}\)</span>, where <span class="math inline">\(c\)</span> is the total attenuation coefficient and <span class="math inline">\(d(x)\)</span> is the distance from camera to the objects. Due to different light absorption abilities of water for different wavelength light beams, the total attenuation coefficients of different color components are diverse. Meanwhile, the average pixel value generally reflects the relative attenuation ratio of each color component, and the lower value of transmission map is, the higher contrast of restored image is. We firstly define the enhanced transmission map is transmission map of the color component with highest average pixel value. In order to define simply, we suppose that blue component has the highest average pixel value.<span class="math display">\[t_{b}(x)=t_{enhanced}(x)=e^{-c_{b}\centerdot d(x)} \tag{23}\]</span>At the same time, the transmission map of other color components can be written as<span class="math display">\[t_{r}(x)=e^{-c_{r}\centerdot d(x)},\quad t_{g}(x)=e^{-c_{g}\centerdot d(x)} \tag{24}\]</span>then using <span class="math inline">\(t_{b}(x)\)</span> to express <span class="math inline">\(t_{r}(x)\)</span> and <span class="math inline">\(t_{g}(x)\)</span><span class="math display">\[t_{r}(x)=\big(t_{b}(x)\big)^{\frac{c_{r}}{c_{b}}}=\big(t_{b}(x)\big)^{\beta_{r}},\quad t_{g}(x)=\big(t_{b}(x)\big)^{\frac{c_{g}}{c_{b}}}=\big(t_{b}(x)\big)^{\beta_{g}} \tag{25}\]</span>where <span class="math inline">\(\beta_{r}\)</span> and <span class="math inline">\(\beta_{g}\)</span> are the relative attenuation ratios and can be estimated by average pixel value. To simplify the computation, we define that <span class="math inline">\(\beta_{r}=\frac{mean(I_{b})}{mean(I_{r})}\)</span>, <span class="math inline">\(\beta_{g}=\frac{mean(I_{b})}{mean(I_{g})}\)</span>. <img src="/images/imageprocessing/backscatter/transmissions.png" alt="Transmissions"> The graph above is the transmission map of each color component, from left to right: red channel, green channel and blue channel.</p>
<h2 id="dehazing-and-color-correction">Dehazing and Color Correction</h2>
<p>After estimating the background light and transmission map of each color component, we can restore the underwater image via the simplified physical model formation <span class="math inline">\((10)\)</span>, say, <span class="math inline">\(J_{\lambda}^{T}(x)=\frac{1}{t_{\lambda}(x)}\centerdot\big(I_{\lambda}(x)-A(\lambda)\big)+A(\lambda)\)</span>. However, the result of this formation can not guarantee the intensities of restored image lie in the display area <span class="math inline">\([0,1]\)</span> or <span class="math inline">\([0,255]\)</span>, so a simple minimum-maximum normalization of intensity values is utilized to map them to the display interval. Meanwhile, although normalization method shows some effect on color correction, <span class="math inline">\(J_{\lambda}^{T}(x)\)</span> still suffers from part of light attenuation in the vertical direction and cause color distortion. Thus, color correction method should be introduced to solve this problem. For this case, a white balance method is used to execute the vertical direction compensation to obtain the final enhanced illuminance component <span class="math inline">\(J_{\lambda}(x)\)</span>, which can achieve good results. <img src="/images/imageprocessing/backscatter/illuminance-component.png" alt="Illuminance Component"> The graph above is the processed result of illuminance component, left: input image and its histogram, right: output image and its histogram.</p>
<h1 id="color-correction-for-reflectance-component">Color Correction for Reflectance Component</h1>
<p>The reflectance component reflects the texture and details of underwater scenes, and we have mentioned that it is considered to be free of backscatter. Thus, the reflectance component only suffers from color distortion caused by energy absorption of water. In order to deal with this issue, the color correction method should be introduced. Color constancy based color correction methods are excellent way to handle this process, which shows great balance between the correction performance and information loss, since the general procedures of such methods include darkest and brightest pixels’ truncation and histogram stretch, which may cause some undesired phenomena of texture information loss. By comparison, the simplest color balance (SCB) method is a better method for this case. The algorithm is fast and efficient, since it only simply stretches the pixel values of the three color channels while preserves the information of image as much as possible by manually setting different truncation ratio of different color channel, so that their histograms are able to occupy the maximal display range <span class="math inline">\([0,255]\)</span> (or <span class="math inline">\([0,1]\)</span>). In order to execute fast stretch process, an affine transform function <span class="math inline">\(ax+b\)</span> is applied on each color channel to map pixel values from minimum 0 to maximum 255 (or 1) by computing proper <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>. However, few aberrant pixels of many images already map the maximum and minimum values, so truncation is used to improve color performance by “clipping” a small percentage of pixels with highest values and lowest values before applying affine transform function. Actually, this process will cause more or less white and black regions in images, which may look unnatural. Thus, the number of truncated pixels must be as less as possible. In general, although this algorithm is not real white balance algorithm since it does not focus on correcting the color distributions, it can provide white balance effect and contrast enhancement to some degree. <img src="/images/imageprocessing/backscatter/reflectance-component.png" alt="Reflectance Component"> The graph above is the processed result of reflectance component, left: input image and its histogram, right: output image and its histogram.</p>
<h1 id="fusion-process">Fusion Process</h1>
<h2 id="weights-of-the-fusion-process">Weights of the Fusion Process</h2>
<p>After deriving the enhanced illuminance and reflectance components, a fusion-based method is introduced to combine these two images and generate the final free backscatter and color distortion image. For fusion techniques, one of the crucial steps is computing the weight maps of input images. In order to represent different features of input images well, we use several weight map methods to measure the features of input images and then combine them together to derive the normalized weighted maps. In practice, we choose three weight maps, luminance weight map, saliency weight map and exposedness weight map. Luminance weight map, <span class="math inline">\(W_{L}\)</span>, which represents the luminance parameter of each image component. This weight map is generated by calculating STD between <span class="math inline">\(r\)</span>, <span class="math inline">\(g\)</span> and <span class="math inline">\(b\)</span> color channels and the luminance value <span class="math inline">\(l\)</span>, where the luminance value <span class="math inline">\(l\)</span> is derived by:<span class="math display">\[l=\alpha\centerdot r+\beta\centerdot g+\gamma\centerdot b \tag{26}\]</span>where <span class="math inline">\(\alpha+\beta+\gamma=1\)</span>, each represents the weight parameter of each color component, and normally we set <span class="math inline">\(\alpha=0.299\)</span>, <span class="math inline">\(\beta=0.587\)</span> and <span class="math inline">\(\gamma=0.114\)</span>. It generates high values correlated with the preservation degree of each input region, while the multi-scale blending ensures a seamless transition between the inputs. However, this weight map is able to correctly reflect the luminance degree of image and show greater enhancement for degraded image, it shows negative effects on contrast and colorfulness. In order to compensate the drawbacks, following weight maps are introduced. Saliency weight map, <span class="math inline">\(W_{S}\)</span>, which reflects the salient objects and points in an image, and it aims to emphasize these discriminating objects of underwater scenes. In order to obtain the saliency map, the algorithm of Achanta et al. based on biological concept of center-surround contrast is applied due to its computationally efficient and time saving. One of the drawbacks of applying saliency map is over-estimation of highlighted areas, thus exposedness weight map is utilized to guarantee the accuracy and protect the mid tones of image. Exposedness weight map, <span class="math inline">\(W_{E}\)</span>, which evaluates the status of exposed pixels. It provides an operator to protect local contrast to be non-exaggerated or non-understated. Generally, pixel values close to mean value is likely to have higher exposed appearance. The map is written as Gaussian-modeled distance to the mean value:<span class="math display">\[W_{E}=e^{-\frac{(I(x)-\bar{I})^{2}}{2\centerdot\sigma^{2}}} \tag{27}\]</span>where <span class="math inline">\(\sigma\)</span> is the standard deviation, <span class="math inline">\(I(x)\)</span> denotes pixel value locates at position <span class="math inline">\(x\)</span> and <span class="math inline">\(\bar{I}\)</span> represents mean value. From the formation, pixels close to mean value have higher weight while pixels with larger distances are associated with over-exposed and under-exposed regions. Consequently, these three weight maps are able to produce well preserved appearance of fused images.</p>
<h2 id="multi-scale-fusion-process">Multi-scale Fusion Process</h2>
<p>Practically, in order to prevent undesirable halos and improve the performance, we utilize a multi-scale Gaussian and Laplacian pyramid decomposition technology to execute fusion process. In this method, each input is decomposed to several layers with different scales by Laplacian operator and Gaussian kernel. Meanwhile, higher layers are generated by differentiating the original image and filtered image of lower layer in Gaussian pyramid. Thus, the Laplacian pyramid is a set of quasi-bandpass versions of image. At the same time, the Gaussian pyramid of normalized weight map <span class="math inline">\(W_{norm}\)</span> is calculated, so that both Laplacian and Gaussian pyramids have same levels, and fusion process can be written as:<span class="math display">\[\mathcal{R}_{\lambda}^{l}=\sum_{n=1}^{N}G^{l}\{W_{norm}^{n}\}\centerdot L^{l}\{J_{\lambda}^{n}\} \tag{28}\]</span>where <span class="math inline">\(L^{l}\{J_{\lambda}^{n}\}\)</span> is Laplacian pyramid of the <span class="math inline">\(\lambda\)</span> component of <span class="math inline">\(n^{th}\)</span> input image, <span class="math inline">\(G^{l}\{W_{norm}^{n}\}\)</span> is <span class="math inline">\(n^{th}\)</span> normalized weight map and <span class="math inline">\(l\)</span> is pyramid levels. Since Laplacian multi-scale strategy performs relatively fast and balances a good trade-off between speed and accuracy, the restored output image can achieve excellent result. <img src="/images/imageprocessing/backscatter/final-result.png" alt="Final Result"> The graph above is the final output image via multi-scale Gaussian and Laplacian fusion process.</p>
<p>The Java Implementation of this method and the experiment results as well as evaluation and analysis is available on my GitHub repository: <a href="https://github.com/IsaacChanghau/OptimizedImageEnhance/blob/master/src/main/java/com/isaac/models/RemoveBackScatter.java" target="_blank" rel="noopener">RemoveBackScatter</a>, the Matlab codes are available here: <a href="https://github.com/IsaacChanghau/OptimizedImageEnhance/tree/master/matlab/RemoveBackScatter" target="_blank" rel="noopener">[link]</a></p>
<h1 id="reference">Reference</h1>
<ul>
<li><a href="https://www.researchgate.net/profile/John_Y_Chiang2/publication/51899044_Underwater_Image_Enhancement_by_Wavelength_Compensation_and_Dehazing/links/0046352c761722d626000000.pdf" target="_blank" rel="noopener">Underwater Image Enhancement by Wavelength Compensation and Dehazing</a></li>
<li><a href="http://dl.acm.org/citation.cfm?id=2808548" target="_blank" rel="noopener">Review on Underwater Image Restoration and Enhancement Algorithms</a></li>
<li><a href="https://asp-eurasipjournals.springeropen.com/articles/10.1155/2010/746052" target="_blank" rel="noopener">Underwater Image Processing: State of the Art of Restoration and Image Enhancement Methods</a></li>
<li><a href="https://www.robots.ox.ac.uk/~vgg/rg/papers/hazeremoval.pdf" target="_blank" rel="noopener">Single Image Haze Removal Using Dark Channel Prior</a></li>
<li><a href="http://www.computervisionbytecnalia.com/wp-content/uploads/2015/02/JVCI-14-173.pdf" target="_blank" rel="noopener">Automatic Red-Channel Underwater Image Restoration</a></li>
<li><a href="http://www.sciencedirect.com/science/article/pii/S0030402613013843" target="_blank" rel="noopener">Region-specialized Underwater Image Restoration in Inhomogeneous Optical Environments</a></li>
<li><a href="http://f4k.dieei.unict.it/proceedings/ICIP2013/pdfs/0003412.pdf" target="_blank" rel="noopener">Underwater Image Enhancement Using Guided Trigonometric Bilateral Filter and Fast Automatic Color Correction</a></li>
<li><a href="http://www.sciencedirect.com/science/article/pii/S0029801814004491" target="_blank" rel="noopener">Deriving Inherent Optical Properties from Background Color and Underwater Image Enhancement</a></li>
<li><a href="https://staff.science.uva.nl/th.gevers/pub/GeversTIP07.pdf" target="_blank" rel="noopener">Edge-based Color Constancy</a></li>
<li><a href="http://www.sciencedirect.com/science/article/pii/0016003280900587" target="_blank" rel="noopener">A Spatial Processor Model for Object Colour Perception</a></li>
<li><a href="http://www.ipol.im/pub/art/2012/g-ace/" target="_blank" rel="noopener">Automatic Color Enhancement (ACE) and Its Fast Implementation</a></li>
<li><a href="http://www.ipol.im/pub/art/2011/llmps-scb/" target="_blank" rel="noopener">Simplest Color Balance</a></li>
<li><a href="http://dl.acm.org/citation.cfm?id=29046" target="_blank" rel="noopener">Adaptive Histogram Equalization and Its Variations</a></li>
<li><a href="http://dl.acm.org/citation.cfm?id=180940" target="_blank" rel="noopener">Contrast Limited Adaptive Histogram Equalization</a></li>
<li><a href="http://ieeexplore.ieee.org/document/6115744/" target="_blank" rel="noopener">Fusion-based Restoration of the Underwater Images</a></li>
<li><a href="http://perso.telecom-paristech.fr/~Gousseau/ProjAnim/2015/ImageSousMarine.pdf" target="_blank" rel="noopener">Enhancing Underwater Images and Videos by Fusion</a></li>
<li><a href="http://smartdsp.xmu.edu.cn/memberpdf/fuxueyang/1.pdf" target="_blank" rel="noopener">A Retinex-based Enhancing Approach for Single Underwater Image</a></li>
<li><a href="http://mccannimaging.com/Retinex/Retinex_files/L%26M1971.pdf" target="_blank" rel="noopener">Lightness and Retinex Theory</a></li>
<li><a href="http://www.sciencedirect.com/science/article/pii/S1568494614005821" target="_blank" rel="noopener">Underwater Image Quality Enhancement through Integrated Color Model with Rayleigh Distribution</a></li>
<li><a href="http://www.sciencedirect.com/science/article/pii/S1568494615005347" target="_blank" rel="noopener">Enhancement of Low Quality Underwater Image through Integrated Global and Local Contrast Correction</a></li>
<li><a href="http://mcl.korea.ac.kr/~dotol1216/Publications/2013_JVCIR_JHKIM.pdf" target="_blank" rel="noopener">Optimized Contrast Enhancement for Real-time Image and Video Dehazing</a></li>
<li><a href="http://www.ics.uci.edu/~majumder/docs/peli.pdf" target="_blank" rel="noopener">Contrast in Complex Images</a></li>
<li><a href="http://students.cec.wustl.edu/~jwaldron/559/project_final/assets/defog_fattal.pdf" target="_blank" rel="noopener">Single Image Dehazing</a></li>
<li><a href="http://kaiminghe.com/eccv10/" target="_blank" rel="noopener">Guided Image Filtering</a></li>
<li><a href="http://ivrlwww.epfl.ch/supplementary_material/RK_CVPR09/" target="_blank" rel="noopener">Frequency-tuned Salient Region Detection</a></li>
<li><a href="https://ntrs.nasa.gov/archive/nasa/casi.ntrs.nasa.gov/20030015730.pdf" target="_blank" rel="noopener">The Statistics of Visual Representation</a></li>
<li><a href="http://ieeexplore.ieee.org/abstract/document/7305804/" target="_blank" rel="noopener">Human Visual System Inspired Underwater Image Quality Measures</a></li>
<li><a href="http://ieeexplore.ieee.org/abstract/document/1164279/" target="_blank" rel="noopener">Alpha-trimmed Means and Their Relationship to Median Filters</a></li>
</ul>

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        
  <ul class="post-copyright">
    <li class="post-copyright-author">
      <strong>Author:</strong>
      Isaac Changhau
    </li>
    <li class="post-copyright-link">
      <strong>Link:</strong>
      <a href="https://isaacchanghau.github.io/2017/04/20/Removing-Backscatter-to-Enhance-the-Visibility-of-Underwater-Object/" title="Removing Backscatter to Enhance the Visibility of Underwater Object">https://isaacchanghau.github.io/2017/04/20/Removing-Backscatter-to-Enhance-the-Visibility-of-Underwater-Object/</a>
    </li>
    <li class="post-copyright-license">
      <strong>Notice: </strong>
      All articles are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/3.0/" rel="external nofollow" target="_blank">CC BY-NC-SA 3.0</a> unless stating additionally. Contact me via email for questions or discussion.
    </li>
  </ul>


      
    </div>

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/image-processing/" rel="tag"># image processing</a>
          
            <a href="/tags/java/" rel="tag"># java</a>
          
            <a href="/tags/opencv/" rel="tag"># opencv</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2017/04/15/Underwater-Image-Enhance-via-Fusion/" rel="next" title="Underwater Image Enhance via Fusion and Its Java Implementation">
                <i class="fa fa-chevron-left"></i> Underwater Image Enhance via Fusion and Its Java Implementation
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2017/04/24/Generating-New-SSH-Key-Adding-It-to-the-ssh-agent-and-GitHub-Account/" rel="prev" title="Generating New SSH Key, Adding It to the ssh-agent and GitHub Account [Reprinted]">
                Generating New SSH Key, Adding It to the ssh-agent and GitHub Account [Reprinted] <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            Overview
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.gif"
               alt="Isaac Changhau" />
          <p class="site-author-name" itemprop="name">Isaac Changhau</p>
           
              <p class="site-description motion-element" itemprop="description">Stay Hungry, Stay Foolish</p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives/">
                <span class="site-state-item-count">46</span>
                <span class="site-state-item-name">posts</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/categories/index.html">
                <span class="site-state-item-count">8</span>
                <span class="site-state-item-name">categories</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">42</span>
                <span class="site-state-item-name">tags</span>
              </a>
            </div>
          

        </nav>

        
          <div class="feed-link motion-element">
            <a href="/atom.xml" rel="alternate">
              <i class="fa fa-rss"></i>
              RSS
            </a>
          </div>
        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="mailto:isaac.changhau@gmail.com" target="_blank" title="Email">
                  
                    <i class="fa fa-fw fa-envelope"></i>
                  
                  Email
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://www.linkedin.com/in/isaac-changhau" target="_blank" title="Linkedin">
                  
                    <i class="fa fa-fw fa-linkedin"></i>
                  
                  Linkedin
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://github.com/IsaacChanghau" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                  GitHub
                </a>
              </span>
            
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#general-schema"><span class="nav-number">1.</span> <span class="nav-text">General Schema</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#image-decomposition"><span class="nav-number">2.</span> <span class="nav-text">Image Decomposition</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#dehazing-and-color-correction-for-illuminance-component"><span class="nav-number">3.</span> <span class="nav-text">Dehazing and Color Correction for Illuminance Component</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#global-underwater-background-light-estimation"><span class="nav-number">3.1.</span> <span class="nav-text">Global Underwater Background Light Estimation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#transmission-map-estimation"><span class="nav-number">3.2.</span> <span class="nav-text">Transmission Map Estimation</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#coarse-estimation"><span class="nav-number">3.2.1.</span> <span class="nav-text">Coarse Estimation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#transmission-map-refinement"><span class="nav-number">3.2.2.</span> <span class="nav-text">Transmission Map Refinement</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#transmission-map-enhancement"><span class="nav-number">3.2.3.</span> <span class="nav-text">Transmission Map Enhancement</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#enhanced-transmission-map-for-each-color-component"><span class="nav-number">3.2.4.</span> <span class="nav-text">Enhanced Transmission Map for Each Color Component</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#dehazing-and-color-correction"><span class="nav-number">3.3.</span> <span class="nav-text">Dehazing and Color Correction</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#color-correction-for-reflectance-component"><span class="nav-number">4.</span> <span class="nav-text">Color Correction for Reflectance Component</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#fusion-process"><span class="nav-number">5.</span> <span class="nav-text">Fusion Process</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#weights-of-the-fusion-process"><span class="nav-number">5.1.</span> <span class="nav-text">Weights of the Fusion Process</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#multi-scale-fusion-process"><span class="nav-number">5.2.</span> <span class="nav-text">Multi-scale Fusion Process</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#reference"><span class="nav-number">6.</span> <span class="nav-text">Reference</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      
        <div class="back-to-top">
          <i class="fa fa-arrow-up"></i>
          
            <span id="scrollpercent"><span>0</span>%</span>
          
        </div>
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy;  2017 - 
  <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-star"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Isaac Changhau</span>
</div>


<div class="powered-by">
  Powered by <a class="theme-link" href="https://hexo.io">Hexo</a>
</div>

<div class="theme-info">
  Theme -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT
  </a>
</div>


        

        
      </div>
    </footer>

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.1"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.1"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.1"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.1"></script>



  


  




	





  





  





  






  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->





  

  

  

  
  


  

  

</body>
</html>
