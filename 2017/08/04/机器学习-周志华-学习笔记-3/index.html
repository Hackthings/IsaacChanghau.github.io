<!doctype html>



  


<html class="theme-next mist use-motion" lang="en">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="java,machine learning,spark,scala,support vector machine,weka," />





  <link rel="alternate" href="/atom.xml" title="Isaac Changhau" type="application/atom+xml" />




  <link rel="shortcut icon" type="image/x-icon" href="/tltimg.jpg?v=5.1.1" />






<meta name="description" content="第六章–支持向量机 (Support Vector Machine)">
<meta name="keywords" content="java,machine learning,spark,scala,support vector machine,weka">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习(周志华)--学习笔记(三) 支持向量机(Support Vector Machine)">
<meta property="og:url" content="https://isaacchanghau.github.io/2017/08/04/机器学习-周志华-学习笔记-3/index.html">
<meta property="og:site_name" content="Isaac Changhau">
<meta property="og:description" content="第六章–支持向量机 (Support Vector Machine)">
<meta property="og:locale" content="en">
<meta property="og:image" content="https://isaacchanghau.github.io/images/machinelearning/svm/1.png">
<meta property="og:image" content="https://isaacchanghau.github.io/images/machinelearning/svm/2.png">
<meta property="og:image" content="https://isaacchanghau.github.io/images/machinelearning/svm/3.png">
<meta property="og:image" content="https://isaacchanghau.github.io/images/machinelearning/svm/4.png">
<meta property="og:image" content="https://isaacchanghau.github.io/images/machinelearning/svm/5.png">
<meta property="og:image" content="https://isaacchanghau.github.io/images/machinelearning/svm/6.png">
<meta property="og:updated_time" content="2018-02-20T04:07:51.485Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="机器学习(周志华)--学习笔记(三) 支持向量机(Support Vector Machine)">
<meta name="twitter:description" content="第六章–支持向量机 (Support Vector Machine)">
<meta name="twitter:image" content="https://isaacchanghau.github.io/images/machinelearning/svm/1.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    sidebar: {"position":"right","display":"post","offset":12,"offset_float":0,"b2t":true,"scrollpercent":true},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://isaacchanghau.github.io/2017/08/04/机器学习-周志华-学习笔记-3/"/>





  <title>机器学习(周志华)--学习笔记(三) 支持向量机(Support Vector Machine) | Isaac Changhau</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  














</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-right page-post-detail ">
    <div class="headband"></div>
    
    <!--<a href="https://github.com/IsaacChanghau"><img style="position: absolute; top: 0; right: 0; border: 0;" src="https://camo.githubusercontent.com/38ef81f8aca64bb9a64448d0d70f1308ef5341ab/68747470733a2f2f73332e616d617a6f6e6177732e636f6d2f6769746875622f726962626f6e732f666f726b6d655f72696768745f6461726b626c75655f3132313632312e706e67" alt="Fork me on GitHub" data-canonical-src="https://s3.amazonaws.com/github/ribbons/forkme_right_darkblue_121621.png"></a>-->

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Isaac Changhau</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">Stay Hungry, Stay Foolish</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th-list"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-vcard"></i> <br />
            
            About
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            Search
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="Searching..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://isaacchanghau.github.io/2017/08/04/机器学习-周志华-学习笔记-3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Isaac Changhau">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Isaac Changhau">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">机器学习(周志华)--学习笔记(三) 支持向量机(Support Vector Machine)</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-08-04T15:15:24+08:00">
                2017-08-04
              </time>
            

            
              <span class="post-updated">
                &nbsp; | &nbsp; <i class="fa fa-calendar-check-o"></i> Updated on
                <time itemprop="dateUpdated" datetime="2018-02-20T12:07:51+08:00" content="2018-02-20">
                  2018-02-20
                </time>
              </span>
            

            

            
          </span>

          
            <span class="post-letters-count">
              &nbsp; | &nbsp;
              <i class="fa fa-file-word-o"></i>
            <span>Count 10,754 words</span>
              <!--&nbsp; | &nbsp;
              <i class="fa fa-clock-o"></i>
            <span>Reading 52 min</span>-->
            </span>
          

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Machine Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>第六章–支持向量机 (Support Vector Machine)<a id="more"></a></p>
<h1 id="间隔与支持向量">间隔与支持向量</h1>
<p>给定数据集<span class="math inline">\(D=\{(\boldsymbol{x}_{1},y_{1}),\dots,(\boldsymbol{x}_{m},y_{m})\}\)</span>，<span class="math inline">\(y_{i}\in\{-1,+1\}\)</span>，分类学习最基本的思想是基于训练集<span class="math inline">\(D\)</span>在样本空间中找到一个划分超平面，将不同类别的样本分开。但能将训练样本粉来的划分超平面可能有很多，直观上，应该去找两类训练样本“正中间”的划分超平面，即下图中粗线表示的划分超平面，因为该划分对训练样本<strong><em>局部扰动</em></strong>的<em>“容忍性”</em>最好。 <img src="/images/machinelearning/svm/1.png" alt="1.png"> 在样本空间中，划分超平面由下述线性方程表示：<span class="math display">\[
\boldsymbol{w}^{T}\boldsymbol{x}+b=0\tag{1}
\]</span>其中<span class="math inline">\(\boldsymbol{w}=(w_{1},\dots,w_{d})\)</span>为<strong><em>法向量</em></strong>，决定了超平面的方向，<span class="math inline">\(b\)</span>为<strong><em>位移项</em></strong>，决定了超平面与原点之间的距离。记超平面为<span class="math inline">\((\boldsymbol{w},b)\)</span>，样本空间中任意点<span class="math inline">\(\boldsymbol{x}\)</span>到超平面<span class="math inline">\((\boldsymbol{w},b)\)</span>的距离可写为<span class="math display">\[
r=\frac{\vert\boldsymbol{w}^{T}\boldsymbol{x}+b\vert}{\Vert\boldsymbol{w}\Vert}\tag{2}
\]</span>假设<span class="math inline">\((\boldsymbol{w},b)\)</span>能将训练样本正确分类，即对于<span class="math inline">\((\boldsymbol{x}_{i},y_{i})\in D\)</span>，若<span class="math inline">\(y_{i}=+1\)</span>，则有<span class="math inline">\(\boldsymbol{w}^{T}\boldsymbol{x}_{i}+b&gt;0\)</span>；若<span class="math inline">\(y_{i}=-1\)</span>，则<span class="math inline">\(\boldsymbol{w}^{T}\boldsymbol{x}_{i}+b&lt;0\)</span>。令<span class="math display">\[
\begin{cases} \boldsymbol{w}^{T}\boldsymbol{x}_{i}+b\geq+1,&amp;y_{i}=+1;\\ 
\boldsymbol{w}^{T}\boldsymbol{x}_{i}+b\leq-1,&amp;y_{i}=-1; \end{cases}\tag{3}\]</span>如下图，距离超平面最近的几个训练样本点使公式(3)成立，它们被称为<strong><em>“支持向量”</em></strong> (support vector)，两个异类支持向量到超平面的距离之和为<span class="math display">\[
\gamma=\frac{2}{\Vert\boldsymbol{w}\Vert}\tag{4}\]</span>它被称为<strong><em>“间隔”</em></strong> (margin)。 <img src="/images/machinelearning/svm/2.png" alt="2.png"> 求解<strong><em>“最大间隔”</em></strong> (maximum margin)的划分超平面，即找到满足公式(3)中的约束参数<span class="math inline">\(\boldsymbol{w}\)</span>和<span class="math inline">\(b\)</span>，使得<span class="math inline">\(\gamma\)</span>最大：<span class="math display">\[\begin{aligned}
&amp; \max_{\boldsymbol{w},b}\frac{2}{\Vert\boldsymbol{w}\Vert}\\
&amp; s.t.\quad y_{i}(\boldsymbol{w}^{T}\boldsymbol{x}_{i}+b)\geq 1,\quad i=1,2,\dots,m
\end{aligned}\tag{5}
\]</span>最大化间隔，仅需要最大化<span class="math inline">\(\Vert\boldsymbol{w}\Vert^{-1}\)</span>，等价于最小化<span class="math inline">\(\Vert\boldsymbol{w}\Vert^{2}\)</span>，于是有<span class="math display">\[\begin{aligned}
&amp; \min_{\boldsymbol{w},b}\frac{1}{2}\Vert\boldsymbol{w}\Vert^{2}\\
&amp; s.t.\quad y_{i}(\boldsymbol{w}^{T}\boldsymbol{x}_{i}+b)\geq 1,\quad i=1,2,\dots,m
\end{aligned}\tag{6}
\]</span>上式为支持向量机(Support Vector Machine，SVM)的基本型。</p>
<h1 id="对偶问题">对偶问题</h1>
<p>我们希望求解式(6)来得到最大间隔划分超平面所对应的模型<span class="math display">\[
f(\boldsymbol{x})=\boldsymbol{w}^{T}\boldsymbol{x}+b\tag{7}
\]</span>公式(6)本身为一个<strong><em>凸二次规划</em></strong> (convex quadratic programming)问题，为求解(6)式，对其使用拉格朗日乘子法可得其<strong><em>“对偶问题”</em></strong> (dual problem)：<span class="math display">\[
L(\boldsymbol{w},b,\boldsymbol{\alpha})=\frac{1}{2}\Vert\boldsymbol{w}\Vert^{2}+
\sum_{i=1}^{m}\alpha_{i}\big(1-y_{i}(\boldsymbol{w}^{T}\boldsymbol{x}_{i}+b)\big)\tag{8}
\]</span>其中<span class="math inline">\(\boldsymbol{\alpha}=(\alpha_{1};\dots;\alpha_{m})\)</span>。令<span class="math inline">\(L(\boldsymbol{w},b,\boldsymbol{\alpha})\)</span>对<span class="math inline">\(\boldsymbol{w}\)</span>和<span class="math inline">\(b\)</span>的偏导为零可得<span class="math display">\[
\boldsymbol{w}=\sum_{i=1}^{m}\alpha_{i}y_{i}\boldsymbol{x}_{i}\tag{9}\]</span><span class="math display">\[
0=\sum_{i=1}^{m}\alpha_{i}y_{i}\tag{10}
\]</span>将(9)代入(8)，即可将<span class="math inline">\(L(\boldsymbol{w},b,\boldsymbol{\alpha})\)</span>中的<span class="math inline">\(\boldsymbol{w}\)</span>和<span class="math inline">\(b\)</span>消去，再考虑式(10)的约束，可得到式(6)的对偶问题<span class="math display">\[
\begin{aligned}
&amp; \max_{\boldsymbol{\alpha}}\sum_{i=1}^{m}\alpha_{i}-\frac{1}{2}\sum_{i=1}^{m}\sum_{j=1}^{m}\alpha_{i}\alpha_{j}y_{i}y_{j}\boldsymbol{x}_{i}^{T}\boldsymbol{x}_{j}\\
&amp; s.t.\quad\sum_{i=1}^{m}\alpha_{i}y_{i}=0,\quad\alpha_{i}\geq 0, i=1,2,\dots,m.
\end{aligned}\tag{11}
\]</span>解出<span class="math inline">\(\boldsymbol{\alpha}\)</span>后，求出<span class="math inline">\(\boldsymbol{w}\)</span>和<span class="math inline">\(b\)</span>即可得到模型<span class="math display">\[
f(\boldsymbol{x})=\boldsymbol{w}^{T}\boldsymbol{x}+b=\sum_{i=1}^{m}\alpha_{i}y_{i}\boldsymbol{x}_{i}^{T}\boldsymbol{x}+b\tag{12}
\]</span>从对偶问题(11)解出的<span class="math inline">\(\alpha_{i}\)</span>是式(8)中的拉格朗日乘子，它恰好对应训练样本<span class="math inline">\((\boldsymbol{x}_{i},y_{i})\)</span>。考虑到式(6)中有不等式约束，因此上述过程需要满足KKT (<a href="https://en.wikipedia.org/wiki/Karush%E2%80%93Kuhn%E2%80%93Tucker_conditions" target="_blank" rel="noopener">Karush-Kuhn-Tucker</a>)条件，即<span class="math display">\[
\begin{cases} \alpha_{i}\geq 0;\\
y_{i}f(\boldsymbol{x}_{i})-1\geq 0;\\
\alpha_{i}(y_{i}f(\boldsymbol{x}_{i})-1)=0.
\end{cases}\tag{13}\]</span>于是，对于任意训练样本<span class="math inline">\((\boldsymbol{x}_{i},y_{i})\)</span>，总有<span class="math inline">\(\alpha_{i}=0\)</span>或<span class="math inline">\(y_{i}f(\boldsymbol{x}_{i})=1\)</span>。若<span class="math inline">\(\alpha_{i}=0\)</span>，则该样本不会在式(12)的求和中出现。若<span class="math inline">\(\alpha_{i}&gt;0\)</span>，则必有<span class="math inline">\(y_{i}f(\boldsymbol{x}_{i})=1\)</span>，所对应的样本点位于最大间隔边界上，是一个支持向量。这显示出支持向量机的一个重要性质：<strong><em>训练完成后，大部分的训练样本都不需要保留，最终模型仅与支持向量有关</em></strong>。 对于式(11)的求解，这是一个二次规划问题，常用SMO (<a href="https://en.wikipedia.org/wiki/Sequential_minimal_optimization" target="_blank" rel="noopener">Sequential Minimal Optimization</a>)等高效的方法来求解。SMO的基本思路是先固定<span class="math inline">\(\alpha_{i}\)</span>以外的所有参数，然后求解<span class="math inline">\(\alpha_{i}\)</span>上的极值。由于存在约束<span class="math inline">\(\sum_{i=1}^{m}\alpha_{i}y_{i}=0\)</span>，若固定<span class="math inline">\(\alpha_{i}\)</span>之外的其他变量，则<span class="math inline">\(\alpha_{i}\)</span>可由其他变量导出。于是，SMO每次选择两个变量<span class="math inline">\(\alpha_{i}\)</span>和<span class="math inline">\(\alpha_{j}\)</span>，并固定其他参数。这样在参数初始化后，SMO不断执行如下两个操作直到收敛： - 选取一对需要更新的变量<span class="math inline">\(\alpha_{i}\)</span>和<span class="math inline">\(\alpha_{j}\)</span>。 - 固定<span class="math inline">\(\alpha_{i}\)</span>和<span class="math inline">\(\alpha_{j}\)</span>以外的参数，求解(11)获得更新后的<span class="math inline">\(\alpha_{i}\)</span>和<span class="math inline">\(\alpha_{j}\)</span>。</p>
<p>注意到只需要选取的<span class="math inline">\(\alpha_{i}\)</span>和<span class="math inline">\(\alpha_{j}\)</span>中有一个不满足KKT条件，目标函数就会在迭代后见效，且KKT条件违背程度越大，则变量更新后可能导致的目标函数值减幅越大。因此SMO先选取违背KKT条件程度最大的变量。第二个变量应选择一个使目标函数值减小最快的变量，但比较各变量所对应的目标函数数值减幅的复杂度过高，SMO采用了一个启发式：使选取的两个变量所对应样本之间的间隔最大。这样的两个变量有很大的差别，与对两个相似变量进行更新相比，对它们进行更新会带给目标函数值更大的变化。具体来说，仅考虑<span class="math inline">\(\alpha_{i}\)</span>和<span class="math inline">\(\alpha_{j}\)</span>时，(11)中的约束可重写为<span class="math display">\[
\alpha_{i}y_{i}+\alpha_{j}y_{j}=c,\quad\alpha_{i}\geq 0,\quad\alpha_{j}\geq 0\tag{14}\]</span>其中<span class="math display">\[
c=-\sum_{k\neq i,j}\alpha_{k}y_{k}\tag{15}\]</span>是使<span class="math inline">\(\sum_{i=1}^{m}\alpha_{i}y_{i}=0\)</span>成立的常数。用<span class="math display">\[
\alpha_{i}y_{i}+\alpha_{j}y_{j}=c\tag{16}\]</span>消去(11)中的变量<span class="math inline">\(\alpha_{j}\)</span>，则得到一个关于<span class="math inline">\(\alpha_{i}\)</span>的单变量二次规划问题，仅有约束<span class="math inline">\(\alpha_{i}\geq 0\)</span>。这样的二次规划具有封闭解，能高效地计算出更新后的<span class="math inline">\(\alpha_{i}\)</span>和<span class="math inline">\(\alpha_{j}\)</span>。对于偏移项<span class="math inline">\(b\)</span>，对任意支持向量<span class="math inline">\((\boldsymbol{x}_{s},y_{s})\)</span>都有<span class="math inline">\(y_{s}f(\boldsymbol{x}_{s})=1\)</span>，即<span class="math display">\[
y_{s}\bigg(\sum_{i\in S}\alpha_{i}y_{i}\boldsymbol{x}_{i}^{T}\boldsymbol{x}+b\bigg)=1\tag{17}
\]</span>其中<span class="math inline">\(S=\{i|\alpha_{i}&gt;0,i=1,2,\dots,m\}\)</span>为所有支持向量的下标集。理论上，可选取任意支持向量，通过求解(17)获得<span class="math inline">\(b\)</span>，但实际中采用一个更加鲁棒的做法：使用所有支持向量求解的平均值<span class="math display">\[
b=\frac{1}{\vert S\vert}\sum_{s\in S}\bigg(y_{s}-\sum_{i\in S}\alpha_{i}y_{i}\boldsymbol{x}_{i}^{T}\boldsymbol{x}_{s}\bigg)\tag{18}\]</span></p>
<h1 id="核函数">核函数</h1>
<p>对于原始样本空间不存在一个能正确划分两类样本的超平面的问题，可将样本从原始空间映射到一个更高维德特征空间，使得样本在这个特征空间内线性可分。令<span class="math inline">\(\phi(\boldsymbol{x})\)</span>表示将<span class="math inline">\(\boldsymbol{x}\)</span>映射后的特征向量，在特征空间中划分超平面所对应的模型可表示为<span class="math display">\[
f(\boldsymbol{x})=\boldsymbol{w}^{T}\phi(\boldsymbol{x})+b\tag{19}
\]</span>类似(6)有<span class="math display">\[\begin{aligned}
&amp;\min_{\boldsymbol{w},b}\frac{1}{2}\Vert\boldsymbol{w}\Vert^{2}\\
&amp;s.t.\quad y_{i}(\boldsymbol{w}^{T}\phi(\boldsymbol{x}_{i})+b)\geq 1,i=1,2,\dots,m
\end{aligned}\tag{20}\]</span>其对偶问题是<span class="math display">\[
\begin{aligned}
&amp; \max_{\boldsymbol{\alpha}}\sum_{i=1}^{m}\alpha_{i}-\frac{1}{2}\sum_{i=1}^{m}\sum_{j=1}^{m}\alpha_{i}\alpha_{j}y_{i}y_{j}\phi(\boldsymbol{x}_{i})^{T}\phi(\boldsymbol{x}_{j})\\
&amp; s.t.\quad\sum_{i=1}^{m}\alpha_{i}y_{i}=0,\quad\alpha_{i}\geq 0, i=1,2,\dots,m.
\end{aligned}\tag{21}
\]</span>上式需要计算<span class="math inline">\(\phi(\boldsymbol{x}_{i})^{T}\phi(\boldsymbol{x}_{j})\)</span>，由于特征空间维数较高，直接计算比较困难，通常设计一个函数：<span class="math display">\[
\kappa(\boldsymbol{x}_{i},\boldsymbol{x}_{j})=\langle\phi(\boldsymbol{x}_{i}),\phi(\boldsymbol{x}_{j})\rangle=\phi(\boldsymbol{x}_{i})^{T}\phi(\boldsymbol{x}_{j})\tag{22}
\]</span>即<span class="math inline">\(\boldsymbol{x}_{i}\)</span>与<span class="math inline">\(\boldsymbol{x}_{j}\)</span>在特征空间的内积等于它们在原始样本空间中通过函数<span class="math inline">\(\kappa(\centerdot,\centerdot)\)</span>计算的结果，于是，(21)可重写为<span class="math display">\[
\begin{aligned}
&amp; \max_{\boldsymbol{\alpha}}\sum_{i=1}^{m}\alpha_{i}-\frac{1}{2}\sum_{i=1}^{m}\sum_{j=1}^{m}\alpha_{i}\alpha_{j}y_{i}y_{j}\kappa(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\\
&amp; s.t.\quad\sum_{i=1}^{m}\alpha_{i}y_{i}=0,\quad\alpha_{i}\geq 0, i=1,2,\dots,m.
\end{aligned}\tag{23}
\]</span>求解后得到<span class="math display">\[
f(\boldsymbol{x})=\boldsymbol{w}^{T}\phi(\boldsymbol{x})+b=\sum_{i=1}^{m}\alpha_{i}y_{i}\kappa(\boldsymbol{x},\boldsymbol{x}_{i})+b\tag{24}
\]</span>这里的函数<span class="math inline">\(\kappa(\centerdot,\centerdot)\)</span>就是<strong><em>“核函数”</em></strong> (kernel function)。式(24)显示出模型最优解可以通过训练样本的核函数展开，这一展式亦称“支持向量展式” (support vector expansion)。 <strong><em>定理1 (核函数)</em></strong>：令<span class="math inline">\(\chi\)</span>为输入空间，<span class="math inline">\(\kappa(\centerdot,\centerdot)\)</span>是定义在 <span class="math inline">\(\chi\times\chi\)</span> 上的对称函数，则<span class="math inline">\(\kappa\)</span>是核函数当且仅当对于任意数据<span class="math inline">\(D={\boldsymbol{x}_{1},\dots,\boldsymbol{x}_{m}}\)</span>，“核矩阵” (kernel matrix) <span class="math inline">\(\boldsymbol{K}\)</span>总是半正定的：<span class="math display">\[\boldsymbol{K}=\begin{bmatrix}
\kappa(\boldsymbol{x}_{1},\boldsymbol{x}_{1}) &amp; \dots &amp; \kappa(\boldsymbol{x}_{1},\boldsymbol{x}_{j}) &amp; \dots &amp; \kappa(\boldsymbol{x}_{1},\boldsymbol{x}_{m})\\
\vdots &amp; \ddots &amp; \vdots &amp; \ddots &amp; \vdots\\
\kappa(\boldsymbol{x}_{i},\boldsymbol{x}_{1}) &amp; \dots &amp; \kappa(\boldsymbol{x}_{i},\boldsymbol{x}_{j}) &amp; \dots &amp; \kappa(\boldsymbol{x}_{i},\boldsymbol{x}_{m})\\
\vdots &amp; \ddots &amp; \vdots &amp; \ddots &amp; \vdots\\
\kappa(\boldsymbol{x}_{m},\boldsymbol{x}_{1}) &amp; \dots &amp; \kappa(\boldsymbol{x}_{m},\boldsymbol{x}_{j}) &amp; \dots &amp; \kappa(\boldsymbol{x}_{m},\boldsymbol{x}_{m})\\
\end{bmatrix}\]</span><strong><em>定理1</em></strong>表明只要一个对称函数所对应的核矩阵半正定，它就能作为核函数使用。事实上，对于一个半正定核矩阵，总能找到一个与之对应的映射<span class="math inline">\(\phi\)</span>。即任何一个核函数都隐式地定义了一个称为<strong><em>“再生核希尔伯特空间”</em></strong> (<a href="https://en.wikipedia.org/wiki/Reproducing_kernel_Hilbert_space" target="_blank" rel="noopener">Reproducing Kernel Hilbert Space</a>，RKHS)的特征空间。我们希望样本在特征空间内线性可分，因此特征空间的好坏对支持向量机的性能至关重要。于是，“核函数选择”称为支持向量机的最大变数，若选择不合适，意味着将样本映射到了一个不合适的特征空间，很可能导致性能不佳。以下给出常用的核函数</p>
<table>
<colgroup>
<col style="width: 33%">
<col style="width: 33%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">名称</th>
<th style="text-align: left;">表达式</th>
<th style="text-align: left;">参数</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">线性核</td>
<td style="text-align: left;"><span class="math inline">\(\kappa(\boldsymbol{x}_{i},\boldsymbol{x}_{j})=\boldsymbol{x}_{i}^{T}\boldsymbol{x}_{j}\)</span></td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;">多项式核</td>
<td style="text-align: left;"><span class="math inline">\(\kappa(\boldsymbol{x}_{i},\boldsymbol{x}_{j})=\big(\boldsymbol{x}_{i}^{T}\boldsymbol{x}_{j}\big)^{d}\)</span></td>
<td style="text-align: left;"><span class="math inline">\(d\geq 1\)</span>为多项式的次数</td>
</tr>
<tr class="odd">
<td style="text-align: left;">高斯核</td>
<td style="text-align: left;"><span class="math inline">\(\kappa(\boldsymbol{x}_{i},\boldsymbol{x}_{j})=\exp\big(-\frac{\Vert\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\Vert^{2}}{2\sigma^{2}}\big)\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\sigma&gt;0\)</span>为高斯核的带宽(width)</td>
</tr>
<tr class="even">
<td style="text-align: left;">拉普拉斯核</td>
<td style="text-align: left;"><span class="math inline">\(\kappa(\boldsymbol{x}_{i},\boldsymbol{x}_{j})=\exp\big(-\frac{\Vert\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\Vert}{\sigma}\big)\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\sigma&gt;0\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;">Sigmoid核</td>
<td style="text-align: left;"><span class="math inline">\(\kappa(\boldsymbol{x}_{i},\boldsymbol{x}_{j})=\tanh(\beta\boldsymbol{x}_{i}^{T}\boldsymbol{x}_{j}+\theta)\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\tanh\)</span>为双曲正切函数，<span class="math inline">\(\beta&gt;0\)</span>，<span class="math inline">\(\theta&lt;0\)</span></td>
</tr>
</tbody>
</table>
<p>此外，核函数还可通过函数组合得到，例如： - 若<span class="math inline">\(\kappa_{1}\)</span>和<span class="math inline">\(\kappa_{2}\)</span>为核函数，则对于任意正数<span class="math inline">\(\gamma_{1}\)</span>、<span class="math inline">\(\gamma_{2}\)</span>，其线性组合<span class="math display">\[
\gamma_{1}\kappa_{1}+\gamma_{2}\kappa_{2}\tag{25}
\]</span>也是核函数。 - 若<span class="math inline">\(\kappa_{1}\)</span>和<span class="math inline">\(\kappa_{2}\)</span>为核函数，则核函数的直积<span class="math display">\[
\kappa_{1}\otimes\kappa_{2}(\boldsymbol{x},\boldsymbol{z})=\kappa_{1}(\boldsymbol{x},\boldsymbol{z})\kappa_{2}(\boldsymbol{x},\boldsymbol{z})\tag{26}
\]</span>也是核函数。 - 若<span class="math inline">\(\kappa_{1}\)</span>为核函数，则对于任意函数<span class="math inline">\(g(\boldsymbol{x})\)</span><span class="math display">\[
\kappa(\boldsymbol{x},\boldsymbol{z})=g(\boldsymbol{x})\kappa_{1}(\boldsymbol{x},\boldsymbol{z})g(\boldsymbol{z})\tag{27}
\]</span>也是核函数。</p>
<h1 id="软间隔与正则化">软间隔与正则化</h1>
<p>现实任务中往往很难确定合适的核函数使得训练样本在特征空间中线性可分，因此可以允许支持向量机载一些样本上出错，这里引入<strong><em>“软间隔”</em></strong> (soft margin)的概念，如下图所示 <img src="/images/machinelearning/svm/3.png" alt="3.png"> 之前假设支持向量机形式是要求所有样本均满足约束(3)，这称为<strong><em>“硬间隔”</em></strong> (hard margin)，而软间隔则是允许某些样本不满足约束<span class="math display">\[
y_{i}(\boldsymbol{w}^{T}\boldsymbol{x}_{i}+b)\geq 1\tag{28}
\]</span>当然，在最大化间隔的同时，不满足约束的样本尽可能少，于是优化目标写为<span class="math display">\[
\min_{\boldsymbol{w},b}\frac{1}{2}\Vert\boldsymbol{w}\Vert^{2}+C\sum_{i=1}^{m}\ell_{0/1}\big(y_{i}(\boldsymbol{w}^{T}\boldsymbol{x}_{i}+b)-1\big)\tag{29}
\]</span>其中<span class="math inline">\(C&gt;0\)</span>是一个常数，<span class="math inline">\(\ell_{0/1}\)</span>是“<span class="math inline">\(0/1\)</span>损失函数”<span class="math display">\[
\ell_{0/1}=\begin{cases}
1,\quad z&lt;0\\
0,\quad otherwise
\end{cases}\tag{30}\]</span>显然，当<span class="math inline">\(C\)</span>无穷大时，(29)迫使所有样本满足约束(28)，于是(29)等价于(6)，当<span class="math inline">\(C\)</span>取有限值时，(29)允许一些样本不满足约束。然而<span class="math inline">\(\ell_{0/1}\)</span>非凸、非连续，使得(29)不易直接求解。于是常采用一些函数来替代它，称为“替代损失” (surrogate loss)，这些函数通常侍凸的连续函数且是<span class="math inline">\(\ell_{0/1}\)</span>的上界。下图给出了常用的三种替代损失函数： <img src="/images/machinelearning/svm/4.png" alt="4.png"> - hinge损失：<span class="math inline">\(\ell_{hinge}(z)=\max(0,1-z)\tag{31}\)</span> - 指数损失(exponential loss)：<span class="math inline">\(\ell_{exp}(z)=\exp(-z)\tag{32}\)</span> - 对率损失(logistic loss)：<span class="math inline">\(\ell_{log}(z)=\log(1+\exp(-z))\tag{33}\)</span></p>
<p>若采用hinge损失，则(29)变为<span class="math display">\[
\min_{\boldsymbol{w},b}\frac{1}{2}\Vert\boldsymbol{w}\Vert^{2}+C\sum_{i=1}^{m}\max\big(0,1-y_{i}(\boldsymbol{w}^{T}\boldsymbol{x}_{i}+b)\big)\tag{34}
\]</span>引入<strong><em>“松弛变量”</em></strong> (slack variables) <span class="math inline">\(\xi\geq 0\)</span>，则可将上式重写为<span class="math display">\[
\min_{\boldsymbol{w},b,\xi_{i}}\frac{1}{2}\Vert\boldsymbol{w}\Vert^{2}+C\sum_{i=1}^{m}\xi_{i}\tag{35}
\]</span>这就是常用的<strong><em>“软间隔支持向量机”</em></strong>。(35)中每个样本都有一个对应的松弛向量，用以表征该样本不满足约束(28)的程度，但是，与(6)相似，这是一个二次规划问题。于是，类似(8)，通过拉格朗日乘子法和得到式(35)的拉格朗日函数<span class="math display">\[
L(\boldsymbol{w},b,\boldsymbol{\alpha},\boldsymbol{\xi},\boldsymbol{\mu})=
\frac{1}{2}\Vert\boldsymbol{w}\Vert^{2}+C\sum_{i=1}^{m}\xi_{i}+
\sum_{i=1}^{m}\alpha_{i}\big(1-\xi_{i}-y_{i}(\boldsymbol{w}^{T}\boldsymbol{x}_{i}+b)\big)-
\sum_{i=1}^{m}\mu_{i}\xi_{i}\tag{36}
\]</span>其中<span class="math inline">\(\alpha_{i}\geq 0\)</span>，<span class="math inline">\(\mu_{i}\geq 0\)</span>是拉格朗日乘子。令<span class="math inline">\(L(\boldsymbol{w},b,\boldsymbol{\alpha},\boldsymbol{\xi},\boldsymbol{\mu})\)</span>对<span class="math inline">\(\boldsymbol{w}\)</span>，<span class="math inline">\(b\)</span>，<span class="math inline">\(\xi_{i}\)</span>的偏导为零可得<span class="math display">\[
\boldsymbol{w}=\sum_{i}^{m}\alpha_{i}y_{i}\boldsymbol{x}_{i}\tag{37}\]</span><span class="math display">\[
0=\sum_{i=1}^{m}\alpha_{i}y_{i}\tag{38}\]</span><span class="math display">\[
C=\alpha_{i}+\mu_{i}\tag{39}
\]</span>将上面三个式子代入(36)可得到(35)的对偶问题<span class="math display">\[\begin{aligned}
&amp; \max_{\boldsymbol{\alpha}}\sum_{i=1}^{m}\alpha_{i}-\frac{1}{2}\sum_{i}^{m}\sum_{j=1}^{m}\alpha_{i}\alpha_{j}y_{i}y_{j}\boldsymbol{x}_{i}^{T}\boldsymbol{x}_{j}\\
&amp; s.t.\quad\sum_{i=1}^{m}\alpha_{i}y_{i}=0,\quad 0\leq\alpha_{i}\leq C, i=1,2,\dots,m
\end{aligned}\tag{40}\]</span>上式与(11)对比可得，两者唯一的差别就在于对偶变量的约束不同：前者是<span class="math inline">\(0\leq\alpha_{i}\leq C\)</span>，后者是<span class="math inline">\(0\leq\alpha_{i}\)</span>。于是，可采用同样的方法(SMO)求解(40)。在引入核函数后能得到与式(24)同样的支持向量展式。类似(13)，对软间隔支持向量机，KKT条件要求<span class="math display">\[\begin{cases}
\alpha_{i}\geq 0,\quad\mu_{i}\geq 0,\\
y_{i}f(\boldsymbol{x}_{i})-1+\xi_{i}\geq 0,\\
\alpha_{i}\big(y_{i}f(\boldsymbol{x}_{i})-1+\xi_{i}\big)=0,\\
\xi_{i}\geq 0,\quad\mu_{i}\xi_{i}=0,
\end{cases}\tag{41}\]</span>于是，对于任意训练样本<span class="math inline">\((\boldsymbol{x}_{i},y_{i})\)</span>，总有<span class="math inline">\(\alpha_{i}=0\)</span>或<span class="math inline">\(y_{i}f(\boldsymbol{x}_{i})=1-\xi_{i}\)</span>。若<span class="math inline">\(\alpha_{i}=0\)</span>，则该样本不会对<span class="math inline">\(f(\boldsymbol{x})\)</span>有任何影响；若<span class="math inline">\(\alpha_{i}&gt;0\)</span>，则必有<span class="math inline">\(y_{i}f(\boldsymbol{x}_{i})=1-\xi_{i}\)</span>，即该样本是支持向量：由(39)可知，若<span class="math inline">\(\alpha_{i}&lt; C\)</span>，则<span class="math inline">\(\mu_{i}&gt;0\)</span>，进而有<span class="math inline">\(\xi_{i}=0\)</span>，即该样本恰在最大间隔边界上；若<span class="math inline">\(\alpha_{i}=C\)</span>，则有<span class="math inline">\(\mu_{i}=0\)</span>，此时若<span class="math inline">\(\xi_{i}\leq 1\)</span>则该样本落在最大间隔内部，若<span class="math inline">\(\xi_{i}&gt;1\)</span>，则该样本被错误分类。因此，软间隔支持向量机的最终模型仅与支持向量有关，即通过采用hinge损失函数仍保持了<strong><em>稀疏性</em></strong>。 除了hinge损失函数，也可以使用其他的替代损失函数，若使用对率损失函数替代(29)中的<span class="math inline">\(0/1\)</span>损失函数，则几乎得到对率回归模型(参考：<a href="https://isaacchanghau.github.io/2017/05/04/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%91%A8%E5%BF%97%E5%8D%8E-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-2/">link</a>)。实际上，<strong><em>支持向量机与对率回归的优化目标相近，通常情况下它们的性能也相当</em></strong>。对率回归的优势主要在于其输出具有自然的概率意义，即在给出预测标记的同时也给出了概率，而支持向量机的输出不具有概率意义，欲得到概率输出需要特殊处理。此外，对率回归能直接用于多任务分类，支持向量机则需进行推广。 而从上图能看出，<strong><em>hinge损失有一块“平坦”的零区域，这使得支持向量机的解具有稀疏性</em></strong>，而对率损失是光滑的单调递减函数，不能导出类似的支持向量概念，因此对率回归的解依赖于更多的训练样本，其预测开销更大。将<span class="math inline">\(0/1\)</span>损失函数换成别的替代函数可以得到不同的学习模型，这些模型的性质与所用的替代函数直接相关，但它们具有一个共性：优化目标中的第一项用来描述划分超平面的“间隔”大小，另一项<span class="math inline">\(\sum_{i=1}^{m}\ell\big(f(\boldsymbol{x}_{i},y_{i})\big)\)</span>涌来表述训练集上的误差，可写为更一般的形式<span class="math display">\[
\min_{f}\Omega(f)+C\sum_{i=1}^{m}\ell\big(f(\boldsymbol{x}_{i},y_{i})\big)\tag{42}
\]</span>其中<span class="math inline">\(\Omega(f)\)</span>称为<strong><em>“结构风险”</em></strong> (structural risk)，用于描述模型<span class="math inline">\(f\)</span>的某些性质；第二项<span class="math inline">\(\sum_{i=1}^{m}\ell\big(f(\boldsymbol{x}_{i},y_{i})\big)\)</span>称为<strong><em>“经验风险”</em></strong> (empirical risk)，用于描述模型与训练数据的契合程度。<span class="math inline">\(C\)</span>用于对二者进行折中。从经验风险最小化的角度来看，<span class="math inline">\(\Omega(f)\)</span>表述了我们希望获得具有何种性质的模型(例如希望获得复杂度较小的模型)，这为引入领域知识和用户意图提供了途径；另一方面，该信息有助于消减假设空间，从而降低了最小化训练误差的过拟合风险。从这个角度来说，(42)称为“正则化” (regularization)问题，<span class="math inline">\(\Omega(f)\)</span>称为正则化项，<span class="math inline">\(C\)</span>则称为正则化常数。<span class="math inline">\(\boldsymbol{L}_{p}\)</span>范数(norm)是常用的正则化项，其中<span class="math inline">\(\boldsymbol{L}_{2}\)</span>范数<span class="math inline">\(\Vert\boldsymbol{w}\Vert_{2}\)</span>倾向于<span class="math inline">\(\boldsymbol{w}\)</span>的分量取值尽量均衡，即非零分量个数尽量稠密，而<span class="math inline">\(\boldsymbol{L}_{0}\)</span>范数<span class="math inline">\(\Vert\boldsymbol{w}\Vert_{0}\)</span>和<span class="math inline">\(\boldsymbol{L}_{1}\)</span>范数<span class="math inline">\(\Vert\boldsymbol{w}\Vert_{1}\)</span>则倾向于<span class="math inline">\(boldsymbol{w}\)</span>的分量尽量稀疏，即非零分量个数尽量少。</p>
<h1 id="支持向量回归">支持向量回归</h1>
<p>给定训练样本 <span class="math inline">\(D=\{(\boldsymbol{x}_{1},y_{1}),\dots,(\boldsymbol{x}_{m},y_{m})\}\)</span>，<span class="math inline">\(y_{i}\in\mathbb{R}\)</span>，我们希望学得一个形如式(7)的回归模型，使得<span class="math inline">\(f(\boldsymbol{x})\)</span>与<span class="math inline">\(y\)</span>尽可能的接近，<span class="math inline">\(\boldsymbol{w}\)</span>和<span class="math inline">\(b\)</span>是待定的模型参数。对于此问题，支持向量回归(Support Vector Regression，SVR)假设能容忍<span class="math inline">\(f(\boldsymbol{x})\)</span>与<span class="math inline">\(y\)</span>之间最多有<span class="math inline">\(\epsilon\)</span>的偏差，即仅当<span class="math inline">\(f(\boldsymbol{x})\)</span>与<span class="math inline">\(y\)</span>之间的差别绝对值大于<span class="math inline">\(\epsilon\)</span>时才计算损失。如下图所示，相当于以<span class="math inline">\(f(\boldsymbol{x})\)</span>为中心，构建一个宽度为<span class="math inline">\(2\epsilon\)</span>的间隔带，若样本落入此间隔带，则认为是被预测正确的。 <img src="/images/machinelearning/svm/5.png" alt="5.png"> 于是，SVR问题可形式化为<span class="math display">\[
\min_{\boldsymbol{w},b}\frac{1}{2}\Vert\boldsymbol{w}\Vert^{2}+C\sum_{i=1}^{m}\ell_{\epsilon}\big(f(\boldsymbol{x}_{i})-y_{i}\big)\tag{43}
\]</span>其中<span class="math inline">\(C\)</span>为正则化常数，<span class="math inline">\(\ell_{\epsilon}\)</span>是如下图所示的<span class="math inline">\(\epsilon\)</span>-不敏感损失(<span class="math inline">\(\epsilon\)</span>-insensitive loss)函数<span class="math display">\[
\ell_{\epsilon}(z)=\begin{cases}
0, &amp; ivertz\vert\leq\epsilon;
\vert z\vert-\epsilon, &amp; otherwise.
\end{cases}\tag{44}\]</span> <img src="/images/machinelearning/svm/6.png" alt="6.png"> 引入松弛变量<span class="math inline">\(\xi_{i}\)</span>和<span class="math inline">\(\hat{\xi}_{i}\)</span>，可将(43)重写为<span class="math display">\[
\min_{\boldsymbol{w},b,\xi_{i},\hat{\xi}_{i}}\frac{1}{2}\Vert\boldsymbol{w}\Vert^{2}+C\sum_{i=1}^{m}(\xi_{i}+\hat{\xi}_{i})\tag{45}\]</span><span class="math display">\[\begin{aligned}
s.t.\quad &amp; f(\boldsymbol{x}_{i})-y_{i}\leq\epsilon+\xi_{i},\\
&amp; y_{i}-f(\boldsymbol{x}_{i})\leq\epsilon+\hat{\xi}_{i},\\
&amp; \xi_{i}\geq 0,\quad\hat{\xi}_{i}\geq 0,\quad i=1,2,\dots,m
\end{aligned}\]</span>类似(36)，通过引入拉格朗日乘子 <span class="math inline">\(\mu_{i}\geq 0\)</span>，<span class="math inline">\(\hat{\mu}_{i}\geq 0\)</span>，<span class="math inline">\(\alpha_{i}\geq 0\)</span>，<span class="math inline">\(\hat{\alpha}_{i}\geq 0\)</span>，由拉格朗日乘子法可得到(45)的拉格朗日函数<span class="math display">\[\begin{aligned}
&amp; L(\boldsymbol{w},b,\boldsymbol{\alpha},\hat{\boldsymbol{\alpha}},\boldsymbol{\xi},\hat{\boldsymbol{\xi}},\boldsymbol{\mu},\hat{\boldsymbol{\mu}})\\
&amp; =\frac{1}{2}\Vert\boldsymbol{w}\Vert^{2}+C\sum_{i=1}^{m}(\xi_{i}+\hat{\xi}_{i})-\sum_{i=1}^{m}\mu_{i}\xi_{i}-\sum_{i=1}^{m}\hat{\mu}_{i}\hat{\xi}_{i}\\
&amp; +\sum_{i=1}^{m}\alpha_{i}\big(f(\boldsymbol{x}_{i})-y_{i}-\epsilon-\xi_{i}\big)+\sum_{i=1}^{m}\hat{\alpha}_{i}\big(y_{i}-f(\boldsymbol{x}_{i})-\epsilon-\hat{\xi}_{i}\big)
\end{aligned}\tag{46}\]</span>将(7)代入，再令<span class="math inline">\(L(\boldsymbol{w},b,\boldsymbol{\alpha},\hat{\boldsymbol{\alpha}},\boldsymbol{\xi},\hat{\boldsymbol{\xi}},\boldsymbol{\mu},\hat{\boldsymbol{\mu}})\)</span>对<span class="math inline">\(\boldsymbol{w}\)</span>，<span class="math inline">\(b\)</span>，<span class="math inline">\(\xi_{i}\)</span> 和 <span class="math inline">\(\hat{\xi}_{i}\)</span> 的偏导为零可得<span class="math display">\[
\boldsymbol{w}=\sum_{i=1}^{m}(\hat{\alpha}_{i}-\alpha_{i})\boldsymbol{x}_{i}\tag{47}
\]</span><span class="math display">\[
0=\sum_{i=1}^{m}(\hat{\alpha}_{i}-\alpha_{i})\tag{48}
\]</span><span class="math display">\[
C=\alpha_{i}+\mu_{i}\tag{49}
\]</span><span class="math display">\[
C=\hat{\alpha}_{i}+\hat{\mu}_{i}\tag{50}
\]</span>将上述四个式子代入(46)可得到SVR的对偶问题<span class="math display">\[
\begin{aligned}
&amp; \max_{\boldsymbol{\alpha},\hat{\boldsymbol{\alpha}}}\sum_{i=1}^{m}y_{i}(\hat{\alpha}_{i}-\alpha_{i})-\epsilon(\hat{\alpha}_{i}+\alpha_{i})-\frac{1}{2}\sum_{i=1}^{m}\sum_{j=1}^{m}(\hat{\alpha}_{i}-\alpha_{i})(\hat{\alpha}_{j}-\alpha_{j})\boldsymbol{x}_{i}^{T}\boldsymbol{x}_{j}\\
&amp; s.t.\quad\sum_{i=1}^{m}(\hat{\alpha}_{i}-\alpha_{i})=0,\quad 0\leq\alpha_{i},\hat{\alpha}_{i}\leq C.
\end{aligned}
\tag{51}
\]</span>上述过程需满足KKT条件，即<span class="math display">\[
\begin{cases}
\alpha_{i}\big(f(\boldsymbol{x}_{i})-y_{i}-\epsilon-\xi_{i}\big)=0,\\
\hat{\alpha}_{i}\big(y_{i}-f(\boldsymbol{x}_{i})-\epsilon-\hat{\xi}_{i}\big)=0,\\
\alpha_{i}\hat{\alpha}_{i}=0,\quad\xi_{i}\hat{\xi}_{i}=0,\\
(C-\alpha_{i})\xi_{i}=0,\quad(C-\hat{\alpha}_{i})\hat{\xi}_{i}=0.
\end{cases}\tag{52}
\]</span>可以看出，当且仅当<span class="math inline">\(f(\boldsymbol{x}_{i})-y_{i}-\epsilon-\xi_{i}=0\)</span>时<span class="math inline">\(\alpha_{i}\)</span>能取非零值，当且仅当<span class="math inline">\(y_{i}-f(\boldsymbol{x}_{i})-\epsilon-\hat{\xi}_{i}=0\)</span>时<span class="math inline">\(\hat{\alpha}_{i}\)</span>能取非零值。换言之仅当样本<span class="math inline">\((\boldsymbol{x}_{i},y_{i})\)</span>不落入<span class="math inline">\(\epsilon\)</span>-间隔带中，相应的<span class="math inline">\(\alpha_{i}\)</span>和<span class="math inline">\(\hat{\alpha}_{i}\)</span>能去非零值。此外，这两个约束不能同时成立，即<span class="math inline">\(\alpha_{i}\)</span>和<span class="math inline">\(\hat{\alpha}_{i}\)</span>中至少有一个为零。将(47)代入(7)，则SVR的解形如<span class="math display">\[
f(\boldsymbol{x})=\sum_{i=1}^{m}(\hat{\alpha}_{i}-\alpha_{i})\boldsymbol{x}_{i}^{T}\boldsymbol{x}+b\tag{53}
\]</span>使(53)中的<span class="math inline">\((\hat{\alpha}_{i}-\alpha_{i})\neq 0\)</span>的样本即为SVR的支持向量，它们必须落在<span class="math inline">\(\epsilon\)</span>-间隔带之外。显然，SVR只是的支持向量只是训练样本的一部分，即其解仍具有稀疏性。而由KKT条件(52)可看出，对于每个样本<span class="math inline">\((\boldsymbol{x}_{i},y_{i})\)</span>都有<span class="math inline">\((C-\alpha_{i})\xi_{i}=0\)</span>且<span class="math inline">\(\alpha_{i}\big(f(\boldsymbol{x}_{i})-y_{i}-\epsilon-\xi_{i}\big)=0\)</span>。于是，在得到<span class="math inline">\(\alpha_{i}\)</span>后，若<span class="math inline">\(0&lt;\alpha_{i}&lt; C\)</span>，则必有<span class="math inline">\(\xi_{i}=0\)</span>，进而有<span class="math display">\[
b=y_{i}+\epsilon-\sum_{i=1}^{m}(\hat{\alpha}_{i}-\alpha_{i})\boldsymbol{x}_{i}^{T}\boldsymbol{x}\tag{54}
\]</span>因此，在求解(51)得到<span class="math inline">\(\alpha_{i}\)</span>后，理论上来说，可任意选取满足<span class="math inline">\(0&lt;\alpha_{i}&lt; C\)</span>的样本通过(54)求得<span class="math inline">\(b\)</span>。实际中常采用更加鲁棒的方法：选取多个(或所有)满足该条件的样本求解<span class="math inline">\(b\)</span>后取平均值。若考虑特征映射形式(19)，则相应的(47)将形如<span class="math display">\[
\boldsymbol{w}=\sum_{i=1}^{m}(\hat{\alpha}_{i}-\alpha_{i})\phi(\boldsymbol{x}_{i})\tag{55}
\]</span>将(55)代入(19)，则SVR可表示为<span class="math display">\[
f(\boldsymbol{x})=\sum_{i=1}^{m}(\hat{\alpha}_{i}-\alpha_{i})\kappa(\boldsymbol{x},\boldsymbol{x}_{i})+b\tag{56}
\]</span>其中，<span class="math inline">\(\kappa(\boldsymbol{x}_{i},\boldsymbol{x}_{j})=\phi(\boldsymbol{x}_{i})^{T}\phi(\boldsymbol{x}_{j}))\)</span>为核函数。</p>
<h1 id="核方法">核方法</h1>
<p>根据(24)和(56)，给定训练样本<span class="math inline">\(\{(\boldsymbol{x}_{1},y_{1}),\dots,(\boldsymbol{x}_{m},y_{m})\}\)</span>，若不考虑偏移项<span class="math inline">\(b\)</span>，无论SVM还是SVR，学得的模型总能表述成<span class="math inline">\(\kappa(\boldsymbol{x},\boldsymbol{x}_{i})\)</span>的线性组合。 <strong><em>定理2 表示定理(representer theorem)</em></strong>：令 <span class="math inline">\(\mathbb{H}\)</span> 为核函数 <span class="math inline">\(\kappa\)</span> 对应的再生核希尔伯特空间，<span class="math inline">\(\Vert h\Vert_{\mathbb{H}}\)</span> 表示 <span class="math inline">\(\mathbb{H}\)</span> 空间中关于 <span class="math inline">\(h\)</span> 的范数，对于任意单调递增函数 <span class="math inline">\(\Omega:[0,\infty]\mapsto\mathbb{R}\)</span> 和任意非负损失函数 <span class="math inline">\(\ell:\mathbb{R}^{m}\mapsto[0,\infty]\)</span>，优化问题<span class="math display">\[
\min_{h\in\mathbb{H}}F(h)=\Omega\big(\Vert h\Vert_{\mathbb{H}}\big)+\ell\big(h(\boldsymbol{x}_{1}),\dots,h(\boldsymbol{x}_{m})\big)\tag{57}
\]</span>的解总可写为<span class="math display">\[
h^{*}(\boldsymbol{x})=\sum_{i=1}^{m}\alpha_{i}\kappa(\boldsymbol{x},h(\boldsymbol{x}_{i}))\tag{58}
\]</span>表示定理对损失函数没有限制，对正则化项 <span class="math inline">\(\Omega\)</span> 仅要求单调递增，甚至不要求 <span class="math inline">\(\Omega\)</span> 是凸函数，意味着对于一般的损失函数正则项，优化问题(57)的最优解 <span class="math inline">\(h^{*}(\boldsymbol{x})\)</span> 都可表示为核函数 <span class="math inline">\(\kappa(\boldsymbol{x},\boldsymbol{x}_{i})\)</span> 的线性组合。 通常将基于核函数的学习方法统称为<strong><em>“核方法”</em></strong> (kernel methods)。如通过“核化” (即引入核函数)来将线性学习器拓展为非线性学习器。以<strong><em>“核线性判别分析”</em></strong> (<a href="https://en.wikipedia.org/wiki/Kernel_Fisher_discriminant_analysis#Kernel_trick_with_LDA" target="_blank" rel="noopener">Kernelized Linear Discriminant Analysis</a>，KLDA) 为例，假设可通过某种映射 <span class="math inline">\(\phi:\boldsymbol{\chi}\mapsto\mathbb{F}\)</span> 将样本映射到一个特征空间 <span class="math inline">\(\mathbb{F}\)</span>，然后在 <span class="math inline">\(\mathbb{F}\)</span> 中执行线性判别分析，以求得<span class="math display">\[
h(\boldsymbol{x})=\boldsymbol{w}^{T}\phi(\boldsymbol{x})\tag{59}
\]</span>类似<a href="https://en.wikipedia.org/wiki/Linear_discriminant_analysis" target="_blank" rel="noopener">LDA</a>，KLDA的学习目标是<span class="math display">\[
\max_{\boldsymbol{w}}J(\boldsymbol{w})=\frac{\boldsymbol{w}^{T}\boldsymbol{S}_{b}^{\phi}\boldsymbol{w}}{\boldsymbol{w}^{T}\boldsymbol{S}_{\boldsymbol{w}}^{\phi}\boldsymbol{w}}\tag{60}
\]</span>其中 <span class="math inline">\(\boldsymbol{S}_{b}^{\phi}\)</span> 和 <span class="math inline">\(\boldsymbol{S}_{\boldsymbol{w}}^{\phi}\)</span> 分别为训练样本在特征空间 <span class="math inline">\(\mathbb{F}\)</span> 中的类间散度矩阵 (<a href="https://stats.stackexchange.com/questions/123490/what-is-the-correct-formula-for-between-class-scatter-matrix-in-lda" target="_blank" rel="noopener">between-class scatter matrix</a>) 和类内散度矩阵 (<a href="https://stats.stackexchange.com/questions/123490/what-is-the-correct-formula-for-between-class-scatter-matrix-in-lda" target="_blank" rel="noopener">within-class scatter matrix</a>)。令 <span class="math inline">\(X_{i}\)</span> 表示第 <span class="math inline">\(i\in\{0,1\}\)</span> 类样本的集合，其样本数为 <span class="math inline">\(m_{i}\)</span>；总样本数 <span class="math inline">\(m=m_{0}+m_{1}\)</span>。第 <span class="math inline">\(i\)</span> 类样本在特征空间 <span class="math inline">\(\mathbb{F}\)</span> 中的均值为<span class="math display">\[
\boldsymbol{\mu}_{i}^{\phi}=\frac{1}{m_{i}}\sum_{\boldsymbol{x}\in X_{i}}\phi(\boldsymbol{x})\tag{61}
\]</span>两个散度矩阵分别为<span class="math display">\[
\boldsymbol{S}_{b}^{\phi}=(\boldsymbol{\mu}_{1}^{\phi}-\boldsymbol{\mu}_{0}^{\phi})(\boldsymbol{\mu}_{1}^{\phi}-\boldsymbol{\mu}_{0}^{\phi})^{T}\tag{62}
\]</span><span class="math display">\[
\boldsymbol{S}_{\boldsymbol{w}}^{\phi}=\sum_{i=0}^{1}\sum_{\boldsymbol{x}\in X_{i}}\big(\phi(\boldsymbol{x})-\boldsymbol{\mu}_{i}^{\phi}\big)\big(\phi(\boldsymbol{x})-\boldsymbol{\mu}_{i}^{\phi}\big)^{T}\tag{63}
\]</span>通常映射 <span class="math inline">\(\phi\)</span> 的具体形式很难得到，因此使用核函数 <span class="math inline">\(\kappa(\boldsymbol{x},\boldsymbol{x}_{i})=\phi(\boldsymbol{x}_{i})^{T}\phi(\boldsymbol{x})\)</span> 来隐式地表达这个映射和特征空间 <span class="math inline">\(\mathbb{F}\)</span>。把 <span class="math inline">\(J(\boldsymbol{w})\)</span> 作为(57) 中的损失函数 <span class="math inline">\(\ell\)</span>，再令 <span class="math inline">\(\Omega=0\)</span>，由表示定理，函数 <span class="math inline">\(h(\boldsymbol{x})\)</span> 可写为<span class="math display">\[
h(\boldsymbol{x})=\sum_{i=1}^{m}\alpha_{i}\kappa(\boldsymbol{x},\boldsymbol{x}_{i})\tag{64}
\]</span>于是由(59)可得<span class="math display">\[
\boldsymbol{w}=\sum_{i=1}^{m}\alpha_{i}\phi(\boldsymbol{x}_{i})\tag{65}
\]</span>令 <span class="math inline">\(\boldsymbol{K}\in\mathbb{R}^{m\times m}\)</span> 为核函数 <span class="math inline">\(\kappa\)</span> 所对应的核矩阵，<span class="math inline">\((\boldsymbol{K})_{ij}=\kappa(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\)</span>。令 <span class="math inline">\(\boldsymbol{1}_{i}\in\{1,0\}^{m\times 1}\)</span> 为第 <span class="math inline">\(i\)</span> 类样本的指示向量，即 <span class="math inline">\(\boldsymbol{1}_{i}\)</span> 的第 <span class="math inline">\(j\)</span> 个分量为1当且仅当 <span class="math inline">\(\boldsymbol{x}_{j}\in X_{i}\)</span>，否则 <span class="math inline">\(\boldsymbol{1}_{i}\)</span> 的第 <span class="math inline">\(j\)</span> 个分量为0.再令<span class="math display">\[
\hat{\boldsymbol{\mu}}_{0}=\frac{1}{m_{0}}\boldsymbol{K}\boldsymbol{1}_{0}\tag{66}
\]</span><span class="math display">\[
\hat{\boldsymbol{\mu}}_{1}=\frac{1}{m_{1}}\boldsymbol{K}\boldsymbol{1}_{1}\tag{67}
\]</span><span class="math display">\[
\boldsymbol{M}=(\hat{\boldsymbol{\mu}}_{0}-\hat{\boldsymbol{\mu}}_{1})(\hat{\boldsymbol{\mu}}_{0}-\hat{\boldsymbol{\mu}}_{1})^{T}\tag{68}
\]</span><span class="math display">\[
\boldsymbol{N}=\boldsymbol{K}\boldsymbol{K}^{T}-\sum_{i=0}^{1}m_{i}\hat{\boldsymbol{\mu}}_{i}\hat{\boldsymbol{\mu}}_{i}^{T}\tag{69}
\]</span>于是(60)等价为<span class="math display">\[
\max_{\boldsymbol{\alpha}}J(\boldsymbol{\alpha})=\frac{\boldsymbol{\alpha}^{T}\boldsymbol{M}\boldsymbol{\alpha}}{\boldsymbol{\alpha}^{T}\boldsymbol{N}\boldsymbol{\alpha}}\tag{70}
\]</span>显然，使用线性判别分析求解方法即可得到 <span class="math inline">\(\boldsymbol{\alpha}\)</span>，进而可由(64)得到投影函数 <span class="math inline">\(h(\boldsymbol{x})\)</span>。</p>
<h1 id="习题">习题</h1>
<p><strong>6.1 试证明样本空间中任意点 <span class="math inline">\(\boldsymbol{x}\)</span> 到超平面 <span class="math inline">\((\boldsymbol{w},b)\)</span>的距离为公式(2)。</strong> Ans: 超平面 <span class="math inline">\((\boldsymbol{w},b)\)</span> 的平面法向量为 <span class="math inline">\(\boldsymbol{w}\)</span> ，任取平面上一点 <span class="math inline">\(\boldsymbol{x}_{0}\)</span>，有 <span class="math inline">\(\boldsymbol{w}^{T}\boldsymbol{x}_{0}+b=0\)</span>。<span class="math inline">\(\boldsymbol{x}\)</span> 到平面的距离就是 <span class="math inline">\(\boldsymbol{x}\)</span> 到 <span class="math inline">\(\boldsymbol{x}_{0}\)</span> 的距离往 <span class="math inline">\(\boldsymbol{w}\)</span> 方向的投影，就是 <span class="math inline">\(\frac{\vert\boldsymbol{w}^{T}(\boldsymbol{x}-\boldsymbol{x}_{0})\vert}{\vert\boldsymbol{w}\vert}=\frac{\vert\boldsymbol{w}^{T}\boldsymbol{x}+b\vert}{\vert\boldsymbol{w}\vert}\)</span>。</p>
<p><strong>6.2 试使用LIBSVM，在<a href="https://isaacchanghau.github.io/2017/05/04/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%91%A8%E5%BF%97%E5%8D%8E-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-2/">西瓜数据集3.0<span class="math inline">\(\alpha\)</span></a>上分别用线性核和高斯核训练一个SVM，并比较其支持向量的差别。</strong> Ans: 由于西瓜数据集3.0<span class="math inline">\(\alpha\)</span>数据量太小，我们这里直接采用<a href="https://archive.ics.uci.edu/ml/datasets.html" target="_blank" rel="noopener">UCI数据集</a>的<a href="https://archive.ics.uci.edu/ml/datasets/iris" target="_blank" rel="noopener">Iris数据集</a>，并使用<a href="http://www.cs.waikato.ac.nz/ml/weka/" target="_blank" rel="noopener">Weka机器学习包</a>进行试验。首先下载数据并转换为ARFF格式，如下： <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">@relation Iris</span><br><span class="line">@attribute sepallength NUMERIC</span><br><span class="line">@attribute sepalwidth NUMERIC</span><br><span class="line">@attribute petallength NUMERIC </span><br><span class="line">@attribute petalwidth NUMERIC</span><br><span class="line">@attribute class &#123;setosa, versicolor, virginica&#125;</span><br><span class="line">@data</span><br><span class="line">5.1,3.5,1.4,0.2,setosa</span><br><span class="line">4.9,3.0,1.4,0.2,setosa</span><br><span class="line">4.7,3.2,1.3,0.2,setosa</span><br><span class="line">4.6,3.1,1.5,0.2,setosa</span><br><span class="line">5.0,3.6,1.4,0.2,setosa</span><br><span class="line">...</span><br></pre></td></tr></table></figure></p>
<p>之后读入数据： <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Instances data = <span class="keyword">new</span> Instances(<span class="keyword">new</span> FileReader(<span class="string">"src/main/resources/iris.data"</span>));</span><br><span class="line">data.setClassIndex(data.numAttributes() - <span class="number">1</span>); <span class="comment">// set label</span></span><br><span class="line">data.randomize(<span class="keyword">new</span> Random(<span class="number">123</span>)); <span class="comment">// shuffling data</span></span><br></pre></td></tr></table></figure></p>
<p>现在我们使用Weka机器学习包的<a href="https://weka.wikispaces.com/LibSVM" target="_blank" rel="noopener">LibSVM</a>分别构建线性核和高斯核的SVM并训练数据： <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// build linear SVM</span></span><br><span class="line">LibSVM linearSVM = <span class="keyword">new</span> LibSVM();</span><br><span class="line">linearSVM.setSVMType(<span class="keyword">new</span> SelectedTag(LibSVM.SVMTYPE_C_SVC, LibSVM.TAGS_SVMTYPE));</span><br><span class="line">linearSVM.setKernelType(<span class="keyword">new</span> SelectedTag(LibSVM.KERNELTYPE_LINEAR, LibSVM.TAGS_KERNELTYPE));</span><br><span class="line">linearSVM.buildClassifier(data); <span class="comment">// build classifier and train</span></span><br><span class="line"><span class="comment">// build gaussian SVM</span></span><br><span class="line">LibSVM gaussSVM = <span class="keyword">new</span> LibSVM();</span><br><span class="line">gaussSVM.setSVMType(<span class="keyword">new</span> SelectedTag(LibSVM.SVMTYPE_C_SVC, LibSVM.TAGS_SVMTYPE));</span><br><span class="line">gaussSVM.setKernelType(<span class="keyword">new</span> SelectedTag(LibSVM.KERNELTYPE_RBF, LibSVM.TAGS_KERNELTYPE));</span><br><span class="line">gaussSVM.buildClassifier(data); <span class="comment">// build classifier and train</span></span><br></pre></td></tr></table></figure></p>
<p>由于Weka封装了LibSVM，无法直接使用Weka得到训练后模型的支持向量，这里我们使用<a href="https://gist.github.com/DavidWiesner/2de8a6b2b89ffeaa6d0f" target="_blank" rel="noopener">如下方法</a>得到训练后的<a href="https://github.com/cjlin1/libsvm/blob/master/java/libsvm/svm_model.java" target="_blank" rel="noopener">模型</a>参数： <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> svm_model <span class="title">getModel</span><span class="params">(LibSVM svm)</span> <span class="keyword">throws</span> IllegalAccessException, NoSuchFieldException </span>&#123;</span><br><span class="line">    Field modelField = svm.getClass().getDeclaredField(<span class="string">"m_Model"</span>);</span><br><span class="line">    modelField.setAccessible(<span class="keyword">true</span>);</span><br><span class="line">    <span class="keyword">return</span> (svm_model) modelField.get(svm);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>我们使用上面的函数来获得训练后的模型，并打印支持向量 <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">svm_model linearModel = getModel(linearSVM);</span><br><span class="line">svm_model gaussModel = getModel(gaussSVM);</span><br><span class="line"><span class="keyword">int</span>[] indices = linearModel.sv_indices;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i : indices) System.out.println(i + <span class="string">": "</span> + data.instance(i - <span class="number">1</span>));</span><br><span class="line">System.out.println();</span><br><span class="line">indices = gaussModel.sv_indices;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i : indices) System.out.println(i + <span class="string">": "</span> + data.instance(i - <span class="number">1</span>));</span><br></pre></td></tr></table></figure></p>
<p>于是我们得到线性核和高斯核模型的支持向量：</p>
<table>
<thead>
<tr class="header">
<th style="text-align: left;">线性核</th>
<th style="text-align: left;">高斯核</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">4: 5.4,3,4.5,1.5,versicolor</td>
<td style="text-align: left;">4: 5.4,3,4.5,1.5,versicolor</td>
</tr>
<tr class="even">
<td style="text-align: left;">5: 6.3,2.5,4.9,1.5,versicolor</td>
<td style="text-align: left;">5: 6.3,2.5,4.9,1.5,versicolor</td>
</tr>
<tr class="odd">
<td style="text-align: left;">37: 6.8,2.8,4.8,1.4,versicolor</td>
<td style="text-align: left;">13: 4.9,2.4,3.3,1,versicolor</td>
</tr>
<tr class="even">
<td style="text-align: left;">42: 6.7,3,5,1.7,versicolor</td>
<td style="text-align: left;">37: 6.8,2.8,4.8,1.4,versicolor</td>
</tr>
<tr class="odd">
<td style="text-align: left;">45: 6.2,2.2,4.5,1.5,versicolor</td>
<td style="text-align: left;">41: 6.7,3.1,4.7,1.5,versicolor</td>
</tr>
<tr class="even">
<td style="text-align: left;">61: 6.3,3.3,4.7,1.6,versicolor</td>
<td style="text-align: left;">42: 6.7,3,5,1.7,versicolor</td>
</tr>
<tr class="odd">
<td style="text-align: left;">63: 5.6,3,4.5,1.5,versicolor</td>
<td style="text-align: left;">45: 6.2,2.2,4.5,1.5,versicolor</td>
</tr>
<tr class="even">
<td style="text-align: left;">99: 5.9,3.2,4.8,1.8,versicolor</td>
<td style="text-align: left;">61: 6.3,3.3,4.7,1.6,versicolor</td>
</tr>
<tr class="odd">
<td style="text-align: left;">104: 6,2.7,5.1,1.6,versicolor</td>
<td style="text-align: left;">63: 5.6,3,4.5,1.5,versicolor</td>
</tr>
<tr class="even">
<td style="text-align: left;">108: 6.1,2.9,4.7,1.4,versicolor</td>
<td style="text-align: left;">77: 6.5,2.8,4.6,1.5,versicolor</td>
</tr>
<tr class="odd">
<td style="text-align: left;">124: 6.9,3.1,4.9,1.5,versicolor</td>
<td style="text-align: left;">82: 6,2.9,4.5,1.5,versicolor</td>
</tr>
<tr class="even">
<td style="text-align: left;">126: 5.1,2.5,3,1.1,versicolor</td>
<td style="text-align: left;">99: 5.9,3.2,4.8,1.8,versicolor</td>
</tr>
<tr class="odd">
<td style="text-align: left;">7: 6.3,2.5,5,1.9,virginica</td>
<td style="text-align: left;">104: 6,2.7,5.1,1.6,versicolor</td>
</tr>
<tr class="even">
<td style="text-align: left;">15: 4.9,2.5,4.5,1.7,virginica</td>
<td style="text-align: left;">108: 6.1,2.9,4.7,1.4,versicolor</td>
</tr>
<tr class="odd">
<td style="text-align: left;">21: 7.2,3,5.8,1.6,virginica</td>
<td style="text-align: left;">113: 7,3.2,4.7,1.4,versicolor</td>
</tr>
<tr class="even">
<td style="text-align: left;">33: 6,3,4.8,1.8,virginica</td>
<td style="text-align: left;">124: 6.9,3.1,4.9,1.5,versicolor</td>
</tr>
<tr class="odd">
<td style="text-align: left;">39: 6.3,2.8,5.1,1.5,virginica</td>
<td style="text-align: left;">126: 5.1,2.5,3,1.1,versicolor</td>
</tr>
<tr class="even">
<td style="text-align: left;">57: 6.3,2.7,4.9,1.8,virginica</td>
<td style="text-align: left;">146: 6,3.4,4.5,1.6,versicolor</td>
</tr>
<tr class="odd">
<td style="text-align: left;">64: 6.5,3,5.2,2,virginica</td>
<td style="text-align: left;">148: 5,2,3.5,1,versicolor</td>
</tr>
<tr class="even">
<td style="text-align: left;">72: 6.5,3.2,5.1,2,virginica</td>
<td style="text-align: left;">7: 6.3,2.5,5,1.9,virginica</td>
</tr>
<tr class="odd">
<td style="text-align: left;">90: 6.1,3,4.9,1.8,virginica</td>
<td style="text-align: left;">15: 4.9,2.5,4.5,1.7,virginica</td>
</tr>
<tr class="even">
<td style="text-align: left;">98: 5.9,3,5.1,1.8,virginica</td>
<td style="text-align: left;">21: 7.2,3,5.8,1.6,virginica</td>
</tr>
<tr class="odd">
<td style="text-align: left;">139: 6.2,2.8,4.8,1.8,virginica</td>
<td style="text-align: left;">25: 6.1,2.6,5.6,1.4,virginica</td>
</tr>
<tr class="even">
<td style="text-align: left;">147: 6,2.2,5,1.5,virginica</td>
<td style="text-align: left;">33: 6,3,4.8,1.8,virginica</td>
</tr>
<tr class="odd">
<td style="text-align: left;">56: 4.5,2.3,1.3,0.3,setosa</td>
<td style="text-align: left;">39: 6.3,2.8,5.1,1.5,virginica</td>
</tr>
<tr class="even">
<td style="text-align: left;">89: 5.1,3.3,1.7,0.5,setosa</td>
<td style="text-align: left;">48: 6.3,3.3,6,2.5,virginica</td>
</tr>
<tr class="odd">
<td style="text-align: left;">117: 4.8,3.4,1.9,0.2,setosa</td>
<td style="text-align: left;">57: 6.3,2.7,4.9,1.8,virginica</td>
</tr>
<tr class="even">
<td style="text-align: left;">done</td>
<td style="text-align: left;">…</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Total: 27</td>
<td style="text-align: left;">Total: 45</td>
</tr>
</tbody>
</table>
<p><strong>6.3 选择两个<a href="http://archive.ics.uci.edu/ml/" target="_blank" rel="noopener">UCI</a>数据集，分别用线性核和高斯核训练一个SVM，并与BP神经网络核C4.5决策树进行试验比较。</strong> Ans: 这里也选用UCI的Iris数据集进行测试。首先我们将数据拆分为训练集(80%)和测试集(20%)： <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// load train dataset</span></span><br><span class="line">Instances trainData = <span class="keyword">new</span> Instances(<span class="keyword">new</span> FileReader(<span class="string">"src/main/resources/iris-train.data"</span>));</span><br><span class="line">trainData.setClassIndex(trainData.numAttributes() - <span class="number">1</span>);</span><br><span class="line">trainData.randomize(<span class="keyword">new</span> Random(<span class="number">123</span>));</span><br><span class="line"><span class="comment">// load test dataset</span></span><br><span class="line">Instances testData = <span class="keyword">new</span> Instances(<span class="keyword">new</span> FileReader(<span class="string">"src/main/resources/iris-test.data"</span>));</span><br><span class="line">testData.setClassIndex(testData.numAttributes() - <span class="number">1</span>);</span><br></pre></td></tr></table></figure></p>
<p>之后我们用Weka机器学习包分别构建线性核和高斯核的SVM、C4.5决策树和BP神经网络，如下(SVM的构建与6.2相同，这里省略)： <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// build C4.5 decision tree</span></span><br><span class="line">J48 tree = <span class="keyword">new</span> J48();</span><br><span class="line">tree.setUnpruned(<span class="keyword">false</span>);</span><br><span class="line">tree.setReducedErrorPruning(<span class="keyword">true</span>);</span><br><span class="line">tree.buildClassifier(trainData);</span><br><span class="line"><span class="comment">// build BP neural networks</span></span><br><span class="line">MultilayerPerceptron mlp = <span class="keyword">new</span> MultilayerPerceptron();</span><br><span class="line">mlp.setLearningRate(<span class="number">0.3</span>);</span><br><span class="line">mlp.setMomentum(<span class="number">0.2</span>);</span><br><span class="line">mlp.setSeed(<span class="number">123</span>);</span><br><span class="line">mlp.setHiddenLayers(<span class="string">"3"</span>);</span><br><span class="line">mlp.setTrainingTime(<span class="number">10000</span>);</span><br><span class="line">mlp.buildClassifier(trainData);</span><br></pre></td></tr></table></figure></p>
<p>之后我们使用<code>Evaluation</code>对构建的四个模型分别进行评价： <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Evaluate</span></span><br><span class="line">Evaluation evalLinear = <span class="keyword">new</span> Evaluation(testData);</span><br><span class="line">evalLinear.evaluateModel(linearSVM, testData);</span><br><span class="line">System.out.println(<span class="string">"Linear SVM"</span> + <span class="string">"\n"</span> + evalLinear.toSummaryString() + <span class="string">"\n"</span> + evalLinear.toMatrixString() + <span class="string">"\n"</span>);</span><br><span class="line">Evaluation evalGauss = <span class="keyword">new</span> Evaluation(testData);</span><br><span class="line">evalGauss.evaluateModel(gaussSVM, testData);</span><br><span class="line">System.out.println(<span class="string">"Gaussian SVM"</span> + <span class="string">"\n"</span> + evalGauss.toSummaryString() + <span class="string">"\n"</span> + evalGauss.toMatrixString() + <span class="string">"\n"</span>);</span><br><span class="line">Evaluation evalTree = <span class="keyword">new</span> Evaluation(testData);</span><br><span class="line">evalTree.evaluateModel(tree, testData);</span><br><span class="line">System.out.println(<span class="string">"C4.5 Decision Tree"</span> + <span class="string">"\n"</span> + evalTree.toSummaryString() + <span class="string">"\n"</span> + evalTree.toMatrixString() + <span class="string">"\n"</span>);</span><br><span class="line">Evaluation evalMlp = <span class="keyword">new</span> Evaluation(testData);</span><br><span class="line">evalMlp.evaluateModel(mlp, testData);</span><br><span class="line">System.out.println();</span><br><span class="line">System.out.println(<span class="string">"BP Neural Network"</span> + <span class="string">"\n"</span> + evalMlp.toSummaryString() + <span class="string">"\n"</span> + evalMlp.toMatrixString());</span><br></pre></td></tr></table></figure></p>
<p>得到如下结果： <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line">Linear SVM</span><br><span class="line">Correctly Classified Instances          30              100      %</span><br><span class="line">Incorrectly Classified Instances         0                0      %</span><br><span class="line">Kappa statistic                          1     </span><br><span class="line">Mean absolute error                      0     </span><br><span class="line">Root mean squared error                  0     </span><br><span class="line">Relative absolute error                  0      %</span><br><span class="line">Root relative squared error              0      %</span><br><span class="line">Total Number of Instances               30     </span><br><span class="line">=== Confusion Matrix ===</span><br><span class="line">  a  b  c   &lt;-- classified as</span><br><span class="line"> 10  0  0 |  a = setosa</span><br><span class="line">  0 10  0 |  b = versicolor</span><br><span class="line">  0  0 10 |  c = virginica</span><br><span class="line"></span><br><span class="line">Gaussian SVM</span><br><span class="line">Correctly Classified Instances          30              100      %</span><br><span class="line">Incorrectly Classified Instances         0                0      %</span><br><span class="line">Kappa statistic                          1     </span><br><span class="line">Mean absolute error                      0     </span><br><span class="line">Root mean squared error                  0     </span><br><span class="line">Relative absolute error                  0      %</span><br><span class="line">Root relative squared error              0      %</span><br><span class="line">Total Number of Instances               30     </span><br><span class="line">=== Confusion Matrix ===</span><br><span class="line">  a  b  c   &lt;-- classified as</span><br><span class="line"> 10  0  0 |  a = setosa</span><br><span class="line">  0 10  0 |  b = versicolor</span><br><span class="line">  0  0 10 |  c = virginica</span><br><span class="line"></span><br><span class="line">C4.5 Decision Tree</span><br><span class="line">Correctly Classified Instances          29               96.6667 %</span><br><span class="line">Incorrectly Classified Instances         1                3.3333 %</span><br><span class="line">Kappa statistic                          0.95  </span><br><span class="line">Mean absolute error                      0.0466</span><br><span class="line">Root mean squared error                  0.149 </span><br><span class="line">Relative absolute error                 10.4945 %</span><br><span class="line">Root relative squared error             31.6147 %</span><br><span class="line">Total Number of Instances               30     </span><br><span class="line">=== Confusion Matrix ===</span><br><span class="line">  a  b  c   &lt;-- classified as</span><br><span class="line">  9  1  0 |  a = setosa</span><br><span class="line">  0 10  0 |  b = versicolor</span><br><span class="line">  0  0 10 |  c = virginica</span><br><span class="line"></span><br><span class="line">BP Neural Network</span><br><span class="line">Correctly Classified Instances          30              100      %</span><br><span class="line">Incorrectly Classified Instances         0                0      %</span><br><span class="line">Kappa statistic                          1     </span><br><span class="line">Mean absolute error                      0.0062</span><br><span class="line">Root mean squared error                  0.0121</span><br><span class="line">Relative absolute error                  1.3956 %</span><br><span class="line">Root relative squared error              2.5607 %</span><br><span class="line">Total Number of Instances               30     </span><br><span class="line">=== Confusion Matrix ===</span><br><span class="line">  a  b  c   &lt;-- classified as</span><br><span class="line"> 10  0  0 |  a = setosa</span><br><span class="line">  0 10  0 |  b = versicolor</span><br><span class="line">  0  0 10 |  c = virginica</span><br></pre></td></tr></table></figure></p>
<p><strong>6.4 试讨论线性判别分析与线性核支持向量机在何种条件下等价。</strong> Ans: 当线性SVM和LDA求出的 <span class="math inline">\(\boldsymbol{w}\)</span> 互相垂直时，两者是等价的，SVM此时也就比LDA多了个偏移项<span class="math inline">\(b\)</span>。因为首先，如果可以使用软间隔的线性SVM，其实线性可分这个条件是不必要的，如果是硬间隔线性SVM，那么线性可分是必要条件。这个题只说了是线性SVM，就没必要关心数据是不是可分，毕竟LDA是都可以处理的。其次假如当前样本线性可分，且SVM与LDA求出的结果相互垂直。当SVM的支持向量固定时，再加入新的样本，并不会改变求出的w，但是新加入的样本会改变原类型数据的协方差和均值，从而导致LDA求出的结果发生改变。这个时候两者的 <span class="math inline">\(\boldsymbol{w}\)</span> 就不垂直了，但是数据依然是可分的。</p>
<p><strong>6.5 试述高斯核SVM与RBF神经网络之间的联系。</strong> Ans: RBF网络的径向基函数与SVM都可以采用高斯核，也就分别得到了高斯核RBF网络与高斯核SVM。神经网络是最小化累计误差，将参数作为惩罚项，而SVM相反，主要是最小化参数，将误差作为惩罚项。在二分类问题中，如果将RBF中隐层数为样本个数，且每个样本中心就是样本参数，得出的RBF网络与核SVM基本等价，非支持向量将得到很小的<span class="math inline">\(\boldsymbol{w}\)</span>。</p>
<p><strong>6.6 试分析SVM对噪声敏感的原因。</strong> Ans: SVM的目的是求出与支持向量有最大化距离的直线，以每个样本为圆心，该距离为半径做圆，可以近似认为圆内的点与该样本属于相同分类。如果出现了噪声，那么这个噪声所带来的错误分类也将最大化，所以SVM对噪声是很敏感的。</p>
<p><strong>6.7 试给出(52)的完整KKT条件。</strong> Ans: 非等式约束写成拉格朗日乘子式，取最优解要满足两个条件 - 拉格朗日乘子式对所有非拉格朗日参数的一阶偏导为0。 - 非等式约束对应的拉格朗日项，要么非等式的等号成立，要么对应的拉格朗日参数为0。</p>
<p>因此完整的KKT条件为<span class="math display">\[\begin{cases}
\boldsymbol{w}=\sum_{i=1}^{m}(\hat{\alpha}_{i}-\alpha_{i})\boldsymbol{x}_{i},\\
0=\sum_{i=1}^{m}(\hat{\alpha}_{i}-\alpha_{i}),\\
C=\alpha_{i}+\mu_{i},\\
C=\hat{\alpha}_{i}+\hat{\mu}_{i},\\
\alpha_{i}\big(f(\boldsymbol{x}_{i})-y_{i}-\epsilon-\xi_{i}\big)=0,\\
\hat{\alpha}_{i}\big(y_{i}-f(\boldsymbol{x}_{i})-\epsilon-\hat{\xi}_{i}\big)=0,\\
(C-\alpha_{i})\xi_{i}=0,\\
(C-\hat{\alpha}_{i})\hat{\xi}_{i}=0.
\end{cases}\]</span></p>
<p><strong>6.8 以西瓜数据集3.0<span class="math inline">\(\alpha\)</span>的“密度”为输入，“含糖率”为输出，试使用LIBSVM训练一个SVR。</strong> Ans: 首先将西瓜数据集转换为ARFF格式，然后搭建SVR进行试验。 <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Instances data = <span class="keyword">new</span> Instances(<span class="keyword">new</span> FileReader(<span class="string">"src/main/resources/watermelon.data"</span>));</span><br><span class="line">data.setClassIndex(data.numAttributes() - <span class="number">1</span>);</span><br><span class="line">LibSVM svr = <span class="keyword">new</span> LibSVM();</span><br><span class="line">svr.setSVMType(<span class="keyword">new</span> SelectedTag(LibSVM.SVMTYPE_EPSILON_SVR, LibSVM.TAGS_SVMTYPE));</span><br><span class="line">svr.setKernelType(<span class="keyword">new</span> SelectedTag(LibSVM.KERNELTYPE_RBF, LibSVM.TAGS_KERNELTYPE));</span><br><span class="line">svr.buildClassifier(data);</span><br><span class="line"><span class="keyword">for</span> (Instance inst : data)  System.out.println(svr.classifyInstance(inst) + <span class="string">"\t"</span> + inst.classValue());</span><br><span class="line">Evaluation eval = <span class="keyword">new</span> Evaluation(data);</span><br><span class="line">eval.evaluateModel(svr, data);</span><br><span class="line">System.out.println(eval.toSummaryString());</span><br></pre></td></tr></table></figure></p>
<p>得到结果： <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">0.23163745020672713 0.46</span><br><span class="line">0.24295566116372483 0.376</span><br><span class="line">0.22201597399900289 0.264</span><br><span class="line">0.21799998153899916 0.318</span><br><span class="line">0.2099638655436369  0.215</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">Correlation coefficient                  0.1952</span><br><span class="line">Mean absolute error                      0.0963</span><br><span class="line">Root mean squared error                  0.1142</span><br><span class="line">Relative absolute error                101.723  %</span><br><span class="line">Root relative squared error             98.1963 %</span><br><span class="line">Total Number of Instances               17</span><br></pre></td></tr></table></figure></p>
<p><strong>6.9 试使用核技巧推广对率回归，产生“核对率回归”。</strong> Ans: 由表示定理可知，一般优化问题的解可 <span class="math inline">\(h(\boldsymbol{x})\)</span> 以写成核函数的线性组合。即<span class="math display">\[
h(\boldsymbol{x})=\sum_{i=1}^{m}\boldsymbol{w}\kappa(\boldsymbol{x},\boldsymbol{x}_{i})
\]</span>可推出<span class="math display">\[
\boldsymbol{w}=\sum_{i=1}^{m}\alpha_{i}\phi(\boldsymbol{x}_{i})
\]</span>其中 <span class="math inline">\(\phi(\boldsymbol{x})\)</span> 是 <span class="math inline">\(\boldsymbol{x}\)</span> 在更高维的映射，于是 <span class="math inline">\(\boldsymbol{w}^{T}\boldsymbol{x}+b\)</span> 可以改写为<span class="math display">\[
\sum_{i=1}^{m}\alpha_{i}\phi(\boldsymbol{x}_{i})\phi(\boldsymbol{x})=\sum_{i=1}^{m}\alpha_{i}\kappa(\boldsymbol{x},\boldsymbol{x}_{i})+b
\]</span>令<span class="math inline">\(\boldsymbol{\beta}=(\boldsymbol{\alpha};b)\)</span>，<span class="math inline">\(\hat{\boldsymbol{t}}_{i}=(\kappa_{i};1)\)</span>，其中 <span class="math inline">\(\kappa_{i}\)</span> 表示核矩阵 <span class="math inline">\(\kappa\)</span> 的第 <span class="math inline">\(i\)</span> 列，可得<span class="math display">\[
\mathcal{l}(\boldsymbol{\beta})=\sum_{i}\big(-y_{i}\boldsymbol{\beta}^{T}\boldsymbol{t}_{i}+\ln(1+e^{\boldsymbol{\beta}^{T}\boldsymbol{t}_{i}})\big)
\]</span>之后的推导于线性回归中的<a href="https://isaacchanghau.github.io/2017/05/04/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%91%A8%E5%BF%97%E5%8D%8E-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-2/">对数几率回归</a>相同。</p>
<p><strong>6.10 试设计一个能显著减少SVM中支持向量的数目儿不显著降低泛化性能的方法。</strong> Ans: 对于线性的SVM，三个属性不完全一样的支持向量就能确定这个SVM，而其他的落在边缘上的点都可以舍弃。</p>
<p>注：以上习题的代码试验并没有考虑模型的优化问题，本人本身对Weka并不熟练，只是知道一些用法，所以直接使用Weka进行模型搭建和输出的演示。本人更习惯使用Spark的MLlib，但是由于MLlib对SVM的实现很少，只实现了线性二分类。没有非线性(核函数)，也没有多分类和回归，这也是无奈之举。(其实使用Python的Scikit-Learn机器学习包更方便)。</p>
<h1 id="spark-mllib实现二分类svm">Spark MLlib实现二分类SVM</h1>
<p>这里我们同样使用Iris数据集，由于Spark MLlib 中的SVM模型只支持线性二分类，所以我们去掉<code>Iris-virginica</code>类型数据，只保留<code>Iris-versicolor</code>(记为1)和<code>Iris-setosa</code>(记为0)进行实验。 <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SVM</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">parsingIris</span> </span>(str: <span class="type">String</span>): <span class="type">LabeledPoint</span> = &#123; <span class="comment">// create Iris parser: CSV Data =&gt; LabeledPoint</span></span><br><span class="line">    <span class="keyword">val</span> fields = str.split(<span class="string">","</span>)</span><br><span class="line">    assert(fields.size == <span class="number">5</span>)</span><br><span class="line">    <span class="keyword">val</span> label = fields(<span class="number">4</span>) <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> <span class="string">"Iris-setosa"</span> =&gt; <span class="number">0.0</span></span><br><span class="line">      <span class="keyword">case</span> <span class="string">"Iris-versicolor"</span> =&gt; <span class="number">1.0</span></span><br><span class="line">      <span class="keyword">case</span> <span class="string">"Iris-virginica"</span> =&gt; <span class="number">2.0</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="type">LabeledPoint</span>.apply(label, <span class="type">Vectors</span>.dense(fields(<span class="number">0</span>).toDouble, fields(<span class="number">1</span>).toDouble, fields(<span class="number">2</span>).toDouble, fields(<span class="number">3</span>).toDouble))</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span> </span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="type">Logger</span>.getLogger(<span class="string">"org"</span>).setLevel(<span class="type">Level</span>.<span class="type">OFF</span>) <span class="comment">// close logger</span></span><br><span class="line">    <span class="comment">/** create Session */</span></span><br><span class="line">    <span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder().master(<span class="string">"local"</span>).appName(<span class="string">"SVM"</span>).getOrCreate()</span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line">    <span class="comment">/** load data and transform to dataframe */</span></span><br><span class="line">    <span class="keyword">val</span> iris : <span class="type">RDD</span>[<span class="type">LabeledPoint</span>] = spark.read.textFile(<span class="keyword">new</span> <span class="type">ClassPathResource</span>(<span class="string">"iris.txt"</span>).getFile.getAbsolutePath)</span><br><span class="line">      .map(parsingIris) <span class="comment">// convert Iris data to LabeledPoint</span></span><br><span class="line">      .filter(e =&gt; e.label != <span class="number">2.0</span>) <span class="comment">// drop third class to build a two classes data</span></span><br><span class="line">      .rdd <span class="comment">// convert to RDD</span></span><br><span class="line">    <span class="keyword">val</span> splits = iris.randomSplit(<span class="type">Array</span>(<span class="number">0.8</span>, <span class="number">0.2</span>), seed=<span class="number">123</span>L) <span class="comment">// 80% for training, 20% for testing</span></span><br><span class="line">    <span class="keyword">val</span> training = splits(<span class="number">0</span>).cache()</span><br><span class="line">    <span class="keyword">val</span> test = splits(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">val</span> model: <span class="type">SVMModel</span> = <span class="type">SVMWithSGD</span>.train(training, <span class="number">1000</span>) <span class="comment">// build model</span></span><br><span class="line">    model.clearThreshold() <span class="comment">// to get the score, not classified result</span></span><br><span class="line">    println(<span class="string">"Weight: "</span>.concat(model.weights.toString)) <span class="comment">// print trained weight</span></span><br><span class="line">    println(<span class="string">"Test Result: "</span>)</span><br><span class="line">    test.map(point =&gt; (model.predict(point.features), <span class="keyword">if</span>(model.predict(point.features) &gt; <span class="number">0</span>) <span class="number">1.0</span> <span class="keyword">else</span> <span class="number">0.0</span>, point.label)).foreach(println) <span class="comment">// print score, classified result and label</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>以下是分类结果： <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">Weight: [-0.5139153688335307,-1.5350451605908375,2.1700813440361717,0.8963829165055544]</span><br><span class="line">Test Result: </span><br><span class="line">(-3.9059303241050616,0.0,0.0)</span><br><span class="line">(-4.327164416860141,0.0,0.0)</span><br><span class="line">(-3.688252095110469,0.0,0.0)</span><br><span class="line">(-3.495468123629213,0.0,0.0)</span><br><span class="line">(-5.020411486531796,0.0,0.0)</span><br><span class="line">(-5.582160204156088,0.0,0.0)</span><br><span class="line">(-3.3835161794399573,0.0,0.0)</span><br><span class="line">(-4.610619380646923,0.0,0.0)</span><br><span class="line">(-3.676140013649289,0.0,0.0)</span><br><span class="line">(-5.6212847686519805,0.0,0.0)</span><br><span class="line">(-4.172989806210081,0.0,0.0)</span><br><span class="line">(-4.969019949648443,0.0,0.0)</span><br><span class="line">(2.908737548495826,1.0,1.0)</span><br><span class="line">(3.3041995740088774,1.0,1.0)</span><br><span class="line">(2.8216798618199084,1.0,1.0)</span><br><span class="line">(1.6480335988062373,1.0,1.0)</span><br><span class="line">(4.08563451107511,1.0,1.0)</span><br><span class="line">(4.325889225283133,1.0,1.0)</span><br><span class="line">(3.57481724420649,1.0,1.0)</span><br><span class="line">(2.7216874014911334,1.0,1.0)</span><br><span class="line">(2.415040975436961,1.0,1.0)</span><br><span class="line">(2.8969329555616268,1.0,1.0)</span><br></pre></td></tr></table></figure></p>

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        
  <ul class="post-copyright">
    <li class="post-copyright-author">
      <strong>Author:</strong>
      Isaac Changhau
    </li>
    <li class="post-copyright-link">
      <strong>Link:</strong>
      <a href="https://isaacchanghau.github.io/2017/08/04/机器学习-周志华-学习笔记-3/" title="机器学习(周志华)--学习笔记(三) 支持向量机(Support Vector Machine)">https://isaacchanghau.github.io/2017/08/04/机器学习-周志华-学习笔记-3/</a>
    </li>
    <li class="post-copyright-license">
      <strong>Notice: </strong>
      All articles are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/3.0/" rel="external nofollow" target="_blank">CC BY-NC-SA 3.0</a> unless stating additionally. Contact me via email for questions or discussion.
    </li>
  </ul>


      
    </div>

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/java/" rel="tag"># java</a>
          
            <a href="/tags/machine-learning/" rel="tag"># machine learning</a>
          
            <a href="/tags/spark/" rel="tag"># spark</a>
          
            <a href="/tags/scala/" rel="tag"># scala</a>
          
            <a href="/tags/support-vector-machine/" rel="tag"># support vector machine</a>
          
            <a href="/tags/weka/" rel="tag"># weka</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2017/08/02/Seq2Seq-Learning-and-Neural-Conversational-Model/" rel="next" title="Seq2Seq Learning and Neural Conversational Model">
                <i class="fa fa-chevron-left"></i> Seq2Seq Learning and Neural Conversational Model
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2017/08/08/机器学习-周志华-学习笔记-4/" rel="prev" title="机器学习(周志华)--学习笔记(四) 集成学习(Ensemble Learning)">
                机器学习(周志华)--学习笔记(四) 集成学习(Ensemble Learning) <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            Overview
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.gif"
               alt="Isaac Changhau" />
          <p class="site-author-name" itemprop="name">Isaac Changhau</p>
           
              <p class="site-description motion-element" itemprop="description">Stay Hungry, Stay Foolish</p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives/">
                <span class="site-state-item-count">43</span>
                <span class="site-state-item-name">posts</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/categories/index.html">
                <span class="site-state-item-count">8</span>
                <span class="site-state-item-name">categories</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">40</span>
                <span class="site-state-item-name">tags</span>
              </a>
            </div>
          

        </nav>

        
          <div class="feed-link motion-element">
            <a href="/atom.xml" rel="alternate">
              <i class="fa fa-rss"></i>
              RSS
            </a>
          </div>
        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="mailto:isaac.changhau@gmail.com" target="_blank" title="Email">
                  
                    <i class="fa fa-fw fa-envelope"></i>
                  
                  Email
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://www.linkedin.com/in/isaac-changhau" target="_blank" title="Linkedin">
                  
                    <i class="fa fa-fw fa-linkedin"></i>
                  
                  Linkedin
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://github.com/IsaacChanghau" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                  GitHub
                </a>
              </span>
            
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#间隔与支持向量"><span class="nav-number">1.</span> <span class="nav-text">间隔与支持向量</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#对偶问题"><span class="nav-number">2.</span> <span class="nav-text">对偶问题</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#核函数"><span class="nav-number">3.</span> <span class="nav-text">核函数</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#软间隔与正则化"><span class="nav-number">4.</span> <span class="nav-text">软间隔与正则化</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#支持向量回归"><span class="nav-number">5.</span> <span class="nav-text">支持向量回归</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#核方法"><span class="nav-number">6.</span> <span class="nav-text">核方法</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#习题"><span class="nav-number">7.</span> <span class="nav-text">习题</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#spark-mllib实现二分类svm"><span class="nav-number">8.</span> <span class="nav-text">Spark MLlib实现二分类SVM</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      
        <div class="back-to-top">
          <i class="fa fa-arrow-up"></i>
          
            <span id="scrollpercent"><span>0</span>%</span>
          
        </div>
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy;  2017 - 
  <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-star"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Isaac Changhau</span>
</div>


<div class="powered-by">
  Powered by <a class="theme-link" href="https://hexo.io">Hexo</a>
</div>

<div class="theme-info">
  Theme -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT
  </a>
</div>


        

        
      </div>
    </footer>

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.1"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.1"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.1"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.1"></script>



  


  




	





  





  





  






  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->





  

  

  

  
  


  

  

</body>
</html>
