<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[How to update multiple tensors using a single value with tf.scan [Reprinted]]]></title>
    <url>%2F2018%2F02%2F20%2FHow-to-update-multiple-tensors-using-a-single-value-with-tf-scan%2F</url>
    <content type="text"><![CDATA[This post is reprinted from Wilka Carvalho · How to update multiple tensors using a single value with tf.scan. Corresponding Jupyter Notebook I assume that you have a set of Tensors that you want to update with a sequence iteratively. E.g. you have a neural network that you’d like to update with a point at time t in a sequence and values from the network at time t-1. If you want to see this in full-fledged use, look at my jupyter notebook where I recreate the Variational Recurrent Neural Network! This is the definition of scan: 12345678910tf.scan( fn, elems, initializer=None, parallel_iterations=10, back_prop=True, swap_memory=False, infer_shape=True, name=None) fn should follow the form fn(parameter_that_changes,parameter_you_change_with). This means that you can assume that your input from elem will always go to parameter_you_change_with, and that what you return should be parameter_that_changes. Writing it like a function looks something like the following 12def fn(x, elem): return new_x where new_x will be x the next time fn is called. That took me some time to figure out. 123456789101112131415import tensorflow as tfimport numpy as np# Super Simple scan example as per: https://stackoverflow.com/questions/43841782/scan-function-in-theano-and-tensorflowdef f(x, ys): (y1, y2) = ys return x + y1 * y2a = tf.constant([1, 2, 3, 4, 5])b = tf.constant([2, 3, 2, 2, 1])c = tf.scan(f, (a, b), initializer=0)with tf.Session() as sess: print(sess.run(c)) output: 1[ 2 8 14 22 27] Another example: 1234567891011121314151617181920212223242526# updating 3 tensors with a single sequencea1 = tf.Variable([0,0])a2 = tf.Variable([1,1])a3 = tf.Variable([2,2])sequence = tf.Variable([1,2,3])# using tf.multiply istead of '*', e.g. tf.multiply(x,2) instead of 2*x was key to this compiling...def replace_one(old, x): a1, a2, a3 = old a1 = tf.add(a1,tf.multiply(x,1)) a2 = tf.add(a2,tf.multiply(x,2)) a3 = tf.add(a3,tf.multiply(x,3)) return [a1,a2,a3]# key things that worked: initializer needed to match output. # dumb mistake I can see tripping up many peopleupdate = tf.scan(replace_one, sequence, initializer=[a1, a2, a3])a1 = a1.assign(a2)with tf.Session() as sess: sess.run(tf.global_variables_initializer()) print(sess.run(update)) outputs: 1234567[array([[1, 1], [3, 3], [6, 6]], dtype=int32), array([[ 3, 3], [ 7, 7], [13, 13]], dtype=int32), array([[ 5, 5], [11, 11], [20, 20]], dtype=int32)] A few notes So this ws more difficult to implement than I expected. I had to get all the ingredients perfectly right. While I can assign outside of scan, for some reason the tensors a1, a2, a3 couldn’t be assigned, i.e. a1.assign(tf.add(a1,tf.multiply(x,1))), inside of scan You can have all your values inside a single tensor for the initializer and update them via indexing. This also doesn’t work. i.e. with T=tf.concat([a1,a2,a3]), you can’t do T[0]=x I spent a long time trying to manually concatonate the values so that I could track them in the future only to learn that scan does this by default!! E.g., for a1, the corresponding output vector is [a1+1, a1+1+2, a1+1+2+3] since the elements were [1,2,3]. Hope you found this useful !!]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python魔术方法 - 让自定义类更像内置类型 [转载]]]></title>
    <url>%2F2018%2F02%2F19%2Fpython%E9%AD%94%E6%9C%AF%E6%96%B9%E6%B3%95-%E8%AE%A9%E8%87%AA%E5%AE%9A%E4%B9%89%E7%B1%BB%E6%9B%B4%E5%83%8F%E5%86%85%E7%BD%AE%E7%B1%BB%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[本文转载自程序员大咖·python 魔术方法: 让自定义类更像内置类型。 Python的魔术方法是Python中那些预定义的像__XXX__类型的函数。使用Python的魔术方法的最大优势在于python提供了简单的方法让对象可以表现得像内置类型一样。 __str__函数 __str__函数用于处理打印实例本身的时候的输出内容。如果没有覆写该函数，则默认输出一个对象名称和内存地址。例如： 12345&gt;&gt;&gt; class Student(object):... def __init__(self,name):... self._name = name...&gt;&gt;&gt; print Student() 输出： 1&lt;__main__.Student object at 0x0000000002A929E8&gt;. 那么我们如何让输出的结果可读性更高一点呢？我们可以覆写__str__函数。例如 1234567&gt;&gt;&gt; class Student(object):... def __init__(self, name):... self._name = name... def __str__(self):... return "I'm a student, named %s" % self._name...&gt;&gt;&gt; print Student("Charlie") 输出结果就是：I’m a student, named Charlie. 我们将str()函数作用于该对象的时候，其实是调用了该对象的__str__函数。 __repr__函数 __repr__也是将对象序列化，但是__repr__更多的是给python编译器看的。__str__更多的是可读性(readable)。 我们将repr()函数作用于摸某一个对象的时候，调用的其实就是该函数的__repr__函数。与repr()成对的是eval()函数。eval()函数是将序列化后的对象重新转为对象。前提是该对象实现了__repr__函数。 &gt; 上面这一段话基于自己的理解，不知道对错。 123456&gt;&gt;&gt; item = [1,2,3]&gt;&gt;&gt; repr(item)'[1, 2, 3]'&gt;&gt;&gt; other_item = eval(repr(item))&gt;&gt;&gt; other_item[1]2 __iter__函数 我们经常对list或者tuple使用for…in…来迭代。那是list继承自Iterable。Iterable实现了__iter__函数。 要想将一个自定义的对象变成一个可迭代的对象，那么必须要实现两个方法：__iter__和next. __iter__函数返回一个对象。迭代的时候则会不断地调用next函数拿到下一个值，直到捕获到StopIteration停止。 &gt; 廖雪峰老师教程里写的是__next__方法，不知道为啥。 12345678910111213141516class Fib(object): def __init__(self): self.a, self.b = 0, 1 def __iter__(self): return self def next(self): self.a, self.b = self.b, self.a + self.b if self.a &gt; 10000: raise StopIteration return self.afor i in Fib(): print i __getitem__函数 上面通过实现__iter__函数实现对象的迭代。那么如何实现对象按下标取出元素呢。这是通过实现对象的__getitem__方法。 我们来举一个例子。我们新建了一个类MyList，我们要办它实现普通list的一些功能，比如 1. 根据下标获取值 2. 正数顺序单步长切片 3. 任意步长切片 12345678910class MyList(object): def __init__(self, *args): self.numbers = args def __getitem__(self, item): return self.numbers[item] my_list = MyList(1, 2, 3, 4, 6, 5, 3)print my_list[2] 当然，上面实现了根据下标获取值。但是这还不够。我们还需要实现切片功能。例如my_list[1:3]。我们对对象进行切片操作的时候，调用的气势也是__getitem__函数。此时，该函数获取到的并不是int对象，而是slice对象。例如下面的代码 123456789101112131415161718class MyList(object): def __init__(self, *args): self.numbers = args def __getitem__(self, item): if isinstance(item, int): return self.numbers[item] elif isinstance(item, slice): # 写习惯了其他语言，差点忘记了三元运算符的格式了，吼吼吼。 # 下面句三元运算符的意思是，若为空，则为切片从0开始。 start = item.start if item.start is not None else 0 # 下面句三元运算符的意思是，若为空，则为切片到最末端结束。 stop = item.stop if item.stop is not None else len(self.numbers) return self.numbers[start:stop] my_list = MyList(1, 2, 3, 4, 6, 5, 3)print my_list[2:5] 上面的代码终于实现了切片功能，但是还没考虑负数呢。那么我们加一把劲再来改一下。代码如下： 123456789101112131415161718class MyList(object): def __init__(self, *args): self.numbers = args def __getitem__(self, item): if isinstance(item, int): return self.numbers[item] elif isinstance(item, slice): start = item.start if item.start is not None else 0 stop = item.stop if item.stop is not None else len(self.numbers) length = len(self.numbers) start = length + start + 1 if start &lt; 0 else start stop = length + stop + 1 if stop &lt; 0 else stop return self.numbers[start:stop] my_list = MyList(1, 2, 3, 4, 6, 5, 3)print my_list[1:-1] __getattar__ 在调用某一个对象不存在的属性或者方法的时候，会抛出一个一个AttributeError错误。但是如果我们实现了类中的魔术方法__getattar__，那么在调用不存在的属性或者方法的时候，就会调用该魔术方法。 1234567891011class Apple(object): def __getattr__(self, item): if item == "attar1": return "print" if item == "method1": return lambda x: "hello %s" % x apple = Apple()print apple.attar1print apple.method1 __getattar__函数一个重要的适用场景就是实现链式调用。例如我们在调用某一个api的时候： &gt; GET users/articles/index 那么我们就希望我们的代码可以实现Api.users.articles.index这么调用。思考一下，要实现链式调用，最重要的就是每一个调用都是返回一个实例～～。 123456789101112131415# coding=utf-8class Api(object): def __init__(self, path=''): self._path = path def __getattr__(self, name): return Api("%s/%s" % (self._path, name)) # 定义一个Post方法来发送请求 def post(self): print self._path api = Api()api.user.articles.index.post() 廖雪峰在他的教程中给我们出了一个题目： 例如调用github的api：users/:user/repos一样，中间的user名需要动态替换。 我们希望能api.users(&quot;charlie&quot;).repos这么调用。那么代码该如何实现呢？这可能需要用到另一个方法__call__ __call__函数 一个对象既有属性，又有方法。我们在调用一个实例的方法的时候，我们可以使用instance.method()的形式调用。其实也可以将实例本身看成一个函数用来调用，我们需要做的就是实现__call__函数本身。 1234567class Apple(object): def __call__(self, *args, **kwargs): return args apple = Apple()print apple("yes", "no") 此时我们再来看一下上面提到的实现api.users(&quot;charlie&quot;).repos链式调用的方法。 12345678910111213141516171819# coding=utf-8class Api(object): def __init__(self, path=''): self._path = path def __getattr__(self, name): return Api("%s/%s" % (self._path, name)) def __call__(self, args): self._path = "%s/%s" % (self._path, args) return Api(self._path) # 定义一个Post方法来发送请求 def post(self): print self._path api = Api()api.users("Charlie").index.post()]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[super没那么简单 [转载]]]></title>
    <url>%2F2018%2F02%2F19%2Fsuper%E6%B2%A1%E9%82%A3%E4%B9%88%E7%AE%80%E5%8D%95%2F</url>
    <content type="text"><![CDATA[本文转载自程序员大咖·super 没那么简单。 约定 在开始之前我们来约定一下本文所使用的 Python 版本。默认用的是 Python 3 ，也就是说：本文所定义的类都是新式类。如果你用到是 Python 2 的话，记得继承object: 1234567# default, Python 3class A: pass# Python 2class A(object): pass Python 3 和 Python 2 的另一个区别是: Python 3 可以使用直接使用super().xxx代替super(Class, self).xxx: 123456789# default, Python 3class B(A): def add(self, x): super().add(x)# Python 2class B(A): def add(self, x): super(B, self).add(x) 所以，你如果用的是 Python 2 的话，记得将本文的super()替换为suepr(Class, self)。如果还有其他不兼容 Python 2 的情况，我会在文中注明的。 单继承 在单继承中super就像大家所想的那样，主要是用来调用父类的方法的。 1234567891011121314151617class A: def __init__(self): self.n = 2 def add(self, m): print('self is &#123;0&#125; @A.add'.format(self)) self.n += mclass B(A): def __init__(self): self.n = 3 def add(self, m): print('self is &#123;0&#125; @B.add'.format(self)) super().add(m) self.n += 3 你觉得执行下面代码后， b.n 的值是多少呢？ 123b = B()b.add(2)print(b.n) 执行结果如下: 123self is &lt;__main__.B object at 0x106c49b38&gt; @B.addself is &lt;__main__.B object at 0x106c49b38&gt; @A.add8 这个结果说明了两个问题: 1. super().add(m)确实调用了父类A的add方法。 2. super().add(m)调用父类方法def add(self, m)时, 此时父类中self并不是父类的实例而是子类的实例, 所以b.add(2)之后的结果是5而不是4。 不知道这个结果是否和你想到一样呢？下面我们来看一个多继承的例子。 多继承 这次我们再定义一个class C，一个class D: 123456789101112131415161718class C(A): def __init__(self): self.n = 4 def add(self, m): print('self is &#123;0&#125; @C.add'.format(self)) super().add(m) self.n += 4class D(B, C): def __init__(self): self.n = 5 def add(self, m): print('self is &#123;0&#125; @D.add'.format(self)) super().add(m) self.n += 5 下面的代码又输出啥呢？ 123d = D()d.add(2)print(d.n) 这次的输出如下: 12345self is &lt;__main__.D object at 0x10ce10e48&gt; @D.addself is &lt;__main__.D object at 0x10ce10e48&gt; @B.addself is &lt;__main__.D object at 0x10ce10e48&gt; @C.addself is &lt;__main__.D object at 0x10ce10e48&gt; @A.add19 你说对了吗？你可能会认为上面代码的输出类似: 1234self is &lt;__main__.D object at 0x10ce10e48&gt; @D.addself is &lt;__main__.D object at 0x10ce10e48&gt; @B.addself is &lt;__main__.D object at 0x10ce10e48&gt; @A.add15 为什么会跟预期的不一样呢？下面我们将一起来看看 super 的奥秘。 super是个类 当我们调用super()的时候，实际上是实例化了一个super类。你没看错，super是个类，既不是关键字也不是函数等其他数据结构: 123456&gt;&gt;&gt; class A: pass...&gt;&gt;&gt; s = super(A)&gt;&gt;&gt; type(s)&lt;class 'super'&gt;&gt;&gt;&gt; 在大多数情况下， super包含了两个非常重要的信息: 一个MRO (Method Resolution Order「方法解析顺序」) 以及MRO中的一个类。当以如下方式调用super时: 1super(a_type, obj) MRO指的是type(obj)的MRO, MRO中的那个类就是a_type, 同时isinstance(obj, a_type) == True。 当这样调用时: 1super(type1, type2) MRO指的是type2的MRO, MRO中的那个类就是type1，同时issubclass(type2, type1) == True。 那么，super()实际上做了啥呢？简单来说就是：提供一个MRO以及一个MRO中的类C，super()将返回一个从MRO中C之后的类中查找方法的对象。也就是说，查找方式时不是像常规方法一样从所有的MRO类中查找，而是从MRO的tail中查找。 举个栗子, 有个MRO: 1[A, B, C, D, E, object] super只会从C之后查找，即: 只会在D或E或object中查找foo方法。 多继承中super的工作方式 再回到前面的 123d = D()d.add(2)print(d.n) 现在你可能已经有点眉目，为什么输出会是: 12345self is &lt;__main__.D object at 0x10ce10e48&gt; @D.addself is &lt;__main__.D object at 0x10ce10e48&gt; @B.addself is &lt;__main__.D object at 0x10ce10e48&gt; @C.addself is &lt;__main__.D object at 0x10ce10e48&gt; @A.add19 下面我们来具体分析一下: - D的MRO是: [D, B, C, A, object]。 &gt; 备注: 可以通过D.mro() (Python 2 使用D.__mro__) 来查看D的MRO信息） - 详细的代码分析如下: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172class A: def __init__(self): self.n = 2 def add(self, m): # 第四步 # 来自 D.add 中的 super # self == d, self.n == d.n == 5 print('self is &#123;0&#125; @A.add'.format(self)) self.n += m # d.n == 7class B(A): def __init__(self): self.n = 3 def add(self, m): # 第二步 # 来自 D.add 中的 super # self == d, self.n == d.n == 5 print('self is &#123;0&#125; @B.add'.format(self)) # 等价于 suepr(B, self).add(m) # self 的 MRO 是 [D, B, C, A, object] # 从 B 之后的 [C, A, object] 中查找 add 方法 super().add(m) # 第六步 # d.n = 11 self.n += 3 # d.n = 14class C(A): def __init__(self): self.n = 4 def add(self, m): # 第三步 # 来自 B.add 中的 super # self == d, self.n == d.n == 5 print('self is &#123;0&#125; @C.add'.format(self)) # 等价于 suepr(C, self).add(m) # self 的 MRO 是 [D, B, C, A, object] # 从 C 之后的 [A, object] 中查找 add 方法 super().add(m) # 第五步 # d.n = 7 self.n += 4 # d.n = 11class D(B, C): def __init__(self): self.n = 5 def add(self, m): # 第一步 print('self is &#123;0&#125; @D.add'.format(self)) # 等价于 super(D, self).add(m) # self 的 MRO 是 [D, B, C, A, object] # 从 D 之后的 [B, C, A, object] 中查找 add 方法 super().add(m) # 第七步 # d.n = 14 self.n += 5 # self.n = 19d = D()d.add(2)print(d.n) 调用过程图如下: 12345678910D.mro() == [D, B, C, A, object]d = D()d.n == 5d.add(2) class D(B, C): class B(A): class C(A): class A: def add(self, m): def add(self, m): def add(self, m): def add(self, m): super().add(m) 1.---&gt; super().add(m) 2.---&gt; super().add(m) 3.---&gt; self.n += m self.n += 5 &lt;------6. self.n += 3 &lt;----5. self.n += 4 &lt;----4. &lt;--| (14+5=19) (11+3=14) (7+4=11) (5+2=7) 现在你知道为什么 d.add(2) 后 d.n 的值是 19 了吧。That’s all! 希望这篇文章能对你有所帮助。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[A Letter to the Past October]]></title>
    <url>%2F2017%2F11%2F15%2F%E5%86%99%E7%BB%99%E5%8D%81%E6%9C%88%E7%9A%84%E4%B8%80%E5%B0%81%E4%BF%A1%2F</url>
    <content type="text"><![CDATA[任何一种环境或一个人，初次见面就预感到离别的隐痛时，你必定是爱上TA了…… 过去的十月，很漫长，发生了太多。 一段三年的感情走到尽头，所有的经历，所有的回忆，顷刻间，化为乌有… 养了快一年的乔巴宝贝，一场车祸… 可能是上帝觉得它太可爱了，所以起了私心，想在天堂里让它作伴，希望你能在新的“家里”也过得好… 也是十月，我的生命中出现了一个意外，不！更诚实，更具体的说，那是一场命中注定… 如果…把心掏给你是我的错…我会一点一点把它赎回来…剩下的、赎不回来的，就放在你那儿吧，反正它本来也属于你，就让它慢慢失去温度、冰冷、然后死去… In the end, I struggled, ran out of my enthusiasm, lost my own pain at your lips, a touch of sweet, it’s just a laniated miracle. 你出现了，像光那样，从一颗星到达另外一颗星。后来，你又离开了。]]></content>
      <categories>
        <category>Life</category>
      </categories>
      <tags>
        <tag>life</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PTransE -- Modeling Relation Paths for Representation Learning of Knowledge Bases]]></title>
    <url>%2F2017%2F09%2F19%2FPTransE%2F</url>
    <content type="text"><![CDATA[It is a summary of the paper Modeling Relation Paths for Representation Learning of Knowledge Bases (PTransE). PTransE is a novel extension of TransE, which is a path-based representation learning model, to model relation paths for representation learning of knowledge bases. The authors argue that multiple-step relation paths also contain rich inference patterns between entities, thus, PTransE also considers relation paths as translations between entities for representation learning, and addresses two key challenges: - Since not all relation paths are reliable, so a path-constraint resource allocation (PCRA) algorithm is designed to measure the reliability of relation paths. - Relation paths is represented via semantic composition of relation embeddings. Despite the success of TransE, TransH, TransR and TransD in modeling relational facts, these methods only take direct relations between entities into consideration (More details in TransX – Embedding Entities and Relationships of Multi-relational Data). Actually, there are also substantial multiple-step relation paths between entities indicating their semantic relationships. For instance:\[ \mathbf{h}\xrightarrow{BornInCity}\mathbf{e}_{1}\xrightarrow{CityInState}\mathbf{e}_{2}\xrightarrow{StateInCountry}\mathbf{t} \]which indicates the relation Nationality between \(\mathbf{h}\) and \(\mathbf{t}\), i.e., \((\mathbf{h},\textrm{Nationality},\mathbf{t})\). However, in PTransE, in addition to direct connected relational facts, the authors also build triples from knowledge bases using entity pairs connected with relation paths. As shown below TransE only considers direct relations between entities and optimizes the objective \(h+r\approx t\), while PTransE generalizes TransE by regarding multiple-step relation paths as connections between entities, where \(\circ\) is an operation to join the relations \(r_{1}\) and \(r_{2}\) together into a unified relation path representation. PTransE: Define entity set \(\mathbf{E}\) and relation set \(\mathbf{R}\) are in \(\mathbb{R}^{k}\), \(S=\{(h,r,t)\}\) represents a set of triples. For each triple \((h,r,t)\), the energy of TransE is computed by\[ E(h,r,t)=\Vert h+r-t\Vert\tag{1} \]Since TransE only consider the direct relations, PTransE also considers the relation paths for representation. Suppose the multiple relation paths \(P(h,t)=\{p_{1},\dots,p_{N}\}\) connecting two entities \(h\) and \(t\), where relation path \(p=\{r_{1},\dots,r_{l}\}\) indicates \(h\xrightarrow{r_{1}}\dots\xrightarrow{r_{l}}t\). For each triple \((h,r,t)\), the energy function of PTransE is defined as\[ G(h,r,t)=E(h,r,t)+E(h,P,t)\tag{2} \]where \(E(h,r,t)\) models correlations between relations and entities with direct relation triples, \(E(h,P,t)\) models the inference correlations between relations with multiple-step relation path triples:\[ E(h,P,t)=\frac{1}{Z}\sum_{p\in P(h,t)}R(p|h,t)E(h,p,t)\tag{3} \] where \(R(p|h,t)\) indicates the reliability of the relation path \(p\) given the entity pair \((h,t)\), \(Z=\sum_{p\in P(h,t)}R(p|h,t)\) is a normalization factor, and \(E(h,p,t)\) is the energy function of the triple \((h,p,t)\). The component \(R(p|h,t)\) concerns about relation path reliability, and \(E(h,p,t)\) concerns about relation path representation. Relation Path Reliability: Since not all relation paths are meaningful and reliable for learning. the authors propose a path-constraint resource allocation (PCRA) algorithm to measure the reliability of relation paths and only the reliable relation paths are selected. The basic idea is that a certain amount of resource is associated with the head entity \(h\), and will flow following the given path \(p\). Using the resource amount that eventually flows to the tail entity \(t\) to measure the reliability of the path \(p\) as a meaningful connection between \(h\) and \(t\). For a triple \((h,p,t)\), the resource amount flowing from \(h\) to \(t\) is computed as follows. The flowing path is writen as \(S_{0}\xrightarrow{r_{1}}S_{1}\xrightarrow{r_{2}}\dots\xrightarrow{r_{l}}S_{l}\), where \(S_{0}=h\) and \(t\in S_{l}\). For any entity \(m\in S_{i}\), denote its direct predecessors along relation \(r_{i}\) in \(S_{i-1}\) as \(S_{i-1}(\centerdot,m)\). The resource flowing to m is defined as\[ R_{p}(m)=\sum_{n\in S_{i-1}(\centerdot,m)}\frac{1}{\vert S_{i}(n,\centerdot)\vert}R_{p}(n)\tag{4} \]where \(S_{i}(n,\centerdot)\) is the direct successors of \(n\in S_{i-1}\) following the relation \(r_{i}\), and \(R_{p}(n)\) is the resource obtained from the entity \(n\). For each relation path \(p\), we set the initial resource in \(h\) as \(R_{p}(h)=1\). By performing resource allocation recursively from \(h\) through \(p\), tail entity \(t\) eventually obtains the resource \(R_{p}(t)\) which indicates how much information of head entity \(h\) can be well translated, thus, using \(R_{p}(t)\) to measure the reliability of the path \(p\) given \((h,t)\), i.e., \(R(p\vert h,t)=R_{p}(t)\). Relation Path Representation: For relation path representation, the authors argue that the semantic meaning of a relation path depends on all relations in this path, thus, given a relation path \(p=(r_{1},\dots,r_{l})\), they define and learn a binary (composition) operation function (\(\circ\)) to obtain the path embedding \(p\) by recursively composing multiple relations. Formally, for a path \(p=(r_{1},\dots,r_{l})\), the path embeddingas is obtained by \(p=r_{1}\circ\centerdot\centerdot\centerdot\circ r_{l}\). As shown below The authors propose three types of composition operation: - Addition (ADD): The addition operation obtains the vector of a path by summing up the vectors of all relations\[ p=r_{1}+\dots+r_{l}\tag{5} \] - Multiplication (MUL): The multiplication operation obtains the vector of a path as the cumulative product of the vectors of all relations\[ p=r_{1}\times\dots\times r_{l}\tag{6} \] - Recurrent Neural Network (RNN): RNN is a recent neural-based model for semantic composition. The composition operation is realized using a matrix \(\mathbf{W}\):\[ c_{i}=f(\mathbf{W}[c_{i-1};r_{i}])\tag{7} \]where \(f\) is a non-linearity or identical function, and \([a;b]\) represents the concatenation of two vectors. By setting \(c_{1}=r_{1}\) and recursively performing RNN following the relation path, finally get \(p=c_{n}\). Since \(E(h,p,t)=\Vert h+p-t\Vert\), and \(\Vert h+r-t\Vert\) has minimized with the direct relation triple \((h,r,t)\) to make sure \(r\approx t-h\). So we can derive\[ E(h,p,t)=\Vert h+p-t\Vert=\Vert p-(t-h)\Vert\approx\Vert p-r\Vert=E(p,r)\tag{8} \]Objective Formalization: The objective of PTransE is given as\[ \mathcal{L}(S)=\sum_{(h,r,t)\in S}\big[L(h,r,t)+\frac{1}{Z}\sum_{p\in P(h,t)}R(p\vert h,t)L(p,r)\big]\tag{9} \]Here the \(L(\centerdot)\) is the margin-based loss functions\[\begin{aligned} L(h,r,t)&amp;=\sum_{(h&#39;,r&#39;,t&#39;)\in S^{-}}[\gamma+E(h,r,t)-E(h&#39;,r&#39;,t&#39;)]_{+}\\ L(p,r)&amp;=\sum_{(h,r&#39;,t)\in S^{-}}[\gamma+E(p,r)-E(p,r&#39;)]_{+} \end{aligned}\]where \([x]_{+}=\max(0,x)\) returns the maximum between \(0\) and \(x\), \(\gamma\) is the margin, \(S\) is the set of valid triples, and \(S^{-}\) is the set of invalid triples derived by\[ S^{-}=\{(h&#39;,r,t)\}\cup\{(h,r&#39;,t)\}\cup\{(h,r,t&#39;)\}\tag{10} \]The objective function is also under the constraints that \(\forall h,r,t\), \(\Vert h\Vert_{2}\leq 1\), \(\Vert r\Vert_{2}\leq 1\), \(\Vert t\Vert_{2}\leq 1\). Reverse Relation Addition: Besides, the authors only consider the relation paths following one direction, so in order to solve the cases, like \(e_{1}\xrightarrow{BornInCity}e_{2}\xleftarrow{CityOfCountry}e_{3}\), they add reverse relations for each relation, say, for each triple \((h,r,t)\), build another \((t,r^{-1},h)\), thus, the above-mentioned path can be set as \(e_{1}\xrightarrow{BornInCity}e_{2}\xrightarrow{CityOfCountry^{-1}}e_{3}\). Path Selection Limitation: This process normally depends on the scale of knowledge database, generally, 3-steps are quiet enough for most cases. Resources: Fast-TransX, KB2E from Natural Language Processing Lab at Tsinghua University (THUNLP).]]></content>
      <categories>
        <category>Natural Language Processing</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
        <tag>natural language processing</tag>
        <tag>c plus plus</tag>
        <tag>word embeddings</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TransX -- Embedding Entities and Relationships of Multi-relational Data]]></title>
    <url>%2F2017%2F09%2F14%2FTransX%2F</url>
    <content type="text"><![CDATA[It is a summary of several papers about embedding entities and relationships of multi-relational knowledge database, while each paper contains one kind of specific method. Generally, the set of methods are called TransX. Here, I only describe the general idea and mathematical process of each method, for more information about those methods in parameters setting, experiment results and discussion, you can directly read the original papers through the links I provide. Multi-relational Data Overview There are a lot of relational knowledge database available nowadays, like: - ConceptNet, which is a freely-available semantic network, designed to help computers understand the meanings of words that people use. - WordNet, which is a large lexical database of English. Nouns, verbs, adjectives and adverbs are grouped into sets of cognitive synonyms (synsets), each expressing a distinct concept. WordNet’s structure makes it a useful tool for computational linguistics and natural language processing. - VerbNet, which is the largest on-line verb lexicon currently available for English. It is a hierarchical domain-independent, broad-coverage verb lexicon with mappings to other lexical resources such as WordNet, Xtag and FrameNet. - FreeBase, which is a large collaborative knowledge base consisting of data composed mainly by its community members. It was an online collection of structured data harvested from many sources, including individual, user-submitted wiki contributions. Freebase aimed to create a global resource that allowed people (and machines) to access common information more effectively. - YaGo, which is a huge semantic knowledge base, derived from Wikipedia, WordNet and GeoNames. It has knowledge of more than 10 million entities and contains more than 120 million facts about these entities. Actually, there are still a lot of different relational knowledge database exist, here we only show some representative databases above. Before we discuss those algorithms, let’s see some examples of multi-relational knowledge database to get the idea about how the database looks like. In the graph above, I show an example of two knowledge DB. For ConceptNet, each entity (or, node) is a concept, and entities are connected to each other through some specific relations, like, oven--UsedFor--&gt;cook, cake--IsA--&gt;dessert, here oven, cook, cake and dessert are entities, while UsedFor and IsA are relations. For WordNet, there are two different entities, one is called Synset (blue node), another is Word (green one). Each synset in WordNet is connected with other synsets through hypernym and hyponym relations, while Word only connect to its own synset and has some links with other words in the same synset, but never connected to words outside. Although there are amount of relations and connections within a knowledge database, generally, all of the relations are in the several certain forms, like one-to-many relation, many-to-one relation, one-to-one relation, co-relation, reflexive relation and so on. For example, one-to-many relation, which means one entity with one relation, links to several different entities, say, apple--is_a--&gt;fruit, apple--is_a--&gt;computer_brand, apple--is_a--&gt;computer_manufacturer and so on. For many-to-one relation, it is similar. However, the reflexive relation is unique, since two entities connect to each other with same relation, like plate--is_a-&gt;dish, while dish--is_a--&gt;plate too. Dispite the scale of relational knowledge databases and how many different relation forms they have, all of them are able to be decomposed to the basic component (triple), i.e., an entity connect to another entity with a certain relation, as shown below In order to convert those relational data into embeddings, which are convenient and easy to access via statistical approach, researchers proposed several methods to handle this issue, like RESCAL, SE, SME(linear), SME(bilinear), LFM, TransE, TransH, TransR, TransD and so on, while the TransE, TransH, TransR, TransD are a group of similar methods, thus, we put them together and named as TransX. So, TransX is a set of methods to create embeddings for entities and relations while remembering their connection information. TransE TransE – Translating Embeddings for Modeling Multi-relational Data. This paper considers the problem of embedding entities and relationships of multi-relational data in low-dimensional vector spaces, its objective is to propose a canonical model which is easy to train, contains a reduced number of parameters and can scale up to very large databases. So the TransE is proposed, which models relationships by interpreting them as translations operating on the low-dimensional embeddings of the entities. TransE, an energy-based model for learning low-dimensional embeddings of entities. In TransE, relationships are represented as translations in the embedding space: if \((h,r,t)\) holds, then the embedding of the tail entity \(t\) should be close to the embedding of the head entity \(h\) plus some vector that depends on the relationship \(r\), while it is also the general idea of the training process of TransE. The main motivation behind this translation-based parameterization is that hierarchical relationships are extremely common in KBs and translations are the natural transformations for representing them. Another motivation comes from Distributed Representations of Words and Phrases and Their Compositionality, in which the authors learn word embeddings from free text, and some 1-to-1 relationships between entities of different types are (coincidentally rather than willingly) represented by the model as translations in the embedding space. This suggests that there may exist embedding spaces in which 1-to-1 relationships between entities of different types may, as well, be represented by translations. Translation-based model: Given a training set \(S\) of triplets \((h,r,t)\) composed of two entities \(h,t\in E\) (the set of entities) and a relationship \(r\in R\) (the set of relationships), TransE learns vector embeddings of the entities and the relationships. The embeddings take values in \(\mathbb{R}^{k}\) (\(k\) is a model hyperparameter) and are denoted with the same letters, in boldface characters. The basic idea behind the model is that the functional relation induced by the \(r\)-labeled edges corresponds to a translation of the embeddings, i.e. \(h+r\approx t\) when \((h,r,t)\) holds (\(t\) should be a nearest neighbor of \(h+r\)), while \(h+r\) should be far away from \(t\) otherwise, as the graph below shows. Following an energy-based framework, the energy of a triplet is equal to \(d(h+r,t)\) for some dissimilarity measure \(d\), which we take to be either the \(L_{1}\) or the \(L_{2}\)-norm. To learn such embeddings, we minimize a margin-based ranking criterion over the training set:\[ \mathcal{L}=\sum_{(h,r,t)\in S}\sum_{(h&#39;,r,t&#39;)\in S&#39;_{(h,r,t)}}\big[\gamma+d(h+r,t)-d(h&#39;+r,t&#39;)\big]_{+}\tag{1} \]where \([x]_{+}\) denotes the positive part of \(x\), \(\gamma&gt;0\) is a margin hyperparameter, and the dissimilarity measure \(d\) is the squared euclidean distance, which computed by\[ d(h+r,t)=\lVert h\rVert^{2}_{2}+\lVert r\rVert^{2}_{2}+\lVert t\rVert^{2}_{2}-2\big(h^{T}t+r^{T}(t-h)\big)\tag{2} \]with the norm constraints that \(\lVert h\rVert^{2}_{2}=\lVert t\rVert^{2}_{2}=1\), and the set of corrupted triplets, constructed according to the equation (3) below, is composed of training triplets with either the head or tail replaced by a random entity (but not both at the same time)\[ S&#39;_{(h,r,t)}=\big\{(h&#39;,r,t)\vert h&#39;\in E\big\}\bigcup\big\{(h,r,t&#39;)\vert t&#39;\in E\big\}\tag{3} \]The loss function (1) favors lower values of the energy for training triplets than for corrupted triplets, and is thus a natural implementation of the intended criterion. The optimization is carried out by stochastic gradient descent (in minibatch mode), over the possible \(h\), \(r\), and \(t\), with the additional constraints that the \(L_{2}\)-norm of the embeddings of the entities is 1 (no regularization or norm constraints are given to the label embeddings \(r\)). This constraint is important, because it prevents the training process to trivially minimize \(\mathcal{L}\) by artificially increasing entity embeddings norms. The detailed optimization procedure is described in the graph below. TransH TransH – Knowledge Graph Embedding by Translating on Hyperplanes. Before talking about TransH, let’s see some problems that TransE holds. TransE models a relation \(r\) as a translation vector \(r\in\mathbb{R}^{k}\), and assumes the error \(\Vert h+r-t\Vert_{l_{1}/l_{2}}\) is low if \((h,r,t)\) is a golden triple. It applies well to irreflexive and one-to-one relations but has problems when dealing with reflexive or many-to-one, one-to-many, many-to-many relations. Considering the ideal case of no-error embedding where \(h+r-t=0\), if \((h,r,t)\in\Delta\), we can get the following consequences directly from TransE model. - If \((h,r,t)\in\Delta\) and \((t,r,h)\in\Delta\), i.e., \(r\) is a reflexive map, then \(r=0\) and \(h=t\). - If \(\forall i\in\{0,1,\dots,m\}\), \((h_{i},r,t)\in\Delta\), i.e., \(r\) is a many-to-one map, then \(h_{0}=\dots=h_{m}\). Similarly, if \((h,r,t_{i})\in\Delta\), i.e., \(r\) is a one-to-many map, then \(t_{0}=\dots=t_{m}\). The reason leading to the above consequences is, in TransE, the representation of an entity is the same when involved in any relations, ignoring distributed representations of entities when involved in different relations. Hence, TransH is proposed to handle the problems of TransE in modeling reflexive, one-to-many, many-to-one and many-to-many relations. The general idea of TransH is shown below, which introduces a relation-specific hyperplane to project the entities to the hyperplane, and the translation process is done in such hyperplane too. For a relation \(r\), the authors position the relation-specific translation vector \(d_{r}\) in the relation-specific hyperplane \(w_{r}\) (the normal vector) rather than in the same space of entity embeddings. Specifically, for a triplet \((h,r,t)\), the embedding \(h\) and \(t\) are first projected to the hyperplane \(w_{r}\). The projections are denoted as \(h_{\bot}\) and \(t_{\bot}\), respectively, while\[\begin{aligned} h_{\bot} &amp;=h-w_{r}^{T}hw_{r}\\ t_{\bot} &amp;=t-w_{r}^{T}tw_{r} \end{aligned}\]The authors expect \(h_{\bot}\) and \(t_{\bot}\) can be connected by a translation vector \(d_{r}\) on the hyperplane with low error if \((h,r,t)\) is a golden triplet. And the graph below shows how the TransH sloves the problems in TransE. Thus, in TransH, by introducing the mechanism of projecting to the relation-specific hyperplane, it enables different roles of an entity in different relations/triplets. Generally, the training process of TransH is similar to TransE, its cost function is also a margin-based ranking loss\[ \mathcal{L}=\sum_{(h,r,t)\in S}\sum_{(h&#39;,r,t&#39;)\in S&#39;_{(h,r,t)}}\big[f_{r}(h,t)+\gamma-f_{r&#39;}(h&#39;,t&#39;)\big]_{+}\tag{4} \]where \([x]_{+}=\max(0,x)\), \(S\) is a set of golden triples, \(S&#39;_{(h,r,t)}\) denotes the set of negative triplets constructed by corrupting \((h,r,t)\), \(\gamma\) is the margin separating positive and negative triplets, and \(f_{r}(h,t)\) is a score function and derived by\[ f_{r}(h,t)=\Vert h_{\bot}+d_{r}-t_{\bot}\Vert_{2}^{2}=\Vert(h-w_{r}^{T}hw_{r})+d_{r}-(t-w_{r}^{T}tw_{r})\Vert_{2}^{2}\tag{5} \]When minimizing the loss \(\mathcal{L}\), the following constraints are considered: - \(\forall e\in E\), \(\Vert e\Vert_{2}\leq 1\) - \(\forall r\in R\), \(\vert w_{r}^{T}d_{r}\vert/\Vert d_{r}\Vert_{2}\leq\epsilon\) - \(\forall r\in R\), \(\Vert w_{r}\Vert_{2}=1\) Reducing False Negative Labels: Here shows a new method to sample the corrupted triples to reduce the false. 1. Give more chance to replacing the head entity if the relation is one-to-many and give more chance to replacing the tail entity if the relation is many-to-one. 2. Among all the triplets of a relation r, we first get the following two statistics: (1) the average number of tail entities per head entity, denoted as \(tph\) (2) the average number of head entities per tail entity, denoted as \(hpt\) Then defining a Bernoulli distribution with parameter \(\frac{thp}{thp+hpt}\) for sampling: given a golden triplet \((h,r,t)\) of the relation \(r\), with probability \(\frac{thp}{thp+hpt}\) to corrupt the triplet by replacing the head, and with probability \(\frac{pht}{thp+hpt}\) to corrupt the triplet by replacing the tail. TransR and CTransR TransR – Learning Entity and Relation Embeddings for Knowledge Graph Completion. TransR is proposed since the authors realize that TransH still have some limitations, since both TransE and TransH assume embeddings of entities and relations being in the same space. However, an entity may have multiple aspects, and various relations focus on different aspects of entities. Hence, it is intuitive that some entities are similar and thus close to each other in the entity space, but are comparably different in some specific aspects and thus far away from each other in the corresponding relation spaces. Thus, TransR models entities and relations in distinct spaces, i.e., entity space and multiple relation spaces (i.e., relation-specific entity spaces), and performs translation in the corresponding relation space. In TransR, for each triple \((h,r,t)\), entities embeddings are set as \(h,t\in\mathbb{R}^{k}\), and relation embedding is set as \(r\in\mathbb{R}^{d}\), while the dimensions of entity embeddings and relation embeddings are not necessarily identical. For each relation \(r\), set a projection matrix \(M_{r}\in\mathbb{R}^{k\times d}\) maps entities from entity space to relation space. With the mapping matrix, the projected vectors of entities are computed by\[ h_{r}=hM_{r},\quad t_{r}=tM_{r} \]And the score function is correspondingly defined as\[ f_{r}(h,t)=\Vert h_{r}+r-t_{r}\Vert_{2}^{2}\tag{6} \]with the constraints that \(\forall h,r,t\), \(\Vert h\Vert_{2}\leq 1\), \(\Vert r\Vert_{2}\leq 1\), \(\Vert t\Vert_{2}\leq 1\), \(\Vert hM_{r}\Vert_{2}\leq 1\), \(\Vert tM_{r}\Vert_{2}\leq 1\). And the following margin-based score function as objective for training:\[ \mathcal{L}=\sum_{(h,r,t)\in S}\sum_{(h&#39;,r,t&#39;)\in S&#39;_{(h,r,t)}}\max\big[0, f_{r}(h,t)+\gamma-f_{r}(h&#39;,t&#39;)\big]\tag{7} \]where \(\max(x, y)\) aims to get the maximum between \(x\) and \(y\), \(\gamma\) is the margin, \(S\) is the set of correct triples and \(S&#39;\) is the set of incorrect triples. Cluster-based TransR (CTransR): The above mentioned models, including TransE, TransH and TransR, learn a unique vector for each relation, which may be under-representative to fit all entity pairs under this relation, because these relations are usually rather diverse. In order to better model these relations, the authors incorporate the idea of piecewise linear regression to extend TransR. The basic idea is that first segment input instances into several groups. Formally, for a specific relation \(r\), all entity pairs \((h,t)\) in the training data are clustered into multiple groups, and entity pairs in each group are expected to exhibit similar \(r\) relation. All entity pairs \((h,t)\) are represented with their vector offsets \((h−t)\) for clustering, where \(h\) and \(t\) are obtained with TransE. Afterwards, learn a separate relation vector \(r_{c}\) for each cluster and matrix \(M_{r}\) for each relation, respectively. The authors define the projected vectors of entities as \(h_{r,c}=hM_{r}\), \(t_{r,c}=tM_{r}\) and the score function is defined as\[ f_{r}(h,t)=\Vert h_{r,c}+r_{c}-t_{r,c}\Vert_{2}^{2}+\alpha\Vert r_{c}-r\Vert_{2}^{2}\tag{8} \]where \(\Vert r_{c}-r\Vert_{2}^{2}\) aims to ensure cluster-specific relation vector \(r_{c}\) not too far away from the original relation vector \(r\), and \(\alpha\) controls the effect of this constraint. Besides, same to TransR, CTransR also enforce constraints on norm of embeddings \(h\), \(r\), \(t\) and mapping matrices. TransD TransD – Knowledge Graph Embedding via Dynamic Mapping Matrix Despite that TransR/CTransR has significant improvements compared with previous state-of-the-art models. However, it also has several flaws: 1. For a typical relation \(r\), all entities share the same mapping matrix \(M_{r}\). However, the entities linked by a relation always contains various types and attributes. 2. The projection operation is an interactive process between an entity and a relation, it is unreasonable that the mapping matrices are determined only by relations. 3. Matrix-vector multiplication makes it has large amount of calculation, and when relation number is large, it also has much more parameters than TransE and TransH. To solve these flaws, TransD is proposed and its basic idea is shown in the graph below. In TransD, two vectors are defined for each entity and relation. The first vector represents the meaning of an entity or a relation, the other one (called projection vector) represents the way that how to project a entity embedding into a relation vector space and it will be used to construct mapping matrices. Therefore, every entity-relation pair has an unique mapping matrix. In addition, TransD has no matrix-by-vector operations which can be replaced by vectors operations. In the graph above, each shape represents an entity pair appearing in a triplet of relation \(r\). \(M_{rh}\) and \(M_{rt}\) are mapping matrices of \(h\) and \(t\), respectively. \(h_{ip}\), \(t_{ip}\) (\(i=1,2,3\)) and \(r_{p}\) are projection vectors. \(h_{i\bot}\) and \(t_{i\bot}\) (\(i=1,2,3\)) are projected vectors of entities. The projected vectors satisfy \(h_{i\bot}+r\approx t_{i\bot}\) (\(i=1,2,3\)). TransD Model: each named symbol object (entities and relations) is represented by two vectors. The first one captures the meaning of entity (relation), the other one is used to construct mapping matrices. For example, given a triplet \((h,r,t)\), its vectors are \(h\), \(h_{p}\), \(r\), \(r_{p}\), \(t\), \(t_{p}\), where subscript \(p\) marks the projection vectors, \(h,h_{p},t,t_{p}\in\mathbb{R}^{n}\) and \(r,r_{p}\in\mathbb{R}^{m}\). For each triplet \((h,r,t)\) the authors set two mapping matrices \(M_{rh},M_{rt}\in\mathbb{R}^{m\times n}\) to project entities from entity space to relation space:\[\begin{aligned} M_{rh}&amp;=r_{p}h_{p}^{T}+I^{m\times n}\\ M_{rt}&amp;=r_{p}t_{p}^{T}+I^{m\times n} \end{aligned}\tag{9}\]Therefore, the mapping matrices are determined by both entities and relations, and this kind of operation makes the two projection vectors interact sufficiently because each element of them can meet every entry comes from another vector. As the authors initialize each mapping matrix with an identity matrix, thus, the \(I^{m\times n}\) is added to \(M_{rh}\) and \(M_{rt}\). With the mapping matrices, the projected vectors are defined as follows:\[\begin{aligned} h_{\bot}&amp;=M_{rh}h\\ t_{\bot}&amp;=M_{rt}t \end{aligned}\tag{10}\]and the score function is set as\[ f_{r}(h,t)=-\Vert h_{\bot}+r-t_{\bot}\Vert_{2}^{2}\tag{11} \]with the constraints that \(\Vert h\Vert_{2}\leq 1\), \(\Vert t\Vert_{2}\leq 1\), \(\Vert r\Vert_{2}\leq 1\), \(\Vert h_{\bot}\Vert_{2}\leq 1\) and \(\Vert t_{\bot}\Vert_{2}\leq 1\). For training process, the authors first denote that \(S=\{(h_{i},r_{i},t_{i})\vert y_{i}=1\}\) as the golden triples, and \(S&#39;=\{(h_{i},r_{i},t_{i})\vert y_{i}=0\}\) as the negative triples, and the negative triples is derived by\[ S&#39;=\{(h_{l},r_{k},t_{k})\vert h_{l}\neq h_{k}\land y_{k}=1\}\cup\{(h_{k},r_{k},t_{l})\vert t_{l}\neq t_{k}\land y_{k}=1\} \]while the authors also use two strategies “unif” and “bern” described in TransH to replace the head or tail entity. using \(\xi\) and \(\xi&#39;\) to denote a golden triplet and a corresponding negative triplet, respectively. The training objective is\[ \mathcal{L}=\sum_{\xi\in S}\sum_{\xi&#39;\in S&#39;}\big[\gamma+f_{r}(\xi&#39;)-f_{r}(\xi)\big]_{+}\tag{12} \]Connections with TransE/H/R and CTransR: - TransE is a special case of TransD when the dimension of vectors satisfies \(m=n\) and all projection vectors are set zero. - TransH is related to TransD when we set \(m=n\). Under the setting, projected vectors of entities can be rewritten as follows:\[\begin{aligned} h_{\bot}&amp;=M_{rh}h=h+h_{p}^{T}hr_{p}\\ t_{\bot}&amp;=M_{rt}t=t+t_{p}^{T}tr_{p} \end{aligned}\]Hence, when \(m = n\), the difference between TransD and TransH is that projection vectors are determinded only by relations in TransH, but TransD’s projection vectors are determinded by both entities and relations. - As to TransR/CTransR, TransD is an improvement of it. TransR/CTransR directly defines a mapping matrix for each relation, TransD consturcts two mapping matrices dynamically for each triplet by setting a projection vector for each entity and relation. In addition, TransD has no matrix-vector multiplication operation which can be replaced by vector operations. Without loss of generality, assuming \(m\geq n\), the projected vectors can be computed as follows:\[\begin{aligned} h_{\bot}&amp;=M_{rh}h=h_{p}^{T}hr_{p}+\big[h^{T},0^{T}\big]^{T}\\ t_{\bot}&amp;=M_{rt}t=t_{p}^{T}tr_{p}+\big[t^{T},0^{T}\big]^{T} \end{aligned}\]Therefore, TransD has less calculation than TransR/CTransR, which makes it train faster and can be applied on large-scale knowledge graphs. Python and C++ Codes The Python and C++ codes of the methods above are available in the GitHub page of Natural Language Processing Lab at Tsinghua University (THUNLP). Python version: TensorFlow-TransX. Converted TensorFlow-TransX for Python 2 and Tensorflow 0.12.0: link C++ version: Fast-TransX, KB2E.]]></content>
      <categories>
        <category>Natural Language Processing</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
        <tag>python</tag>
        <tag>natural language processing</tag>
        <tag>c plus plus</tag>
        <tag>word embeddings</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 浅析 "战狼II" 170000+影评数据]]></title>
    <url>%2F2017%2F09%2F10%2FPython-%E6%B5%85%E6%9E%90-%E6%88%98%E7%8B%BC2-170000-%E5%BD%B1%E8%AF%84%E6%95%B0%E6%8D%AE%2F</url>
    <content type="text"><![CDATA[本文参考数据分析小王子·&lt;&gt;豆瓣十二万影评浅析，利用 Python 网络爬虫爬取豆瓣影评数据并进行简要分析。 想要分析“战狼II”的影评数据，首先需要获取这些数据，这里使用 Python 的 requests 包进行网页请求，并使用正则表达式匹配出我们需要的数据。首先使用Chrome打开豆瓣影评·战狼II的网页 (https://movie.douban.com/subject/26363254/comments?start=0)，使用 Developer Tools 对当前页面做一个简单的了解和分析，如下图： 我们发现页面的所有评论、评论者、投票、评价等级等信息均存储在 &lt;div class=&quot;comment-item&quot; ...&gt; 标签下。而转向下一页面的链接信息存储在 &lt;div id=&quot;paginator&quot;&gt; 标签的 &lt;a href=&quot;?start=26&amp;amp;limit=20&amp;amp;sort=new_score&amp;amp;status=P&quot; ... class=&quot;next&quot;&gt;... 中。因此，可以针对这部分 HTML 标签创建相应的正则表达式来获取数据。简易的爬虫代码如下： 123456789101112131415161718import requests import reimport pandas as pdurl_first = 'https://movie.douban.com/subject/26363254/comments?start=0' # start pagehead = &#123;'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.79 Safari/537.36'&#125;cookies = &#123;'cookie':'you own cookies&#125; #cookie of your accounthtml = requests.get(url_first, headers=head, cookies=cookies) # get first pagere_page = re.compile(r'&lt;a href="(.*?)&amp;amp;.*?class="next"&gt;') # next pagere_content = re.compile(r'&lt;span class="votes"&gt;(.*?)&lt;/span&gt;.*?comment"&gt;(.*?)&lt;/a&gt;.*?&lt;/span&gt;.*?&lt;span.*?class=""&gt;(.*?)&lt;/a&gt;.*?&lt;span&gt;(.*?)&lt;/span&gt;.*?title="(.*?)"&gt;&lt;/span&gt;.*?title="(.*?)"&gt;.*?class=""&gt; (.*?)\n', re.S)while html.status_code==200: url_next = 'https://movie.douban.com/subject/26363254/comments' + re.findall(re_page, html.text)[0] data = re.findall(re_content, html.text) print(url_next) frame = pd.DataFrame(data) frame.to_csv('./data/comments.csv', header=False, index=False, mode='a+', encoding='utf-8') frame = [] data = [] html = requests.get(url_next, cookies=cookies, headers=head) 这里需要设置自己的 User-Agent 和 cookie，如果使用的是Chrome，可以直接进入 Developer Tools，在 Network 中找到当前页面，便能获得这些信息。以上，代码没有考虑豆瓣的验证码问题，因此爬取大概20000～30000条数据时便会报出 443 Error，由于验证码机器识别问题涉及到一些其他的领域，这里喔直接忽略这个问题，因为数据量不是很大，所以每次保存停止，只需要重新启动程序即可。这里爬取的数据为7类：赞同数，是否有用，用户名，是否看过，评分，日期以及评论内容。 完成19万+条数据爬取之后，由于存在一些格式错误等脏数据，仍需要做一些简单的数据清理，清理后的数据量为18万+。代码如下： 12345678910111213141516171819202122232425262728293031import reimport pandas as pdreg_correct = re.compile(r'(.*),(.*),(.*),(.*),(.*),(.*),(.*)')reg_dirty = re.compile(r'.*这条短评跟影片无关.*&lt;span class=""votes""&gt;(.*?)&lt;/span&gt;.*?class=""j a_vote_comment""&gt;(.*?)&lt;/a&gt;.*?class=""""&gt;(.*?)&lt;/a&gt;&lt;span&gt;(.*?)&lt;/span&gt;.*?title=""(.*?)",(.*),(.*)')data = [] # 格式化数据with open('./data/comments.csv') as file: dirty_data = [] # 错误格式数据 dirty_lines = '' flag = 0 # load and clean data for line in file: arr = re.findall(reg_correct, line) if len(arr) == 0: dirty_lines = ''.join([dirty_lines, line.strip()]) flag = 1 else: value = (arr[0][0].replace('"', ''), arr[0][1].replace('"', ''), arr[0][2].replace('"', ''), arr[0][3].replace('"', ''), arr[0][4].replace('"', ''), arr[0][5].replace('"', ''), '`' + arr[0][6].replace('"', '') + '`') data.append(value) if flag == 1: flag = 0 dirty_data.append(dirty_lines) dirty_lines = '' # add cleaned data to data array for line in dirty_data: arr = re.findall(reg_dirty, line) if len(arr) != 0: value = (arr[0][0].replace('"', ''), arr[0][1].replace('"', ''), arr[0][2].replace('"', ''), arr[0][3].replace('"', ''), arr[0][4].replace('"', ''), arr[0][5].replace('"', ''), '`' + arr[0][6].replace('"', '') + '`') data.append(value)path = './data/comments_clean.csv'pd.DataFrame(data).to_csv(path, header=False, index=False, encoding='utf-8')print('Done...') 将数据保存为 .csv 文件，前5行数据如下： 之后便是进行数据分析，在数据处理过程中发现，经过清理的数据中仍然有一些数据的格式错误，由于这部分数据量很小，所以直接将这部分数据删除： 123456789101112131415161718import pandas as pd# load datacolumn_names = ['Votes', 'Useful', 'User', 'Watched', 'Score', 'Date', 'Comment']data = pd.read_csv('./data/comments_clean.csv', header=None, names=column_names, skipinitialspace = True, quotechar = '`')# set value as stringdata['Votes'] = data['Votes'].astype(str)data['Useful'] = data['Useful'].astype(str)data['User'] = data['User'].astype(str)data['Watched'] = data['Watched'].astype(str)data['Score'] = data['Score'].astype(str)data['Date'] = data['Date'].astype(str)data['Comment'] = data['Comment'].astype(str)# clean up the data with error formatdata = data[data['Score'].map(len) == 6]data = data[data['Score'] != '看过']data = data[data['Date'].map(len) == 19]print('rows:', data.shape[0], ', columns: ', data.shape[1]) # count rows of total dataset# out: ('rows:', 176875, ', columns: ', 7) 然后我们对不同评分的人数做一个简单的统计并作图： 123456789101112131415161718import matplotlib.pyplot as pltimport numpy as npprint(data['Score'].value_counts())index = np.arange(5)score_counts = data['Score'].value_counts()values = (score_counts[0], score_counts[1], score_counts[2], score_counts[4], score_counts[3])bar_width = 0.35plt.figure(figsize=(20, 10))plt.bar(index, values, bar_width, alpha=0.6, color='rgbym')plt.xlabel('Score', fontsize=16) plt.ylabel('Counts', fontsize=16)plt.title('Comments Level', fontsize=18) plt.xticks(index, ('5-star', '4-star', '3-star', '2-star', '1-star'), fontsize=14, rotation=20)plt.ylim(0, 90000)plt.grid()for idx, value in zip(index, values): plt.text(idx, value + 0.1, '%d' % value, ha='center', va='bottom', fontsize=14, color='black')plt.show() 输出为： 123456力荐 79361推荐 47724还行 29337很差 10774较差 9679Name: Score, dtype: int64 从输出数据和上图中可以很明显的发现，对这部电影持好评 (推荐、力荐) 的人占大多数，部分人觉得还行，少数人评价差。 最后，我们对所有的评论内容进行云图展示，首先定义两个函数，一个用于对评论内容进行清理和分词，另一个进行云图生成。 评论内容清理和分词： 这里同样使用结巴分词进行中文分词操作和 Python 内置正则表达式进行数据清洗操作。(注释掉的部分是移除中文停用词，为了加快运行速度，这里喔忽略了这个操作) 12345678910111213141516171819202122232425262728293031323334import reimport jiebadef segment_words(stars): comments = None if stars == 'all': comments = data['Comment'] else: comments = data[data['Score'] == stars]['Comment'] comments_list = [] for comment in comments: comment = str(comment).strip().replace('span', '').replace('class', '').replace('emoji', '') comment = re.compile('1f\d+\w*|[&lt;&gt;/=]').sub('', comment) if (len(comment) &gt; 0): comments_list.append(comment) text = ''.join(comments_list) word_list = jieba.cut(text, cut_all=True) ''' stopwords_list = [] # load chinese stop words with open('./data/中文停用词表(1208个).txt') as file: for line in file: stopwords_list.append(line.strip()) print(len(stopwords_list)) with open('./data/停用词表.txt') as file: for line in file: line = line.strip() if line not in stopwords_list: stopwords_list.append(line) print(len(stopwords_list)) # remove stop words from word_list word_list = [word for word in word_list if word not in stopwords_list] ''' words = ' '.join(word_list) return words 云图生成： 云图生成使用 Python 的 WordCloud 包和 Pillow 图像处理包。代码如下： 123456789101112from wordcloud import WordCloud, ImageColorGeneratorimport PIL.Image as Imagedef plot_word_cloud(words): coloring = np.array(Image.open('./data/chinese.jpg')) wc = WordCloud(background_color='white', max_words=2000, mask=coloring, max_font_size=60, random_state=42, font_path='./data/DroidSansFallbackFull.ttf', scale=2).generate(words) image_color = ImageColorGenerator(coloring) plt.figure(figsize=(32, 16)) plt.imshow(wc.recolor(color_func=image_color)) plt.imshow(wc) plt.axis('off') plt.show() 首先我们对所有的评论内容生成云图： 12all_words = segment_words('all')plot_word_cloud(all_words) 然后分别对力荐、推荐，以及较差、很差生成评论云图，并进行比较： 1234five_start_words = segment_words('力荐')plot_word_cloud(five_start_words)four_start_words = segment_words('推荐')plot_word_cloud(four_start_words) 1234two_start_words = segment_words('较差')plot_word_cloud(two_start_words)one_start_words = segment_words('很差')plot_word_cloud(one_start_words) 以上便是影评数据的简单分析和展示，进一步的可以对影评数据进行更精确的清理和分词操作等，并且根据评分等级，可以用该数据为基础搭建一个用户正负情感的机器学习模型等。以后有时间，再对这些数据进行进一步的处理。 代码可在我的 GitHub Repository 中找到：AmusingPythonCodes/wolf_warriors_comments。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>data analysis</tag>
        <tag>natural language processing</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python itchat包分析微信好友]]></title>
    <url>%2F2017%2F09%2F10%2FPython-itchat%E5%8C%85%E5%88%86%E6%9E%90%E5%BE%AE%E4%BF%A1%E6%9C%8B%E5%8F%8B%2F</url>
    <content type="text"><![CDATA[本文参照大数据·“一件有趣的事：我用 Python 爬了爬自己的微信朋友”，利用Python的 itchat 包简单的分析了一下自己的朋友圈。 首先需要安装 itchat 包：sudo pip3 install itchat 或者 sudo pip install itchat。 成功安装 itchat 后，便可正式使用这个包来爬一爬自己的微信朋友了。这里，我先导入之后需要用到的Python 包： 123456789101112import itchat # itchat documentation -- https://itchat.readthedocs.io/zh/latest/api/import matplotlib.pyplot as pltimport seaborn as snsimport numpy as npimport pandas as pdimport refrom wordcloud import WordCloud, ImageColorGeneratorimport PIL.Image as Image # pillowimport jieba # chinese word segementation toolfrom matplotlib.font_manager import FontProperties# since matplotlib and pandas.plot cannot display chinesefont = FontProperties(fname='./data/DroidSansFallbackFull.ttf', size=14) # load chinese font 现在我们需要登录网页版微信并获取朋友信息： 1234# login, default a QR code will be generated, scan for loginitchat.login()friends = itchat.get_friends(update=True)[0:] # get all friendsprint(friends[0]) # the first one is yourself 这里itchat.login()默认将生成一个二维码图片，利用手机微信扫描登录即可，之后获取所有的朋友信息，其中第一条数据是自己的信息，信息以dict形式存储。打印输出如下： 1&#123;'UserName': u'...', 'City': '', ..., 'Province': '', ..., 'Signature': u'\u5982\u679c\u5fc3\u4f1a\u6210\u4e3a\u963b\u788d\n\u8ba9\u5b83\u6d88\u5931\u5c31\u597d\u4e86', ..., 'NickName': u'Isaac Changhau', ..., 'Sex': 1, ...&#125; 分析上面的输出可以得出，城市、省份信息存储在 key 为 City 和 Province 的记录中，用户名信息存储在 key 为 NickName 的记录中，签名信息存储在 key 为 Signature 的记录中，性别信息存储在 key 为 Sex 的记录中，其中 1 表示男性，2 表示女性，其他就是不明性别的 (通常是没有填写)。 所以我先对自己的微信朋友的男女分布做一个统计，首先进行数据提取和累加： 1234567891011121314151617181920# get male-female-ratiodef get_male_female_count(friends): male = 0 female = 0 others = 0 for friend in friends: sex = friend['Sex'] if sex == 1: male += 1 elif sex == 2: female += 1 else: others += 1 return male, female, othersmale, female, others = get_male_female_count(friends[1:])total = len(friends[1:])print('Male population: &#123;:d&#125;, ratio: &#123;:.4f&#125;'.format(male, male / float(total)))print('Female population: &#123;:d&#125;, ratio: &#123;:.4f&#125;'.format(female, female / float(total)))print('Others: &#123;:d&#125;, ratio: &#123;:.4f&#125;'.format(others, others / float(total))) 得到输出： 123Male population: 190, ratio: 0.5352Female population: 151, ratio: 0.4254Others: 14, ratio: 0.0394 然后，可视化上述信息： 1234567891011121314# plot male-female-ratioindex = np.arange(3)genders = (male, female, others)bar_width = 0.35plt.figure(figsize=(14, 7))plt.bar(index, genders, bar_width, alpha=0.6, color='rgb')plt.xlabel('Gender', fontsize=16) plt.ylabel('Population', fontsize=16)plt.title('Male-Female Population', fontsize=18) plt.xticks(index, ('Male', 'Female', 'Others'), fontsize=14, rotation=20)plt.ylim(0,220)for idx, gender in zip(index, genders): plt.text(idx, gender + 0.1, '%.0f' % gender, ha='center', va='bottom', fontsize=14, color='black')plt.show() 从上图可以直观的看到，我的男性朋友比女性朋友多！(是不是说明我还是比较正直～)。另外，我们也可以对好友的城市分布做一个简单的统计并作出统计图。同样，我先提取出了感兴趣的数据段，并储存为 DataFrame： 1234567891011# extract the variables: NickName, Sex, City, Province, Signaturedef get_features(friends): features = [] for friend in friends: feature = &#123;'NickName': friend['NickName'], 'Sex': friend['Sex'], 'City': friend['City'], 'Province': friend['Province'], 'Signature': friend['Signature']&#125; features.append(feature) return pd.DataFrame(features)features = get_features(friends[1:])print(features.columns)features.head() 输出数据为： 12345678Index([u'City', u'NickName', u'Province', u'Sex', u'Signature'], dtype='object') City NickName Province Sex Signature0 成都 娇姐 四川 2 忧郁的娇姐，愤怒的小豪！1 乌鲁木齐 樱桃小兔子 ❤ 新疆 2 路遥知马力，日久见人心2 北碚 waitings 重庆 1 做一个傻子多么好3 AlexShi 1 A true procrastinator4 沈阳 崔智语 辽宁 1 之后提取省份和城市信息 (注：部分好友在海外，因此对于省份和城市的划分和国内不同)，然后进行简单的数据清理，之后按省份和城市进行数据聚合并且统计各个城市的人数。这里我取排名前二十的省份进行堆叠直方图展示： 1234567891011121314151617181920locations = features.loc[:, ['Province', 'City']] # get location columnslocations = locations[locations['Province'] != ''] # clean empty city or province recordsdata = locations.groupby(['Province', 'City']).size().unstack() # group by and countcount_subset = data.take(data.sum(1).argsort())[-20:] # obtain the 20 highest data# plotsubset_plot = count_subset.plot(kind='bar', stacked=True, figsize=(24, 24))# set fontsxtick_labels = subset_plot.get_xticklabels()for label in xtick_labels: label.set_fontproperties(font)legend_labels = subset_plot.legend().textsfor label in legend_labels: label.set_fontproperties(font) label.set_fontsize(10)plt.xlabel('Province', fontsize=20)plt.ylabel('Number', fontsize=20)plt.show() 上面这个图画得比较丑，哎，对Python画图不是太熟练，colormap 也没有好好设置，不过大概意思是能表明的。 最后，我根据好友的个性签名生成自定义云图。这里需要用到 Python 的 jieba 包对中文进行分词 (之前已经引入了该包)。首先提取出所有的个性签名，并组合成一个text： 12345678910sigature_list = []for signature in features['Signature']: signature = signature.strip().replace('span', '').replace('class', '').replace('emoji', '') # re.compile(ur'[^a-zA-Z0-9\u4e00-\u9fa5 ]').sub('', signature) signature = re.compile('1f\d+\w*|[&lt;&gt;/=]').sub('', signature) if (len(signature) &gt; 0): sigature_list.append(signature)text = ''.join(sigature_list)# print(text) 之后，利用结巴分词，对text 进行划分： 123word_list = jieba.cut(text, cut_all=True)words = ' '.join(word_list)# print(words) 最后，基于 Python 的 WordCloud 包进行云图生成，WordCloud 可以根据自己想要的图片、形状、颜色画出相似的图形。这里喔使用我自己的头像和 Wechat logo 分别生成了词云进行展示。 123456789coloring = np.array(Image.open('./data/avatar.jpg'))wc = WordCloud(background_color='white', max_words=2000, mask=coloring, max_font_size=60, random_state=42, font_path='./data/DroidSansFallbackFull.ttf', scale=2).generate(words)image_color = ImageColorGenerator(coloring)plt.figure(figsize=(32, 16))plt.imshow(wc.recolor(color_func=image_color))plt.imshow(wc)plt.axis('off')plt.show() 最后的最后，当然，itchat并不是只有这些无聊的功能，它更强大的地方在于可以进行消息收发，自定义个人号机器人等等…… 这些都值得花时间去考究探索。 代码可在我的 GitHub Repository 中找到：AmusingPythonCodes/wechat_exploration。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>data analysis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Autoencoder and Sparsity]]></title>
    <url>%2F2017%2F08%2F19%2FAutoencoder-and-Sparsity%2F</url>
    <content type="text"><![CDATA[The article is excerpted from Andrew Ng’s CS294A Lecture notes: Sparse Autoencoder with some personal understanding. Before going through this article, you would better get some information from the Backpropagation Algorithm. Suppose we have only unlabeled training examples set \(\{x^{(1)},x^{(2)},x^{(3)},\dots\}\), where \(x^{(i)}\in\mathbb{R}^{n}\). An autoencoder neural network is an unsupervised learning algorithm that applies backpropagation, setting the target values to be equal to the inputs. i.e., it uses \(y^{(i)}=x^{(i)}\). Below is an autoencoder: The autoencoder tries to learn a function \(h_{W,b}(x)\approx x\). In other words, it is trying to learn an approximation to the identity function, so as to output \(\hat{x}\) that is similar to \(x\). The identity function seems a particularly trivial function to be trying to learn; but by placing constraints on the network, such as by limiting the number of hidden units, we can discover interesting structure about the data. As a concrete example, suppose the inputs x are the pixel intensity values from a \(10\times 10\) image (100 pixels) so \(n=100\), and there are \(s_{2}=50\) hidden units in layer \(L_{2}\). Note that we also have \(y\in\mathbb{R}^{100}\). Since there are only 50 hidden units, the network is forced to learn a compressed representation of the input. That is, given only the vector of hidden unit activations \(a^{(2)}\in\mathbb{R}^{50}\), it must try to reconstruct the 100-pixel input \(x\). If the input were completely random, say, each \(x_{i}\) comes from an I.I.D. (Independently and Identically Distribute) Gaussian independent of the other features, then this compression task would be very difficult. But if there is structure in the data, for example, if some of the input features are correlated, then this algorithm will be able to discover some of those correlations. &gt; In fact, this simple autoencoder oftern ends up learning a low-dimensional representation very similar to PCA’s. Our argument above relied on the number of hidden units \(s_{2}\) being small. But even when the number of hidden units is large (perhaps even greater than the number of input pixels), we can still discover interesting structure, by imposing other constraints on the network. In particular, if we impose a sparsity constraint on the hidden units, then the autoencoder will still discover interesting structure in the data, even if the number of hidden units is large. Informally, we will think of a neuron as being “active” (or as “firing”) if its output value is close to 1, or as being “inactive” if its output value is close to 0. We would like to constrain the neurons to be inactive most of the time. &gt; This discussion assumes a sigmoid activation function. If you are using a tanh activation function, then we think of a neuron as being inactive when it outputs values close to -1. Recall that \(a_{j}^{(2)}\) denotes the activation of hidden unit \(j\) in the autoencoder. However, this notation does not make explicit what was the input \(x\) that led to that activation. Thus, we will write \(a_{j}^{(2)}(x)\) to denote the activation of this hidden unit when the network is given a specific input \(x\). Further, let\[ \hat{\rho}_{j}=\frac{1}{m}\sum_{i=1}^{m}\big[a_{j}^{(2)}(x^{(i)})\big]\tag{1} \]be the average activation of hidden unit \(j\) (averaged over the training set). We would like to (approximately) enforce the constraint\[ \hat{\rho}_{j}=\rho\tag{2} \]where \(\rho\) is a sparsity parameter, typically, a small value close to zero (say \(\rho=0.05\)). In other words, we would like the average activation of each hidden neuron \(j\) to be close to 0.05 (say). To satisfy this constraint, the hidden unit’s activations must mostly be near 0. To achieve this, we will add an extra penalty term to our optimization objective that penalizes \(\hat{\rho}_{j}\) deviating significantly from \(\rho\). Many choices of the penalty term will give reasonable results. We will choosr the following:\[ \sum_{j=1}^{s_{2}}\rho\log\frac{\rho}{\hat{\rho}_{j}}+(1-\rho)\log\frac{1-\rho}{1-\hat{\rho}_{j}}\tag{3} \]Here, \(s_{2}\) is the number of neurons in the hidden layer, and the index \(j\) is summing over the hidden units in our network. If you are familiar with the concept of KL divergence, this penalty term is based on it, and can also be written\[ \sum_{j=1}^{s_{2}}KL(\rho\Vert\hat{\rho}_{j})\tag{4} \]where \(KL(\rho\Vert\hat{\rho}_{j})=\rho\log\frac{\rho}{\hat{\rho}_{j}}+(1-\rho)\log\frac{1-\rho}{1-\hat{\rho}_{j}}\) is the Kullback-Leibler (KL) divergence between a Bernoulli random variable with mean \(\rho\) and a Bernoulli random variable with mean \(\hat{\rho}_{j}\). KL-divergence is a standard function for measuring how different two different distributions are. This penalty function has the property that \(KL(\rho\Vert\hat{\rho}_{j})=0\) if \(\hat{\rho}_{j}=\rho\), and otherwise it increases monotonically as \(\hat{\rho}_{j}\) diverges from \(\rho\). For example, in the figure below, we have set \(\rho=0.2\), and plotted \(KL(\rho\Vert\hat{\rho}_{j})\) for a range of values of \(\hat{\rho}_{j}\) We see that the KL-divergence reaches its minimum of 0 at \(\hat{\rho}_{j}=\rho\), and blows up (it actually approaches \(\infty\)) as \(\hat{\rho}_{j}\) approaches 0 or 1. Thus, minimizing this penalty term has the effect of causing \(\hat{\rho}_{j}\) to be close to \(\rho\). Our overall cost function is now\[ J_{sparse}(W,b)=J(W,b)+\beta\sum_{j=1}^{s_{2}}KL(\rho\Vert\hat{\rho}_{j})\tag{5} \]where \(J(W,b)\) is as defined previously, and \(\beta\) controls the weight of the sparsity penalty term. The term \(\hat{\rho}_{j}\) (implicitly) depends on \(W\), \(b\) also, because it is the average activation of hidden unit \(j\), and the activation of a hidden unit depends on the parameters \(W\), \(b\). To incorporate the KL-divergence term into your derivative calculation, there is a simple-to-implement trick involving only a small change to your code. Specifically, where previously for the second layer (\(l = 2\)), during backpropagation you would have computed\[ \delta_{i}^{(2)}=\bigg(\sum_{j=1}^{s_{2}}W_{ji}^{(2)}\delta_{j}^{(3)}\bigg)f&#39;(z_{i}^{(2)})\tag{6} \]now instead compute\[ \delta_{i}^{(2)}=\bigg(\big(\sum_{j=1}^{s_{2}}W_{ji}^{(2)}\delta_{j}^{(3)}\big)+\beta\big(-\frac{\rho}{\hat{\rho}_{i}}+\frac{1-\rho}{1-\hat{\rho}_{i}}\big)\bigg)f&#39;(z_{i}^{(2)})\tag{7} \] Here is how to derive the \(\frac{\partial J_{sparse}(W,b)}{\partial W_{i,j}^{(l)}}\) and \(\frac{\partial J_{sparse}(W,b)}{\partial b_{i}^{(l)}}\) Denote \(S(W,b)=\sum_{j=1}^{s_{2}}KL(\rho\Vert\hat{\rho}_{j})\), so (5) can be written as\[ J_{sparse}(W,b)=J(W,b)+\beta S(W,b)\tag{8} \]Then the two partial derivations in gradient descent algorithm are\[\begin{cases} \frac{\partial J_{sparse}(W,b)}{\partial W_{i,j}^{(l)}} &amp;=\frac{\partial J(W,b)}{\partial W_{i,j}^{(l)}}+\beta\frac{\partial S(W,b)}{\partial W_{i,j}^{(l)}}\\ \frac{\partial J_{sparse}(W,b)}{\partial b_{i}^{(l)}} &amp;=\frac{\partial J(W,b)}{\partial b_{i}^{(l)}}+\beta\frac{\partial S(W,b)}{\partial b_{i}^{(l)}} \end{cases}\tag{9}\]The calculation of \(\frac{\partial J(W,b)}{\partial W_{i,j}^{(l)}}\) and \(\frac{\partial J(W,b)}{\partial b_{i}^{(l)}}\) have discussed in Backpropagation Algorithm, here we only discuss the computation of \(\frac{\partial S(W,b)}{\partial W_{i,j}^{(l)}}\) and \(\frac{\partial S(W,b)}{\partial b_{i}^{(l)}}\). First, expand \(S(W,b)\), we have\[ S(W,b)=\sum_{t=1}^{s_{2}}KL(\rho\Vert\hat{\rho}_{t})=\sum_{t=1}^{s_{2}}\big[\rho\ast\log\frac{\rho}{\hat{\rho}_{t}}+(1-\rho)\ast\log\frac{1-\rho}{1-\hat{\rho}_{t}}\big]\tag{10} \]where\[ \hat{\rho}_{t}=\frac{1}{m}\sum_{k=1}^{m}a_{t}^{(2)}\big(x^{(k)}\big)=\frac{1}{m}\sum_{k=1}^{m}f\big(z_{t}^{(2)}\big)\tag{11} \]here\[ z_{t}^{(2)}=z_{t}^{(2)}\big(x^{(k)}\big)=\bigg(\sum_{s=1}^{s_{1}}W_{t,s}^{(1)}x_{s}^{(k)}\bigg)+b_{t}^{(1)}\tag{12} \]according to (11) and (12), we can get the following two points: when \(l\neq 1\), \(\frac{\partial S(W,b)}{\partial W_{i,j}^{(l)}}=0\), \(\frac{\partial S(W,b)}{\partial b_{i}^{(l)}}=0\). \(\frac{\partial S(W,b)}{\partial W_{i,j}^{(l)}}=\frac{\partial\sum_{t=1}^{s_{2}}KL(\rho\Vert\hat{\rho}_{t})}{\partial W_{i,j}^{(l)}}=\frac{\partial KL(\rho\Vert\hat{\rho}_{i})}{\partial W_{i,j}^{(l)}}\); \(\frac{\partial S(W,b)}{\partial b_{i}^{(l)}}=\frac{\partial\sum_{t=1}^{s_{2}}KL(\rho\Vert\hat{\rho}_{t})}{\partial b_{i}^{(l)}}=\frac{\partial KL(\rho\Vert\hat{\rho}_{i})}{\partial b_{i}^{(l)}}\). According to the above two points, we have\[\begin{aligned} \frac{\partial S(W,b)}{\partial W_{i,j}^{(1)}} &amp;=\frac{\partial}{\partial W_{i,j}^{(1)}}\big[\rho\ast\log\frac{\rho}{\hat{\rho}_{i}}+(1-\rho)\ast\log\frac{1-\rho}{1-\hat{\rho}_{i}}\big]\\ &amp;= \frac{\partial}{\partial W_{i,j}^{(1)}}\big\{\rho(\log\rho-\log\hat{\rho}_{i})+(1-\rho)[\log(1-\rho)-\log(1-\hat{\rho}_{i})]\big\}\quad (\textrm{According to }\log\frac{A}{B}=\log A-\log B)\\ &amp;=\rho\big(0-\frac{1}{\hat{\rho}_{i}}\frac{\partial\hat{\rho}_{i}}{\partial W_{i,j}^{(1)}}\big)+(1-\rho)\big(0+\frac{1}{1-\hat{\rho}_{i}}\frac{\partial\hat{\rho}_{i}}{\partial W_{i,j}^{(1)}}\big)\quad (\rho\textrm{ is constant})\\ &amp;=\big(-\frac{\rho}{\hat{\rho}_{i}}+\frac{1-\rho}{1-\hat{\rho}_{i}}\big)\frac{\partial\hat{\rho}_{i}}{\partial W_{i,j}^{(1)}} \end{aligned}\]Samely, we have\[ \frac{\partial S(W,b)}{\partial b_{i}^{(1)}}=\big(-\frac{\rho}{\hat{\rho}_{i}}+\frac{1-\rho}{1-\hat{\rho}_{i}}\big)\frac{\partial\hat{\rho}_{i}}{\partial b_{i}^{(1)}} \]Then, we need to compute \(\frac{\partial\hat{\rho}_{i}}{\partial W_{i,j}^{(1)}}\) and \(\frac{\partial\hat{\rho}_{i}}{\partial b_{i}^{(1)}}\), according to (11), we have\[\begin{aligned} \frac{\partial\hat{\rho}_{i}}{\partial W_{i,j}^{(1)}} &amp;=\frac{1}{m}\sum_{k=1}^{m}f&#39;\big(z_{i}^{(2)}\big)\centerdot\frac{\partial z_{i}^{(2)}}{\partial W_{i,j}^{(1)}}\\ &amp;=\frac{1}{m}\sum_{k=1}^{m}f&#39;\big(z_{i}^{(2)}\big)\centerdot x_{j}^{(k)}\quad (\textrm{according to (12), we have }\frac{\partial z_{i}^{(2)}}{\partial W_{i,j}^{(1)}}=x_{j}^{(k)}) \end{aligned}\]Samely, we have\[\begin{aligned} \frac{\partial\hat{\rho}_{i}}{\partial b_{i}^{(1)}} &amp;=\frac{1}{m}\sum_{k=1}^{m}f&#39;\big(z_{i}^{(2)}\big)\centerdot\frac{\partial z_{i}^{(2)}}{\partial b_{i}^{(1)}}\\ &amp;=\frac{1}{m}\sum_{k=1}^{m}f&#39;\big(z_{i}^{(2)}\big)\quad (\textrm{according to (12), we have }\frac{\partial z_{i}^{(2)}}{\partial b_{i}^{(1)}}=1) \end{aligned}\]In summary, we can obtain\[\begin{cases} \frac{\partial S(W,b)}{\partial W_{i,j}^{(1)}} &amp;=\big(-\frac{\rho}{\hat{\rho}_{i}}+\frac{1-\rho}{1-\hat{\rho}_{i}}\big)\centerdot\frac{1}{m}\sum_{k=1}^{m}f&#39;\big(z_{i}^{(2)}\big)\centerdot x_{j}^{(k)}\\ \frac{\partial S(W,b)}{\partial b_{i}^{(1)}}&amp;=\big(-\frac{\rho}{\hat{\rho}_{i}}+\frac{1-\rho}{1-\hat{\rho}_{i}}\big)\centerdot\frac{1}{m}\sum_{k=1}^{m}f&#39;\big(z_{i}^{(2)}\big) \end{cases}\tag{13}\]In Backpropagation Algorithm, we already obtain that\[\begin{cases} \frac{\partial J(W,b)}{\partial W_{i,j}^{(1)}}&amp;=\big[\frac{1}{m}\sum_{k=1}^{m}a_{j}^{(1)}\delta_{i}^{(2)}\big]+\lambda W_{i,j}^{(1)}\\ \frac{\partial J(W,b)}{\partial b_{i}^{(1)}}&amp;=\frac{1}{m}\sum_{k=1}^{m}\delta_{i}^{(2)} \end{cases}\tag{14}\]Thus, put (13), (14) into (9), we will get\[\begin{cases} \frac{\partial J_{sparse}(W,b)}{\partial W_{i,j}^{(l)}} &amp;=\big[\frac{1}{m}\sum_{k=1}^{m}a_{j}^{(1)}\delta_{i}^{(2)}\big]+\lambda W_{i,j}^{(1)}+\beta\big(-\frac{\rho}{\hat{\rho}_{i}}+\frac{1-\rho}{1-\hat{\rho}_{i}}\big)\centerdot\frac{1}{m}\sum_{k=1}^{m}f&#39;\big(z_{i}^{(2)}\big)\centerdot x_{j}^{(k)}\\ \frac{\partial J_{sparse}(W,b)}{\partial b_{i}^{(l)}} &amp;=\frac{1}{m}\sum_{k=1}^{m}\delta_{i}^{(2)}+\beta\big(-\frac{\rho}{\hat{\rho}_{i}}+\frac{1-\rho}{1-\hat{\rho}_{i}}\big)\centerdot\frac{1}{m}\sum_{k=1}^{m}f&#39;\big(z_{i}^{(2)}\big) \end{cases}\tag{15}\]Note that \(a_{j}^{(1)}\), \(\delta_{i}^{(2)}\) and \(z_{i}^{(2)}\) are related to \(x^{(k)}\) (or \(y^{(k)}\)), particularly, we have \(a_{j}^{(1)}=x_{j}^{(k)}\). After simplify (15), we get\[\begin{cases} \frac{\partial J_{sparse}(W,b)}{\partial W_{i,j}^{(l)}} &amp;=\frac{1}{m}\sum_{k=1}^{m}\bigg\{a_{j}^{(1)}\big[\delta_{i}^{(2)}+\beta\big(-\frac{\rho}{\hat{\rho}_{i}}+\frac{1-\rho}{1-\hat{\rho}_{i}}\big)f&#39;\big(z_{i}^{(2)}\big)\big]\bigg\}+\lambda W_{i,j}^{(1)}\\ \frac{\partial J_{sparse}(W,b)}{\partial b_{i}^{(l)}} &amp;=\frac{1}{m}\sum_{k=1}^{m}\big[\delta_{i}^{(2)}+\beta\big(-\frac{\rho}{\hat{\rho}_{i}}+\frac{1-\rho}{1-\hat{\rho}_{i}}\big)f&#39;\big(z_{i}^{(2)}\big)\big] \end{cases}\]Now, we obtain how to change (6) to (7). One subtlety is that you’ll need to know \(\hat{\rho}_{i}\) to compute this term. Thus, you’ll need to compute a forward pass on all the training examples first to compute the average activations on the training set, before computing backpropagation on any example. If your training set is small enough to fit comfortably in computer memory (this will be the case for the programming assignment), you can compute forward passes on all your examples and keep the resulting activations in memory and compute the \(\hat{\rho}_{i}\)s. Then you can use your precomputed activations to perform backpropagation on all your examples. If your data is too large to fit in memory, you may have to scan through your examples computing a forward pass on each to accumulate (sum up) the activations and compute \(\hat{\rho}_{i}\) (discarding the result of each forward pass after you have taken its activations \(a_{i}^{(2)}\) into account for computing \(\hat{\rho}_{i}\)). Then after having computed \(\hat{\rho}_{i}\), you’d have to redo the forward pass for each example so that you can do backpropagation on that example. In this latter case, you would end up computing a forward pass twice on each example in your training set, making it computationally less efficient. The full derivation showing that the algorithm above results in gradient descent is beyond the scope of these notes. But if you implement the autoencoder using backpropagation modified this way, you will be performing gradient descent exactly on the objective \(J_{sparse}(W,b)\). Using the derivative checking method, you will be able to verify this for yourself as well. Below is the summary of notation table th:first-of-type { width: 70px; } Expression Explanation \(x\) Input features for a training example, \(x\in\mathbb{R}^{n}\). \(y\) Output/target values. Here, \(y\) can be vector valued. In the case of an autoencoder, \(y = x\). \((x^{(i)},y^{(i)})\) The \(i\)-th training example. \(h_{W,b}(x)\) Output of our hypothesis on input \(x\), using parameters \(W\), \(b\). This should be a vector of the same dimension as the target value \(y\). \(W_{i,j}^{(l)}\) The parameter associated with the connection between unit \(j\) in layer \(l\), and unit \(i\) in layer \(l+1\). \(b_{i}^{(l)}\) The bias term associated with unit \(i\) in layer \(l+1\). Can also be thought of as the parameter associated with the connection between the bias unit in layer \(l\) and unit \(i\) in layer \(l+1\). \(\theta\) Our parameter vector. It is useful to think of this as the result of taking the parameters \(W\), \(b\) and “unrolling” them into a long column vector. \(a_{i}^{(l)}\) Activation (output) of unit \(i\) in layer \(l\) of the network. In addition, since layer \(L_{1}\) is the input layer, we also have \(a_{i}^{(1)}=x_{i}\). \(f(\centerdot)\) The activation function. Throughout these notes, we used \(f(z)=tanh(z)\). \(z_{i}^{(l)}\) Total weighted sum of inputs to unit \(i\) in layer \(l\). Thus, \(a_{i}^{(l)}=f\big(z_{i}^{(l)}\big)\) \(\alpha\) Learning rate parameter. \(s_{l}\) Number of units in layer \(l\) (not counting the bias unit). \(n_{l}\) Number layers in the network. Layer \(L_{1}\) is usually the input layer, and layer $L_{n_{l}} the output layer. \(\lambda\) Weight decay parameter. \(\hat{x}\) For an autoencoder, its output; i.e., its reconstruction of the input \(x\). Same meaning as \(h_{W,b}(x)\). \(\rho\) Sparsity parameter, which specifies our desired level of sparsity. \(\hat{\rho}_{i}\) The average activation of hidden unit \(i\) (in the sparse autoencoder). \(\beta\) Weight of the sparsity penalty term (in the sparse autoencoder objective).]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
        <tag>machine learning</tag>
        <tag>mathematics</tag>
        <tag>autoencoder</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Backpropagation Algorithm]]></title>
    <url>%2F2017%2F08%2F17%2FBackpropagation-Algorithm%2F</url>
    <content type="text"><![CDATA[The article is excerpted from Andrew Ng’s CS294A Lecture notes: Sparse Autoencoder, then I add some personal understanding. In this article, we will let \(n_{l}\) denote the number of layers in our network, label \(l\) as \(L_{l}\), so layer \(L_{1}\) is the input layer, and layer \(L_{n_{l}}\) the output layer. Neural network has parameters \((W,b)=(W^{(1)},b^{(1)},\dots,W^{(n_{l}-1)},b^{(n_{l}-1)})\), where we write \(W_{ij}^{(l)}\) to denote the parameter (or weight) associated with the connection between unit \(j\) in layer \(l\) and unit \(i\) in layer \(l+1\). (Note the order of the indices). Also, \(b_{i}^{(l)}\) is the bais associated with unit \(i\) in layer \(l+1\). We also let \(s_{l}\) denote the number of nodes in layer \(l\) (not counting the bias unit). Plus, we will write \(a_{i}^{(l)}\) to denote the activation (meaning output value) of unit \(i\) in layer \(l\). Suppose we have a fixed training set \(\{(x^{(1)},y^{(1)}),\dots,(x^{(m)},y^{(m)})\}\) of \(m\) training examples. We can train our neural network using batch gradient descent. In detail, for a single training example \((x,y)\), we define the cost function with respect to that single example to be\[ J(W,b;x,y)=\frac{1}{2}\Vert h_{W,b}(x)-y\Vert^{2}\tag{1} \]This is a (one-half) squared-error cost function. Given a training set of \(m\) examples, we then define the overall cost function to be\[\begin{aligned} J(W,b) &amp; =\bigg[\frac{1}{m}\sum_{i=1}^{m}J(W,b;x^{(i)},y^{(i)})\bigg]+\frac{\lambda}{2}\sum_{l=1}^{n_{l}-1}\sum_{i=1}^{s_{l}}\sum_{j=1}^{s_{l}+1}\big(W_{ji}^{(l)}\big)^{2}\\ &amp; =\bigg[\frac{1}{m}\sum_{i=1}^{m}\big(\frac{1}{2}\Vert h_{W,b}(x^{(i)})-y^{(i)}\Vert^{2}\big)\bigg]+\frac{\lambda}{2}\sum_{l=1}^{n_{l}-1}\sum_{i=1}^{s_{l}}\sum_{j=1}^{s_{l}+1}\big(W_{ji}^{(l)}\big)^{2} \end{aligned}\tag{2}\]The first term is an average sum-of-squares error term. The second term is a regularization term (also called a weight decay term) that tends to decrease the magnitude of the weights, and helps prevent overfitting. Usually weight decay is not applied to the bias terms \(b_{i}^{(l)}\), as reflected in the difinition of \(J(W,b)\). Applying weight decay to the bias units usually makes only a small different to the final network, however. If you took CS229, you may also recognize weight decay this as essentially a variant of the Bayesian regularization method you saw there, where we placed a Gaussian prior on the parameters and did MAP (instead of maximum likelihood) estimation. The weight decay parameter \(\lambda\) controls the relative importance of the two terms. Note also the slightly overloaded notation: \(J(W,b;x,y)\) is the squared error cost with respect to a single example; \(J(W,b)\) is the overall cost function, which includes the weight decay term. This cost function above is often used both for classification and for regression problems. For classification, we let \(y=0\) or \(1\) represent the two class labels (recall that the sigmoid activation function outputs values in \([0,1]\); if we were using a tanh activation function, we would instead use \(-1\) and \(+1\) to denote the labels). For regression problems, we first scale our outputs to ensure that they lie in the \([0,1]\) range (or if we were using a tanh activation function, then the \([−1,1]\) range). Our goal is to minimize \(J(W,b)\) as a function of \(W\) and \(b\). To train our neural network, we will initialize each parameter \(W_{ij}^{(l)}\) and each \(b_{i}^{(l)}\) to a small random value near zero (say according to a \(\mathfrak{N}(0,\epsilon^{2})\) distribution for some small \(\epsilon\), say \(0.01\)), and then apply an optimization algorithm such as batch gradient descent. Since \(J(W,b)\) is a non-convex function, gradient descent is susceptible to local optima; however, in practice gradient descent usually works fairly well. Finally, note that it is important to initialize the parameters randomly, rather than to all 0’s. If all the parameters start off at identical values, then all the hidden layer units will end up learning the same function of the input (more formally, \(W_{ij}^{(1)}\) will be the same for all values of \(i\), so that \(a_{1}^{(2)}=a_{2}^{(2)}=a_{3}^{(2)}=\dots\) for any input \(x\)). The random initialization serves the purpose of symmetry breaking. One iteration of gradient descent updates the parameters \(W\), \(b\) as follows:\[ W_{ij}^{(l)}:=W_{ij}^{(l)}-\alpha\frac{\partial J(W,b)}{\partial W_{ij}^{(l)}}\tag{3} \]\[ b_{i}^{(l)}:=b_{i}^{(l)}-\alpha\frac{\partial J(W,b)}{\partial b_{i}^{(l)}}\tag{4} \]where \(\alpha\) is the learning rate. The key step is computing the partial derivatives above. We will now describe the backpropagation algorithm, which gives an efficient way to compute these partial derivatives. We will first describe how backpropagation can be used to compute \(\frac{\partial}{\partial W_{ij}^{(l)}}J(W,b;x,y)\) and \(\frac{\partial}{\partial b_{i}^{(l)}}J(W,b;x,y)\), the partial derivatives of the cost function \(J(W,b;x,y)\) defined with respect to a single example \((x,y)\). Once we can compute these, then by referring to Equation (2), we see that the derivative of the overall cost function \(J(W,b)\) can be computed as\[ \frac{\partial J(W,b)}{\partial W_{ij}^{(l)}}=\bigg[\frac{1}{m}\sum_{i=1}^{m}\frac{\partial}{\partial W_{ij}^{(l)}}J(W,b;x^{(i)},y^{(i)})\bigg]+\lambda W_{ij}^{(l)}\tag{5} \]\[ \frac{\partial J(W,b)}{\partial b_{i}^{(l)}}=\frac{1}{m}\sum_{i=1}^{m}\frac{\partial}{\partial b_{i}^{(l)}}J(W,b;x^{(i)},y^{(i)})\tag{6} \]The two lines above differ slightly because weight decay is applied to \(W\) but not \(b\). The intuition behind the backpropagation algorithm is as follows. Given a training example \((x,y)\), we will first run a “forward pass” to compute all the activations throughout the network, including the output value of the hypothesis \(h_{W,b}(x)\). Then, for each node i in layer l, we would like to compute an “error term” \(\delta_{i}^{(l)}\) that measures how much that node was “responsible” for any errors in our output. For an output node, we can directly measure the difference between the network’s activation and the true target value, and use that to define \(\delta_{i}^{(n_{l})}\) (where layer \(n_{l}\) is the output layer). How about hidden units? For those, we will compute \(\delta_{i}^{(l)}\) based on a weighted average of the error terms of the nodes that uses \(a_{i}^{(l)}\) (the \(i\)-th activation value of \(l\) layer) as an input. The reason why we need to introduce error term \(\delta_{i}^{(l)}\): Using the chain rule of derivation, we have\[ \frac{\partial J(w,b;x,y)}{\partial W_{ij}^{(l)}}=\frac{\partial J(w,b;x,y)}{\partial z_{i}^{(l+1)}}\frac{\partial z_{i}^{(l+1)}}{\partial W_{ij}^{(l)}} \]\[ \frac{\partial J(w,b;x,y)}{\partial b_{i}^{(l)}}=\frac{\partial J(w,b;x,y)}{\partial z_{i}^{(l+1)}}\frac{\partial z_{i}^{(l+1)}}{\partial b_{i}^{(l)}} \]Since \(z^{(l+1)}=W^{(l)}a^{(l)}+b^{(l)}\), then\[ z_{i}^{(l+1)}=\sum_{k=1}^{s_{l}}W_{ik}^{(l)}a_{k}^{(l)}+b_{i}^{(l)}\]Thus we have\[\frac{\partial z_{i}^{(l+1)}}{\partial W_{ij}^{(l)}}=a_{j}^{(l)},\quad\frac{\partial z_{i}^{(l+1)}}{b_{i}^{(l)}}=1 \]If we let \(\delta_{i}^{(l)}=\frac{\partial J(w,b;x,y)}{\partial z_{i}^{(l)}}\), then we have\[\frac{\partial J(w,b;x,y)}{\partial W_{ij}^{(l)}}=a_{i}^{(l)}\delta_{i}^{(l+1)}\]\[\frac{\partial J(w,b;x,y)}{\partial b_{i}^{(l)}}=\delta_{i}^{(l+1)} \]Thus, in order to compute \(\frac{\partial J(W,b)}{\partial W_{ij}^{(l)}}\) and \(\frac{\partial J(W,b)}{\partial b_{i}^{(l)}}\), the key is to compute \(\delta_{i}^{(l+1)}\) In detail, here is the backpropagation algorithm: Perform a feedforward pass, computing the activations \(a_{i}^{(l)}\) for layers \(L_{2}\), \(L_{3}\), and so on up to the output layer \(L_{n_{l}}\). For each output unit \(i\) in layer \(n_{l}\) (the output layer), set\[ \delta_{i}^{(n_{l})}=\frac{\partial}{\partial z_{i}^{(n_{l})}}\frac{1}{2}\Vert y-h_{W,b}(x)\Vert^{2}=-(y_{i}-a_{i}^{(n_{l})})\centerdot f&#39;(z_{i}^{(n_{l})})\tag{7}\] Its computation process as follow:\[\begin{aligned} \delta_{i}^{(n_{l})} &amp;=\frac{\partial J(W,b;x,y)}{\partial z_{i}^{(n_{l})}}=\frac{\partial}{\partial z_{i}^{(n_{l})}}\bigg(\frac{1}{2}\Vert y-h_{W,b}(x)\Vert^{2}\bigg)\quad (\textrm{The definition of }J(W,b;x,y))\\ &amp;=\frac{1}{2}\frac{\partial}{\partial z_{i}^{(n_{l})}}\bigg(\sum_{j=1}^{s_{n_{l}}}\big(y_{j}-a_{j}^{(n_{l})}\big)^{2}\bigg)\quad (\textrm{The definition of vector norm }\Vert\centerdot\Vert)\\ &amp;=\frac{1}{2}\frac{\partial}{\partial z_{i}^{(n_{l})}}\bigg(\sum_{j=1}^{s_{n_{l}}}\big(y_{j}-f(z_{j}^{(n_{l})})\big)^{2}\bigg)\quad (\textrm{Using }a_{j}^{(n_{l})}=f(z_{j}^{(n_{l})}))\\ &amp;=-(y_{i}-f(z_{i}^{(n_{l})}))\centerdot f&#39;(z_{i}^{(n_{l})})\quad (\textrm{compute partial derivative of sum term wrt. }z_{i}^{(n_{l})})\\ &amp;=-(y_{i}-a_{i}^{(n_{l})})\centerdot f&#39;(z_{i}^{(n_{l})})\quad (\textrm{Using }a_{j}^{(n_{l})}=f(z_{j}^{(n_{l})})) \end{aligned}\] For \(l=n_{l}-1, n_{l}-2,\dots,2\), each node \(i\) in layer \(l\), set\[ \delta_{i}^{(l)}=\bigg(\sum_{j=1}^{s_{l}+1}W_{ji}^{(l)}\delta_{j}^{(l+1)}\bigg)f&#39;(z_{i}^{(l)})\tag{8}\] Its computation process as follow:\[\begin{aligned} \delta_{i}^{(n_{l}-1)} &amp;=\frac{\partial J(W,b;x,y)}{\partial z_{i}^{(n_{l}-1)}}=\frac{\partial}{\partial z_{i}^{(n_{l}-1)}}\bigg(\frac{1}{2}\Vert y-h_{W,b}(x)\Vert^{2}\bigg)\quad (\textrm{The definition of }J(W,b;x,y))\\ &amp;=\frac{1}{2}\frac{\partial}{\partial z_{i}^{(n_{l}-1)}}\bigg(\sum_{j=1}^{s_{n_{l}}}\big(y_{j}-a_{j}^{(n_{l})}\big)^{2}\bigg)\quad (\textrm{The definition of vector norm }\Vert\centerdot\Vert)\\ &amp;=\frac{1}{2}\sum_{j=1}^{s_{n_{l}}}\frac{\partial}{\partial z_{i}^{(n_{l}-1)}}\big(y_{j}-a_{j}^{(n_{l})}\big)^{2}\\ &amp;=\frac{1}{2}\sum_{j=1}^{s_{n_{l}}}\frac{\partial}{\partial z_{i}^{(n_{l}-1)}}\big(y_{j}-f(z_{j}^{(n_{l})})\big)^{2}\quad (\textrm{Using }a_{j}^{(n_{l})}=f(z_{j}^{(n_{l})}))\\ &amp;=\sum_{j=1}^{s_{n_{l}}}-\big(y_{j}-f(z_{j}^{(n_{l})})\big)\frac{\partial f(z_{j}^{(n_{l})})}{\partial z_{i}^{(n_{l}-1)}}\quad (\textrm{Compute partial derivative})\\ &amp;=\sum_{j=1}^{s_{n_{l}}}-\big(y_{j}-f(z_{j}^{(n_{l})})\big)\centerdot f&#39;(z_{j}^{(n_{l})})\centerdot\frac{\partial z_{i}^{(n_{l})}}{\partial z_{i}^{(n_{l}-1)}}\quad (\textrm{Using Chain rule to compute }\frac{\partial f(z_{j}^{(n_{l})})}{\partial z_{i}^{(n_{l}-1)}})\\ &amp;=\sum_{j=1}^{s_{n_{l}}}\delta_{j}^{(n_{l})}\centerdot\frac{\partial z_{i}^{(n_{l})}}{\partial z_{i}^{(n_{l}-1)}}\quad (\textrm{Using Equation (7)})\\ &amp;=\sum_{j=1}^{s_{n_{l}}}\delta_{j}^{(n_{l})}\centerdot\frac{\partial}{\partial z_{i}^{(n_{l}-1)}}\bigg(\sum_{k=1}^{s_{n_{l}-1}}f(z_{k}^{(n_{l}-1)})\centerdot W_{jk}^{(n_{l}-1)}+b_{j}^{(n_{l}-1)}\bigg)\quad (\textrm{According to the definition of }z_{j}^{(n_{l})})\\ &amp;=\sum_{j=1}^{s_{n_{l}}}\delta_{j}^{(n_{l})}\centerdot W_{ji}^{(n_{l}-1)}\centerdot f&#39;(z_{i}^{(n_{l}-1)})\quad (\textrm{Compute partial derivative of }z_{i}^{(n_{l}-1)})\\ &amp;=\bigg(\sum_{j=1}^{s_{n_{l}}}W_{ji}^{(n_{l}-1)}\centerdot\delta_{j}^{(n_{l})}\bigg)\centerdot f&#39;(z_{i}^{(n_{l}-1)}) \end{aligned}\]By replacing the relationship between \(n_{l}-1\) and \(n_{l}\) to the relationship between \(l\) and \(l+1\), we can derive the Equation (8). And the above derivation process from backward to forward is the essence of “Backpropagation”. Compute the desired partial derivatives, which are given as:\[\begin{aligned} \frac{\partial}{\partial W_{ij}^{(l)}}J(W,b;x,y) &amp;=a_{j}^{(l)}\delta_{i}^{(l+1)}\\ \frac{\partial}{\partial b_{i}^{(l)}}J(W,b;x,y) &amp;=\delta_{i}^{(l+1)} \end{aligned}\] Below gives a graph to illustrate the above steps, we use a sample \((x,y)\) as an example. Finally, we can also rewrite the algorithm using matrix-vectorial notation. We will use “\(\bullet\)” to denote the element-wise product operator (also called the Hadamard product), so that if \(a=b\bullet c\), then \(a_{i}=b_{i}c_{i}\). Similar to how we extended the definition of \(f(\centerdot)\) to apply element-wise to vectors, we also do the same for \(f&#39;(\centerdot)\) (so that \(f&#39;([z_{1},z_{2},z_{3}])=\big[\frac{\partial f(z_{1})}{\partial z_{1}},\frac{\partial f(z_{2})}{\partial z_{2}},\frac{\partial f(z_{3})}{\partial z_{3}}\big]\)). The algorithm can then be written: Perform a feedforward pass, computing the activations for layers \(L_{2}\), \(L_{3}\), up to the output layer \(L_{n_{l}}\), using Equations\[\begin{aligned} z^{(l+1)} &amp;=W^{(l)}a^{(l)}+b^{(l)}\\ a^{(l+1)} &amp;=f(z^{(l+1)}) \end{aligned}\] For the output layer (layer \(n_{l}\)), set\[ \delta^{(n_{l})}=-(y-a^{(n_{l})})\bullet f&#39;(z^{(n)}) \] For \(l=n_{l}-1, n_{l}-2,\dots,2\), set\[ \delta^{(l)}=\big((W^{(l)})^{T}\delta^{(l+1)}\big)\bullet f&#39;(z^{(l)}) \] Compute the desired partial derivatives:\[\begin{aligned} \nabla_{W^{(l)}}J(W,b;x,y) &amp;=\delta^{(l+1)}\big(a^{(l)}\big)^{T}\\ \nabla_{b^{(l)}}J(W,b;x,y) &amp;=\delta^{(l+1)} \end{aligned}\] Implementation note: In steps 2 and 3 above, we need to compute \(f&#39;(z^{(l)})\) for each value of \(i\). Assuming \(f(z)\) is the sigmoid activation function, we would already have \(a_{i}^{(l)}\) stored away from the forward pass through the network. Thus, using the expression that we worked out earlier for \(f&#39;(z)\), we can compute this as\[ f&#39;(z^{(l)})=a_{i}^{(l)}(1-a_{i}^{(l)}) \]Finally, we are ready to describe the full gradient descent algorithm. In the pseudo-code below, \(\Delta W^{(l)}\) is a matrix (of the same dimension as \(W^{(l)}\)), and \(\Delta b^{(l)}\) is a vector (of the same dimension as \(b^{(l)}\)). Note that in this notation, “\(\Delta W^{(l)}\)” is a matrix, and in particular it isn’t “\(\Delta\) times \(W^{(l)}\)”. We implement one iteration of batch gradient descent as follows: Set \(\Delta W^{(l)}:=0\), \(\Delta b^{(l)}:=0\) (matrix/vector of zeros) for all \(l\). For \(i=1\) to \(m\): Use backpropagation to compute \(\nabla_{W^{(l)}}J(W,b;x,y)\) and \(\nabla_{b^{(l)}}J(W,b;x,y)\). Set \(\Delta W^{(l)}:=\Delta W^{(l)}+\nabla_{W^{(l)}}J(W,b;x,y)\) Set \(\Delta b^{(l)}:=\Delta b^{(l)}+\nabla_{b^{(l)}}J(W,b;x,y)\) Update the parameters\[\begin{aligned} W^{(l)} &amp;:= W^{(l)}-\alpha\bigg[\big(\frac{1}{m}\Delta W^{(l)}\big)+\lambda W^{(l)}\bigg]\\ b^{(l)} &amp;:= b^{(l)}-\alpha\bigg[\frac{1}{m}\Delta b^{(l)}\bigg] \end{aligned}\] To train our neural network, we can now repeatedly take steps of gradient descent to reduce our cost function \(J(W,b)\).]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
        <tag>machine learning</tag>
        <tag>mathematics</tag>
        <tag>backpropagation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Seq2Seq中的Beam Search算法过程 [转载]]]></title>
    <url>%2F2017%2F08%2F10%2FSeq2Seq%E4%B8%AD%E7%9A%84Beam-Search%E7%AE%97%E6%B3%95%E8%BF%87%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[本文转载自知乎专栏·机器学习算法与自然语言处理的seq2seq中的beam search算法过程。 在 Sequence2Sequence 模型中，beam search 的方法只用在测试的情况，因为在训练过程中，每一个 decoder 的输出是有正确答案的，也就不需要 beam search 去加大输出的准确率。假设现在我们用机器翻译作为例子来说明，我们需要翻译： &gt; “我是中国人” —&gt; “I am Chinese” 假设我们的词表大小只有三个单词就是 I am Chinese。那么如果我们的 beam size 为 2 的话，我们现在来解释。 如下图所示，我们在 decoder 的过程中，有了beam search 方法后，在第一次的输出，我们选取概率最大的 “I” 和 “am” 两个单词，而不是只挑选一个概率最大的单词。 然后接下来我们要做的就是，把 “I” 单词作为下一个 decoder 的输入算一遍得到 \(y_{2}\) 的输出概率分布，把 “am” 单词作为下一个 decoder 的输入算一遍也得到 \(y_{2}\) 的输出概率分布。 比如将 “I” 单词作为下一个 decoder 的输入算一遍得到 \(y_{2}\) 的输出概率分布如下： 比如将 “am” 单词作为下一个 decoder 的输入算一遍得到 \(y_{2}\) 的输出概率分布如下： 那么此时我们由于我们的 beam size 为 2，也就是我们只能保留概率最大的两个序列，此时我们可以计算所有的序列概率：\[\begin{aligned} &amp; &quot;I\quad I&quot;=0.4\times 0.3=0.12,\qquad &amp; &quot;I\quad am&quot;=0.4\times 0.6=0.24,\\ &amp; &quot;I\quad Chinese&quot;=0.4\times 0.1=0.04,\qquad &amp; &quot;am\quad I&quot;=0.5\times 0.3=0.15,\\ &amp; &quot;am\quad am&quot;=0.5\times 0.3=0.15,\qquad &amp; &quot;am\quad Chinese&quot;=0.5\times 0.4=0.20 \end{aligned}\]我们很容易得出俩个最大概率的序列为 “I am” 和 “am Chinese”，然后后面会不断重复这个过程，直到遇到结束符为止。 最终输出 2 个得分最高的序列。这就是seq2seq中的 beam search 算法过程，但是可能有些同学有一个疑问，就是但 \(i-1\) 时刻选择的单词不同的时候，下一时刻的输出概率分布为什么会改变？ 这是由于解码的过程中，第i时刻的模型的输入，包括了第 \(i-1\) 时刻模型的输出，那么很自然在第 \(i-1\) 时刻模型的输出不同的时候，就会导致下一时刻模型的输出概率分布会不同，因为第 \(i-1\) 时刻的输出作为参数影响了后一时刻模型的学习。 如下图用了一个 slides 的法语翻译为英文的例子，可以更容易理解上面的解释。 参考：谁能解释下seq2seq中的beam search算法过程?]]></content>
      <categories>
        <category>Natural Language Processing</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
        <tag>natural language processing</tag>
        <tag>seq2seq</tag>
        <tag>beam search</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习(周志华)--学习笔记(四) 集成学习(Ensemble Learning)]]></title>
    <url>%2F2017%2F08%2F08%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%91%A8%E5%BF%97%E5%8D%8E-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-4%2F</url>
    <content type="text"><![CDATA[第八章-集成学习 (Ensemble Learning) 个体与集成 集成学习 (ensemble learning) 通过构建并结合多个学习器来完成学习任务，也被称为多分类器系统 (multi-classifier system)、基于委员会的学习 (committee-based learning) 等。 下图展示了集成学习的一般结构：先产生一组“个体学习器” (individual learner)，再用某种策略将它们结合。若集成中只包含同种类型的个体学习器，这样的集成是“同质的” (homogeneous)，同质集成中的个体学习器称为“基学习器” (base learner)，相应的学习算法称为“基学习算法” (base learning algorithm)。集成也可以包含不同类型的个体学习器，即“异质的” (heterogenous)。异质集成中的个体学习器由不同的学习算法生成，此时不再有基学习算法，而个体学习器也常称为“组件学习器” (component learner)。 集成学习通过将多个学习器进行结合，通常可以获得比单一学习器显著优越的泛化性能。这对“弱学习器” (weak learner) 尤为明显。实际中，要获得好的集成，个体学习器通常应该“好二不同”，即个体学习器要有一定的“准确性”，并且要有“多样性”，即学习器间具有差异。如下图所示 举个例子，考虑二分类问题 \(y\in\{-1,+1\}\) 和真实函数 \(\boldsymbol{f}\)，假设基分类器的错误率为 \(\epsilon\)，即对每个基分类器 \(h_{i}\) 有\[ P(h_{i}(\boldsymbol{x})\neq\boldsymbol{f}(\boldsymbol{x}))=\epsilon\tag{1} \]假设集成通过简单投票法结合 \(T\) 个基分类器，若有超过半数的基分类器正确，则集成分类就正确：\[ H(\boldsymbol{x})=sign\bigg(\sum_{i=1}^{T}h_{i}(\boldsymbol{x})\bigg)\tag{2} \]假设基分类器的错误率相互独立，则由 Hoeffding 不等式可知，集成的错误率为\[ P(H(\boldsymbol{x})\neq\boldsymbol{f}(\boldsymbol{x}))=\sum_{k=0}^{\lfloor T/2\rfloor}{T\choose k}(1-\epsilon)^{k}\epsilon^{T-k}\leq\exp\big(-\frac{1}{2}T(1-2\epsilon)^{2}\big)\tag{3} \]由上式可得，随着个体集成中个体分类器数目 \(T\) 的增大，集成的错误率将指数级下降，最终趋向于零。上面的分析有一个关键假设：基学习器的误差相互独立。实际上，个体学习器是为解决同一个问题训练出来的，显然不能相互独立。而“准确性”和“多样性”本身就存在冲突，当准确性很高之后，增加多样性就需要牺牲准确性。 根据个体学习器的生成方式，集成学习大致分为两类： - 个体学习器间存在强依赖关系、必须串行生成的序列化方法，如 Boosting。 - 个体学习器不存在强依赖关系、可同时生成的并行化方法，如 Bagging 和“随即森林” (Random Forest)。 Boosting Boosting 是一族可将弱学习器提升为强学习器的算法，其工作机制类似：先从初始训练集训练出一个基学习器，再根据基学习器的表现对训练样本进行调整，使得先前基学习器做错的训练样本在后续受到更多的关注，然后基于调整后的样本训练下一个基学习器，如此重复进行，直至基学习器达到事先指定的值 \(T\)，最终将这 \(T\) 个基学习器进行加权结合。如 AdaBoost 算法， &gt; AdaBoost 算法 &gt; 输入：训练集 \(D=\{(\boldsymbol{x}_{1},y_{1}),\dots,(\boldsymbol{x}_{m},y_{m})\}\)，\(y\in\{-1,+1\}\)；基学习算法 \(\mathfrak{L}\)；训练轮数 \(T\)。 &gt; 过程： &gt; \(\qquad\)\(\mathcal{D}_{1}(\boldsymbol{x})=\frac{1}{m}\) &gt; \(\qquad\)for \(t=1,2,\dots,T\) do &gt; \(\qquad\)\(\qquad\)\(h_{t}=\mathfrak{L}(D,\mathcal{D}_{t})\) &gt; \(\qquad\)\(\qquad\)\(\epsilon_{t}=P_{\boldsymbol{x}\sim\mathcal{D}_{t}}\big(h_{t}(\boldsymbol{x})\neq\boldsymbol{f}(\boldsymbol{x})\big)\) &gt; \(\qquad\)\(\qquad\)if \(\epsilon_{t}&gt;0.5\) then break\(\qquad\) (检测是否优于随机猜测) &gt; \(\qquad\)\(\qquad\)\(\alpha_{t}=\frac{1}{2}\ln\big(\frac{1-\epsilon_{t}}{\epsilon_{t}}\big)\) &gt; \(\qquad\)\(\qquad\)\(\mathcal{D}_{t+1}(\boldsymbol{x})=\frac{\mathcal{D}_{t}(\boldsymbol{x})}{Z_{t}}\times\begin{cases}\exp(-\alpha_{t}), &amp; h_{t}(\boldsymbol{x})=\boldsymbol{f}(\boldsymbol{x})\\\exp(\alpha_{t}), &amp; h_{t}(\boldsymbol{x})\neq\boldsymbol{f}(\boldsymbol{x})\end{cases}=\frac{\mathcal{D}_{t}(\boldsymbol{x})\exp\big(-\alpha_{t}\boldsymbol{f}(\boldsymbol{x})h_{t}(\boldsymbol{x})\big)}{Z_{t}}\) (\(Z_{t}\)是规范化因子，确保\(\mathcal{D}_{t+1}\)是一个分布) &gt; \(\qquad\)end for &gt; 输出：\(H(\boldsymbol{x})=sign\big(\sum_{t=1}^{T}\alpha_{t}h_{t}(\boldsymbol{x})\big)\) AdaBoost 可以理解为一个“加性模型” (additive model)，即基学习器的线性组合\[ H(\boldsymbol{x})=\sum_{t=1}^{T}\alpha_{t}h_{t}(\boldsymbol{x})\tag{4} \]来最小化指数损失函数 (exponential loss function)\[ \ell_{\exp}(H|\mathcal{D})=\mathbb{E}_{\boldsymbol{x}\sim\mathcal{D}}\big[e^{-\boldsymbol{f}(\boldsymbol{x})H(\boldsymbol{x})}\big]\tag{5} \]若 \(H(\boldsymbol{x})\) 能令指数损失函数最小化，则考虑式(5)对 \(H(\boldsymbol{x})\) 的偏导，并令导数为零，可得\[ H(\boldsymbol{x})=\frac{1}{2}\ln\frac{P\big(\boldsymbol{f}(\boldsymbol{x})=1|\boldsymbol{x}\big)}{P\big(\boldsymbol{f}(\boldsymbol{x})=-1|\boldsymbol{x}\big)}\tag{6} \]因此有\[ sign\big(H(\boldsymbol{x})\big) =sign\bigg(\frac{1}{2}\ln\frac{P\big(\boldsymbol{f}(\boldsymbol{x})=1|\boldsymbol{x}\big)}{P\big(\boldsymbol{f}(\boldsymbol{x})=-1|\boldsymbol{x}\big)}\bigg) =\dots =\arg\max_{y\in\{-1,1\}}P\big(\boldsymbol{f}(\boldsymbol{x})=y|\boldsymbol{x}\big)\tag{7} \]这意味着 \(sign\big(H(\boldsymbol{x})\big)\) 达到零贝叶斯最优错误率。换言之，若指数损失函数最小化，则分类错误率也将最小化；说明指数损失函数式分类任务原本 \(0/1\) 损失函数的一致 (consistent) 替代损失函数。 在AdaBoost算法中，第一个基分类器 \(h_{1}\) 是通过直接将基学习算法用于初始数据分布而得；此后迭代生成 \(h_{t}\) 和 \(\alpha_{t}\)，当基分类器 \(h_{t}\) 基于分布 \(\mathcal{D}_{t}\) 产生后，该基分类器的权重 \(\alpha_{t}\)应使得 \(\alpha_{t}h_{t}\) 最小化指数损失函数\[ \ell_{\exp}(\alpha_{t}h_{t}|\mathcal{D}_{t})=\mathbb{E}_{\boldsymbol{x}\sim\mathcal{D}_{t}}\big[e^{-f(\boldsymbol{x})\alpha_{t}h_{t}(\boldsymbol{x})}\big]=\dots=e^{-\alpha_{t}}(1-\epsilon_{t})+e^{\alpha_{t}}\epsilon_{t}\tag{8} \]其中 \(\epsilon_{t}=P_{\boldsymbol{x}\sim\mathcal{D}_{t}}\big(h_{t}(\boldsymbol{x})\neq f(\boldsymbol{x})\big)\)。考虑指数损失函数关于 \(\alpha_{t}\) 的导数并令导数为零可得\[ \alpha_{t}=\frac{1}{2}\ln\bigg(\frac{1-\epsilon_{t}}{\epsilon_{t}}\bigg)\tag{9} \]这就是分类器权重更新公式，而 AdaBoost 算法在获得 \(H_{t-1}\) 之后样本分布将进行调整，使下一轮的基学习器 \(h_{t}\) 能纠正 \(H_{t-1}\) 的一些错误，即最小化 \(\ell_{\exp}(H_{t-1}+h_{t}|\mathcal{D})\)。通过泰勒展开，数学期望定义等一系列变化和相关关系 (具体请参照书中推导) 可得到理想基学习器\[ h_{t}(\boldsymbol{x})=\arg\min_{h}\mathbb{E}_{\boldsymbol{x}\sim\mathcal{D}_{t}}\big[\Pi(f(\boldsymbol{x})\neq h(\boldsymbol{x}))\big]\tag{10} \]由此可见，理想的 \(h_{t}\) 将在分布 \(\mathcal{D}_{t}\) 下最小化分类误差。因此，弱分类器将基于分布 \(\mathcal{D}_{t}\) 来训练，且针对 \(\mathcal{D}_{t}\) 的分类误差应小于0.5，这在一定程度上类似“残差逼近”的思想。考虑到 \(\mathcal{D}_{t}\) 和 \(\mathcal{D}_{t+1}\) 的关系，有\[ \mathcal{D}_{t+1}(\boldsymbol{x})=\mathcal{D}_{t}(\boldsymbol{x})\centerdot e^{-f(\boldsymbol{x})\alpha_{t}h_{t}(\boldsymbol{x})}\frac{\mathbb{E}_{\boldsymbol{x}\sim\mathcal{D}}\big[e^{-f(\boldsymbol{x})H_{t-1}(\boldsymbol{x})}\big]}{\mathbb{E}_{\boldsymbol{x}\sim\mathcal{D}}\big[e^{-f(\boldsymbol{x})H_{t}(\boldsymbol{x})}\big]}\tag{11} \]这便是样本分布更新公式。以上是从基于加性模型迭代式优化指数损失函数的角度推导出了AdaBoost算法。 Boosting算法要求基学习器能对特定的数据分布进行学习，这可通过“重赋权法” (re-weighting) 实施，即在训练过程的每一轮中，根据样本分布为每一个训练样本重新赋予一个权重。对无法接受带权样本的基学习算法，则可通过“重采样法” (re-sampling) 来处理，即每一轮学习中，根据样本分布对训练数据重新进行采样，在用重采样而得的样本集对基学习器进行训练。两种方法没有显著优劣差别。但是，若采用重采样法，则面对学习过程停止问题 (流程图中检测是否优于随机猜测，否抛弃当前基学习器，学习过程停止)，可获得“重启动”机会以避免训练过程过早停止。 从偏差-方差分解角度看，Boosting主要关注降低偏差，因此Boosting能基于泛化性能相当弱的学习器构建很强的集成。 Bagging与随机森林 欲得到泛化能力强的集成，集成中的个体学习器应尽可能相互独立，，虽然在实际中无法做到，但可以设法使基学习器尽可能具有较大的差异。给定一个数据集，一种方法是对样本进行采样，产生若干不同的子集，再从每个子集中训练出一个基学习器。为了避免因为采样导致每个基学习器训练数据不足，常采用相互有交叠的采样子集。 Bagging Bagging 是并行式集成学习的代表，它直接基于自助采样法 (bootstrap sampling)。给定包含 \(m\) 个样本的数据集，先随机取出一个样本放入采样集，再把该样本放回初始数据集，使得下次采样时该样本仍有可能被选中，经过 \(m\) 次随机采样，得到含 \(m\) 个样本的采样集，初始训练集中有的样本在采样集多次出现，有的则从未出现。极限状态下 (\(m\mapsto\infty\))，初始训练集中约有 \(63.2%\) 的样本出现在采样集中。这样可采样出 \(T\) 个含 \(m\) 个训练样本的采样集，然后基于每个采样集训练出一个基学习器，再将这些基学习器结合。这就是 Bagging 的基本流程。在对预测输出进行结合时，Bagging 通常对分类任务使用简单投票法，对回归任务使用简单平均法。若分类预测时出现两个类收到同样票数的情形，则最简单的方法是随机选择一个，也可进一步考察学习器投票的置信度来确定最终胜者。Bagging 算法流程如下： &gt; 输入：训练集 \(D=\{(\boldsymbol{x}_{1},y_{1}),\dots,(\boldsymbol{x}_{m},y_{m})\}\)，\(y\in\{-1,+1\}\)；基学习算法 \(\mathfrak{L}\)；训练轮数 \(T\)。 &gt; 过程： &gt; \(\qquad\)for \(t=1,2,\dots,T\) do &gt; \(\qquad\)\(\qquad\)\(h_{t}=\mathfrak{L}(D,\mathcal{D}_{bs})\) &gt; \(\qquad\)end for &gt; 输出：\(H(\boldsymbol{x})=\arg\max_{y\in\mathcal{Y}}\sum_{t=1}^{T}\Pi\big(h_{t}(\boldsymbol{x})=y\big)\) 假设基学习器的计算复杂度为 \(\mathcal{O}(m)\)，则 Bagging 的复杂度大致为 \(T\big(\mathcal{O}(m)+\mathcal{O}(s)\big)\)，考虑到采样与投票/平均过程的复杂度 \(\mathcal{O}(s)\) 很小，而 \(T\) 通常是一个不太大的常数，因此训练一个 Bagging 集成与直接使用基学习器训练一个学习器的复杂度同阶。此外，与标准 AdaBoost 只适用于二分类任务不同，Bagging能不经修改的用于多分类、回归任务。而自助采样过程还给 Bagging 带来了另一个优点：由于每个基学习器只使用了初始训练集中约 \(63.2%\) 的样本，剩下的样本可用作验证集来对泛化性能进行“包外估计” (out-of-bag estimate)。 令 \(D_{t}\) 表示 \(h_{t}\) 实际使用的训练集，令 \(H^{oob}(\boldsymbol{x})\) 表示对样本 \(\boldsymbol{x}\) 的包外预测，即仅考虑那些未使用 \(\boldsymbol{x}\) 训练的基学习器在 \(\boldsymbol{x}\) 上的预测，有\[ H^{oob}(\boldsymbol{x})=\arg\max_{y\in\mathcal{Y}}\sum_{t=1}^{T}\Pi\big(h_{t}(\boldsymbol{x})=y\big)\centerdot\Pi(\boldsymbol{x}\notin D_{t}) \]则 Bagging 泛化误差的包外估计为\[ \epsilon^{oob}=\frac{1}{\vert D\vert}\sum_{(\boldsymbol{x},y)\in D}\Pi\big(H^{oob}(\boldsymbol{x})\neq y\big) \]当基学习器是决策树时，包外样本还可以用于辅助剪枝，或用于估计决策树中各结点的后验概率以辅助对零训练样本结点的处理；当基学习器是神经网络时，可使用包外样本辅助早期停止以减小过拟合风险。从偏差-方差分解角度看， Bagging 主要关注降低方差，因此它在不剪枝决策树、神经网络等易受样本扰动的学习器中效用更为明显。 随机森林 随机森林 (Random Forset，RF) 是 Bagging 的一个扩展。RF 在以决策树为基学习器构建 Bagging 集成的基础上，在决策树的训练过程中引入随机属性选择。传统决策树在选择划分属性时是在当前节点的属性集合 (假定有 \(d\) 个属性) 中选择一个最优属性；而在 RF 中，对基决策树的每个结点，先从该结点的属性集合中随机选择一个包含 \(k\) 个属性的子集，然后再从这个子集中选择一个最优属性用于划分。参数 \(k\) 控制了随机性的引入程度： - 若令 \(k=d\)，则基决策树的构建与传统决策树相同； - 若令 \(k=1\)，则是随机选择一个属性划分； - 一般情况下，推荐 \(k=\log_{2}d\)。 与 Bagging 中基学习器的“多样性”仅通过样本扰动而来不同，随机森林中基学习器的多样性不仅来自样本扰动，还来自属性扰动，这使得最终集成的泛化性能可通过个体学习器之间异度的增加而进一步提升。此外，随机森林的收敛性与 Bagging 相似，随机森林的起始性能往往相对较差，特别是在集成中只包含一个基学习器时，因为通过引入属性扰动，随机森林中个体学习器的性能往往有所降低。然而，随着个体学习器数目的增加，随机森林通常会收敛到更低的泛化误差。值得一提的是，随机森林的训练效率常优于 Bagging，因为在个体决策树的构建过程中，Bagging 使用的是“确定型“决策树，在选择划分属性时要对结点的所有属性进行考察，而随机森林shiyongde”随机型“决策树则只需考察一个属性子集。 结合策略 学习器结合可能会从三个方面带来好处： - 从统计方面，由于学习任务的假设空间往往很大，可能有多个假设在训练集上达到同等性能，此时若使用单学习器可能因误选而导致泛化性能不佳，结合多个学习器则会减小这一风险； - 从计算方面，学习算法往往会陷入局部极小，有的局部极小点对应的泛化性能可能很糟糕，而通过多次运行之后进行结合，可降低陷入糟糕局部极小点的风险； - 从表示方面，某些学习任务的真实假设可能不在当前学习算法所考虑的假设空间中，此时若使用单学习器则肯定无效，而通过结合多个学习器，由于假设空间有所扩大，有可能学得更好的近似。 假定集成包含 \(T\) 个基学习器 \(\{h_{1},h_{2},\dots,h_{T}\}\)，其中 \(h_{i}\) 在示例 \(\boldsymbol{x}\) 上的输出为 \(h_{i}(\boldsymbol{x})\)。 平均法 简单平均法 (simple averaging)：\[H(\boldsymbol{x})=\frac{1}{T}\sum_{i=1}^{T}h_{i}(\boldsymbol{x})\] 加权平均法 (weighted averaging)：\[H(\boldsymbol{x})=\sum_{i=1}^{T}w_{i}h_{i}(\boldsymbol{x})\]其中 \(w_{i}\) 是个体学习器 \(h_{i}\) 的权重，通常要求 \(w_{i}\geq 0\)，\(\sum_{i=1}^{T}w_{i}=1\)。 加权平均法的权重一般从训练数据中学习而得，但由于样本不充分或噪声，通常学习的权重并不完全可靠，对于规模较大的集成，由于权重较多，甚至可能导致过拟合。一般而言，在个体学习器性能相差较大时宜使用加权平均法，而在个体学习器性能相近时宜使用简单平均法。 投票法 对分类任务，学习器 \(h_{i}\) 将从类别标记集合 \(\{c_{1},\dots,c_{N}\}\) 中预测出一个标记。将 \(h_{i}\) 的预测输出表示为一个 \(N\) 维向量 \(\big(h_{i}^{1}(\boldsymbol{x}),\dots,h_{i}^{N}(\boldsymbol{x})\big)\)，其中 \(h_{i}^{j}(\boldsymbol{x})\) 表示 \(h_{i}\) 在类别标记 \(c_{j}\) 上的输出。 - 绝对多数投票法 (majority voting)：\[H(\boldsymbol{x})=\begin{cases} c_{j}, &amp; \sum_{i=1}^{T}h_{i}^{j}(\boldsymbol{x})&gt;0.5\sum_{k=1}^{N}\sum_{i=1}^{T}h_{i}^{k}(\boldsymbol{x});\\ reject, &amp; otherwise. \end{cases}\]即若某标记得票过半数，则预测为该标记；否则拒绝预测。 - 相对多数投票法 (plurality voting)：\[H(\boldsymbol{x})=c_{\arg\max_{j}\sum_{i=1}^{T}h_{i}^{j}(\boldsymbol{x})}\]即预测为的票最多的标记，若同时有多个标记获最高票，则从中随机选择一个。 - 加权投票法 (weighted voting)：\[H(\boldsymbol{x})=c_{\arg\max_{j}\sum_{i=1}^{T}w_{i}h_{i}^{j}(\boldsymbol{x})}\]与加权平均法类似，\(w_{i}\) 是 \(h_{i}\) 的权重，通常 \(w_{i}\geq 0\)，\(\sum_{i=1}^{T}w_{i}=1\)。 以上没有限制个体学习器输出值的类型，实际中，不同类型的个体学习器可能产生不同类型的输出值，常见的有： - 类标记：\(h_{i}^{j}(\boldsymbol{x})\in\{0,1\}\)，若 \(h_{i}\) 将样本 \(\boldsymbol{x}\) 预测为类别 \(c_{j}\) 则取值为1，否则为0.使用类标记的投票亦称“硬投票” (hard voting)。 - 类概率：\(h_{i}^{j}(\boldsymbol{x})\in[0,1]\)，相当于对后验概率 \(P(c_{j}|\boldsymbol{x})\) 的一个估计。使用概率的投票亦称“软投票” (soft voting)。 注意：不同类型的 \(h_{i}^{j}(\boldsymbol{x})\) 值不能混用。 学习法 当训练数据很多时，一种更为强大的结合策略是使用“学习法”，即通过另一个学习器来进行结合。Stacking是学习法的典型代表，这里称个体学习器为初级学习器，用于结合的学习器称为次级学习器或元学习器 (meta-learner)。 Stacking先从初始数据集训练出初级学习器，然后生成一个新数据集用于训练次级学习器。在这个新数据集中，初级学习器的输出被当作样例输入特征，而初始样本的标记仍被当作样例标记。Stacking算法描述如下，这里假定初级学习器使用不同学习算法产生，即初级集成是异质的。 &gt; 输入：训练集 \(D=\{(\boldsymbol{x}_{1},y_{1}),\dots,(\boldsymbol{x}_{m},y_{m})\}\)，初级学习算法 \(\mathfrak{L}_{1},\mathfrak{L}_{2},\dots,\mathfrak{L}_{T}\)，次级学习算法 \(\mathfrak{L}\)。 &gt; 过程： &gt; \(\qquad\)for \(t=1,2,\dots,T\) do &gt; \(\qquad\qquad h_{t}=\mathfrak{L}_{t}(D)\) &gt; \(\qquad\)end for &gt; \(\qquad D&#39;=\emptyset\) &gt; \(\qquad\)for \(i=1,2,\dots,m\) do &gt; \(\qquad\qquad\)for \(t=1,2,\dots,T\) do &gt; \(\qquad\qquad\qquad z_{it}=h_{t}(\boldsymbol{x}_{i})\) &gt; \(\qquad\qquad\)end for &gt; \(\qquad\qquad D&#39;=D&#39;\bigcup\big((z_{i1},\dots,z_{iT}),y_{i}\big)\) &gt; \(\qquad\)end for &gt; \(\qquad h&#39;=\mathfrak{L}(D&#39;)\) &gt; 输出：\(H(\boldsymbol{x})=h&#39;\big(h_{1}(\boldsymbol{x}),\dots,h_{T}(\boldsymbol{x})\big)\) 在训练阶段，次级训练集释利用初级学习器产生的，若直接用初级学习器的训练集来产生次级训练集，则过拟合风险比较大。因此一般通过交叉验证或留一法这样的方式，用训练初级学习器未使用的样本来产生次级学习器的训练样本。次级学习器的输入属性表示和次级学习算法对Stacking集成的泛化性能优很大影响。将初级学习器的输出类概率作为次级学习器的输入属性，用多响应线性回归 (Multi-response Linear Regression，MLR) 作为次级学习算法效果较好，在MLR中使用不同的属性集效果更佳。 此外，贝叶斯模型平均 (Bayes Model Averaging，BMA) 基于后验概率来为不同模型赋予权重，可视为加权平均法的一种特殊实现，理论上来说，若数据生成模型恰在当前考虑的模型中，切数据噪声很少，则BMA不差于Stacking，然而实际中很难确保这一要求。因此 Stacking 通常优于 BMA，因为其鲁棒性比 BMA好，且 BMA 对模型近似误差非常敏感。 多样性 误差-分歧分解 设集成泛化误差为 \(E\)，令 \(\bar{E}=\sum_{i=1}^{T}w_{i}E_{i}\) 表示个体学习器泛化误差的加权均值，\(\bar{A}=\sum_{i=1}^{T}w_{i}A_{i}\) 表示个体学习器的加权分歧值，有\[ E=\bar{E}-\bar{A} \]这个式子明确提示出：个体学习器准确性越高、多样性越大，则集成越好。(推导此处省略，书中推导过程只适用于回归学习，难以直接推广到分类学习任务。) 多样性度量 多样性度量 (diversity measure) 是用于度量集成中个体分类器的多样性，即估算个体学习器的多样化程度。典型做法是考虑个体分类器两两相似/不相似性。对二分类任务，分类器 \(h_{i}\) 与 \(h_{j}\) 的预测结果列联表 (contingency table) 为 \(h_{i}=+1\) \(h_{i}=-1\) \(h_{j}=+1\) a c \(h_{j}=-1\) b d 其中，a表示 \(h_{i}\) 与 \(h_{j}\) 均预测为正类的样本数目；b、c、d 含义类推；\(a+b+c+d=m\)。基于此列联遍，给出以下常见多样性度量。 - 不合度量 (disafreement measure)：\[dis_{ij}=\frac{b+c}{m}\]\(dis_{ij}\in[0,1]\)，值越大多样性越大。 - 相关系数 (correlation coefficient)：\[\rho_{ij}=\frac{ad-bc}{\sqrt{(a+b)(a+c)(c+d)(b+d)}}\]\(\rho_{ij}\in[-1,1]\)，若 \(h_{i}\) 与 \(h_{j}\) 无关，则值为0；若 \(h_{i}\) 与 \(h_{j}\) 正相关则值为正，否则为负。 - \(Q\)-统计量 (\(Q\)-statistic)：\[Q_{ij}=\frac{ad-bc}{ad+bc}\]\(Q_{ij}\) 与相关系数 \(\rho_{ij}\)的符号相同，且 \(\vert Q_{ij}\vert\leq\vert\rho_{ij}\vert\)。 - \(\kappa\)-统计量 (\(\kappa\)-statistic)：\[\kappa=\frac{p_{1}-p_{2}}{1-p_{2}}\]其中，\(p_{1}\) 是两个分类器取得一致的概率；\(p_{2}\) 是两个分类器偶然达成一致的概率，它们可由数据集 \(D\) 估算：\[p_{1}=\frac{a+d}{m}\]\[p_{2}=\frac{(a+b)(a+c)+(c+d)(b+d)}{m^{2}}\]若分类器 \(h_{i}\) 与 \(h_{j}\) 在 \(D\) 上完全一致，则 \(\kappa=1\)；若它们仅是偶然达成一致，则 \(\kappa=0\)。\(\kappa\) 通常非负，仅在 \(h_{i}\) 与 \(h_{j}\) 达成一致的概率甚至低于偶然性的情况下取负值。 以上都是“成对型” (pairwise) 多样性度量，可以通过二维图绘制出来，如著名的“\(\kappa\)-误差图”，如下面的示例 其中横坐标是这对分类器的 \(\kappa\) 值，纵坐标是它们的平均误差，显然，数据点云的位置越高，则个体分类器准确性越低；点云的位置越靠右，则个体学习器的多样性越小。 多样性增强 集成学习中需有效地生成多样性大的个体学习器，一般方法是在学习过程中引入随机性，常见方法是对数据样本、输入属性、输出表示、算法参数进行扰动。 - 样本数据扰动：给定初始数据集，可从中残生不同数据子集，再利用不同的数据子集训练处不同的个体学习器。数据样本扰动通常基于采样法，简单高效。这种方法对“不稳定基学习器”，如决策树、神经网络等很有效，但对于对数据样本扰动不敏感的基学习器 (稳定基学习器)，如线性学习器、支持向量机、朴素贝叶斯、\(k\)-近邻学习器等效果不明显。 - 输入属性扰动：训练样本通常由一组属性描述，不同的“子空间”提供了不同的数据观察视角。显然，从不同子空间训练出的个体学习器必然有所不同。随机子空间 (random subspace) 算法是一种代表性方法。 - 输出表示扰动：通过对输出表示进行操纵以增强多样性。可对训练样本的类标记稍作变动，如“翻转法” (Flipping Output) 随机改变一些训练样本标记；也可对输出表示进行转化，如“输出调制法” (Output Smearing) 将分类输出转化为回归输出后构建个体学习器；还可将原任务拆解为多个可同时求解的子任务，如ECOC法利用纠错输出码将多分类任务拆解为一系列二分类任务来训练基学习器。 - 算法参数扰动：基学习算法一般都有参数需要进行设置，因此可以通过随机设置不同的参数，往往可产生差别较大的个体学习器，如“负相关法” (Negative Correlation) 显式地通过正则化项来强制个体神经网络使用不同的参数。对参数较少的算法，可通过将其学习过程中某些环节用其它类似方式代替，从而达到扰动目的。 习题 8.1 假设抛硬币正面朝上的概率为 \(p\)，反面朝上的概率为 \(1-p\)，令 \(H(n)\) 代表抛 \(n\) 次硬币所得正面朝上的次数，则最多 \(k\) 次正面朝上的概率为\[ P(H(n)\leq k)=\sum_{i=0}^{k}{n\choose i}p^{i}(1-p)^{n-i} \]对 \(\delta&gt;0\)，\(k=(p-\delta)n\)，有 Hoeffding 不等式\[ P(H(n)\leq(p-\delta)n)\leq e^{-2\delta^{2}n} \]试推导出式(3)。 Ans: 取\(p-\delta=\frac{1}{2}\)，则\(\delta=p-\frac{1}{2}=\frac{1}{2}-\epsilon\)，\[P(H(n)\leq\frac{n}{2})=\sum_{i=0}{\frac{n}{2}}{n \choose i}p^{i}(1-p)^{n-i}\leq e^{-2(\frac{1}{2}-\epsilon)^{2}n}=e^{-\frac{1}{2}(1-2\epsilon)^{2}n}\] 8.2 对于 \(0/1\) 损失函数来说，指数损失函数并非仅有的一致替代函数。考虑式(5)，试证明：任意损失函数 \(\ell\big(-\boldsymbol{f}(\boldsymbol{x})H(\boldsymbol{x})\big)\)，若对于 \(H(\boldsymbol{x})\) 在区间 \([-\infty,\delta]\) \((\delta&gt;0)\) 上单调递减，则 \(\ell\) 是 \(0/1\) 损失函数的一致替代函数。 Ans: 总损失\[ \mathcal{L}=\ell\big(-H(x)f(x)\big)P\big(f(x)|x\big)=\ell\big(-H(x)\big)\centerdot P\big(f(x)=1|x\big)+\ell\big(H(x)\big)\centerdot P\big(f(x)=0|x\big),\quad H(x)\in\{-1,1\} \]要使\(\mathcal{L}\)最小，当\(P(f(x)=1|x)&gt;P(f(x)=0|x)\)时，会希望\(\ell(-H(x))&lt;\ell(H(x))\)，由于\(\ell\)是递减的，得\(H(x)&gt;H(-x)\)，的\(H(x)=1\)。同理当\(P(f(x)=1|x)&lt;P(f(x)=0|x)\)时，\(H(x)=−1\)。\(\ell(−H(x)f(x))\)是对\(H(x)\)的单调递减函数，那么可以认为\(\ell(−H(x)f(x))\)是对\(−H(x)\)的单调递增函数。此时\(H(x)=\arg\max_{y\in0,1}P(f(x)=y|x)\),即达到了贝叶斯最优错误率，说明\(\ell\)是\(0/1\)损失函数的一致替代函数。 8.3 从网上下载或自己编程实现AdaBoost，以不剪枝决策树为基学习器，在西瓜数据集3.0\(\alpha\)上训练一个AdaBoost集成。 Ans: 由于西瓜数据集数据量太小，这里我们使用UCI数据集的Iris数据进行实验，并使用Sci-kit Learn机器学习包编程，具体细节见代码。 123456789101112131415161718192021222324252627282930313233343536373839import pandas as pdimport warningsfrom sklearn.ensemble import AdaBoostClassifierfrom sklearn.tree import DecisionTreeClassifierfrom sklearn.model_selection import train_test_split, cross_val_scorefrom sklearn.utils import shufflewarnings.filterwarnings('ignore')data = pd.read_csv('iris.csv')labels = data['class'] # labelsdata.drop(['class'], axis=1, inplace=True) # data# shuffling data and labelsdata, labels = shuffle(data, labels, random_state=5)# split into train and testtrain_data, test_data, train_labels, test_labels = train_test_split(data, labels, test_size=0.1, random_state=200)# create base learnerbase_learner = DecisionTreeClassifier(criterion='gini', splitter='best', max_depth=2, random_state=100, max_leaf_nodes=5)# create AdaBoost classifier and fit train dataada_boost = AdaBoostClassifier(base_learner, n_estimators=10, algorithm='SAMME', learning_rate=0.5, random_state=200)print(ada_boost, '\n') # print AdaBoost configuration# fit dataada_boost.fit(train_data, train_labels)print(ada_boost.base_estimator_, '\n') # print base learnerprint('Base Learner error and weight:')for idx, err, weight in zip(range(1, 11), ada_boost.estimator_errors_, ada_boost.estimator_weights_): print('Base Learner-%d\t' % idx, err, '\t', weight)print('\n')print('Feature Importance:')for feature, importance in zip(list(train_data), ada_boost.feature_importances_): print(feature, '\t', importance)print('\n')# Accuracyscores = cross_val_score(ada_boost, train_data, train_labels, cv=5)print("Accuracy: %0.2f (+/- %0.2f)\n" % (scores.mean(), scores.std() * 2))# predictpredict_labels = ada_boost.predict(test_data)for predict, test_label in zip(predict_labels, test_labels): print(predict, '\t', test_label) 得到结果： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950AdaBoostClassifier(algorithm='SAMME', base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=2, max_features=None, max_leaf_nodes=5, min_impurity_split=1e-07, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, presort=False, random_state=100, splitter='best'), learning_rate=0.5, n_estimators=10, random_state=200) DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=2, max_features=None, max_leaf_nodes=5, min_impurity_split=1e-07, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, presort=False, random_state=100, splitter='best') Base Learner error and weight:Base Learner-1 0.0296296296296 2.09102507132Base Learner-2 0.104626988622 1.41999302436Base Learner-3 0.111889828638 1.38236413421Base Learner-4 0.116849602012 1.3578775189Base Learner-5 0.136963254896 1.26694588583Base Learner-6 0.180878117849 1.1017783246Base Learner-7 0.174611689277 1.12321827288Base Learner-8 0.169743533665 1.14029657839Base Learner-9 0.273137055613 0.835955706868Base Learner-10 0.127787913186 1.30690391613Feature Importance:sepalLength 0.134583407492sepalWidth 0.0584706598454petalLength 0.382968813518petalWidth 0.423977119144Accuracy: 0.94 (+/- 0.04)Predict ActualIris-setosa Iris-setosaIris-virginica Iris-virginicaIris-versicolor Iris-versicolorIris-versicolor Iris-versicolorIris-versicolor Iris-versicolorIris-virginica Iris-virginicaIris-setosa Iris-setosaIris-setosa Iris-setosaIris-setosa Iris-setosaIris-versicolor Iris-versicolorIris-virginica Iris-virginicaIris-virginica Iris-virginicaIris-versicolor Iris-versicolorIris-versicolor Iris-versicolorIris-versicolor Iris-versicolor 8.4 GradientBoosting是一种常用的Boosting算法，试分析其与AdaBoost的异同。 Ans: GradientBoosting与AdaBoost相同的地方在于要生成多个分类器以及每个分类器都有一个权值，最后将所有分类器加权累加起来。不同在于：AdaBoost通过每个分类器的分类结果改变每个样本的权值用于新的分类器和生成权值，但不改变每个样本不会改变。GradientBoosting将每个分类器对样本的预测值与真实值的差值传入下一个分类器来生成新的分类器和权值(这个差值就是下降方向)，而每个样本的权值不变。 8.5 试编程实现Bagging，以决策树桩为基学习器，在西瓜数据集3.0\(\alpha\)上训练一个Bagging集成。 Ans: 这里同样适用UCI数据集的Iris数据进行实验，具体细节见代码。 1234567891011121314151617181920212223242526272829303132333435import pandas as pdimport warningsfrom sklearn.ensemble import BaggingClassifierfrom sklearn.tree import DecisionTreeClassifierfrom sklearn.utils import shufflefrom sklearn.model_selection import train_test_split, cross_val_scorewarnings.filterwarnings('ignore')data = pd.read_csv('iris.csv')labels = data['class'] # labelsdata.drop(['class'], axis=1, inplace=True) # data# shuffling data and labelsdata, labels = shuffle(data, labels, random_state=5)# split into train and testtrain_data, test_data, train_labels, test_labels = train_test_split(data, labels, test_size=0.1, random_state=200)# create base learnerbase_learner = DecisionTreeClassifier(criterion='gini', splitter='best', max_depth=2, random_state=100, max_leaf_nodes=5)# create bagging classifierbagging = BaggingClassifier(base_learner, n_estimators=10, bootstrap=True, bootstrap_features=True, oob_score=True, max_samples=0.5, random_state=200, n_jobs=-1)print(bagging, '\n') # print bagging configuration# fit databagging.fit(train_data, train_labels)print(bagging.base_estimator_, '\n') # print base learnerprint('Base Learner Features:')for idx, feature in zip(range(1, 11), bagging.estimators_features_): print('Base Learner-%d\t' % idx, feature)print('\n')scores = cross_val_score(bagging, train_data, train_labels, cv=5)print("Accuracy: %0.2f (+/- %0.2f)\n" % (scores.mean(), scores.std() * 2))# predictpredict_labels = bagging.predict(test_data)for predict, test_label in zip(predict_labels, test_labels): print(predict, '\t', test_label) 输出结果为： 1234567891011121314151617181920212223242526272829303132333435363738394041424344BaggingClassifier(base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=2, max_features=None, max_leaf_nodes=5, min_impurity_split=1e-07, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, presort=False, random_state=100, splitter='best'), bootstrap=True, bootstrap_features=True, max_features=1.0, max_samples=0.5, n_estimators=10, n_jobs=-1, oob_score=True, random_state=200, verbose=0, warm_start=False) DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=2, max_features=None, max_leaf_nodes=5, min_impurity_split=1e-07, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, presort=False, random_state=100, splitter='best') Base Learner Features:Base Learner-1 [1 1 1 0]Base Learner-2 [1 2 3 0]Base Learner-3 [3 3 1 1]Base Learner-4 [2 2 1 2]Base Learner-5 [3 1 0 2]Base Learner-6 [2 2 1 2]Base Learner-7 [3 2 0 2]Base Learner-8 [3 0 2 0]Base Learner-9 [1 1 3 3]Base Learner-10 [1 0 1 3]Accuracy: 0.93 (+/- 0.03)Iris-setosa Iris-setosaIris-virginica Iris-virginicaIris-versicolor Iris-versicolorIris-versicolor Iris-versicolorIris-versicolor Iris-versicolorIris-virginica Iris-virginicaIris-setosa Iris-setosaIris-setosa Iris-setosaIris-setosa Iris-setosaIris-versicolor Iris-versicolorIris-virginica Iris-virginicaIris-virginica Iris-virginicaIris-versicolor Iris-versicolorIris-versicolor Iris-versicolorIris-versicolor Iris-versicolor 8.6 试析Bagging通常为何难以提升朴素贝叶斯分类器的性能。 Ans: Bagging主要是降低分类器的方差，而朴素贝叶斯分类器没有方差可以减小。对全训练样本生成的朴素贝叶斯分类器是最优的分类器，不能用随机抽样来提高泛化性能。 8.7 试析随机森林为何比决策树Bagging集成的训练速度更快。 Ans: 随机森林不仅会随机样本，还会在所有样本属性中随机几种出来计算。这样每次生成分类器时都是对部分属性计算最优，速度会比Bagging计算全属性要快。 8.8 MultiBoosting算法将AdaBoost作为Bagging的基学习器，Iterative Bagging算法则是将Bagging作为AdaBoost的基学习器。试比较二者的优缺点。 Ans: MultiBoosting由于集合了Bagging，Wagging，AdaBoost，可以有效的降低误差和方差，特别是误差。但是训练成本和预测成本都会显著增加。Iterative Bagging相比Bagging会降低误差，但是方差上升。由于Bagging本身就是一种降低方差的算法，所以Iterative Bagging相当于Bagging与单分类器的折中。 8.9 试设计一种可视的多样性度量，对8.3和8.5中得到的集成进行评估，并与 \(\kappa\)-误差图比较。 Ans: TODO 8.10 试设计一种能提升\(k\)近邻分类器性能的集成学习方法。 Ans: 可以使用Bagging来提升k近邻分类器的性能，每次随机抽样出一个子样本，并训练一个k近邻分类器，对测试样本进行分类。最终取最多的一种分类。]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>python</tag>
        <tag>boosting</tag>
        <tag>random forest</tag>
        <tag>bagging</tag>
        <tag>decision tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习(周志华)--学习笔记(三) 支持向量机(Support Vector Machine)]]></title>
    <url>%2F2017%2F08%2F04%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%91%A8%E5%BF%97%E5%8D%8E-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-3%2F</url>
    <content type="text"><![CDATA[第六章–支持向量机 (Support Vector Machine) 间隔与支持向量 给定数据集\(D=\{(\boldsymbol{x}_{1},y_{1}),\dots,(\boldsymbol{x}_{m},y_{m})\}\)，\(y_{i}\in\{-1,+1\}\)，分类学习最基本的思想是基于训练集\(D\)在样本空间中找到一个划分超平面，将不同类别的样本分开。但能将训练样本粉来的划分超平面可能有很多，直观上，应该去找两类训练样本“正中间”的划分超平面，即下图中粗线表示的划分超平面，因为该划分对训练样本局部扰动的“容忍性”最好。 在样本空间中，划分超平面由下述线性方程表示：\[ \boldsymbol{w}^{T}\boldsymbol{x}+b=0\tag{1} \]其中\(\boldsymbol{w}=(w_{1},\dots,w_{d})\)为法向量，决定了超平面的方向，\(b\)为位移项，决定了超平面与原点之间的距离。记超平面为\((\boldsymbol{w},b)\)，样本空间中任意点\(\boldsymbol{x}\)到超平面\((\boldsymbol{w},b)\)的距离可写为\[ r=\frac{\vert\boldsymbol{w}^{T}\boldsymbol{x}+b\vert}{\Vert\boldsymbol{w}\Vert}\tag{2} \]假设\((\boldsymbol{w},b)\)能将训练样本正确分类，即对于\((\boldsymbol{x}_{i},y_{i})\in D\)，若\(y_{i}=+1\)，则有\(\boldsymbol{w}^{T}\boldsymbol{x}_{i}+b&gt;0\)；若\(y_{i}=-1\)，则\(\boldsymbol{w}^{T}\boldsymbol{x}_{i}+b&lt;0\)。令\[ \begin{cases} \boldsymbol{w}^{T}\boldsymbol{x}_{i}+b\geq+1,&amp;y_{i}=+1;\\ \boldsymbol{w}^{T}\boldsymbol{x}_{i}+b\leq-1,&amp;y_{i}=-1; \end{cases}\tag{3}\]如下图，距离超平面最近的几个训练样本点使公式(3)成立，它们被称为“支持向量” (support vector)，两个异类支持向量到超平面的距离之和为\[ \gamma=\frac{2}{\Vert\boldsymbol{w}\Vert}\tag{4}\]它被称为“间隔” (margin)。 求解“最大间隔” (maximum margin)的划分超平面，即找到满足公式(3)中的约束参数\(\boldsymbol{w}\)和\(b\)，使得\(\gamma\)最大：\[\begin{aligned} &amp; \max_{\boldsymbol{w},b}\frac{2}{\Vert\boldsymbol{w}\Vert}\\ &amp; s.t.\quad y_{i}(\boldsymbol{w}^{T}\boldsymbol{x}_{i}+b)\geq 1,\quad i=1,2,\dots,m \end{aligned}\tag{5} \]最大化间隔，仅需要最大化\(\Vert\boldsymbol{w}\Vert^{-1}\)，等价于最小化\(\Vert\boldsymbol{w}\Vert^{2}\)，于是有\[\begin{aligned} &amp; \min_{\boldsymbol{w},b}\frac{1}{2}\Vert\boldsymbol{w}\Vert^{2}\\ &amp; s.t.\quad y_{i}(\boldsymbol{w}^{T}\boldsymbol{x}_{i}+b)\geq 1,\quad i=1,2,\dots,m \end{aligned}\tag{6} \]上式为支持向量机(Support Vector Machine，SVM)的基本型。 对偶问题 我们希望求解式(6)来得到最大间隔划分超平面所对应的模型\[ f(\boldsymbol{x})=\boldsymbol{w}^{T}\boldsymbol{x}+b\tag{7} \]公式(6)本身为一个凸二次规划 (convex quadratic programming)问题，为求解(6)式，对其使用拉格朗日乘子法可得其“对偶问题” (dual problem)：\[ L(\boldsymbol{w},b,\boldsymbol{\alpha})=\frac{1}{2}\Vert\boldsymbol{w}\Vert^{2}+ \sum_{i=1}^{m}\alpha_{i}\big(1-y_{i}(\boldsymbol{w}^{T}\boldsymbol{x}_{i}+b)\big)\tag{8} \]其中\(\boldsymbol{\alpha}=(\alpha_{1};\dots;\alpha_{m})\)。令\(L(\boldsymbol{w},b,\boldsymbol{\alpha})\)对\(\boldsymbol{w}\)和\(b\)的偏导为零可得\[ \boldsymbol{w}=\sum_{i=1}^{m}\alpha_{i}y_{i}\boldsymbol{x}_{i}\tag{9}\]\[ 0=\sum_{i=1}^{m}\alpha_{i}y_{i}\tag{10} \]将(9)代入(8)，即可将\(L(\boldsymbol{w},b,\boldsymbol{\alpha})\)中的\(\boldsymbol{w}\)和\(b\)消去，再考虑式(10)的约束，可得到式(6)的对偶问题\[ \begin{aligned} &amp; \max_{\boldsymbol{\alpha}}\sum_{i=1}^{m}\alpha_{i}-\frac{1}{2}\sum_{i=1}^{m}\sum_{j=1}^{m}\alpha_{i}\alpha_{j}y_{i}y_{j}\boldsymbol{x}_{i}^{T}\boldsymbol{x}_{j}\\ &amp; s.t.\quad\sum_{i=1}^{m}\alpha_{i}y_{i}=0,\quad\alpha_{i}\geq 0, i=1,2,\dots,m. \end{aligned}\tag{11} \]解出\(\boldsymbol{\alpha}\)后，求出\(\boldsymbol{w}\)和\(b\)即可得到模型\[ f(\boldsymbol{x})=\boldsymbol{w}^{T}\boldsymbol{x}+b=\sum_{i=1}^{m}\alpha_{i}y_{i}\boldsymbol{x}_{i}^{T}\boldsymbol{x}+b\tag{12} \]从对偶问题(11)解出的\(\alpha_{i}\)是式(8)中的拉格朗日乘子，它恰好对应训练样本\((\boldsymbol{x}_{i},y_{i})\)。考虑到式(6)中有不等式约束，因此上述过程需要满足KKT (Karush-Kuhn-Tucker)条件，即\[ \begin{cases} \alpha_{i}\geq 0;\\ y_{i}f(\boldsymbol{x}_{i})-1\geq 0;\\ \alpha_{i}(y_{i}f(\boldsymbol{x}_{i})-1)=0. \end{cases}\tag{13}\]于是，对于任意训练样本\((\boldsymbol{x}_{i},y_{i})\)，总有\(\alpha_{i}=0\)或\(y_{i}f(\boldsymbol{x}_{i})=1\)。若\(\alpha_{i}=0\)，则该样本不会在式(12)的求和中出现。若\(\alpha_{i}&gt;0\)，则必有\(y_{i}f(\boldsymbol{x}_{i})=1\)，所对应的样本点位于最大间隔边界上，是一个支持向量。这显示出支持向量机的一个重要性质：训练完成后，大部分的训练样本都不需要保留，最终模型仅与支持向量有关。 对于式(11)的求解，这是一个二次规划问题，常用SMO (Sequential Minimal Optimization)等高效的方法来求解。SMO的基本思路是先固定\(\alpha_{i}\)以外的所有参数，然后求解\(\alpha_{i}\)上的极值。由于存在约束\(\sum_{i=1}^{m}\alpha_{i}y_{i}=0\)，若固定\(\alpha_{i}\)之外的其他变量，则\(\alpha_{i}\)可由其他变量导出。于是，SMO每次选择两个变量\(\alpha_{i}\)和\(\alpha_{j}\)，并固定其他参数。这样在参数初始化后，SMO不断执行如下两个操作直到收敛： - 选取一对需要更新的变量\(\alpha_{i}\)和\(\alpha_{j}\)。 - 固定\(\alpha_{i}\)和\(\alpha_{j}\)以外的参数，求解(11)获得更新后的\(\alpha_{i}\)和\(\alpha_{j}\)。 注意到只需要选取的\(\alpha_{i}\)和\(\alpha_{j}\)中有一个不满足KKT条件，目标函数就会在迭代后见效，且KKT条件违背程度越大，则变量更新后可能导致的目标函数值减幅越大。因此SMO先选取违背KKT条件程度最大的变量。第二个变量应选择一个使目标函数值减小最快的变量，但比较各变量所对应的目标函数数值减幅的复杂度过高，SMO采用了一个启发式：使选取的两个变量所对应样本之间的间隔最大。这样的两个变量有很大的差别，与对两个相似变量进行更新相比，对它们进行更新会带给目标函数值更大的变化。具体来说，仅考虑\(\alpha_{i}\)和\(\alpha_{j}\)时，(11)中的约束可重写为\[ \alpha_{i}y_{i}+\alpha_{j}y_{j}=c,\quad\alpha_{i}\geq 0,\quad\alpha_{j}\geq 0\tag{14}\]其中\[ c=-\sum_{k\neq i,j}\alpha_{k}y_{k}\tag{15}\]是使\(\sum_{i=1}^{m}\alpha_{i}y_{i}=0\)成立的常数。用\[ \alpha_{i}y_{i}+\alpha_{j}y_{j}=c\tag{16}\]消去(11)中的变量\(\alpha_{j}\)，则得到一个关于\(\alpha_{i}\)的单变量二次规划问题，仅有约束\(\alpha_{i}\geq 0\)。这样的二次规划具有封闭解，能高效地计算出更新后的\(\alpha_{i}\)和\(\alpha_{j}\)。对于偏移项\(b\)，对任意支持向量\((\boldsymbol{x}_{s},y_{s})\)都有\(y_{s}f(\boldsymbol{x}_{s})=1\)，即\[ y_{s}\bigg(\sum_{i\in S}\alpha_{i}y_{i}\boldsymbol{x}_{i}^{T}\boldsymbol{x}+b\bigg)=1\tag{17} \]其中\(S=\{i|\alpha_{i}&gt;0,i=1,2,\dots,m\}\)为所有支持向量的下标集。理论上，可选取任意支持向量，通过求解(17)获得\(b\)，但实际中采用一个更加鲁棒的做法：使用所有支持向量求解的平均值\[ b=\frac{1}{\vert S\vert}\sum_{s\in S}\bigg(y_{s}-\sum_{i\in S}\alpha_{i}y_{i}\boldsymbol{x}_{i}^{T}\boldsymbol{x}_{s}\bigg)\tag{18}\] 核函数 对于原始样本空间不存在一个能正确划分两类样本的超平面的问题，可将样本从原始空间映射到一个更高维德特征空间，使得样本在这个特征空间内线性可分。令\(\phi(\boldsymbol{x})\)表示将\(\boldsymbol{x}\)映射后的特征向量，在特征空间中划分超平面所对应的模型可表示为\[ f(\boldsymbol{x})=\boldsymbol{w}^{T}\phi(\boldsymbol{x})+b\tag{19} \]类似(6)有\[\begin{aligned} &amp;\min_{\boldsymbol{w},b}\frac{1}{2}\Vert\boldsymbol{w}\Vert^{2}\\ &amp;s.t.\quad y_{i}(\boldsymbol{w}^{T}\phi(\boldsymbol{x}_{i})+b)\geq 1,i=1,2,\dots,m \end{aligned}\tag{20}\]其对偶问题是\[ \begin{aligned} &amp; \max_{\boldsymbol{\alpha}}\sum_{i=1}^{m}\alpha_{i}-\frac{1}{2}\sum_{i=1}^{m}\sum_{j=1}^{m}\alpha_{i}\alpha_{j}y_{i}y_{j}\phi(\boldsymbol{x}_{i})^{T}\phi(\boldsymbol{x}_{j})\\ &amp; s.t.\quad\sum_{i=1}^{m}\alpha_{i}y_{i}=0,\quad\alpha_{i}\geq 0, i=1,2,\dots,m. \end{aligned}\tag{21} \]上式需要计算\(\phi(\boldsymbol{x}_{i})^{T}\phi(\boldsymbol{x}_{j})\)，由于特征空间维数较高，直接计算比较困难，通常设计一个函数：\[ \kappa(\boldsymbol{x}_{i},\boldsymbol{x}_{j})=\langle\phi(\boldsymbol{x}_{i}),\phi(\boldsymbol{x}_{j})\rangle=\phi(\boldsymbol{x}_{i})^{T}\phi(\boldsymbol{x}_{j})\tag{22} \]即\(\boldsymbol{x}_{i}\)与\(\boldsymbol{x}_{j}\)在特征空间的内积等于它们在原始样本空间中通过函数\(\kappa(\centerdot,\centerdot)\)计算的结果，于是，(21)可重写为\[ \begin{aligned} &amp; \max_{\boldsymbol{\alpha}}\sum_{i=1}^{m}\alpha_{i}-\frac{1}{2}\sum_{i=1}^{m}\sum_{j=1}^{m}\alpha_{i}\alpha_{j}y_{i}y_{j}\kappa(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\\ &amp; s.t.\quad\sum_{i=1}^{m}\alpha_{i}y_{i}=0,\quad\alpha_{i}\geq 0, i=1,2,\dots,m. \end{aligned}\tag{23} \]求解后得到\[ f(\boldsymbol{x})=\boldsymbol{w}^{T}\phi(\boldsymbol{x})+b=\sum_{i=1}^{m}\alpha_{i}y_{i}\kappa(\boldsymbol{x},\boldsymbol{x}_{i})+b\tag{24} \]这里的函数\(\kappa(\centerdot,\centerdot)\)就是“核函数” (kernel function)。式(24)显示出模型最优解可以通过训练样本的核函数展开，这一展式亦称“支持向量展式” (support vector expansion)。 定理1 (核函数)：令\(\chi\)为输入空间，\(\kappa(\centerdot,\centerdot)\)是定义在 \(\chi\times\chi\) 上的对称函数，则\(\kappa\)是核函数当且仅当对于任意数据\(D={\boldsymbol{x}_{1},\dots,\boldsymbol{x}_{m}}\)，“核矩阵” (kernel matrix) \(\boldsymbol{K}\)总是半正定的：\[\boldsymbol{K}=\begin{bmatrix} \kappa(\boldsymbol{x}_{1},\boldsymbol{x}_{1}) &amp; \dots &amp; \kappa(\boldsymbol{x}_{1},\boldsymbol{x}_{j}) &amp; \dots &amp; \kappa(\boldsymbol{x}_{1},\boldsymbol{x}_{m})\\ \vdots &amp; \ddots &amp; \vdots &amp; \ddots &amp; \vdots\\ \kappa(\boldsymbol{x}_{i},\boldsymbol{x}_{1}) &amp; \dots &amp; \kappa(\boldsymbol{x}_{i},\boldsymbol{x}_{j}) &amp; \dots &amp; \kappa(\boldsymbol{x}_{i},\boldsymbol{x}_{m})\\ \vdots &amp; \ddots &amp; \vdots &amp; \ddots &amp; \vdots\\ \kappa(\boldsymbol{x}_{m},\boldsymbol{x}_{1}) &amp; \dots &amp; \kappa(\boldsymbol{x}_{m},\boldsymbol{x}_{j}) &amp; \dots &amp; \kappa(\boldsymbol{x}_{m},\boldsymbol{x}_{m})\\ \end{bmatrix}\]定理1表明只要一个对称函数所对应的核矩阵半正定，它就能作为核函数使用。事实上，对于一个半正定核矩阵，总能找到一个与之对应的映射\(\phi\)。即任何一个核函数都隐式地定义了一个称为“再生核希尔伯特空间” (Reproducing Kernel Hilbert Space，RKHS)的特征空间。我们希望样本在特征空间内线性可分，因此特征空间的好坏对支持向量机的性能至关重要。于是，“核函数选择”称为支持向量机的最大变数，若选择不合适，意味着将样本映射到了一个不合适的特征空间，很可能导致性能不佳。以下给出常用的核函数 名称 表达式 参数 线性核 \(\kappa(\boldsymbol{x}_{i},\boldsymbol{x}_{j})=\boldsymbol{x}_{i}^{T}\boldsymbol{x}_{j}\) 多项式核 \(\kappa(\boldsymbol{x}_{i},\boldsymbol{x}_{j})=\big(\boldsymbol{x}_{i}^{T}\boldsymbol{x}_{j}\big)^{d}\) \(d\geq 1\)为多项式的次数 高斯核 \(\kappa(\boldsymbol{x}_{i},\boldsymbol{x}_{j})=\exp\big(-\frac{\Vert\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\Vert^{2}}{2\sigma^{2}}\big)\) \(\sigma&gt;0\)为高斯核的带宽(width) 拉普拉斯核 \(\kappa(\boldsymbol{x}_{i},\boldsymbol{x}_{j})=\exp\big(-\frac{\Vert\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\Vert}{\sigma}\big)\) \(\sigma&gt;0\) Sigmoid核 \(\kappa(\boldsymbol{x}_{i},\boldsymbol{x}_{j})=\tanh(\beta\boldsymbol{x}_{i}^{T}\boldsymbol{x}_{j}+\theta)\) \(\tanh\)为双曲正切函数，\(\beta&gt;0\)，\(\theta&lt;0\) 此外，核函数还可通过函数组合得到，例如： - 若\(\kappa_{1}\)和\(\kappa_{2}\)为核函数，则对于任意正数\(\gamma_{1}\)、\(\gamma_{2}\)，其线性组合\[ \gamma_{1}\kappa_{1}+\gamma_{2}\kappa_{2}\tag{25} \]也是核函数。 - 若\(\kappa_{1}\)和\(\kappa_{2}\)为核函数，则核函数的直积\[ \kappa_{1}\otimes\kappa_{2}(\boldsymbol{x},\boldsymbol{z})=\kappa_{1}(\boldsymbol{x},\boldsymbol{z})\kappa_{2}(\boldsymbol{x},\boldsymbol{z})\tag{26} \]也是核函数。 - 若\(\kappa_{1}\)为核函数，则对于任意函数\(g(\boldsymbol{x})\)\[ \kappa(\boldsymbol{x},\boldsymbol{z})=g(\boldsymbol{x})\kappa_{1}(\boldsymbol{x},\boldsymbol{z})g(\boldsymbol{z})\tag{27} \]也是核函数。 软间隔与正则化 现实任务中往往很难确定合适的核函数使得训练样本在特征空间中线性可分，因此可以允许支持向量机载一些样本上出错，这里引入“软间隔” (soft margin)的概念，如下图所示 之前假设支持向量机形式是要求所有样本均满足约束(3)，这称为“硬间隔” (hard margin)，而软间隔则是允许某些样本不满足约束\[ y_{i}(\boldsymbol{w}^{T}\boldsymbol{x}_{i}+b)\geq 1\tag{28} \]当然，在最大化间隔的同时，不满足约束的样本尽可能少，于是优化目标写为\[ \min_{\boldsymbol{w},b}\frac{1}{2}\Vert\boldsymbol{w}\Vert^{2}+C\sum_{i=1}^{m}\ell_{0/1}\big(y_{i}(\boldsymbol{w}^{T}\boldsymbol{x}_{i}+b)-1\big)\tag{29} \]其中\(C&gt;0\)是一个常数，\(\ell_{0/1}\)是“\(0/1\)损失函数”\[ \ell_{0/1}=\begin{cases} 1,\quad z&lt;0\\ 0,\quad otherwise \end{cases}\tag{30}\]显然，当\(C\)无穷大时，(29)迫使所有样本满足约束(28)，于是(29)等价于(6)，当\(C\)取有限值时，(29)允许一些样本不满足约束。然而\(\ell_{0/1}\)非凸、非连续，使得(29)不易直接求解。于是常采用一些函数来替代它，称为“替代损失” (surrogate loss)，这些函数通常侍凸的连续函数且是\(\ell_{0/1}\)的上界。下图给出了常用的三种替代损失函数： - hinge损失：\(\ell_{hinge}(z)=\max(0,1-z)\tag{31}\) - 指数损失(exponential loss)：\(\ell_{exp}(z)=\exp(-z)\tag{32}\) - 对率损失(logistic loss)：\(\ell_{log}(z)=\log(1+\exp(-z))\tag{33}\) 若采用hinge损失，则(29)变为\[ \min_{\boldsymbol{w},b}\frac{1}{2}\Vert\boldsymbol{w}\Vert^{2}+C\sum_{i=1}^{m}\max\big(0,1-y_{i}(\boldsymbol{w}^{T}\boldsymbol{x}_{i}+b)\big)\tag{34} \]引入“松弛变量” (slack variables) \(\xi\geq 0\)，则可将上式重写为\[ \min_{\boldsymbol{w},b,\xi_{i}}\frac{1}{2}\Vert\boldsymbol{w}\Vert^{2}+C\sum_{i=1}^{m}\xi_{i}\tag{35} \]这就是常用的“软间隔支持向量机”。(35)中每个样本都有一个对应的松弛向量，用以表征该样本不满足约束(28)的程度，但是，与(6)相似，这是一个二次规划问题。于是，类似(8)，通过拉格朗日乘子法和得到式(35)的拉格朗日函数\[ L(\boldsymbol{w},b,\boldsymbol{\alpha},\boldsymbol{\xi},\boldsymbol{\mu})= \frac{1}{2}\Vert\boldsymbol{w}\Vert^{2}+C\sum_{i=1}^{m}\xi_{i}+ \sum_{i=1}^{m}\alpha_{i}\big(1-\xi_{i}-y_{i}(\boldsymbol{w}^{T}\boldsymbol{x}_{i}+b)\big)- \sum_{i=1}^{m}\mu_{i}\xi_{i}\tag{36} \]其中\(\alpha_{i}\geq 0\)，\(\mu_{i}\geq 0\)是拉格朗日乘子。令\(L(\boldsymbol{w},b,\boldsymbol{\alpha},\boldsymbol{\xi},\boldsymbol{\mu})\)对\(\boldsymbol{w}\)，\(b\)，\(\xi_{i}\)的偏导为零可得\[ \boldsymbol{w}=\sum_{i}^{m}\alpha_{i}y_{i}\boldsymbol{x}_{i}\tag{37}\]\[ 0=\sum_{i=1}^{m}\alpha_{i}y_{i}\tag{38}\]\[ C=\alpha_{i}+\mu_{i}\tag{39} \]将上面三个式子代入(36)可得到(35)的对偶问题\[\begin{aligned} &amp; \max_{\boldsymbol{\alpha}}\sum_{i=1}^{m}\alpha_{i}-\frac{1}{2}\sum_{i}^{m}\sum_{j=1}^{m}\alpha_{i}\alpha_{j}y_{i}y_{j}\boldsymbol{x}_{i}^{T}\boldsymbol{x}_{j}\\ &amp; s.t.\quad\sum_{i=1}^{m}\alpha_{i}y_{i}=0,\quad 0\leq\alpha_{i}\leq C, i=1,2,\dots,m \end{aligned}\tag{40}\]上式与(11)对比可得，两者唯一的差别就在于对偶变量的约束不同：前者是\(0\leq\alpha_{i}\leq C\)，后者是\(0\leq\alpha_{i}\)。于是，可采用同样的方法(SMO)求解(40)。在引入核函数后能得到与式(24)同样的支持向量展式。类似(13)，对软间隔支持向量机，KKT条件要求\[\begin{cases} \alpha_{i}\geq 0,\quad\mu_{i}\geq 0,\\ y_{i}f(\boldsymbol{x}_{i})-1+\xi_{i}\geq 0,\\ \alpha_{i}\big(y_{i}f(\boldsymbol{x}_{i})-1+\xi_{i}\big)=0,\\ \xi_{i}\geq 0,\quad\mu_{i}\xi_{i}=0, \end{cases}\tag{41}\]于是，对于任意训练样本\((\boldsymbol{x}_{i},y_{i})\)，总有\(\alpha_{i}=0\)或\(y_{i}f(\boldsymbol{x}_{i})=1-\xi_{i}\)。若\(\alpha_{i}=0\)，则该样本不会对\(f(\boldsymbol{x})\)有任何影响；若\(\alpha_{i}&gt;0\)，则必有\(y_{i}f(\boldsymbol{x}_{i})=1-\xi_{i}\)，即该样本是支持向量：由(39)可知，若\(\alpha_{i}&lt; C\)，则\(\mu_{i}&gt;0\)，进而有\(\xi_{i}=0\)，即该样本恰在最大间隔边界上；若\(\alpha_{i}=C\)，则有\(\mu_{i}=0\)，此时若\(\xi_{i}\leq 1\)则该样本落在最大间隔内部，若\(\xi_{i}&gt;1\)，则该样本被错误分类。因此，软间隔支持向量机的最终模型仅与支持向量有关，即通过采用hinge损失函数仍保持了稀疏性。 除了hinge损失函数，也可以使用其他的替代损失函数，若使用对率损失函数替代(29)中的\(0/1\)损失函数，则几乎得到对率回归模型(参考：link)。实际上，支持向量机与对率回归的优化目标相近，通常情况下它们的性能也相当。对率回归的优势主要在于其输出具有自然的概率意义，即在给出预测标记的同时也给出了概率，而支持向量机的输出不具有概率意义，欲得到概率输出需要特殊处理。此外，对率回归能直接用于多任务分类，支持向量机则需进行推广。 而从上图能看出，hinge损失有一块“平坦”的零区域，这使得支持向量机的解具有稀疏性，而对率损失是光滑的单调递减函数，不能导出类似的支持向量概念，因此对率回归的解依赖于更多的训练样本，其预测开销更大。将\(0/1\)损失函数换成别的替代函数可以得到不同的学习模型，这些模型的性质与所用的替代函数直接相关，但它们具有一个共性：优化目标中的第一项用来描述划分超平面的“间隔”大小，另一项\(\sum_{i=1}^{m}\ell\big(f(\boldsymbol{x}_{i},y_{i})\big)\)涌来表述训练集上的误差，可写为更一般的形式\[ \min_{f}\Omega(f)+C\sum_{i=1}^{m}\ell\big(f(\boldsymbol{x}_{i},y_{i})\big)\tag{42} \]其中\(\Omega(f)\)称为“结构风险” (structural risk)，用于描述模型\(f\)的某些性质；第二项\(\sum_{i=1}^{m}\ell\big(f(\boldsymbol{x}_{i},y_{i})\big)\)称为“经验风险” (empirical risk)，用于描述模型与训练数据的契合程度。\(C\)用于对二者进行折中。从经验风险最小化的角度来看，\(\Omega(f)\)表述了我们希望获得具有何种性质的模型(例如希望获得复杂度较小的模型)，这为引入领域知识和用户意图提供了途径；另一方面，该信息有助于消减假设空间，从而降低了最小化训练误差的过拟合风险。从这个角度来说，(42)称为“正则化” (regularization)问题，\(\Omega(f)\)称为正则化项，\(C\)则称为正则化常数。\(\boldsymbol{L}_{p}\)范数(norm)是常用的正则化项，其中\(\boldsymbol{L}_{2}\)范数\(\Vert\boldsymbol{w}\Vert_{2}\)倾向于\(\boldsymbol{w}\)的分量取值尽量均衡，即非零分量个数尽量稠密，而\(\boldsymbol{L}_{0}\)范数\(\Vert\boldsymbol{w}\Vert_{0}\)和\(\boldsymbol{L}_{1}\)范数\(\Vert\boldsymbol{w}\Vert_{1}\)则倾向于\(boldsymbol{w}\)的分量尽量稀疏，即非零分量个数尽量少。 支持向量回归 给定训练样本 \(D=\{(\boldsymbol{x}_{1},y_{1}),\dots,(\boldsymbol{x}_{m},y_{m})\}\)，\(y_{i}\in\mathbb{R}\)，我们希望学得一个形如式(7)的回归模型，使得\(f(\boldsymbol{x})\)与\(y\)尽可能的接近，\(\boldsymbol{w}\)和\(b\)是待定的模型参数。对于此问题，支持向量回归(Support Vector Regression，SVR)假设能容忍\(f(\boldsymbol{x})\)与\(y\)之间最多有\(\epsilon\)的偏差，即仅当\(f(\boldsymbol{x})\)与\(y\)之间的差别绝对值大于\(\epsilon\)时才计算损失。如下图所示，相当于以\(f(\boldsymbol{x})\)为中心，构建一个宽度为\(2\epsilon\)的间隔带，若样本落入此间隔带，则认为是被预测正确的。 于是，SVR问题可形式化为\[ \min_{\boldsymbol{w},b}\frac{1}{2}\Vert\boldsymbol{w}\Vert^{2}+C\sum_{i=1}^{m}\ell_{\epsilon}\big(f(\boldsymbol{x}_{i})-y_{i}\big)\tag{43} \]其中\(C\)为正则化常数，\(\ell_{\epsilon}\)是如下图所示的\(\epsilon\)-不敏感损失(\(\epsilon\)-insensitive loss)函数\[ \ell_{\epsilon}(z)=\begin{cases} 0, &amp; ivertz\vert\leq\epsilon; \vert z\vert-\epsilon, &amp; otherwise. \end{cases}\tag{44}\] 引入松弛变量\(\xi_{i}\)和\(\hat{\xi}_{i}\)，可将(43)重写为\[ \min_{\boldsymbol{w},b,\xi_{i},\hat{\xi}_{i}}\frac{1}{2}\Vert\boldsymbol{w}\Vert^{2}+C\sum_{i=1}^{m}(\xi_{i}+\hat{\xi}_{i})\tag{45}\]\[\begin{aligned} s.t.\quad &amp; f(\boldsymbol{x}_{i})-y_{i}\leq\epsilon+\xi_{i},\\ &amp; y_{i}-f(\boldsymbol{x}_{i})\leq\epsilon+\hat{\xi}_{i},\\ &amp; \xi_{i}\geq 0,\quad\hat{\xi}_{i}\geq 0,\quad i=1,2,\dots,m \end{aligned}\]类似(36)，通过引入拉格朗日乘子 \(\mu_{i}\geq 0\)，\(\hat{\mu}_{i}\geq 0\)，\(\alpha_{i}\geq 0\)，\(\hat{\alpha}_{i}\geq 0\)，由拉格朗日乘子法可得到(45)的拉格朗日函数\[\begin{aligned} &amp; L(\boldsymbol{w},b,\boldsymbol{\alpha},\hat{\boldsymbol{\alpha}},\boldsymbol{\xi},\hat{\boldsymbol{\xi}},\boldsymbol{\mu},\hat{\boldsymbol{\mu}})\\ &amp; =\frac{1}{2}\Vert\boldsymbol{w}\Vert^{2}+C\sum_{i=1}^{m}(\xi_{i}+\hat{\xi}_{i})-\sum_{i=1}^{m}\mu_{i}\xi_{i}-\sum_{i=1}^{m}\hat{\mu}_{i}\hat{\xi}_{i}\\ &amp; +\sum_{i=1}^{m}\alpha_{i}\big(f(\boldsymbol{x}_{i})-y_{i}-\epsilon-\xi_{i}\big)+\sum_{i=1}^{m}\hat{\alpha}_{i}\big(y_{i}-f(\boldsymbol{x}_{i})-\epsilon-\hat{\xi}_{i}\big) \end{aligned}\tag{46}\]将(7)代入，再令\(L(\boldsymbol{w},b,\boldsymbol{\alpha},\hat{\boldsymbol{\alpha}},\boldsymbol{\xi},\hat{\boldsymbol{\xi}},\boldsymbol{\mu},\hat{\boldsymbol{\mu}})\)对\(\boldsymbol{w}\)，\(b\)，\(\xi_{i}\) 和 \(\hat{\xi}_{i}\) 的偏导为零可得\[ \boldsymbol{w}=\sum_{i=1}^{m}(\hat{\alpha}_{i}-\alpha_{i})\boldsymbol{x}_{i}\tag{47} \]\[ 0=\sum_{i=1}^{m}(\hat{\alpha}_{i}-\alpha_{i})\tag{48} \]\[ C=\alpha_{i}+\mu_{i}\tag{49} \]\[ C=\hat{\alpha}_{i}+\hat{\mu}_{i}\tag{50} \]将上述四个式子代入(46)可得到SVR的对偶问题\[ \begin{aligned} &amp; \max_{\boldsymbol{\alpha},\hat{\boldsymbol{\alpha}}}\sum_{i=1}^{m}y_{i}(\hat{\alpha}_{i}-\alpha_{i})-\epsilon(\hat{\alpha}_{i}+\alpha_{i})-\frac{1}{2}\sum_{i=1}^{m}\sum_{j=1}^{m}(\hat{\alpha}_{i}-\alpha_{i})(\hat{\alpha}_{j}-\alpha_{j})\boldsymbol{x}_{i}^{T}\boldsymbol{x}_{j}\\ &amp; s.t.\quad\sum_{i=1}^{m}(\hat{\alpha}_{i}-\alpha_{i})=0,\quad 0\leq\alpha_{i},\hat{\alpha}_{i}\leq C. \end{aligned} \tag{51} \]上述过程需满足KKT条件，即\[ \begin{cases} \alpha_{i}\big(f(\boldsymbol{x}_{i})-y_{i}-\epsilon-\xi_{i}\big)=0,\\ \hat{\alpha}_{i}\big(y_{i}-f(\boldsymbol{x}_{i})-\epsilon-\hat{\xi}_{i}\big)=0,\\ \alpha_{i}\hat{\alpha}_{i}=0,\quad\xi_{i}\hat{\xi}_{i}=0,\\ (C-\alpha_{i})\xi_{i}=0,\quad(C-\hat{\alpha}_{i})\hat{\xi}_{i}=0. \end{cases}\tag{52} \]可以看出，当且仅当\(f(\boldsymbol{x}_{i})-y_{i}-\epsilon-\xi_{i}=0\)时\(\alpha_{i}\)能取非零值，当且仅当\(y_{i}-f(\boldsymbol{x}_{i})-\epsilon-\hat{\xi}_{i}=0\)时\(\hat{\alpha}_{i}\)能取非零值。换言之仅当样本\((\boldsymbol{x}_{i},y_{i})\)不落入\(\epsilon\)-间隔带中，相应的\(\alpha_{i}\)和\(\hat{\alpha}_{i}\)能去非零值。此外，这两个约束不能同时成立，即\(\alpha_{i}\)和\(\hat{\alpha}_{i}\)中至少有一个为零。将(47)代入(7)，则SVR的解形如\[ f(\boldsymbol{x})=\sum_{i=1}^{m}(\hat{\alpha}_{i}-\alpha_{i})\boldsymbol{x}_{i}^{T}\boldsymbol{x}+b\tag{53} \]使(53)中的\((\hat{\alpha}_{i}-\alpha_{i})\neq 0\)的样本即为SVR的支持向量，它们必须落在\(\epsilon\)-间隔带之外。显然，SVR只是的支持向量只是训练样本的一部分，即其解仍具有稀疏性。而由KKT条件(52)可看出，对于每个样本\((\boldsymbol{x}_{i},y_{i})\)都有\((C-\alpha_{i})\xi_{i}=0\)且\(\alpha_{i}\big(f(\boldsymbol{x}_{i})-y_{i}-\epsilon-\xi_{i}\big)=0\)。于是，在得到\(\alpha_{i}\)后，若\(0&lt;\alpha_{i}&lt; C\)，则必有\(\xi_{i}=0\)，进而有\[ b=y_{i}+\epsilon-\sum_{i=1}^{m}(\hat{\alpha}_{i}-\alpha_{i})\boldsymbol{x}_{i}^{T}\boldsymbol{x}\tag{54} \]因此，在求解(51)得到\(\alpha_{i}\)后，理论上来说，可任意选取满足\(0&lt;\alpha_{i}&lt; C\)的样本通过(54)求得\(b\)。实际中常采用更加鲁棒的方法：选取多个(或所有)满足该条件的样本求解\(b\)后取平均值。若考虑特征映射形式(19)，则相应的(47)将形如\[ \boldsymbol{w}=\sum_{i=1}^{m}(\hat{\alpha}_{i}-\alpha_{i})\phi(\boldsymbol{x}_{i})\tag{55} \]将(55)代入(19)，则SVR可表示为\[ f(\boldsymbol{x})=\sum_{i=1}^{m}(\hat{\alpha}_{i}-\alpha_{i})\kappa(\boldsymbol{x},\boldsymbol{x}_{i})+b\tag{56} \]其中，\(\kappa(\boldsymbol{x}_{i},\boldsymbol{x}_{j})=\phi(\boldsymbol{x}_{i})^{T}\phi(\boldsymbol{x}_{j}))\)为核函数。 核方法 根据(24)和(56)，给定训练样本\(\{(\boldsymbol{x}_{1},y_{1}),\dots,(\boldsymbol{x}_{m},y_{m})\}\)，若不考虑偏移项\(b\)，无论SVM还是SVR，学得的模型总能表述成\(\kappa(\boldsymbol{x},\boldsymbol{x}_{i})\)的线性组合。 定理2 表示定理(representer theorem)：令 \(\mathbb{H}\) 为核函数 \(\kappa\) 对应的再生核希尔伯特空间，\(\Vert h\Vert_{\mathbb{H}}\) 表示 \(\mathbb{H}\) 空间中关于 \(h\) 的范数，对于任意单调递增函数 \(\Omega:[0,\infty]\mapsto\mathbb{R}\) 和任意非负损失函数 \(\ell:\mathbb{R}^{m}\mapsto[0,\infty]\)，优化问题\[ \min_{h\in\mathbb{H}}F(h)=\Omega\big(\Vert h\Vert_{\mathbb{H}}\big)+\ell\big(h(\boldsymbol{x}_{1}),\dots,h(\boldsymbol{x}_{m})\big)\tag{57} \]的解总可写为\[ h^{*}(\boldsymbol{x})=\sum_{i=1}^{m}\alpha_{i}\kappa(\boldsymbol{x},h(\boldsymbol{x}_{i}))\tag{58} \]表示定理对损失函数没有限制，对正则化项 \(\Omega\) 仅要求单调递增，甚至不要求 \(\Omega\) 是凸函数，意味着对于一般的损失函数正则项，优化问题(57)的最优解 \(h^{*}(\boldsymbol{x})\) 都可表示为核函数 \(\kappa(\boldsymbol{x},\boldsymbol{x}_{i})\) 的线性组合。 通常将基于核函数的学习方法统称为“核方法” (kernel methods)。如通过“核化” (即引入核函数)来将线性学习器拓展为非线性学习器。以“核线性判别分析” (Kernelized Linear Discriminant Analysis，KLDA) 为例，假设可通过某种映射 \(\phi:\boldsymbol{\chi}\mapsto\mathbb{F}\) 将样本映射到一个特征空间 \(\mathbb{F}\)，然后在 \(\mathbb{F}\) 中执行线性判别分析，以求得\[ h(\boldsymbol{x})=\boldsymbol{w}^{T}\phi(\boldsymbol{x})\tag{59} \]类似LDA，KLDA的学习目标是\[ \max_{\boldsymbol{w}}J(\boldsymbol{w})=\frac{\boldsymbol{w}^{T}\boldsymbol{S}_{b}^{\phi}\boldsymbol{w}}{\boldsymbol{w}^{T}\boldsymbol{S}_{\boldsymbol{w}}^{\phi}\boldsymbol{w}}\tag{60} \]其中 \(\boldsymbol{S}_{b}^{\phi}\) 和 \(\boldsymbol{S}_{\boldsymbol{w}}^{\phi}\) 分别为训练样本在特征空间 \(\mathbb{F}\) 中的类间散度矩阵 (between-class scatter matrix) 和类内散度矩阵 (within-class scatter matrix)。令 \(X_{i}\) 表示第 \(i\in\{0,1\}\) 类样本的集合，其样本数为 \(m_{i}\)；总样本数 \(m=m_{0}+m_{1}\)。第 \(i\) 类样本在特征空间 \(\mathbb{F}\) 中的均值为\[ \boldsymbol{\mu}_{i}^{\phi}=\frac{1}{m_{i}}\sum_{\boldsymbol{x}\in X_{i}}\phi(\boldsymbol{x})\tag{61} \]两个散度矩阵分别为\[ \boldsymbol{S}_{b}^{\phi}=(\boldsymbol{\mu}_{1}^{\phi}-\boldsymbol{\mu}_{0}^{\phi})(\boldsymbol{\mu}_{1}^{\phi}-\boldsymbol{\mu}_{0}^{\phi})^{T}\tag{62} \]\[ \boldsymbol{S}_{\boldsymbol{w}}^{\phi}=\sum_{i=0}^{1}\sum_{\boldsymbol{x}\in X_{i}}\big(\phi(\boldsymbol{x})-\boldsymbol{\mu}_{i}^{\phi}\big)\big(\phi(\boldsymbol{x})-\boldsymbol{\mu}_{i}^{\phi}\big)^{T}\tag{63} \]通常映射 \(\phi\) 的具体形式很难得到，因此使用核函数 \(\kappa(\boldsymbol{x},\boldsymbol{x}_{i})=\phi(\boldsymbol{x}_{i})^{T}\phi(\boldsymbol{x})\) 来隐式地表达这个映射和特征空间 \(\mathbb{F}\)。把 \(J(\boldsymbol{w})\) 作为(57) 中的损失函数 \(\ell\)，再令 \(\Omega=0\)，由表示定理，函数 \(h(\boldsymbol{x})\) 可写为\[ h(\boldsymbol{x})=\sum_{i=1}^{m}\alpha_{i}\kappa(\boldsymbol{x},\boldsymbol{x}_{i})\tag{64} \]于是由(59)可得\[ \boldsymbol{w}=\sum_{i=1}^{m}\alpha_{i}\phi(\boldsymbol{x}_{i})\tag{65} \]令 \(\boldsymbol{K}\in\mathbb{R}^{m\times m}\) 为核函数 \(\kappa\) 所对应的核矩阵，\((\boldsymbol{K})_{ij}=\kappa(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\)。令 \(\boldsymbol{1}_{i}\in\{1,0\}^{m\times 1}\) 为第 \(i\) 类样本的指示向量，即 \(\boldsymbol{1}_{i}\) 的第 \(j\) 个分量为1当且仅当 \(\boldsymbol{x}_{j}\in X_{i}\)，否则 \(\boldsymbol{1}_{i}\) 的第 \(j\) 个分量为0.再令\[ \hat{\boldsymbol{\mu}}_{0}=\frac{1}{m_{0}}\boldsymbol{K}\boldsymbol{1}_{0}\tag{66} \]\[ \hat{\boldsymbol{\mu}}_{1}=\frac{1}{m_{1}}\boldsymbol{K}\boldsymbol{1}_{1}\tag{67} \]\[ \boldsymbol{M}=(\hat{\boldsymbol{\mu}}_{0}-\hat{\boldsymbol{\mu}}_{1})(\hat{\boldsymbol{\mu}}_{0}-\hat{\boldsymbol{\mu}}_{1})^{T}\tag{68} \]\[ \boldsymbol{N}=\boldsymbol{K}\boldsymbol{K}^{T}-\sum_{i=0}^{1}m_{i}\hat{\boldsymbol{\mu}}_{i}\hat{\boldsymbol{\mu}}_{i}^{T}\tag{69} \]于是(60)等价为\[ \max_{\boldsymbol{\alpha}}J(\boldsymbol{\alpha})=\frac{\boldsymbol{\alpha}^{T}\boldsymbol{M}\boldsymbol{\alpha}}{\boldsymbol{\alpha}^{T}\boldsymbol{N}\boldsymbol{\alpha}}\tag{70} \]显然，使用线性判别分析求解方法即可得到 \(\boldsymbol{\alpha}\)，进而可由(64)得到投影函数 \(h(\boldsymbol{x})\)。 习题 6.1 试证明样本空间中任意点 \(\boldsymbol{x}\) 到超平面 \((\boldsymbol{w},b)\)的距离为公式(2)。 Ans: 超平面 \((\boldsymbol{w},b)\) 的平面法向量为 \(\boldsymbol{w}\) ，任取平面上一点 \(\boldsymbol{x}_{0}\)，有 \(\boldsymbol{w}^{T}\boldsymbol{x}_{0}+b=0\)。\(\boldsymbol{x}\) 到平面的距离就是 \(\boldsymbol{x}\) 到 \(\boldsymbol{x}_{0}\) 的距离往 \(\boldsymbol{w}\) 方向的投影，就是 \(\frac{\vert\boldsymbol{w}^{T}(\boldsymbol{x}-\boldsymbol{x}_{0})\vert}{\vert\boldsymbol{w}\vert}=\frac{\vert\boldsymbol{w}^{T}\boldsymbol{x}+b\vert}{\vert\boldsymbol{w}\vert}\)。 6.2 试使用LIBSVM，在西瓜数据集3.0\(\alpha\)上分别用线性核和高斯核训练一个SVM，并比较其支持向量的差别。 Ans: 由于西瓜数据集3.0\(\alpha\)数据量太小，我们这里直接采用UCI数据集的Iris数据集，并使用Weka机器学习包进行试验。首先下载数据并转换为ARFF格式，如下： 12345678910111213@relation Iris@attribute sepallength NUMERIC@attribute sepalwidth NUMERIC@attribute petallength NUMERIC @attribute petalwidth NUMERIC@attribute class &#123;setosa, versicolor, virginica&#125;@data5.1,3.5,1.4,0.2,setosa4.9,3.0,1.4,0.2,setosa4.7,3.2,1.3,0.2,setosa4.6,3.1,1.5,0.2,setosa5.0,3.6,1.4,0.2,setosa... 之后读入数据： 123Instances data = new Instances(new FileReader("src/main/resources/iris.data"));data.setClassIndex(data.numAttributes() - 1); // set labeldata.randomize(new Random(123)); // shuffling data 现在我们使用Weka机器学习包的LibSVM分别构建线性核和高斯核的SVM并训练数据： 12345678910// build linear SVMLibSVM linearSVM = new LibSVM();linearSVM.setSVMType(new SelectedTag(LibSVM.SVMTYPE_C_SVC, LibSVM.TAGS_SVMTYPE));linearSVM.setKernelType(new SelectedTag(LibSVM.KERNELTYPE_LINEAR, LibSVM.TAGS_KERNELTYPE));linearSVM.buildClassifier(data); // build classifier and train// build gaussian SVMLibSVM gaussSVM = new LibSVM();gaussSVM.setSVMType(new SelectedTag(LibSVM.SVMTYPE_C_SVC, LibSVM.TAGS_SVMTYPE));gaussSVM.setKernelType(new SelectedTag(LibSVM.KERNELTYPE_RBF, LibSVM.TAGS_KERNELTYPE));gaussSVM.buildClassifier(data); // build classifier and train 由于Weka封装了LibSVM，无法直接使用Weka得到训练后模型的支持向量，这里我们使用如下方法得到训练后的模型参数： 12345private static svm_model getModel(LibSVM svm) throws IllegalAccessException, NoSuchFieldException &#123; Field modelField = svm.getClass().getDeclaredField("m_Model"); modelField.setAccessible(true); return (svm_model) modelField.get(svm);&#125; 我们使用上面的函数来获得训练后的模型，并打印支持向量 1234567svm_model linearModel = getModel(linearSVM);svm_model gaussModel = getModel(gaussSVM);int[] indices = linearModel.sv_indices;for (int i : indices) System.out.println(i + ": " + data.instance(i - 1));System.out.println();indices = gaussModel.sv_indices;for (int i : indices) System.out.println(i + ": " + data.instance(i - 1)); 于是我们得到线性核和高斯核模型的支持向量： 线性核 高斯核 4: 5.4,3,4.5,1.5,versicolor 4: 5.4,3,4.5,1.5,versicolor 5: 6.3,2.5,4.9,1.5,versicolor 5: 6.3,2.5,4.9,1.5,versicolor 37: 6.8,2.8,4.8,1.4,versicolor 13: 4.9,2.4,3.3,1,versicolor 42: 6.7,3,5,1.7,versicolor 37: 6.8,2.8,4.8,1.4,versicolor 45: 6.2,2.2,4.5,1.5,versicolor 41: 6.7,3.1,4.7,1.5,versicolor 61: 6.3,3.3,4.7,1.6,versicolor 42: 6.7,3,5,1.7,versicolor 63: 5.6,3,4.5,1.5,versicolor 45: 6.2,2.2,4.5,1.5,versicolor 99: 5.9,3.2,4.8,1.8,versicolor 61: 6.3,3.3,4.7,1.6,versicolor 104: 6,2.7,5.1,1.6,versicolor 63: 5.6,3,4.5,1.5,versicolor 108: 6.1,2.9,4.7,1.4,versicolor 77: 6.5,2.8,4.6,1.5,versicolor 124: 6.9,3.1,4.9,1.5,versicolor 82: 6,2.9,4.5,1.5,versicolor 126: 5.1,2.5,3,1.1,versicolor 99: 5.9,3.2,4.8,1.8,versicolor 7: 6.3,2.5,5,1.9,virginica 104: 6,2.7,5.1,1.6,versicolor 15: 4.9,2.5,4.5,1.7,virginica 108: 6.1,2.9,4.7,1.4,versicolor 21: 7.2,3,5.8,1.6,virginica 113: 7,3.2,4.7,1.4,versicolor 33: 6,3,4.8,1.8,virginica 124: 6.9,3.1,4.9,1.5,versicolor 39: 6.3,2.8,5.1,1.5,virginica 126: 5.1,2.5,3,1.1,versicolor 57: 6.3,2.7,4.9,1.8,virginica 146: 6,3.4,4.5,1.6,versicolor 64: 6.5,3,5.2,2,virginica 148: 5,2,3.5,1,versicolor 72: 6.5,3.2,5.1,2,virginica 7: 6.3,2.5,5,1.9,virginica 90: 6.1,3,4.9,1.8,virginica 15: 4.9,2.5,4.5,1.7,virginica 98: 5.9,3,5.1,1.8,virginica 21: 7.2,3,5.8,1.6,virginica 139: 6.2,2.8,4.8,1.8,virginica 25: 6.1,2.6,5.6,1.4,virginica 147: 6,2.2,5,1.5,virginica 33: 6,3,4.8,1.8,virginica 56: 4.5,2.3,1.3,0.3,setosa 39: 6.3,2.8,5.1,1.5,virginica 89: 5.1,3.3,1.7,0.5,setosa 48: 6.3,3.3,6,2.5,virginica 117: 4.8,3.4,1.9,0.2,setosa 57: 6.3,2.7,4.9,1.8,virginica done … Total: 27 Total: 45 6.3 选择两个UCI数据集，分别用线性核和高斯核训练一个SVM，并与BP神经网络核C4.5决策树进行试验比较。 Ans: 这里也选用UCI的Iris数据集进行测试。首先我们将数据拆分为训练集(80%)和测试集(20%)： 1234567// load train datasetInstances trainData = new Instances(new FileReader("src/main/resources/iris-train.data"));trainData.setClassIndex(trainData.numAttributes() - 1);trainData.randomize(new Random(123));// load test datasetInstances testData = new Instances(new FileReader("src/main/resources/iris-test.data"));testData.setClassIndex(testData.numAttributes() - 1); 之后我们用Weka机器学习包分别构建线性核和高斯核的SVM、C4.5决策树和BP神经网络，如下(SVM的构建与6.2相同，这里省略)： 12345678910111213// build C4.5 decision treeJ48 tree = new J48();tree.setUnpruned(false);tree.setReducedErrorPruning(true);tree.buildClassifier(trainData);// build BP neural networksMultilayerPerceptron mlp = new MultilayerPerceptron();mlp.setLearningRate(0.3);mlp.setMomentum(0.2);mlp.setSeed(123);mlp.setHiddenLayers("3");mlp.setTrainingTime(10000);mlp.buildClassifier(trainData); 之后我们使用Evaluation对构建的四个模型分别进行评价： 1234567891011121314// EvaluateEvaluation evalLinear = new Evaluation(testData);evalLinear.evaluateModel(linearSVM, testData);System.out.println("Linear SVM" + "\n" + evalLinear.toSummaryString() + "\n" + evalLinear.toMatrixString() + "\n");Evaluation evalGauss = new Evaluation(testData);evalGauss.evaluateModel(gaussSVM, testData);System.out.println("Gaussian SVM" + "\n" + evalGauss.toSummaryString() + "\n" + evalGauss.toMatrixString() + "\n");Evaluation evalTree = new Evaluation(testData);evalTree.evaluateModel(tree, testData);System.out.println("C4.5 Decision Tree" + "\n" + evalTree.toSummaryString() + "\n" + evalTree.toMatrixString() + "\n");Evaluation evalMlp = new Evaluation(testData);evalMlp.evaluateModel(mlp, testData);System.out.println();System.out.println("BP Neural Network" + "\n" + evalMlp.toSummaryString() + "\n" + evalMlp.toMatrixString()); 得到如下结果： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859Linear SVMCorrectly Classified Instances 30 100 %Incorrectly Classified Instances 0 0 %Kappa statistic 1 Mean absolute error 0 Root mean squared error 0 Relative absolute error 0 %Root relative squared error 0 %Total Number of Instances 30 === Confusion Matrix === a b c &lt;-- classified as 10 0 0 | a = setosa 0 10 0 | b = versicolor 0 0 10 | c = virginicaGaussian SVMCorrectly Classified Instances 30 100 %Incorrectly Classified Instances 0 0 %Kappa statistic 1 Mean absolute error 0 Root mean squared error 0 Relative absolute error 0 %Root relative squared error 0 %Total Number of Instances 30 === Confusion Matrix === a b c &lt;-- classified as 10 0 0 | a = setosa 0 10 0 | b = versicolor 0 0 10 | c = virginicaC4.5 Decision TreeCorrectly Classified Instances 29 96.6667 %Incorrectly Classified Instances 1 3.3333 %Kappa statistic 0.95 Mean absolute error 0.0466Root mean squared error 0.149 Relative absolute error 10.4945 %Root relative squared error 31.6147 %Total Number of Instances 30 === Confusion Matrix === a b c &lt;-- classified as 9 1 0 | a = setosa 0 10 0 | b = versicolor 0 0 10 | c = virginicaBP Neural NetworkCorrectly Classified Instances 30 100 %Incorrectly Classified Instances 0 0 %Kappa statistic 1 Mean absolute error 0.0062Root mean squared error 0.0121Relative absolute error 1.3956 %Root relative squared error 2.5607 %Total Number of Instances 30 === Confusion Matrix === a b c &lt;-- classified as 10 0 0 | a = setosa 0 10 0 | b = versicolor 0 0 10 | c = virginica 6.4 试讨论线性判别分析与线性核支持向量机在何种条件下等价。 Ans: 当线性SVM和LDA求出的 \(\boldsymbol{w}\) 互相垂直时，两者是等价的，SVM此时也就比LDA多了个偏移项\(b\)。因为首先，如果可以使用软间隔的线性SVM，其实线性可分这个条件是不必要的，如果是硬间隔线性SVM，那么线性可分是必要条件。这个题只说了是线性SVM，就没必要关心数据是不是可分，毕竟LDA是都可以处理的。其次假如当前样本线性可分，且SVM与LDA求出的结果相互垂直。当SVM的支持向量固定时，再加入新的样本，并不会改变求出的w，但是新加入的样本会改变原类型数据的协方差和均值，从而导致LDA求出的结果发生改变。这个时候两者的 \(\boldsymbol{w}\) 就不垂直了，但是数据依然是可分的。 6.5 试述高斯核SVM与RBF神经网络之间的联系。 Ans: RBF网络的径向基函数与SVM都可以采用高斯核，也就分别得到了高斯核RBF网络与高斯核SVM。神经网络是最小化累计误差，将参数作为惩罚项，而SVM相反，主要是最小化参数，将误差作为惩罚项。在二分类问题中，如果将RBF中隐层数为样本个数，且每个样本中心就是样本参数，得出的RBF网络与核SVM基本等价，非支持向量将得到很小的\(\boldsymbol{w}\)。 6.6 试分析SVM对噪声敏感的原因。 Ans: SVM的目的是求出与支持向量有最大化距离的直线，以每个样本为圆心，该距离为半径做圆，可以近似认为圆内的点与该样本属于相同分类。如果出现了噪声，那么这个噪声所带来的错误分类也将最大化，所以SVM对噪声是很敏感的。 6.7 试给出(52)的完整KKT条件。 Ans: 非等式约束写成拉格朗日乘子式，取最优解要满足两个条件 - 拉格朗日乘子式对所有非拉格朗日参数的一阶偏导为0。 - 非等式约束对应的拉格朗日项，要么非等式的等号成立，要么对应的拉格朗日参数为0。 因此完整的KKT条件为\[\begin{cases} \boldsymbol{w}=\sum_{i=1}^{m}(\hat{\alpha}_{i}-\alpha_{i})\boldsymbol{x}_{i},\\ 0=\sum_{i=1}^{m}(\hat{\alpha}_{i}-\alpha_{i}),\\ C=\alpha_{i}+\mu_{i},\\ C=\hat{\alpha}_{i}+\hat{\mu}_{i},\\ \alpha_{i}\big(f(\boldsymbol{x}_{i})-y_{i}-\epsilon-\xi_{i}\big)=0,\\ \hat{\alpha}_{i}\big(y_{i}-f(\boldsymbol{x}_{i})-\epsilon-\hat{\xi}_{i}\big)=0,\\ (C-\alpha_{i})\xi_{i}=0,\\ (C-\hat{\alpha}_{i})\hat{\xi}_{i}=0. \end{cases}\] 6.8 以西瓜数据集3.0\(\alpha\)的“密度”为输入，“含糖率”为输出，试使用LIBSVM训练一个SVR。 Ans: 首先将西瓜数据集转换为ARFF格式，然后搭建SVR进行试验。 12345678910Instances data = new Instances(new FileReader("src/main/resources/watermelon.data"));data.setClassIndex(data.numAttributes() - 1);LibSVM svr = new LibSVM();svr.setSVMType(new SelectedTag(LibSVM.SVMTYPE_EPSILON_SVR, LibSVM.TAGS_SVMTYPE));svr.setKernelType(new SelectedTag(LibSVM.KERNELTYPE_RBF, LibSVM.TAGS_KERNELTYPE));svr.buildClassifier(data);for (Instance inst : data) System.out.println(svr.classifyInstance(inst) + "\t" + inst.classValue());Evaluation eval = new Evaluation(data);eval.evaluateModel(svr, data);System.out.println(eval.toSummaryString()); 得到结果： 123456789101112130.23163745020672713 0.460.24295566116372483 0.3760.22201597399900289 0.2640.21799998153899916 0.3180.2099638655436369 0.215...Correlation coefficient 0.1952Mean absolute error 0.0963Root mean squared error 0.1142Relative absolute error 101.723 %Root relative squared error 98.1963 %Total Number of Instances 17 6.9 试使用核技巧推广对率回归，产生“核对率回归”。 Ans: 由表示定理可知，一般优化问题的解可 \(h(\boldsymbol{x})\) 以写成核函数的线性组合。即\[ h(\boldsymbol{x})=\sum_{i=1}^{m}\boldsymbol{w}\kappa(\boldsymbol{x},\boldsymbol{x}_{i}) \]可推出\[ \boldsymbol{w}=\sum_{i=1}^{m}\alpha_{i}\phi(\boldsymbol{x}_{i}) \]其中 \(\phi(\boldsymbol{x})\) 是 \(\boldsymbol{x}\) 在更高维的映射，于是 \(\boldsymbol{w}^{T}\boldsymbol{x}+b\) 可以改写为\[ \sum_{i=1}^{m}\alpha_{i}\phi(\boldsymbol{x}_{i})\phi(\boldsymbol{x})=\sum_{i=1}^{m}\alpha_{i}\kappa(\boldsymbol{x},\boldsymbol{x}_{i})+b \]令\(\boldsymbol{\beta}=(\boldsymbol{\alpha};b)\)，\(\hat{\boldsymbol{t}}_{i}=(\kappa_{i};1)\)，其中 \(\kappa_{i}\) 表示核矩阵 \(\kappa\) 的第 \(i\) 列，可得\[ \mathcal{l}(\boldsymbol{\beta})=\sum_{i}\big(-y_{i}\boldsymbol{\beta}^{T}\boldsymbol{t}_{i}+\ln(1+e^{\boldsymbol{\beta}^{T}\boldsymbol{t}_{i}})\big) \]之后的推导于线性回归中的对数几率回归相同。 6.10 试设计一个能显著减少SVM中支持向量的数目儿不显著降低泛化性能的方法。 Ans: 对于线性的SVM，三个属性不完全一样的支持向量就能确定这个SVM，而其他的落在边缘上的点都可以舍弃。 注：以上习题的代码试验并没有考虑模型的优化问题，本人本身对Weka并不熟练，只是知道一些用法，所以直接使用Weka进行模型搭建和输出的演示。本人更习惯使用Spark的MLlib，但是由于MLlib对SVM的实现很少，只实现了线性二分类。没有非线性(核函数)，也没有多分类和回归，这也是无奈之举。(其实使用Python的Scikit-Learn机器学习包更方便)。 Spark MLlib实现二分类SVM 这里我们同样使用Iris数据集，由于Spark MLlib 中的SVM模型只支持线性二分类，所以我们去掉Iris-virginica类型数据，只保留Iris-versicolor(记为1)和Iris-setosa(记为0)进行实验。 12345678910111213141516171819202122232425262728293031object SVM &#123; def parsingIris (str: String): LabeledPoint = &#123; // create Iris parser: CSV Data =&gt; LabeledPoint val fields = str.split(",") assert(fields.size == 5) val label = fields(4) match &#123; case "Iris-setosa" =&gt; 0.0 case "Iris-versicolor" =&gt; 1.0 case "Iris-virginica" =&gt; 2.0 &#125; LabeledPoint.apply(label, Vectors.dense(fields(0).toDouble, fields(1).toDouble, fields(2).toDouble, fields(3).toDouble)) &#125; def main (args: Array[String]): Unit = &#123; Logger.getLogger("org").setLevel(Level.OFF) // close logger /** create Session */ val spark = SparkSession.builder().master("local").appName("SVM").getOrCreate() import spark.implicits._ /** load data and transform to dataframe */ val iris : RDD[LabeledPoint] = spark.read.textFile(new ClassPathResource("iris.txt").getFile.getAbsolutePath) .map(parsingIris) // convert Iris data to LabeledPoint .filter(e =&gt; e.label != 2.0) // drop third class to build a two classes data .rdd // convert to RDD val splits = iris.randomSplit(Array(0.8, 0.2), seed=123L) // 80% for training, 20% for testing val training = splits(0).cache() val test = splits(1) val model: SVMModel = SVMWithSGD.train(training, 1000) // build model model.clearThreshold() // to get the score, not classified result println("Weight: ".concat(model.weights.toString)) // print trained weight println("Test Result: ") test.map(point =&gt; (model.predict(point.features), if(model.predict(point.features) &gt; 0) 1.0 else 0.0, point.label)).foreach(println) // print score, classified result and label &#125;&#125; 以下是分类结果： 123456789101112131415161718192021222324Weight: [-0.5139153688335307,-1.5350451605908375,2.1700813440361717,0.8963829165055544]Test Result: (-3.9059303241050616,0.0,0.0)(-4.327164416860141,0.0,0.0)(-3.688252095110469,0.0,0.0)(-3.495468123629213,0.0,0.0)(-5.020411486531796,0.0,0.0)(-5.582160204156088,0.0,0.0)(-3.3835161794399573,0.0,0.0)(-4.610619380646923,0.0,0.0)(-3.676140013649289,0.0,0.0)(-5.6212847686519805,0.0,0.0)(-4.172989806210081,0.0,0.0)(-4.969019949648443,0.0,0.0)(2.908737548495826,1.0,1.0)(3.3041995740088774,1.0,1.0)(2.8216798618199084,1.0,1.0)(1.6480335988062373,1.0,1.0)(4.08563451107511,1.0,1.0)(4.325889225283133,1.0,1.0)(3.57481724420649,1.0,1.0)(2.7216874014911334,1.0,1.0)(2.415040975436961,1.0,1.0)(2.8969329555616268,1.0,1.0)]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>java</tag>
        <tag>spark</tag>
        <tag>scala</tag>
        <tag>support vector machine</tag>
        <tag>weka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Seq2Seq Learning and Neural Conversational Model]]></title>
    <url>%2F2017%2F08%2F02%2FSeq2Seq-Learning-and-Neural-Conversational-Model%2F</url>
    <content type="text"><![CDATA[It is a summary of two papers: Sequence to Sequence Learning with Neural Networks and A Neural Conversational Model, as well as the implementation of Neural Conversation Model via Java with deeplearning4j package. Sequence to Sequence Model In this paper, the author presents a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. The method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. As an example shown below: The Model: The author chooses GravesLSTM as the network layer to avoid long term dependencies issue (vanishing gradient), where the goal of LSTM is to estimate the conditional probability \(p(y_{1},\dots,y_{T&#39;}|x_{1},\dots,x_{T})\), here \((x_{1},\dots,x_{T})\) is an input sequence and \((y_{1},\dots,y_{T&#39;})\) is its corresponding output sequence whose length \(T&#39;\) may differ from \(T\). The LSTM computes this conditional probability by first obtaining the fixed-dimensional representation \(v\) of the input sequence \((x_{1},\dots,x_{T})\) given by the last hidden state of the LSTM, and then computing the probability of \((y_{1},\dots,y_{T&#39;})\) with a standard LSTM-LM formulation whose initial hidden state is set to the representation \(v\) of \((x_{1},\dots,x_{T})\):\[ p(y_{1},\dots,y_{T&#39;}|x_{1},\dots,x_{T})=\prod_{t=1}^{T&#39;}p(y_{t}|v,y_{1},\dots,y_{t-1})\]In this equation, each \(p(y_{t}|v,y_{1},\dots,y_{t-1})\) distribution is represented with a softmax over all the words in the vocabulary. It is important to note that each sentence ends with a special end-of-sentence symbol &lt;EOS&gt;, which enables the model to define a distribution over sequences of all possible lengths. As the example shown below The LSTM computes the representation of A, B, C, &lt;EOS&gt; and then uses this representation to compute the probability of W, X, Y, Z, &lt;EOS&gt;. Thus, the model reads an input sentence ABC and produces WXYZ as the output sentence. The model stops making predictions after outputting the end-of-sentence token &lt;EOS&gt;. The actual model that the author built has three important differences to make the model more sophisticated and robust: - The author used two different LSTMs: one for the input sequence and another for the output sequence, because doing so increases the number model parameters at negligible computational cost and makes it natural to train the LSTM on multiple language pairs simultaneously. - The author chose an LSTM with four layers, since deep LSTMs significantly outperformed shallow LSTMs. - The author found it extremely valuable to reverse the order of the words of the input sentence. For instance, instead of mapping the sentence (\(a\), \(b\), \(c\)) to the sentence (\(\alpha\), \(\beta\), \(\gamma\)), the LSTM is asked to map (\(c\), \(b\), \(a\)) to (\(\alpha\), \(\beta\), \(\gamma\)), where (\(\alpha\), \(\beta\), \(\gamma\)) is the translation of (\(a\), \(b\), \(c\)). This way, \(a\) is in close proximity to \(\alpha\), \(b\) is fairly close to \(\beta\), and so on, a fact that makes it easy for SGD to “establish communication” between the input and the output. The author used the WMT’14 English to French dataset, and trained the model on a subset of 12M sen- tences consisting of 348M French words and 304M English words, which is a clean “selected” subset from here. As typical neural language models rely on a vector representation for each word, the author used a fixed vocabulary for both languages, 160K of the most frequent words for the source language and 80K of the most frequent words for the target language. Every out-of-vocabulary word was replaced with a special UNK token. With the model and the dataset, the author achieved some good results (the best result achieved by direct translation with large neural networks at that time). More details about Decoding and Rescoring, Reversing the Source Sentences, Training and so forth are available in the paper. Neural Conversational Model Actually, this paper did not propose any novel idea, however, it did something interesting that the author applied the Seq2Seq model, described in Sequence to Sequence Learning with Neural Networks, to not translation tasks but the conversation generation tasks. Thus, this paper named as A Neural Conversational Model. Taking a brief look at the Seq2Seq model, which is based on a recurrent neural network, it reads the input sequence one token at a time, and predicts the output sequence, also one token at a time. During training, the true output sequence is given to the model, so learning can be done by backpropagation. And the model is trained to maximize the cross entropy of the correct sequence given its context. Since given that the true output sequence is not observed during inference, so the model simply feed the predicted output token as input to predict the next output. This is a “greedy” inference approach. A less greedy approach would be to use beam search, and feed several candidates at the previous step to the next step. The predicted sequence can be selected based on the probability of the sequence. The author indicated that Seq2Seq model is simple and general, and the way to use Seq2Seq to build conversation modeling is straight: the input sequence can be the concatenation of what has been conversed so far (the context), and the output sequence is the reply. However, the Seq2Seq model will not be able to successfully “solve” the problem of modeling dialogue due to several obvious simplifications: - The objective function being optimized does not capture the actual objective achieved through human communication, which is typically longer term and based on exchange of information rather than next step prediction. - The lack of a model to ensure consistency and general world knowledge is another obvious limitation of a purely unsupervised model. In the paper, the author use this Seq2Seq model to handle two datasets and show the results: one is a closed-domain IT helpdesk troubleshooting dataset and another is an open-domain movie transcript dataset. DL4J Implementation Note that it is an implementation of A Neural Conversation Model and the corpus used here is Cornell Movie Dialogs Corpus, which contains a large metadata-rich collection of fictional conversations extracted from raw movie scripts: - 220,579 conversational exchanges between 10,292 pairs of movie characters. - involves 9,035 characters from 617 movies. - in total 304,713 utterances and so forth. To implement the Sequence to Sequence model, we need to process the following steps: corpus pre-process, dataset iterator creation, dictionary construction, building neural networks model and so forth. Here I only focus on the model construction, for other parts, you can get information directly from codes: CorpusProcessor, CorpusIterator and Dictionaries. Below are some special tokens used in corpus process and training need to be introduced in advance: - &lt;unk&gt;: replaces any word or other token that’s not in the dictionary (too rare to be included or completely unknown) - &lt;eos&gt;: end of sentence, used only in the output to stop the processing; the model input and output length is limited by the ROW_SIZE constant. - &lt;go&gt;: used only in the decoder input as the first token before the model produced anything Generally, the model architecture looks like as follow: Input Layer =&gt; Embedding Layer =&gt; Encoder (LSTM Layer) =&gt; Decoder (LSTM Layer) =&gt; Output Layer(Softmax) The encoder layer produces a so called “thought vector” that contains a neurally-compressed representation of the input. Depending on that vector the model produces different sentences even if they start with the same token. There is one more input, connected directly to the decoder layer, it is used to provide the previous token of the output. For the very first output token, we send a special &lt;go&gt; token there, on the next iteration we use the token that the model produced the last time. On the training stage everything is simple, we apriori know the desired output so the decoder input would be the same token set prepended with the &lt;go&gt; token and without the last &lt;eos&gt; token. For instance: &gt; Input: “how” “do” “you” “do” “?” &gt; Output: “I’m” “fine” “,” “thanks” “!” &quot;&lt;eos&gt;&quot; &gt; Decoder: &quot;&lt;go&gt;&quot; “I’m” “fine” “,” “thanks” “!” It is worth to mention that the input is reversed as per Sequence to Sequence Learning with Neural Networks, since the most important words are usually in the beginning of the phrase and they would get more weight if supplied last (the model “forgets” tokens that were supplied “long ago”, i.e. they have lesser weight than the recent ones). The output and decoder input sequence lengths are always equal. The encoder and decoder layers work sequentially. First the encoder creates the thought vector, that is the last activations of the layer. Those activations are then duplicated for as many time steps as there are elements in the output so that every output element can have its own copy of the thought vector. Then the decoder starts working. It receives two inputs, the thought vector made by the encoder and the token that it should have produced (but usually it outputs something else so we have our loss metric and can compute gradients for the backward pass) on the previous step (or &lt;go&gt; for the very first step). These two vectors are simply concatenated by the merge vertex. The decoder’s output goes to the softmax layer and that’s it (More details are available here). Implementing the Seq2Seq model via deeplearning4j, we need to create a ComputationGraph, which is used to build complex network architectures and allows for greater freedom in network architectures in deeplearning4j. First of all, we need to configure a neural network by setting the iterations, learning rate, optimizations, updater, parameters initialization and etc., as shown below: 123456789final NeuralNetConfiguration.Builder builder = new NeuralNetConfiguration.Builder() .iterations(1) .learningRate(LEARNING_RATE) .rmsDecay(RMS_DECAY) .optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT) .miniBatch(true) .updater(Updater.RMSPROP) .weightInit(WeightInit.XAVIER) .gradientNormalization(GradientNormalization.RenormalizeL2PerLayer); Note that those variables are custom defined, you can modify them as you want in order to achieve better results, and if you are not familiar with those variables or settings in deeplearning4j, you can visit the official website to get more knowledge. Next step is to build the ComputationGraph 123final ComputationGraphConfiguration.GraphBuilder graphBuilder = builder.graphBuilder() .addInputs("inputLine", "decoderInput") .setInputTypes(InputType.recurrent(dict.size()), InputType.recurrent(dict.size())) .addInputs specify the inputs to the network, and their associated labels, while the names of the inputs also defines their order. Here we have two inputs for the computation graph, inputLine is feed to next layer (embedding layer) directly, while decoderInput will be used later. .setInputTypes specify the types of inputs to the network, so that preprocessors can be automatically added, and the nIns (input size) for each layer can be automatically calculated and set. Meanwhile InputType is used to define the types of activations etc used in a ComputationGraph, .recurrent(...) indicates that it is for recurrent neural network (time series) data. Add Embedding Layer: 1234567// following previous codes.addLayer("embeddingEncoder", new EmbeddingLayer.Builder() .nIn(dict.size()) .nOut(EMBEDDING_WIDTH) .build(), "inputLine") Here we use .addLayer to add an Embedding Layer named as embeddingEncoder, where its input is the inputLine, the nIn (input size) is set as the size of dictionary, while nOut (output size) is the pre-defined EMBEDDING_WIDTH. Note that Embedding Layer can only be used as the first layer for a network. Add Encoder (LSTM) Layer: 123456789// following previous codes.addLayer("encoder", new GravesLSTM.Builder() .nIn(EMBEDDING_WIDTH) .nOut(HIDDEN_LAYER_WIDTH) .activation(Activation.TANH) .gateActivationFunction(Activation.HARDSIGMOID) .build(), "embeddingEncoder") After Embedding Layer, we add a LSTM as Encoder Layer, which named as encoder and the Embedding Layer act as its input, the activation function of LSTM gates are set as HARDSIGMOID, while the layer activation function is set as TANH. Add Vertex: 1234// following previous codes.addVertex("thoughtVector", new LastTimeStepVertex("inputLine"), "encoder").addVertex("dup", new DuplicateToTimeSeriesVertex("decoderInput"), "thoughtVector").addVertex("merge", new MergeVertex(), "decoderInput", "dup") This part performs a role of concatenation between Encoder LSTM Layer and Decoder LSTM Layer, it addresses the output from previous layer and does some processes to generate the desired input for next layer. Below is the explanation of three different GraphVertexs used in the neural network construction: - LastTimeStepVertex is used in the context of recurrent neural network activations, to go from 3d (time series) activations to 2d activations, by extracting out the last time step of activations for each example. This can be used for example in sequence to sequence architectures, and potentially for sequence classification. Note that since RNNs may have masking arrays (to allow for examples/time series of different lengths in the same minibatch), it is necessary to provide the same of the network input that has the corresponding mask array. If this input does not have a mask array, the last time step of the input will be used for all examples; otherwise, the time step of the last non-zero entry in the mask array (for each example separately) will be used. &quot;inputLine&quot; here is the name of the input to look at when determining the last time step. Specifically, the mask array of this time series input is used when determining which time step to extract and return. Meanwhile &quot;thoughtVector&quot; is the Vertex Layer name, &quot;encoder&quot; is the Vertex inputs. - DuplicateToTimeSeriesVertex is a vertex that goes from 2d activations to a 3d time series activations, by means of duplication. That is, given a 2d input with shape [numExamples,nIn] duplicate each row to give output of [numExamples,nIn,timeSeriesLength], where the activations are the same for all time steps. This method is used for example in sequence to sequence models. Note that the length of the output time series (number of time steps) is determined by means of referencing one of the inputs in the ComputationGraph, that is, since the length of the time series may differ at runtime, we generally want the number of time steps to match some other input; here, we are specifying the length of the output time series to be the same as one of the input time series. &quot;decoderInput&quot; is the name of the input in the ComputationGraph network to use, to determine how long the output time series should be. This input should exist, and be a time series input. - MergeVertex is used to combine the activations of two or more layers/GraphVertex by means of concatenation/merging. Here &quot;decoderInput&quot; and &quot;dup&quot; is the two layers to be merged, &quot;merge&quot; is the name of this Vertex Layer. Exactly how this is done depends on the type of input: + For 2d (feed forward layer) inputs: MergeVertex([numExamples, layerSize1], [numExamples, layerSize2])-&gt;[numExamples, layerSize1+layerSize2]. + For 3d (time series) inputs: MergeVertex([numExamples, layerSize1, timeSeriesLength], [numExamples, layerSize2, timeSeriesLength])-&gt;[numExamples, layerSize1+layerSize2, timeSeriesLength]. + For 4d (convolutional) inputs: MergeVertex([numExamples, depth1, width, height], [numExamples, depth2, width, height])-&gt;[numExamples, depth1+depth2, width, height]. So, generally, the LastTimeStepVertex extract out the last time step of activations from Encoder Layer as tge &quot;thoughtVector&quot;, then DuplicateToTimeSeriesVertex duplicates the &quot;thoughtVector&quot; according to the time series length of &quot;decoderInput&quot;, finally, MergeVertex concatenate the duplicated &quot;thoughtVector&quot; and &quot;decoderInput&quot; through the method for 3d (time series) inputs. Then using this &quot;merge&quot; result as the input of Decoder Layer. Add Decoder (LSTM) Layer: 123456789// following previous codes.addLayer("decoder", new GravesLSTM.Builder() .nIn(dict.size() + HIDDEN_LAYER_WIDTH) .nOut(HIDDEN_LAYER_WIDTH) .activation(Activation.TANH) .gateActivationFunction(Activation.HARDSIGMOID) // always be a (hard) sigmoid function .build(), "merge") The structure of Decoder Layer is similar to the Encoder Layer. One thing needs to mention here is that the nIn (input size) is dict.size()+HIDDEN_LAYER_WIDTH, since its input is from the &quot;merge&quot; Vertex Layer, which concatenates the &quot;thoughtVector&quot; and &quot;decoderInput&quot;. Add Output Layer and Further Settings: 123456789101112131415// following previous codes.addLayer("output", new RnnOutputLayer.Builder() .nIn(HIDDEN_LAYER_WIDTH) .nOut(dict.size()) .activation(Activation.SOFTMAX) .lossFunction(LossFunctions.LossFunction.MCXENT) // multi-class cross entropy .build(), "decoder").setOutputs("output").backpropType(BackpropType.Standard).tBPTTForwardLength(TBPTT_SIZE).tBPTTBackwardLength(TBPTT_SIZE).pretrain(false).backprop(true); The last layer is RnnOutputLayer, which is a type of layer used as the final layer with many recurrent neural network systems (for both regression and classification tasks). RnnOutputLayer handles things like score calculation, and error calculation (of prediction vs. actual) given a loss function etc. Functionally, it is very similar to the ‘standard’ OutputLayer class (which is used with feed-forward networks); however it both outputs (and expects as labels/targets) 3d time series data sets. After configuring the last layer, we still need to deal with several proper settings to make the whole computation graph works, like setting backpropagation as .backpropType(BackpropType.Standard), setting BPTT forward and backward length as .tBPTTForwardLength(TBPTT_SIZE) and .tBPTTBackwardLength(TBPTT_SIZE) and etc. Build and Initialize ComputetionGraph: 12ComputationGraph net = new ComputationGraph(graphBuilder.build());net.init(); Finally, after configuring all the required settings, we can build the Sequence to Sequence Computation Graph and initialize it. Above is the general graph of building a simple version of Sequence to Sequence model for consersation tasks. More related information are available in DL4J Guidance and DL4J Examples. Full Codes are available in my GitHub repository: EncDecLSTM, and you can also get the original DL4J implementation of encdec based on Seq2Seq as well as Python and Lua implementation codes in the Resources at the end of this article. Reference Sequence to Sequence Learning with Neural Networks A Neural Conversational Model The Unreasonable Effectiveness of Recurrent Neural Networks Recurrent Neural Networks in DL4J Class ComputationGraphConfiguration.GraphBuilder – DL4J API Artificial Intelligence/Search/Heuristic search/Beam search Beam search Beam Search Algorithm, how does it work? Resources Python: [JayParks/tf-seq2seq], [farizrahman4u/seq2seq] Java: [dl4j-examples-encdec], [dl4j-examples-character] Lua: [harvardnlp/seq2seq-attn], [harvardnlp] DataSets: [IT helpdesk troubleshooting dataset], [movie transcript dataset], [Cornell Movie Dialogs Corpus]]]></content>
      <categories>
        <category>Natural Language Processing</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
        <tag>java</tag>
        <tag>lstm</tag>
        <tag>gru</tag>
        <tag>deeplearning4j</tag>
        <tag>natural language processing</tag>
        <tag>seq2seq</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Plain Stock Close-Price Prediction via LSTM]]></title>
    <url>%2F2017%2F07%2F26%2FPlain-Stock-Close-Price-Prediction-via-LSTM-Initial-Exploration%2F</url>
    <content type="text"><![CDATA[This is a practice of using LSTM to do the one day ahead prediction of the stock close price. The dataset I used here is the New York Stock Exchange from Kaggle, which consists of following files: - prices.csv: raw, as-is daily prices. Most of data spans from 2010 to the end 2016, for companies new on stock market date range is shorter. There have been approx. 140 stock splits in that time, this set doesn’t account for that. - prices-split-adjusted.csv: same as prices, but there have been added adjustments for splits. - securities.csv: general description of each company with division on sectors - fundamentals.csv: metrics extracted from annual SEC 10K fillings (2012-2016), should be enough to derive most of popular fundamental indicators. For those four .csv files, I only use the prices-split-adjusted.csv to train the LSTM-based neural networks model. Since the data in prices-split-adjusted.csv does not have any missing values, thus, data preprocessing tasks, like data clean, transforming, can be eliminated. Meanwhile, prices-split-adjusted.csv contains the information of more than 500 different stocks, here I only select one of them and extract all the information of such stock to analyze it. To implement the LSTM-based neural networks for learning, I use Deeplearning4J (DL4J) to process the data and build the model. For more information about how to play with DL4J, you can access their offical website here. Typically, if you wonder how to implement a LSTMs, we may want to visit DL4J’s introduction of “Using Recurrent Neural Networks in DL4J” as well as some examples of DL4J: [link]. Data Preview Here I use Spark Dataframe to analyze the data and show some statistical information. First, we need to configure spark and load the data from file to build the dataframe. 12345678910111213Logger.getLogger("org").setLevel(Level.OFF); // shut down log info.SparkSession spark = SparkSession.builder().master("local").appName("DataProcess").getOrCreate();String filename = "prices-split-adjusted.csv";// load data from csv fileDataset&lt;Row&gt; data = spark.read().format("csv").option("header", true) .load(new ClassPathResource(filename).getFile().getAbsolutePath()) .withColumn("openPrice", functions.col("open").cast("double")).drop("open") .withColumn("lowPrice", functions.col("low").cast("double")).drop("low") .withColumn("highPrice", functions.col("high").cast("double")).drop("high") .withColumn("volumeTmp", functions.col("volume").cast("double")).drop("volume") .withColumn("closePrice", functions.col("close").cast("double")).drop("close") .toDF("date", "symbol", "open", "low", "high", "volume", "close");data.show(); Below gives first 20 lines of the data, inside the prices-split-adjusted.csv, there are 7 fields, date, symbol, open, close, low, high and volume, where symbol denotes the name of stock. 12345678910111213141516171819202122232425+----------+------+----------+----------+----------+----------+---------+| date|symbol| open| close| low| high| volume|+----------+------+----------+----------+----------+----------+---------+|2016-01-05| WLTW| 123.43|125.839996|122.309998| 126.25|2163600.0||2016-01-06| WLTW|125.239998|119.980003|119.940002|125.540001|2386400.0||2016-01-07| WLTW|116.379997|114.949997| 114.93|119.739998|2489500.0||2016-01-08| WLTW|115.480003|116.620003| 113.5|117.440002|2006300.0||2016-01-11| WLTW|117.010002|114.970001|114.089996|117.330002|1408600.0||2016-01-12| WLTW|115.510002|115.550003| 114.5|116.059998|1098000.0||2016-01-13| WLTW|116.459999|112.849998|112.589996| 117.07| 949600.0||2016-01-14| WLTW|113.510002|114.379997|110.050003|115.029999| 785300.0||2016-01-15| WLTW|113.330002|112.529999|111.919998|114.879997|1093700.0||2016-01-19| WLTW|113.660004|110.379997|109.870003|115.870003|1523500.0||2016-01-20| WLTW|109.059998|109.300003| 108.32|111.599998|1653900.0||2016-01-21| WLTW|109.730003| 110.0| 108.32|110.580002| 944300.0||2016-01-22| WLTW|111.879997|111.949997|110.190002|112.949997| 744900.0||2016-01-25| WLTW| 111.32|110.120003| 110.0|114.629997| 703800.0||2016-01-26| WLTW|110.419998| 111.0|107.300003|111.400002| 563100.0||2016-01-27| WLTW|110.769997|110.709999|109.019997| 112.57| 896100.0||2016-01-28| WLTW|110.900002|112.580002|109.900002|112.970001| 680400.0||2016-01-29| WLTW|113.349998|114.470001|111.669998|114.589996| 749900.0||2016-02-01| WLTW| 114.0| 114.5|112.900002|114.849998| 574200.0||2016-02-02| WLTW| 113.25|110.559998| 109.75|113.860001| 694800.0|+----------+------+----------+----------+----------+----------+---------+only showing top 20 rows Then we extract all symbols from the dataset and display some of them: 123Dataset&lt;Row&gt; symbols = data.select("date", "symbol").groupBy("symbol").agg(functions.count("date").as("count"));System.out.println(symbols.count());symbols.show(); Here we got the result, count means the number of data available for each symbol: 1234567891011Number of Symbols: 501+------+-----+|symbol|count|+------+-----+| ALXN| 1762|| GIS| 1762|| K| 1762|| LEN| 1762|| SPGI| 1762|| AVY| 1762|... Actually, with Spark Dataframe, you can easily handle the data and deal with a lot of different tasks, here I only show the general information above and do not go deep in this part, since my own task is to build a lstm networks to train the data and make a prediction. In order to transform the data to the format suitable for training, here I define a StockData class to store the stock information, this class will be used in the “Customize DataSet Iterator” part. 123456789101112131415161718192021222324/** * Created by zhanghao on 25/7/17. * @author ZHANG HAO */public class StockData &#123; private String date; // date private String symbol; // stock name private double open; // open price private double close; // close price private double low; // low price private double high; // high price private double volume; // volume public StockData () &#123;&#125; public StockData (String date, String symbol, double open, double close, double low, double high, double volume) &#123; this.date = date; this.symbol = symbol; this.open = open; this.close = close; this.low = low; this.high = high; this.volume = volume; &#125; ... Getter and Setter&#125; Customize DataSet Iterator For a stock, we have 5 features, say, open, close, low, high and volume, and our task is to predict “future” close price of such stock. So here we set VECTOR_SIZE=5. Since LSTMs is one kind of Recurrent Neural Networks, unlike standard feed-forward network, who expect input and output data that is two-dimensional, that is, data with “shape” [numExamples,inputSize]. It means that the data into a feed-forward network has numExamples rows/examples, where each row consists of inputSize columns. Similarly, output data for a standard feed-forward network is also two dimensional, with shape [numExamples,outputSize]. Conversely, data for RNNs are time series. Thus, they have 3 dimensions: one additional dimension for time. Input data has shape [numExamples,inputSize,timeSeriesLength], and output data has shape [numExamples,outputSize,timeSeriesLength]. It means that the data in INDArray is laid out such that the value at position (i,j,k) is the jth value at the kth time step of the ith example in the minibatch. This data layout is shown below. In this case, we need to indicate the time series length of the data, so I define the exampleLength to represent the time series length for RNNs. Another thing is the miniBatchSize, which is defined to create the mini-batch for training. Given the miniBatchSize, VECTOR_SIZE and exampleLength, we need to create the mini-batch DataSet for training, in the DataSet, we have two things, one is 3-dimensional input data, and another is labels. Since we need to predict one-day ahead close price of a stock, and my strategy is to using the previous one-month information to construct the input time series data, then predict the close price of next day. Suppose that there are 22 working days per month, so the time series length should be 22. For instance, with miniBatchSize=64, assume time start from \(t=0\), our input data has shape [numExamples,inputSize,timeSeriesLength], where numExamples=64, inputSize=5 and timeSeriesLength=0~22, while label (output) data has shape [numExamples,outputSize,timeSeriesLength], where numExamples=64, outputSize=1 (only predict close price) and timeSeriesLength=1~23 (one-day ahead). More implementation details are shown in the codes below, the StockDataSetIterator implements the standard DataSetIterator, which is pre-defined interface by Deeplearning4J to help user to construct a proper custom dataset available for Deeplearning4J neural networks and for iterating over DataSet objects - objects that encapsulate the input and target INDArrays, plus (optionally) the input and labels mask arrays. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556/** * Created by zhanghao on 26/7/17. * Modified by zhanghao on 28/9/17. * @author ZHANG HAO */public class StockDataSetIterator implements DataSetIterator &#123; private final int VECTOR_SIZE = 5; private int miniBatchSize; private int exampleLength = 22; // default 22, say, 22 working days per month private double[] minNum = new double[VECTOR_SIZE]; private double[] maxNum = new double[VECTOR_SIZE]; private PriceCategory category = PriceCategory.CLOSE; // default to train to predict the CLOSE price model private LinkedList&lt;Integer&gt; exampleStartOffsets = new LinkedList&lt;&gt;(); private List&lt;StockData&gt; train; private List&lt;Pair&lt;INDArray, INDArray&gt;&gt; test; public StockDataSetIterator (String filename, String symbol, int miniBatchSize, int exampleLength, double splitRatio, PriceCategory category) &#123; List&lt;StockData&gt; stockDataList = readStockDataFromFile(filename, symbol); this.miniBatchSize = miniBatchSize; this.exampleLength = exampleLength; this.category = category; int split = (int) Math.round(stockDataList.size() * splitRatio); train = stockDataList.subList(0, split); test = generateTestDataSet(stockDataList.subList(split, stockDataList.size())); initializeOffsets(); &#125; private void initializeOffsets () &#123; exampleStartOffsets.clear(); int window = exampleLength + 1; for (int i = 0; i &lt; train.size() - window; i++) &#123; exampleStartOffsets.add(i); &#125; &#125; public List&lt;Pair&lt;INDArray, INDArray&gt;&gt; getTestDataSet() &#123; return test; &#125; public double[] getMaxNum() &#123; return maxNum; &#125; public double[] getMinNum() &#123; return minNum; &#125; @Override public DataSet next(int num) &#123; ... &#125; private double feedLabel(StockData data) &#123; ... &#125; @Override public int totalExamples() &#123; return train.size() - exampleLength - 1; &#125; @Override public int inputColumns() &#123; return VECTOR_SIZE; &#125; @Override public int totalOutcomes() &#123; if (this.category.equals(PriceCategory.ALL)) return VECTOR_SIZE; else return 1; &#125; @Override public boolean resetSupported() &#123; return false; &#125; @Override public boolean asyncSupported() &#123; return false; &#125; @Override public void reset() &#123; initializeOffsets(); &#125; @Override public int batch() &#123; return miniBatchSize; &#125; @Override public int cursor() &#123; return totalExamples() - exampleStartOffsets.size(); &#125; @Override public int numExamples() &#123; return totalExamples(); &#125; @Override public void setPreProcessor(DataSetPreProcessor dataSetPreProcessor) &#123; throw new UnsupportedOperationException("Not Implemented"); &#125; @Override public DataSetPreProcessor getPreProcessor() &#123; throw new UnsupportedOperationException("Not Implemented"); &#125; @Override public List&lt;String&gt; getLabels() &#123; throw new UnsupportedOperationException("Not Implemented"); &#125; @Override public boolean hasNext() &#123; return exampleStartOffsets.size() &gt; 0; &#125; @Override public DataSet next() &#123; return next(miniBatchSize); &#125; private List&lt;Pair&lt;INDArray, INDArray&gt;&gt; generateTestDataSet (List&lt;StockData&gt; stockDataList) &#123; ... &#125; private List&lt;StockData&gt; readStockDataFromFile (String filename, String symbol) &#123; ... &#125;&#125; Build LSTM RNNs After customizing the StockDataSetIterator, we need to build the recurrent neural networks model to train. To handle this task, I construct a four layer neural network, which contains two LSTM layers and two dense layers, GravesLSTM -&gt; GraveLSTM -&gt; DenseLayer -&gt; RNNOutputLayer, and the stucture of the model and its parameters are shown in the codes. 123456789101112131415161718192021222324252627282930313233/** * Created by zhanghao on 25/7/17. * @author ZHANG HAO */public class LSTMNetwork &#123; ... (Parameters definition) public static MultiLayerNetwork buildLstmNetworks (int nIn, int nOut) &#123; MultiLayerConfiguration conf = new NeuralNetConfiguration.Builder() .seed(seed) .iterations(iterations) .learningRate(learningRate) .optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT) .weightInit(WeightInit.XAVIER) .updater(Updater.RMSPROP) .regularization(true) .l2(1e-4) .list() .layer(0, new GravesLSTM.Builder().nIn(nIn).nOut(lstmLayer1Size).activation(Activation.TANH).gateActivationFunction(Activation.HARDSIGMOID).dropOut(dropoutRatio).build()) .layer(1, new GravesLSTM.Builder().nIn(lstmLayer1Size).nOut(lstmLayer2Size).activation(Activation.TANH).gateActivationFunction(Activation.HARDSIGMOID).dropOut(dropoutRatio).build()) .layer(2, new DenseLayer.Builder().nIn(lstmLayer2Size).nOut(denseLayerSize).activation(Activation.RELU).build()) .layer(3, new RnnOutputLayer.Builder().nIn(denseLayerSize).nOut(nOut).activation(Activation.IDENTITY).lossFunction(LossFunctions.LossFunction.MSE).build()) .backpropType(BackpropType.TruncatedBPTT) .tBPTTForwardLength(exampleLength) .tBPTTBackwardLength(exampleLength) .pretrain(false) .backprop(true) .build(); MultiLayerNetwork net = new MultiLayerNetwork(conf); net.init(); net.setListeners(new ScoreIterationListener(100)); return net; &#125;&#125; Train and Predict For training and prediction process, we use the stock GOOG as an example. Firstly, we load the file, create the StockDataSetIterator and split the dataset to training dataset and test dataset. Then, construct the LSTM RNN model, and do the training process. The codes are shown below. 1234567891011121314151617181920212223242526272829303132333435363738/** * Created by zhanghao on 26/7/17. * Modified by zhanghao on 28/9/17. * @author ZHANG HAO */public class StockPricePrediction &#123; private static final Logger log = LoggerFactory.getLogger(StockPricePrediction.class); public static void main (String[] args) throws IOException &#123; String file = new ClassPathResource("prices-split-adjusted.csv").getFile().getAbsolutePath(); String symbol = "GOOG"; // stock name int batchSize = 64; // mini-batch size int exampleLength = 22; // time series length, assume 22 working days per month double splitRatio = 0.9; // 90% for training, 10% for testing int epochs = 100; // training epochs log.info("Create dataSet iterator..."); PriceCategory category = PriceCategory.CLOSE; // CLOSE: predict close price StockDataSetIterator iterator = new StockDataSetIterator(file, symbol, batchSize, exampleLength, splitRatio, category); log.info("Load test dataset..."); List&lt;Pair&lt;INDArray, INDArray&gt;&gt; test = iterator.getTestDataSet(); log.info("Build lstm networks..."); MultiLayerNetwork net = RecurrentNets.buildLstmNetworks(iterator.inputColumns(), iterator.totalOutcomes()); log.info("Training..."); for (int i = 0; i &lt; epochs; i++) &#123; while (iterator.hasNext()) net.fit(iterator.next()); // fit model using mini-batch data iterator.reset(); // reset iterator net.rnnClearPreviousState(); // clear previous state &#125; log.info("Saving model..."); File locationToSave = new File("src/main/resources/StockPriceLSTM_".concat(String.valueOf(category)).concat(".zip")); // saveUpdater: i.e., the state for Momentum, RMSProp, Adagrad etc. Save this to train your network more in the future ModelSerializer.writeModel(net, locationToSave, true); log.info("Load model..."); net = ModelSerializer.restoreMultiLayerNetwork(locationToSave); log.info("Testing..."); ... log.info("Done..."); &#125;&#125; Here is some log information while training, and the final prediction graph is shown below. 12345678910111213141516171819[main] INFO com.isaac.stock.StockClosePricePrediction - create stock dataSet iterator...[main] INFO org.nd4j.linalg.factory.Nd4jBackend - Loaded [CpuBackend] backend...[main] INFO com.isaac.stock.StockClosePricePrediction - load test dataset...[main] INFO com.isaac.stock.StockClosePricePrediction - build lstm networks......[main] INFO com.isaac.stock.StockClosePricePrediction - training...[main] INFO org.deeplearning4j.optimize.listeners.ScoreIterationListener - Score at iteration 0 is 0.23434746144095608[main] INFO org.deeplearning4j.optimize.listeners.ScoreIterationListener - Score at iteration 100 is 0.20113769402560447[main] INFO org.deeplearning4j.optimize.listeners.ScoreIterationListener - Score at iteration 200 is 0.06535478890549833[main] INFO org.deeplearning4j.optimize.listeners.ScoreIterationListener - Score at iteration 300 is 0.007232614184257477[main] INFO org.deeplearning4j.optimize.listeners.ScoreIterationListener - Score at iteration 400 is 0.005167405140985004[main] INFO org.deeplearning4j.optimize.listeners.ScoreIterationListener - Score at iteration 500 is 0.00527686810765035...[main] INFO org.deeplearning4j.optimize.listeners.ScoreIterationListener - Score at iteration 2100 is 0.005341530172120316[main] INFO org.deeplearning4j.optimize.listeners.ScoreIterationListener - Score at iteration 2200 is 0.008277905296288288[main] INFO org.deeplearning4j.optimize.listeners.ScoreIterationListener - Score at iteration 2300 is 0.0065657371367870785[main] INFO org.deeplearning4j.optimize.listeners.ScoreIterationListener - Score at iteration 2400 is 0.006721078631611399... The graph above shows that the prediction result is not bad. Actually, the dataset I used here is quiet small, if the dataset is larger, the result should be more accurate, and another things is that the parameters setting also can be modified to achieve more convinced prediction. Anyway, it is just my first attempt to deal with stock price prediction tasks usring LSTMs. Plus, Quandl Financial and Economic Data provides up to 40 years stock prices information for more than 3000 tickers, you can get more related data here. Full Java Codes are available on my GitHub repository: StockPrediction Reference New York Stock Exchange – Kaggle Using Recurrent Neural Networks in DL4J DL4J examples – GravesLSTMCharModellingExample Understanding LSTM Networks LSTM Neural Network for Time Series Prediction]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
        <tag>machine learning</tag>
        <tag>java</tag>
        <tag>lstm</tag>
        <tag>deeplearning4j</tag>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LSTM and GRU -- Formula Summary]]></title>
    <url>%2F2017%2F07%2F22%2FLSTM-and-GRU-Formula-Summary%2F</url>
    <content type="text"><![CDATA[Long Short-Term Memory (LSTM) unit and Gated Recurrent Unit (GRU) RNNs are among the most widely used models in Deep Learning for NLP today. Both LSTM (1997) and GRU (2014) are designed to combat the vanishing gradient problem prevents standard RNNs from learning long-term dependencies through gating mechanism. Note that, this article heavily rely on the following to articles, Understanding LSTM Networks and Recurrent Neural Network Tutorial, I summary the formula definition and explanation from them to enhance my understanding of LSTM and GRU as well as their similarity and difference. GRU is a simpler variant of LSTMs that share many of the same properties, it combines the forget and input gates into a single “update gate”. And it also merges the cell state and hidden state, and makes some other changes. The resulting model is simpler than standard LSTM models, but its performance comparable to LSTM on sequence modeling, but less parameters and easier to train. Below gives the general graph (screenshot from Understanding LSTM Networks article, LSTM is redrawn) of LSTM and GRU: Denote \(\ast\) as elementwise multiplication and ignore bias term, LSTM calculates a hidden state \(h_{t}\) as\[\begin{aligned} i_{t} &amp; =\sigma\big(x_{t}U^{i}+h_{t-1}W^{i}\big)\\ f_{t} &amp; =\sigma\big(x_{t}U^{f}+h_{t-1}W^{f}\big)\\ o_{t} &amp; =\sigma\big(x_{t}U^{o}+h_{t-1}W^{o}\big)\\ \tilde{C}_{t} &amp; =\tanh\big(x_{t}U^{g}+h_{t-1}W^{g}\big)\\ C_{t} &amp; =\sigma\big(f_{t}\ast C_{t-1}+i_{t}\ast\tilde{C}_{t}\big)\\ h_{t} &amp; =\tanh(C_{t})\ast o_{t} \end{aligned}\]Here, \(i\), \(f\), \(o\) are called the input, forget and output gates, respectively. Note that they have the exact same equations, just with different parameter matrices (\(W\) is the recurrent connection at the previous hidden layer and current hidden layer, \(U\) is the weight matrix connecting the inputs to the current hidden layer). They care called gates because the sigmoid function squashes the values of these vectors between 0 and 1, and by multiplying them elementwise with another vector you define how much of that other vector you want to “let through”. The input gate defines how much of the newly computed state for the current input you want to let through. The forget gate defines how much of the previous state you want to let through. Finally, The output gate defines how much of the internal state you want to expose to the external network (higher layers and the next time step). All the gates have the same dimensions \(d_h\), the size of your hidden state. \(\tilde{C}\) is a “candidate” hidden state that is computed based on the current input and the previous hidden state. \(C\) is the internal memory of the unit. It is a combination of the previous memory, multiplied by the forget gate, and the newly computed hidden state, multiplied by the input gate. Thus, intuitively it is a combination of how we want to combine previous memory and the new input. We could choose to ignore the old memory completely (forget gate all 0’s) or ignore the newly computed state completely (input gate all 0’s), but most likely we want something in between these two extremes. \(h_{t}\) is output hidden state, computed by multiplying the memory with the output gate. Not all of the internal memory may be relevant to the hidden state used by other units in the network. Intuitively, plain RNNs could be considered a special case of LSTMs. If fix the input gate all 1’s, the forget gate to all 0’s (say, always forget the previous memory) and the output gate to all 1’s (say, expose the whole memory), it will almost get a standard RNN. For GRU, the hidden state \(h_{t}\) is computed as\[\begin{aligned} z_{t} &amp; =\sigma\big(x_{t}U^{z}+h_{t-1}W^{z}\big)\\ r_{t} &amp; =\sigma\big(x_{t}U^{r}+h_{t-1}W^{r}\big)\\ \tilde{h}_{t} &amp; =\tanh\big(x_{t}U^{h}+(r_{t}\ast h_{t-1})W^{h}\big)\\ h_{t} &amp; =(1-z_{t})\ast h_{t-1}+z_{t}\ast\tilde{h}_{t} \end{aligned}\]Here \(r\) is a reset gate, and \(z\) is an update gate. Intuitively, the reset gate determines how to combine the new input with the previous memory, and the update gate defines how much of the previous memory to keep around. If set the reset to all 1’s and update gate to all 0’s, it will arrive at the vanilla RNN model. Reference - Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling - Understanding LSTM Networks - Recurrent Neural Network Tutorial, Part 4 – Implementing a GRU/LSTM RNN with Python and Theano - Generating Sequences With Recurrent Neural Networks]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
        <tag>machine learning</tag>
        <tag>lstm</tag>
        <tag>gru</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Neural Responding Machine for Short-Text Conversation (STC) -- Summary]]></title>
    <url>%2F2017%2F07%2F19%2FNeural-Responding-Machine-for-Short-Text-Conversation%2F</url>
    <content type="text"><![CDATA[The paper, Neural Responding Machine for Short-Text Conversation, published by L Shang et. al. in 2015, introduces Neural Responding Machine (NRM), a neural network-based response generator for short-text conversation. The NRM takes the general encoder-decoder framework, which formalizes the generation of response as a decoding process based on the latent representation of the input text, while both encoding and decoding are realized with recurrent neural networks (RNN). In this paper, the author demonstrates that the proposed encoder-decoder-based neural network overperform the traditional Retrivial-based methods and Statistical Machine Translation (SMT) based methods. Dataset for STC The author prepares around 4.4 million pairs of conversations from Weibo to train the model. And the dataset is derived from hundreds of millions of post-response pairs by cleaning this raw data including: - removing trivial responses like “wow” - filtering out potential advertisements - removing the response after first 30 ones for topic consistency. Below is some statistics of the dataset used: Neural Responding Machines The neural responding machine generally contains three parts, encoder, context generator and decoder, as shown in the graph below. Here, both encoder and decoder are Recurrent Neural Networks, beasue of its natural ability to summarize and generate word sequence of arbitrary lengths. where \(\mathbf{x}=(x_{1},\dots,x_{T})\) is the input sequence (\(x_{i}\) denotes the \(i\)-th word), \(\mathbf{h}=(h_{1},\dots,h_{T})\) is a set of high-dimensional hidden representations, \(\alpha_{t}\) is the attention signal at time \(t\), while \(c_{t}\) is the context at time \(t\), \(y_{t}\) is the generated \(t\)-th word of response. And the general process is: 1. The encoder converts the input sequence \(\mathbf{x}\) into the hidden representations \(\mathbf{h}\); 2. \(\mathbf{h}\), along with attention signal \(\alpha_{t}\), are fed to the context-generator to build the context \(c_{t}\) input to decoder at time \(t\). 3. The \(c_{t}\) is linearly transformed by the transformation matrix \(\mathbf{L}\) into a stimulus of generating RNN to produce the \(t\)-th word \(y_{t}\) of response. Here \(\mathbf{L}\) performs the role to transform the representation of post (or some part of it) to the rich representation of many plausible responses, while \(\alpha_{t}\) is to determine which part of the hidden representation \(\mathbf{h}\) should be emphasized during the generation process, normally, \(\alpha_{t}\) is function of historically generated subsequence \((y_{1},\dots,y_{t-1})\), \(\mathbf{x}\) or their latent representations. Encoder and Context Generator The author introduces three types of encoding schemes: global scheme, local scheme and hybrid scheme (combination of global and local schemes). And the context generator of each scheme is different. Global Scheme Graph below visualizes the global scheme of the RNN-encoder and related context generator. The hidden state at time \(t\) is computed by \(h_{t}=\mathcal{f}(x_{t},h_{t-1})\), while the context generator simply uses the last hidden state as the context vector, i.e, \(c_{t}=h_{T}\), it means that final hidden state \(h_{T}\) is used as the global representation of the sentence. However, this scheme has drawbacks that a vectorial summarization of the entire post is often hard to obtain and may lose important details for response generation, especially when the dimension of the hidden state is not big enough. It is worth to mentione that \(\mathcal{f}(\centerdot)\) can be a logistic function, sophisticated Long Short-Term Memory (LSTM) unit, or Gated Recurrent Unit (GRU). Compared to “Ungated” or vanilla logistic function, LSTM and GRU are specially designed for its long term memory, which is able to store information over extended time steps without too much decay. The author chooses GRU for this task, due to its performance comparable to LSTM on sequence modeling, but less parameters and easier to train. Local Scheme Local scheme introduces an attention mechanism, which allows the encoder to dynamically select and linearly combine different parts of the input sequence, say, the contect vector here is computed by\[c_{t}=\sum_{j=1}^{T}\alpha_{tj}h_{j}\]where weighting factor \(\alpha_{tj}\) is the attention signal to determine which part should be selected to generate the rew word \(y_{t}\), which in turn is a function of hidden states \(\alpha_{tj}=q(h_{j},s_{t-1})\), \(s_{t-1}\) is the hidden state of decoder at time \(t-1\), will be introduced later, graph of local scheme is shown below: Basically, the attention mechanism \(\alpha_{tj}\) models the alignment between the inputs around position \(j\) and the output at position \(t\), so it can be viewed as a local matching model. This scheme enjoys the advantage of adaptively focusing on some important words of the input text according to the generated words of response, i.e., it cover the deficit that global scheme lost some important information. Hybrid Scheme As mentioned before, global scheme has the summarization of the entire post, while local scheme can adaptively select the important words in post for various suitable responses. Since post-response pairs in STC are not strictly parallel and a word in different context can have different meanings, the author assumes that the global representation in may provide useful context for extracting the local context, therefore complementary to the local scheme. Thus, hybrid scheme is therefore to combine the global and local schemes by concatenating their encoded hidden states to form an extended hidden representation for each time stamp, as shown below: Here the summarization \(h_{T}^{g}\) is incorporated into \(c_{t}\) and \(\alpha_{tj}\) to provide a global context for local matching, thus the context generator function is\[c_{t}=\sum_{j=1}^{T}\alpha_{tj}\big[h_{j}^{l};h_{T}^{g}\big]\]where \(\big[h_{j}^{l};h_{T}^{g}\big]\) denotes the concatenation of vectors \(h_{j}^{l}\) and \(h_{T}^{g}\). The author also mention that the context generator in hybrid scheme will evoke different encoding mechanisms in the global encoder and the local encoder, although they will be combined later in forming a unified representation. More specifically, the last hidden state of global scheme plays a role different from that of the last state of local scheme, since it has the responsibility to encode the entire input sentence. This role of global scheme, however, tends to be not adequately emphasized in training the hybrid encoder when the parameters of the two encoding RNNs are learned jointly from scratch. For this the author first initialize hybrid scheme with the parameters of local scheme and global scheme trained separately, then fine ture the parameters in encoder along with training the parameters of decoder. Decoder The graph of decoder is shown below, which is essentially a standard RNN language model except conditioned on the context input \(\mathbf{c}\). The generation probability of the \(t\)-th word is computed by\[p(y_{t}|y_{t-1},\dots,y_{1},\mathbf{x})=\mathcal{g}(y_{t-1},s_{t},c_{t})\]where \(y_{t}\) is a one-hot word representation, \(\mathcal{g}(\centerdot)\) is a softmax activation function, and \(s_{t}\) is the hidden state of decoder at time \(t\) computed by\[s_{t}=\mathcal{f}(y_{t-1},s_{t-1},c_{t})\]and \(\mathcal{f}(\centerdot)\) is a non-linear activation function (i.e., still GRU unit), and the transformation \(\mathbf{L}\) is often assigned as parameters of \(\mathcal{f}(\centerdot)\). To learn the parameters of the model, the target is to maximize the likelihood of observing the original response conditioned on the post in the training set. While for a new post, NRMs generate their responses by using a left-to-right beam search with beam size = 10. Implementation Details and Experiments The author uses Stanford Chinese word segmenter to split the posts and responses into sequences of words. Since the number of unique words in post text is 125,237, and that of response text is 679,958. The author therefore construct two separate vocabularies for posts and responses by using 40,000 most frequent words on each side, covering 97.8% usage of words for post and 96.2% for response respectively. All the remaining words are replaced by a special token “UNK”. The dimensions of the hidden states of encoder and decoder are both 1000, and the dimensions of the word-embedding for post and response are both 620. Model parameters are initialized by randomly sampling from a uniform distribution between -0.1 and 0.1. Models are trained by stochastic gradient descent algorithm with mini-batch. Below shows the responses generated by different models, where NRM-glo denotes global scheme, NRM-loc represents local scheme and NRM-hyb is the hybrid scheme, while Rtr.-based represents the Retrieval-based method used as a competitor. Above is a general summary of the paper: “Neural Responding Machine for Short-Text Conversation”, I only describe the algorithm introduced in this paper, and I am trying to build this model by myself in the furture.]]></content>
      <categories>
        <category>Natural Language Processing</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
        <tag>lstm</tag>
        <tag>gru</tag>
        <tag>natural language processing</tag>
        <tag>seq2seq</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Singapore, Two Years]]></title>
    <url>%2F2017%2F07%2F14%2F%E6%96%B0%E5%8A%A0%E5%9D%A1%E4%B8%A4%E5%B9%B4%2F</url>
    <content type="text"><![CDATA[以此纪念在异国漂泊的两年…… 14岁…离开家去另一座城市上高中… 17岁…高中毕业，在家短暂的待了一个月，便启程去了遥远的北方城市，大连… 21岁…大学毕业，匆匆回家，十四天后，登上了飞往新加坡的航班… 这个故事，便从21岁开始。 2015年7月14日一大早，带上前一晚打包好的行李出发… 不舍，因为离别。 新加坡真的是很干净，从机场出来，坐上的士赶往NTU，一路上的风景也算是让我小小的惊呆了。到NTU已经是晚上，学校很安静，但那会儿的我好像没有心思去感受，因为从踏上这片土地时，心里一直有种空虚感，很少像这样，想家…… 似乎攻略没做好，不认路，没有晚餐… 在新朋友的帮助下，好不容易找到一间7-11，吃了来新加坡的第一顿饭：泡面。而回到寝室发现没有被子，没有床单，没有枕头。那晚，我就简单的擦了擦床垫，蜷在上面挨过了异国他乡的第一个夜晚。 那晚，我不停问自己，这个选择，好吗？明知道只有今后漫长的时光能慢慢告诉我答案，可还是忍不住沉思… 假如，我没有选择出国，不用花费大量的时间学英语，也许我会继续大学时的研究方向，接着走下去；不用沉重的签下放弃保研承诺书，这样的话，我最终会去到哪个学校呢？ 很快，我便不再思考这些没用的问题，因为我没有重来一次的机会，也因为新的生活，比我想象中，更有趣。 这里的生活没有国内丰富，但这里的人很nice。 在学校的一年里，主旋律是学习，伴奏是旅行和探索。第一次面对全英文授课时，内心无遗是崩溃的，而论文也是一头雾水。一路跌跌撞撞，参杂着喜和忧，走到了毕业。 一年很短，很快又是离别… 毕业，我送给自己一首诗： 1234567891011121314151617181920212223242526272829小小的屋里弥漫着每个夜晚安抚着伤感焦虑的灵魂伴着芬芳静静飘散在孤单单的梦里启程、离别充满着勇气却没有魄力去忘记几日几年后回首看看也算是无悔的经历若从来没有无谓的坚持又何为无奈的放弃凡事未必结果却并非没有意义我只是努力和憧憬着小小的愿望和私心罢了坚持，为爱的人，为想要的生活坚持下去什么样的方式什么样的存在什么样的结局都无所谓我有强大的内心暂别，Singapore再回来，就不再是可以不管不顾的傻傻学生了... 离开学校，很快就开始工作，连毕业旅行的时间都没留下。而工作，正如很多人所说的那样，教会了我许许多多在学校里学不到和从来不曾想过的事情。这一年里，我最大的体悟便是那曾经最为触动我内心的一句话 – 成熟，不是学会表达，而是学会咽下，当你一点一点学会克制住很多东西，才能驾驭好人生。 本想好好的，写一写，曾经的点滴，但我发现，似乎不太有意义。曾经老师教过，说过程是重要的，但后来，我发现，没人在乎过程，大家想要的似乎都只是结果而已。而慢慢的，我还发现，经历过什么其实也不是那么的重要，重要的是，从这些经历中得到了什么。努力过，拼搏过，感动过，伤感过，哭泣过，跌倒过，战胜过，成功过，又有何用，那些都只是过去，都将被淹没在时间的长河里，被人遗忘，被自己遗忘。而只有刻在内心深处的那些因为经历而留下的印记和感悟或许才会终身受用，这些印记又如何能表达，如何能让人感同身受？我觉得似乎不能。。。 生活从来都不会太美好，但它还会继续，无论是否愿意。倒不是悲观，我很享受现在，即使迷惘，即使彷徨，好奇和期待着接下来的发生的一切。]]></content>
      <categories>
        <category>Life</category>
      </categories>
      <tags>
        <tag>life</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[House Prices Advanced Regression Techniques -- Training and Prediction]]></title>
    <url>%2F2017%2F07%2F10%2FHouse-Price-Advanced-Regression-Techniques-2%2F</url>
    <content type="text"><![CDATA[After cleaning and transforming processes in House Prices Advanced Regression Techniques – Data Analysis Exploration. Here we continue to build machine learning model to train the dataset and make a prediction based on the trained model. We use Elastic Net and Gradient Boosting models to train the dataset and make predictions separately, then average the results of the two models to generate the final output. Elastic Net Regression In statistics and, in particular, in the fitting of linear or logistic regression models, the elastic net is a regularized regression method that linearly combines the L1 and L2 penalties of the lasso and ridge methods. The target function of Lasso is given as:\[\min_{\mathbf{w}}\frac{1}{2n}\Vert\mathbf{X}^{T}\mathbf{w}-\mathbf{y}\Vert_{2}^{2}+\alpha\Vert\mathbf{w}\Vert_{1}\]where \(n\) is number of samples, \(\mathbf{w}\) is the coefficient parameters to learn, \(\Vert\mathbf{w}\Vert_{1}\) term is L1 penality, which promotes sparsity, reduces the redundancy and improves the accurancy and robustness of regression (it alleviates the overfitting in some degree). Also if there is a group of highly correlated variables, then the Lasso tends to select one variable from a group and ignore the others. However, the L1 norm in Lass has some limitations that, for example, in the “large \(p\) and small \(n\)” case (high-dimensional data with few examples), the Lasso selects at most n variables before it saturates. While the target function of Ridge is computed by:\[\min_{\mathbf{w}}\frac{1}{2}\Vert\mathbf{X}^{T}\mathbf{w}-\mathbf{y}\Vert_{2}^{2}+\frac{\alpha}{2}\Vert\mathbf{w}\Vert_{2}\]where \(\Vert\mathbf{w}\Vert_{2}^{2}\) term is L2 penality, which constrains the module of \(\mathbf{w}\) in to a L2 ball to alleviate the overfitting problem. However, L2 norm shrinkages the value of approximated parameters, but does not make it zero, which means it does not perform the function of parameter selection. For Elastic Net, it can be treated as a compromise between Lasso and Ridge, since it integrates the L1 and L2 regularizations. Its target function is described as:\[\min_{\mathbf{w}}\frac{1}{2n}\Vert\mathbf{X}^{T}\mathbf{w}-\mathbf{y}\Vert_{2}^{2}+\alpha\rho\Vert\mathbf{w}\Vert_{1}+\frac{\alpha(1-\rho)}{2}\Vert\mathbf{w}\Vert_{2}^{2}\]where \(\rho\) is an adaptive hyper-parameter to control the contributions of L1 norm and L2 norm. With this compromise, Elastic Net remains part of the parameter selection function in Lasso and part of rotary stability property in Ridge. Meanwhile, compare to Lasso, Elastic Net not only randomly select one of the correlated variables, but also tends to obtain all of them and enahnces their group effect. Gradient Boosting Regression Gradient boosting is a machine learning technique for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees. It builds the model in a stage-wise fashion like other boosting methods do, and it generalizes them by allowing optimization of an arbitrary differentiable loss function. Like other boosting methods, gradient boosting combines weak “learners” into a single strong learner in an iterative fashion. It is easiest to explain in the least-squares regression setting, where the goal is to “teach” a model \(F\) to predict values in the form \(\hat{y} = F(\mathbf{x})\) by minimizing the mean squared error \((\hat{y} - y)^{2}\), averaged over some training set of actual values of the output variable \(y\). Given a training dataset \(\{(\mathbf{x}_{1},y_{1}),(\mathbf{x}_{2},y_{2}),\dots,(\mathbf{x}_{n},y_{n})\}\), the goal is to find an approximation \(\hat{F}(\mathbf{x})\) to a function \(F(\mathbf{x})\) that minimizes the expected value of some specified loss function \(\mathcal{L}(y, F(x))\):\[\hat{F}=\arg\min_{F}\mathbb{E}_{\mathbf{x},y}\big[\mathcal{L}(y, F(\mathbf{x}))\big]\]The gradient boosting method assumes a real-valued \(y\) and seeks an approximation \(\hat{F}(\mathbf{x})\) in the form of a weighted sum of functions \(h_{i}(\mathbf{x})\) from some class \(\mathcal{H}\), called base (or weak) learners:\[F(\mathbf{x})=\sum_{i=1}^{M}\gamma_{i}h_{i}(\mathbf{x})+const.\]In accordance with the empirical risk minimization principle, the method tries to find an approximation \(\hat{F}(\mathbf{x})\) that minimizes the average value of the loss function on the training set. It does so by starting with a model, consisting of a constant function \(F_{0}(\mathbf{x})\), and incrementally expanding it in a greedy fashion:\[F_{0}(\mathbf{x})=\arg\min_{\gamma}\sum_{i=1}^{n}\mathcal{L}(y_{i},\gamma)\\F_{m}(\mathbf{x})=F_{m-1}(\mathbf{x})+\arg\min_{h\in\mathcal{H}}\sum_{i=1}^{n}\mathcal{L}(y_{i},F_{m-1}(\mathbf{x}_{i})+h(\mathbf{x}_{i}))\]where \(h\in\mathcal{H}\) is a base learner function. Unfortunately, choosing the best function \(h\) at each step for an arbitrary loss function \(\mathcal{L}\) is a computationally infeasible optimization problem in general. Therefore, we will restrict to a simplification. The idea is to apply a steepest descent step to this minimization problem. If we considered the continuous case, i.e. \(\mathcal{H}\) the set of arbitrary differentiable functions on \(\mathbb{R}\), we would update the model in accordance with the following equations:\[F_{m}(\mathbf{x})=F_{m-1}(\mathbf{x})-\gamma_{m}\sum_{i=1}^{n}\nabla_{F_{m-1}}\mathcal{L}(y_{i},F_{m-1}(\mathbf{x}_{i}))\\\gamma_{m}=\arg\min_{\gamma}\sum_{i=1}^{n}\mathcal{L}\bigg(y_{i},F_{m-1}(\mathbf{x}_{i})-\gamma\frac{\partial\mathcal{L}(y_{i},F_{m-1}(\mathbf{x}_{i}))}{\partial F_{m-1}(\mathbf{x}_{i})}\bigg)\]where the derivatives are taken with respect to the functions \(F_{i}\) for \(i\in\{1,\dots,m\}\). In the discrete case however, i.e. the set \(\mathcal{H}\) is finite, we will choose the candidate function \(h\) closest to the gradient of \(\mathcal{L}\) for which the coefficient \(\gamma\) may then be calculated with the aid of line search the above equations. Note that this approach is a heuristic and will therefore not yield an exact solution to the given problem, yet a satisfactory approximation. In pseudocode, the generic gradient boosting method is: 1. Input: training dataset \(\{(\mathbf{x}_{i},y_{i})\}_{i=1}^{n}\), a differentiable loss function \(\mathcal{L}(y,F(\mathbf{x}))\), number of iterations \(M\). 2. Initialize model with a constant value: \(F_{0}(\mathbf{x})=\arg\min_{\gamma}\sum_{i=1}^{n}\mathcal{L}(y_{i},\gamma)\). 3. For \(m=1\) to \(M\): 3.1. Compute so-called pseudo-residuals: \(r_{im}=-\big[\frac{\partial\mathcal{L}(y_{i},F(\mathbf{x}_{i}))}{\partial F(\mathbf{x}_{i})}\big]_{F(\mathbf{x})=F_{m-1}(\mathbf{x})}\), for \(i=1,\dots,n\). 3.2. Fit a base learner (e.g. tree) \(h_{m}(\mathbf{x})\) to pseudo-residuals, i.e., train it using the training dataset \(\{\mathbf{x}_{i},r_{im}\}_{i=1}^{n}\). 3.3. Compute multiplier \(\gamma_{m}\) by solving one-dimensional optimization problem: \(\gamma_{m}=\arg\min_{\gamma}\sum_{i=1}^{n}\mathcal{L}(y_{i},F_{m-1}(\mathbf{x}_{i})+\gamma h_{m}(\mathbf{x}_{i}))\). 3.4. Update the model: \(F_{m}(\mathbf{x})=F_{m-1}(\mathbf{x})+\gamma_{m}h_{m}(\mathbf{x})\). 4. Output \(F_{M}(\mathbf{x})\). Summary of Data Analysis Process 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586import pandas as pdimport numpy as npfrom sklearn import ensemble, linear_modelfrom sklearn.model_selection import train_test_split, cross_val_scorefrom sklearn.utils import shufflefrom method2.functions import train_testimport warningswarnings.filterwarnings('ignore')# load datatrain = pd.read_csv('train.csv')test = pd.read_csv('test.csv')train_labels = train.pop('SalePrice')data = pd.concat([train, test], keys=['train', 'test'])# drop the data with high missing percentagedata.drop(['Id', 'MiscFeature', 'Fence', 'PoolQC', 'FireplaceQu', 'Alley'], axis=1, inplace=True)# drop the Bsmt feature group, Garage feature group as well as some trivial featuresdata.drop(['Utilities', 'RoofMatl', 'MasVnrArea', 'MasVnrType', 'Heating', 'LowQualFinSF', 'BsmtFullBath', 'BsmtHalfBath', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'BsmtFinType2', 'Functional', 'WoodDeckSF', 'OpenPorchSF', 'GarageYrBlt', 'GarageCond', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageArea', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal'], axis=1, inplace=True)# merge TotalBsmtSF, 1stFlrSF, 2ndFlrSF to TotalSFdata['TotalBsmtSF'] = data['TotalBsmtSF'].fillna(0)data['1stFlrSF'] = data['1stFlrSF'].fillna(0)data['2ndFlrSF'] = data['2ndFlrSF'].fillna(0)data['TotalSF'] = data['TotalBsmtSF'] + data['1stFlrSF'] + data['2ndFlrSF']data.drop(['TotalBsmtSF', '1stFlrSF', '2ndFlrSF'], axis=1, inplace=True)# MSSubClass as categoricaldata['MSSubClass'] = data['MSSubClass'].astype(str)# MSZoning: filling NA with most popular valuesdata['MSZoning'] = data['MSZoning'].fillna(data['MSZoning'].mode()[0])# LotFrontage: fill NA with mean valuedata['LotFrontage'] = data['LotFrontage'].fillna(data['LotFrontage'].mean())# OverallCond as categoricaldata['OverallCond'] = data['OverallCond'].astype(str)# Electrical: fill NA with most popular valuesdata['Electrical'] = data['Electrical'].fillna(data['Electrical'].mode()[0])# KitchenAbvGr as categoricaldata['KitchenAbvGr'] = data['KitchenAbvGr'].astype(str)# KitchenQual: fill NA with most popular valuesdata['KitchenQual'] = data['KitchenQual'].fillna(data['KitchenQual'].mode()[0])# GarageCars: fill NA with 0data['GarageCars'] = data['GarageCars'].fillna(0.0)# SaleType: fill NA with most popular valuesdata['SaleType'] = data['SaleType'].fillna(data['SaleType'].mode()[0])# Year and Month as categoricaldata['YrSold'] = data['YrSold'].astype(str)data['MoSold'] = data['MoSold'].astype(str)# Exterior1st and Exterior2nd: fill NA with most popular valuesdata['Exterior1st'] = data['Exterior1st'].fillna(data['Exterior2nd'].mode()[0])data['Exterior2nd'] = data['Exterior2nd'].fillna(data['Exterior2nd'].mode()[0])# Standardizing LotFrontage and LotAreanumeric_data = data.loc[:, ['LotFrontage', 'LotArea', 'GrLivArea', 'TotalSF']]numeric_data_standardized = (numeric_data - numeric_data.mean())/numeric_data.std()# Log transformation of labels, GrLivArea and TotalSFtrain_labels = np.log(train_labels)# data['GrLivArea'] = np.log(data['GrLivArea'])# data['TotalSF'] = np.log(data['TotalSF'])# Getting Dummies from Condition1 and Condition2conditions = set([x for x in data['Condition1']] + [x for x in data['Condition2']])dummies = pd.DataFrame(data=np.zeros((len(data.index), len(conditions))), index=data.index, columns=conditions)for i, cond in enumerate(zip(data['Condition1'], data['Condition2'])): dummies.ix[i, cond] = 1data = pd.concat([data, dummies.add_prefix('Condition_')], axis=1)data.drop(['Condition1', 'Condition2'], axis=1, inplace=True)# Getting Dummies from Exterior1st and Exterior2ndexteriors = set([x for x in data['Exterior1st']] + [x for x in data['Exterior2nd']])dummies = pd.DataFrame(data=np.zeros((len(data.index), len(exteriors))), index=data.index, columns=exteriors)for i, ext in enumerate(zip(data['Exterior1st'], data['Exterior2nd'])): dummies.ix[i, ext] = 1data = pd.concat([data, dummies.add_prefix('Exterior_')], axis=1)data.drop(['Exterior1st', 'Exterior2nd'], axis=1, inplace=True)# Getting Dummies from all other categorical varsfor col in data.dtypes[data.dtypes == 'object'].index: for_dummy = data.pop(col) data = pd.concat([data, pd.get_dummies(for_dummy, prefix=col)], axis=1)# copy datadata_standardized = data.copy()# Replacing numeric feature by standardized valuesdata_standardized.update(numeric_data_standardized)# Splitting dataset to train and testtrain_data = data.loc['train'].select_dtypes(include=[np.number]).valuestest_data = data.loc['test'].select_dtypes(include=[np.number]).values# Splitting standardized featurestrain_data_st = data_standardized.loc['train'].select_dtypes(include=[np.number]).valuestest_data_st = data_standardized.loc['test'].select_dtypes(include=[np.number]).values Model Construction, Training and Prediction Shuffling and Splitting Data 12345# Shuffling train setstrain_features_st, train_features, train_labels = shuffle(train_features_st, train_features, train_labels, random_state=5)# Splittingx_train, x_test, y_train, y_test = train_test_split(train_features, train_labels, test_size=0.1, random_state=200)x_train_st, x_test_st, y_train_st, y_test_st = train_test_split(train_features_st, train_labels, test_size=0.1, random_state=200) where **_features_st dataset is used for training Elastic Net, while **_features dataset is used for training Gradient Boosting Regressor. Define two functions to show R2 and RMSE scores for train and validation sets 1234567891011121314151617from sklearn.metrics import r2_score, mean_squared_errorimport numpy as np# Prints R2 and RMSE scoresdef get_score(prediction, labels): print('R2: &#123;&#125;'.format(r2_score(prediction, labels))) print('RMSE: &#123;&#125;'.format(np.sqrt(mean_squared_error(prediction, labels))))# Shows scores for train and validation setsdef train_test(estimator, x_trn, x_tst, y_trn, y_tst): prediction_train = estimator.predict(x_trn) # Printing estimator print(estimator) # Printing train scores get_score(prediction_train, y_trn) prediction_test = estimator.predict(x_tst) # Printing test scores print("Test") get_score(prediction_test, y_tst) Model Construction For Elastic Net, we use cross validation method to select the best parameters group for a given parameters map. 12345ens_test = linear_model.ElasticNetCV(alphas=[0.0001, 0.0005, 0.001, 0.01, 0.1, 1, 10], l1_ratio=[0.01, 0.1, 0.5, 0.9, 0.99], max_iter=5000).fit(x_train_st, y_train_st)train_test(ens_test, x_train_st, x_test_st, y_train_st, y_test_st)# Average R2 score and standard deviation of 5-fold cross-validationscores = cross_val_score(ens_test, train_features_st, train_labels, cv=5)print("Accuracy: %0.2f (+/- %0.2f)" % (scores.mean(), scores.std() * 2)) Here the \(\alpha\) is given as alphas=[0.0001, 0.0005, 0.001, 0.01, 0.1, 1, 10], \(\rho\) is given as l1_ratio=[0.01, 0.1, 0.5, 0.9, 0.99], and the maximal iterations is set as \(5000\), \(K\) of cross validation is set as \(5\). For Gradient Boosting Regressor, the cross validation technique is also used, parameters here are fixed and \(K\) of cross validation is set as \(5\). 12345g_best = ensemble.GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05, max_depth=3, max_features='sqrt', min_samples_leaf=15, min_samples_split=10, loss='huber').fit(x_train, y_train)train_test(g_best, x_train, x_test, y_train, y_test)# Average R2 score and standard deviation of 5-fold cross-validationscores = cross_val_score(g_best, train_features_st, train_labels, cv=5)print("Accuracy: %0.2f (+/- %0.2f)" % (scores.mean(), scores.std() * 2)) After cross validation and obtaining the best model of Elastic Net and Gradient Boosting Regressor, we need to retraining the model: 123# Retraining modelsgb_model = g_best.fit(train_features, train_labels)enst_model = ens_test.fit(train_features_st, train_labels) Then get the predictions and save to file 1234567# Getting our SalePrice estimationfinal_labels = (np.exp(gb_model.predict(test_features)) + np.exp(enst_model.predict(test_features_st))) / 2# print resultoutput = pd.DataFrame(&#123;'Id': test.Id, 'SalePrice': final_labels&#125;)print(output)# Saving to CSVoutput.to_csv('submission.csv', index=False) Below shows some information in training process and the prediction results: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849ElasticNetCV(alphas=[0.0001, 0.0005, 0.001, 0.01, 0.1, 1, 10], copy_X=True, cv=None, eps=0.001, fit_intercept=True, l1_ratio=[0.01, 0.1, 0.5, 0.9, 0.99], max_iter=5000, n_alphas=100, n_jobs=1, normalize=False, positive=False, precompute='auto', random_state=None, selection='cyclic', tol=0.0001, verbose=0)R2: 0.9009282706669409RMSE: 0.1192142029440696TestR2: 0.8967299864999421RMSE: 0.11097041288345283Accuracy: 0.88 (+/- 0.10)GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None, learning_rate=0.05, loss='huber', max_depth=3, max_features='sqrt', max_leaf_nodes=None, min_impurity_split=1e-07, min_samples_leaf=15, min_samples_split=10, min_weight_fraction_leaf=0.0, n_estimators=3000, presort='auto', random_state=None, subsample=1.0, verbose=0, warm_start=False)R2: 0.9617959062813555RMSE: 0.07593410094831428TestR2: 0.9062977017781593RMSE: 0.10586499921275429Accuracy: 0.90 (+/- 0.04)Predictions: Id SalePrice0 1461 119290.1868651 1462 152342.2899282 1463 180487.5616533 1464 201057.8912204 1465 191830.1286955 1466 170253.1958866 1467 170236.2194057 1468 167112.6828148 1469 190918.2395579 1470 123530.454116... ... ...1450 2911 86655.6511231451 2912 149131.1281171452 2913 80815.0706981453 2914 76847.9532241454 2915 83052.3960611455 2916 82826.4066791456 2917 157280.4761851457 2918 119939.1024691458 2919 221954.591126[1459 rows x 2 columns] Reference Elastic Net Regularization Elastic Net in sklearn Cross Validation of Elastic Net in sklearn Gradient Boosting Gradient Boosting Regressor in sklearn]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>python</tag>
        <tag>elastic net</tag>
        <tag>boosting</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[House Prices Advanced Regression Techniques -- Data Analysis Exploration]]></title>
    <url>%2F2017%2F07%2F08%2FHouse-Price-Advanced-Regression-Techniques-1%2F</url>
    <content type="text"><![CDATA[House Prices: Advanced Regression Techniques is a kaggle competition to predict the house prices, which aims to practice feature engineering, RFs, and gradient boosting. After exploring and referring others’ methods, I decide to do it by myself to improve my python skill in data science and data analysis ability. Competition Description Ask a home buyer to describe their dream house, and they probably won’t begin with the height of the basement ceiling or the proximity to an east-west railroad. But this playground competition’s dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence. With 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home. The data contains three parts, one is data_description.txt, which gives brief summary of the data information, another two, train.csv and test.csv, are the datasets used to train and test. More details of the data fields’ description are available here: [link]. Data Analysis and Feature Extraction Preliminary Analysis Data Preview First of all, we need to load the data and have a glance of it. 123456789import pandas as pdtrain = pd.read_csv('train.csv')test = pd.read_csv('test.csv')train_labels = train.pop('SalePrice') # separate labels from train datasetdata = pd.concat([train, test], keys=['train', 'test'])print(data.columns) # check column decorationsprint('rows:', data.shape[0], ', columns:', data.shape[1]) # count rows of total datasetprint('rows in train dataset:', train.shape[0])print('rows in test dataset:', test.shape[0]) the output is shown below: 123456789101112131415161718192021Index(['Id', 'MSSubClass', 'MSZoning', 'LotFrontage', 'LotArea', 'Street', 'Alley', 'LotShape', 'LandContour', 'Utilities', 'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd', 'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType', 'MasVnrArea', 'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinSF1', 'BsmtFinType2', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'Heating', 'HeatingQC', 'CentralAir', 'Electrical', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'KitchenQual', 'TotRmsAbvGrd', 'Functional', 'Fireplaces', 'FireplaceQu', 'GarageType', 'GarageYrBlt', 'GarageFinish', 'GarageCars', 'GarageArea', 'GarageQual', 'GarageCond', 'PavedDrive', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'PoolQC', 'Fence', 'MiscFeature', 'MiscVal', 'MoSold', 'YrSold', 'SaleType', 'SaleCondition'], dtype='object')rows: 2919, columns: 80rows in train dataset: 1460rows in test dataset: 1459 Here we have 2919 lines records in total, 1460 lines for training and 1459 lines for testing. Each line has 79 properties (Id is eliminated). Then we take a look at the situation of data missing: 12nans = pd.concat([train.isnull().sum(), train.isnull().sum() / train.shape[0], test.isnull().sum(), test.isnull().sum() / test.shape[0]], axis=1, keys=['Train', 'Percentage', 'Test', 'Percentage'])print(nans[nans.sum(axis=1) &gt; 0]) we find that Alley, FireplaceQu, PoolQC, Fence and MiscFeature suffer almost or more than 50% data missing. Normally, if the property has more 15% information loss, we should delete it. 1234567891011121314151617181920212223242526272829303132333435 Train Percentage Test PercentageMSZoning 0 0.000000 4 0.002742LotFrontage 259 0.177397 227 0.155586Alley 1369 0.937671 1352 0.926662Utilities 0 0.000000 2 0.001371Exterior1st 0 0.000000 1 0.000685Exterior2nd 0 0.000000 1 0.000685MasVnrType 8 0.005479 16 0.010966MasVnrArea 8 0.005479 15 0.010281BsmtQual 37 0.025342 44 0.030158BsmtCond 37 0.025342 45 0.030843BsmtExposure 38 0.026027 44 0.030158BsmtFinType1 37 0.025342 42 0.028787BsmtFinSF1 0 0.000000 1 0.000685BsmtFinType2 38 0.026027 42 0.028787BsmtFinSF2 0 0.000000 1 0.000685BsmtUnfSF 0 0.000000 1 0.000685TotalBsmtSF 0 0.000000 1 0.000685Electrical 1 0.000685 0 0.000000BsmtFullBath 0 0.000000 2 0.001371BsmtHalfBath 0 0.000000 2 0.001371KitchenQual 0 0.000000 1 0.000685Functional 0 0.000000 2 0.001371FireplaceQu 690 0.472603 730 0.500343GarageType 81 0.055479 76 0.052090GarageYrBlt 81 0.055479 78 0.053461GarageFinish 81 0.055479 78 0.053461GarageCars 0 0.000000 1 0.000685GarageArea 0 0.000000 1 0.000685GarageQual 81 0.055479 78 0.053461GarageCond 81 0.055479 78 0.053461PoolQC 1453 0.995205 1456 0.997944Fence 1179 0.807534 1169 0.801234MiscFeature 1406 0.963014 1408 0.965045SaleType 0 0.000000 1 0.000685 Labels (“SalePrice”) After that, let’s extract some information from the SalePrice, which is also the most important thing, since our task is to predict it. From the information shown below, we can derive the data summary of SalePrice, which provides mean, standard deviation, minimum, maximum and so forth. 1234567891011print(train_labels.describe())count 1460.000000mean 180921.195890std 79442.502883min 34900.00000025% 129975.00000050% 163000.00000075% 214000.000000max 755000.000000Name: SalePrice, dtype: float64 Then we plot its distribution histogram and normal probalility graph. 1234567891011121314from scipy import statsimport seaborn as snssns.plt.figure()sns.plt.subplot(1, 2, 1)sns.plt.title("Sale Prices Dist")sns.distplot(train_labels, fit=stats.norm)sns.plt.subplot(1, 2, 2)stats.probplot(train_labels, plot=sns.plt)sns.plt.show()print("Skewness: %f" % train_labels.skew())print("Kurtosis: %f" % train_labels.kurt())# Skewness and KurtosisSkewness: 1.882876Kurtosis: 6.536282 The graph shows that the distribution of SalePrice deviates from the normal distribution, it has peak value and it is positive biased, but it does not follow the diagonal line (right side), the diagonal line represents the normal distribution in normal probability graph, and a good data distribution should follow this line closely. In order to solve such problems, we can explore to use the logarithmic transformation to see if it is able to handle these problems. 123456789101112sns.plt.figure()sns.plt.subplot(1, 2, 1)sns.plt.title("Sale Prices Dist")sns.distplot(np.log(train_labels), fit=stats.norm)sns.plt.subplot(1, 2, 2)stats.probplot(np.log(train_labels), plot=sns.plt)sns.plt.show()print("Skewness: %f" % np.log(train_labels).skew())print("Kurtosis: %f" % np.log(train_labels).kurt())# Skewness and KurtosisSkewness: 0.121335Kurtosis: 0.809532 See the graph below, it is obvious that the data distribution follows diagonal line better than the previous one, although some outliers exist. thus, it seems that the log transformation is a good choice to transform the data. Systematic Analysis Data Clean, Integration and Correlation Analysis After having some preliminary understanding of the data, next step, we need to explore the relationship between those properties and labels, as well as the internal relationship among those properties and so forth. In the preliminary analysis, we did the analysis on the overall dataset (training dataset + testing dataset). However, in this part, we should only consider the training part (To make the analysis more fair). There are (generally) two types of properties, one is numeric and another is categorical. First of all, we need to drop some unused properties according to the data missing information. As mentioned before, Alley, FireplaceQu, PoolQC, Fence and MiscFeature suffer almost or more than 50% data loss: - Alley: Type of alley access - FireplaceQu: Fireplace quality - PoolQC: Pool quality - Fence: Fence quality - MiscFeature: Miscellaneous feature not covered in other categories Under normal conditions, those features are not within the scope of our consideration when we buy a house, so these features are not important and should be eliminated from the dataset. 1train.drop(['Id', 'MiscFeature', 'Fence', 'PoolQC', 'FireplaceQu', 'Alley'], axis=1, inplace=True) Here we draw the correlation coefficient matrix to explore the coorelation among different properties: 1234567# draw correlation coefficient matrixcorrmat = train.corr()f, ax = sns.plt.subplots(figsize=(12, 9))sns.heatmap(corrmat, vmax=.8, square=True)sns.plt.yticks(rotation=0)sns.plt.xticks(rotation=90)sns.plt.show() From the graph below, we can clearly see that there are two group of features show high correlation (the two big red blocks), one is the correlation coefficient between TotalBsmtSF and 1stFlrSF, another is Garage... feature group. The two examples indicate that these features have strong correlation with each other, actually, the degree of correlation reaches the situation of multicollinearity, thus, we can treat that those features contains almost the same information. If you see the SalePrice, you will find that the SalePrice has strong correlations not only GrLivArea, TotalBsmtSF and OverallQual, but also some other features, here we draw the correlation coefficient matrix of SalePrice to show the top 10 features: 12345678cols = corrmat.nlargest(10, 'SalePrice')['SalePrice'].indexcm = np.corrcoef(train[cols].values.T)sns.set(font_scale=1.25)hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws=&#123;'size': 10&#125;, yticklabels=cols.values, xticklabels=cols.values)sns.plt.yticks(rotation=0)sns.plt.xticks(rotation=90)sns.plt.show() The graph below gives top 10 features which have strong correlation with each other. We can derive here that - OverallQual, GrLivArea and TotalBsmtSF have strong correlation with SalePrice. - GarageCars and GarageArea not only have strong correlation with SalePrice, but also have strong correlation with each other. It is easy to imagine that the number of cars store in garage is strong depends on the area of garage, which means that one of them is enough to represent the relationship between SalePrice and Garage.... Here, we choose GarageCars, since it has slightly higher score (Same for other Garage... features). - The relationship between TotalBsmtSF and 1stFlrSF is almost same as GarageCars and GarageArea, we can also choose one of them, but here we will use another strategy, and will be discussed later. - According to the data description and the graph here, we find that GrLivArea and TotRmsAbvGrd are the similar features too. Their correlation is 0.83, and the GrLivArea represents the “Above grade (ground) living area square feet”, while TotRmsAbvGrd indicates “Total rooms above grade (does not include bathrooms)”. - Another thing is that the FullBath and YearBuilt do not have siginificantly strong correlation with SalePrice. Here we also show the scatter graph between SalePrice and the most important features: 1234sns.set()cols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']sns.pairplot(train[cols], size=2.5)sns.plt.show() This scatters graph gives much more affluent information of the relationships among those features. As mentioned just now, we will drop the feature GarageArea. For Bsmt... features group, if you have studied the data in Excel carefully, you will find that TotalBsmtSF equals to the sum of BsmtFinSF1, BsmtFinSF2 and BsmtUnfSF, which means that by keeping the TotalBsmtSF, BsmtFinSF1, BsmtFinSF2 and BsmtUnfSF are able to be eliminated. Since we also need to handle TotalBsmtSF and 1stFlrSF, here we visually show their relationship with SalePrice: 12345678f, (ax1, ax2, ax3) = sns.plt.subplots(1, 3)data_total = pd.concat([train['SalePrice'], train['TotalBsmtSF']], axis=1)data_total.plot.scatter(x='TotalBsmtSF', y='SalePrice', ylim=(0, 800000), ax=ax1)data1 = pd.concat([train['SalePrice'], train['1stFlrSF']], axis=1)data1.plot.scatter(x='1stFlrSF', y='SalePrice', ylim=(0, 800000), ax=ax2)data2 = pd.concat([train['SalePrice'], train['2ndFlrSF']], axis=1)data2.plot.scatter(x='2ndFlrSF', y='SalePrice', ylim=(0, 800000), ax=ax3)sns.plt.show() we can derive the fighre below, the first graph is TotalBsmtSF-SalePrice, the second is 1stFlrSF-SalePrice and the last one is 2ndFlrSF-SalePrice. Generally, all of these three features have closely relationship with SalePrice, and follow the exponential distribution. Only some specific situations that the TotalBsmtSF has no influence on SalePrice (those zero values), while 2ndFlrSF does also (although there are much more zero values in 2ndFlrSF, the trend is similar). The description of TotalBsmtSF, 1stFlrSF and 2ndFlrSF: - TotalBsmtSF: Total square feet of basement area. - 1stFlrSF: First Floor square feet. - 2ndFlrSF: Second floor square feet. In this case, since all of them are the area information of the house and their relationships with SalePrice are similar, so our strategy is creating a new feature named as TotalSF to add those three features, then drop them. 1234567train.drop(['BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF'], axis=1, inplace=True)train['TotalBsmtSF'] = train['TotalBsmtSF'].fillna(0)train['1stFlrSF'] = train['1stFlrSF'].fillna(0)train['2ndFlrSF'] = train['2ndFlrSF'].fillna(0)train['TotalSF'] = train['TotalBsmtSF'] + train['1stFlrSF'] + train['2ndFlrSF']train.drop(['TotalBsmtSF', '1stFlrSF', '2ndFlrSF'], axis=1, inplace=True)train.drop(['GarageArea'], axis=1, inplace=True) # as analysis before and the relationship between TotalSF and SalePrice is show below: Intuitively, the merged feature has a better exponential relationship with SalePrice, compare to the previous three fatures. The two outliers appear in the graph can be considered as the abnormal values and delete them. In the discuss above, we only anslysis and clear some of the most important numeric features, other numeric also needs to be analyzed as well as the categorical features. Becasue of the space limit, here we do not talk much on this process, and below gives some features that I choose to get rid of, since I think they do not have much correlation to SalePrice or some of these features have strong correlation with the features we already decide to retain (which means they are similar, so keep one of them is enough). 12345train.drop(['Utilities', 'RoofMatl', 'MasVnrArea', 'MasVnrType', 'Heating', 'LowQualFinSF', 'BsmtFullBath', 'BsmtHalfBath', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'Functional', 'GarageYrBlt', 'GarageCond', 'GarageType', 'GarageFinish', 'GarageQual', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal'], axis=1, inplace=True) After this process, the following features are remains: 123456789101112print(train.columns)Index(['MSSubClass', 'MSZoning', 'LotFrontage', 'LotArea', 'Street', 'LotShape', 'LandContour', 'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd', 'RoofStyle', 'Exterior1st', 'Exterior2nd', 'ExterQual', 'ExterCond', 'Foundation', 'HeatingQC', 'CentralAir', 'Electrical', 'GrLivArea', 'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'KitchenQual', 'TotRmsAbvGrd', 'Fireplaces', 'GarageCars', 'PavedDrive', 'MoSold', 'YrSold', 'SaleType', 'SaleCondition', 'SalePrice', 'TotalSF'], dtype='object') There is another important process is to fill the NA values within those features, normally, for categorical features, we can fill the NA with most frequent values or No(such feature), while for the numeric features, we can fill NA with mean value or zero. And we do not talk too much about this here. Next step, we will analysis the relationship between each feature and SalePrice. Univariate Analysis Actually, there are four features which act as siginificantly important roles in this issue. Those four features are OverallQual, YearBuilt, TotalBsmtSF (after data integration process, it becomes TotalSF) and GrLivArea. OverallQual and SalePrice Here we explore the relationship between OverallQual and SalePrice: 12345overall_qual = pd.concat([train['SalePrice'], train['OverallQual']], axis=1)f, ax = sns.plt.subplots(figsize=(8, 6))fig = sns.boxplot(x='OverallQual', y="SalePrice", data=overall_qual)fig.axis(ymin=0, ymax=800000)sns.plt.show() It is obvious that the distribution trends of OverallQual and SalePrice are same. Generally, the SalePrice increase while OverallQual increase. YearBuilt and SalePrice Here is the relationship between YearBuilt and SalePrice: 123456year_built = pd.concat([train['SalePrice'], train['YearBuilt']], axis=1)f, ax = sns.plt.subplots(figsize=(16, 8))fig = sns.boxplot(x='YearBuilt', y="SalePrice", data=year_built)fig.axis(ymin=0, ymax=800000)sns.plt.xticks(rotation=90)sns.plt.show() Although they do not show a strong following trends, this graph also indicates that, generally, the house price is higher while the built year is later. TotalSF and SalePrice Here is the relationship between TotalSF and SalePrice: As discussed before, this two features have strong correlation, and their trends are same. Since TotalSF is one of the most important feature, so let’s go deep. Here we show the distribution histogram and normal probalility graph of TotalSF to explore its normality: 123456789101112sns.plt.figure()sns.plt.subplot(1, 2, 1)sns.plt.title("TotalSF Dist")sns.distplot(train['TotalSF'], fit=stats.norm)sns.plt.subplot(1, 2, 2)stats.probplot(train['TotalSF'], plot=sns.plt)sns.plt.show()print("Skewness: %f" % train['TotalSF'].skew())print("Kurtosis: %f" % train['TotalSF'].kurt())# Skewness and KurtosisSkewness: 1.776700Kurtosis: 12.621968 Compare with the distribution histogram and normal probalility graph of SalePrice we draw before, you will find that their situation are exactly similar， TotalSF shows positive biased, deviates from the normal distribution, has peak value. Same as SalePrice, we process logarithmic transformation on TotalSF and draw the graph again: The graph above shows that after log transformation, the normality of TotalSF becomes better. And below is the homoscedasticity of variance graph of TotalSF and SalePrice: 1234sf = np.log(train['TotalSF'])sp = np.log(train['SalePrice'])sns.plt.scatter(sf[sf &gt; 0], sp[sf &gt; 0])sns.plt.show() This graph shows that SalePrice performs the same level of change within variable range of TotalSF. In this case, we may take the logarithmic transformation for TotalSF to fit SalePrice well. GrLivArea and SalePrice Here we do the same thing as TotalSF for GrLivArea. First, we draw the relationship between GrLivArea and SalePrice: Then plot the distribution histogram and normal probalility graph of GrLivArea to show its normality. 123# Skewness and KurtosisSkewness: 1.366560Kurtosis: 4.895121 And do the logarithmic transformation on GrLivArea and show the graph again: Samely, the normality of GrLivArea becomes better after log transformation. Next, we explore the homoscedasticity of variance graph of GrLivArea and SalePrice: This graph shows that SalePrice performs the same level of change within variable range of GrLivArea. In this case, we may take the logarithmic transformation for GrLivArea to fit SalePrice well. Others Univariables analysis Below shows the relationships between SalePrice and LotFrontage, LotArea, LotShape, LandContour, LandShape and LotConfig. From graph, we can derive that the LogFrontage shows slightly similar trends as SalePrice, it tends to increase while SalePrice increase. However, others do not have such feature, while each value in each feature does not give a clear trends. And the following six features also do not have a clear relationship on SalePrice: However, although YearRemodAdd feature does not give a clear trends, compare to YearBuilt feature, it still indicates that the house price is higher while the RemodAdd year is later. There are still many features do not show here, but each of them are worth to analyze. For analysis, we stop here and make a short summary. - For Bsmt... feature group, drop all of them, except TotalBsmtSF, then merge TotalBsmtSF with 1stFlrSF and 2ndFlrSF to create a new feature named as TotalSF. - For Garage... feature group, drop all of them, except GarageCars. - Drop the features whose missing percentage is large than 20%. - Drop the features that we treat them as the trivial features for the house prices predictation. - Fill NA with most frequent value or zero or mean. and so forth. Conclusion In the above, I describe a procedure to analysis the data and the machine learning model for training and predicting the data will be introduced in next article. Actually, with the data process procedures introduced above and the machine learning model (I choose a model with combination of gradient boosting regressor and elastic net), I achieve the score 0.126. It is not a very good result, but, anyway, this is just a practice after learning the basic knowledge of data science and mechine learning. Below is the data clean, integration and transformation codes: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586import pandas as pdimport numpy as npfrom sklearn import ensemble, linear_modelfrom sklearn.model_selection import train_test_split, cross_val_scorefrom sklearn.utils import shufflefrom method2.functions import train_testimport warningswarnings.filterwarnings('ignore')# load datatrain = pd.read_csv('train.csv')test = pd.read_csv('test.csv')train_labels = train.pop('SalePrice')data = pd.concat([train, test], keys=['train', 'test'])# drop the data with high missing percentagedata.drop(['Id', 'MiscFeature', 'Fence', 'PoolQC', 'FireplaceQu', 'Alley'], axis=1, inplace=True)# drop the Bsmt feature group, Garage feature group as well as some trivial featuresdata.drop(['Utilities', 'RoofMatl', 'MasVnrArea', 'MasVnrType', 'Heating', 'LowQualFinSF', 'BsmtFullBath', 'BsmtHalfBath', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'BsmtFinType2', 'Functional', 'WoodDeckSF', 'OpenPorchSF', 'GarageYrBlt', 'GarageCond', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageArea', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal'], axis=1, inplace=True)# merge TotalBsmtSF, 1stFlrSF, 2ndFlrSF to TotalSFdata['TotalBsmtSF'] = data['TotalBsmtSF'].fillna(0)data['1stFlrSF'] = data['1stFlrSF'].fillna(0)data['2ndFlrSF'] = data['2ndFlrSF'].fillna(0)data['TotalSF'] = data['TotalBsmtSF'] + data['1stFlrSF'] + data['2ndFlrSF']data.drop(['TotalBsmtSF', '1stFlrSF', '2ndFlrSF'], axis=1, inplace=True)# MSSubClass as categoricaldata['MSSubClass'] = data['MSSubClass'].astype(str)# MSZoning: filling NA with most popular valuesdata['MSZoning'] = data['MSZoning'].fillna(data['MSZoning'].mode()[0])# LotFrontage: fill NA with mean valuedata['LotFrontage'] = data['LotFrontage'].fillna(data['LotFrontage'].mean())# OverallCond as categoricaldata['OverallCond'] = data['OverallCond'].astype(str)# Electrical: fill NA with most popular valuesdata['Electrical'] = data['Electrical'].fillna(data['Electrical'].mode()[0])# KitchenAbvGr as categoricaldata['KitchenAbvGr'] = data['KitchenAbvGr'].astype(str)# KitchenQual: fill NA with most popular valuesdata['KitchenQual'] = data['KitchenQual'].fillna(data['KitchenQual'].mode()[0])# GarageCars: fill NA with 0data['GarageCars'] = data['GarageCars'].fillna(0.0)# SaleType: fill NA with most popular valuesdata['SaleType'] = data['SaleType'].fillna(data['SaleType'].mode()[0])# Year and Month as categoricaldata['YrSold'] = data['YrSold'].astype(str)data['MoSold'] = data['MoSold'].astype(str)# Exterior1st and Exterior2nd: fill NA with most popular valuesdata['Exterior1st'] = data['Exterior1st'].fillna(data['Exterior2nd'].mode()[0])data['Exterior2nd'] = data['Exterior2nd'].fillna(data['Exterior2nd'].mode()[0])# Standardizing LotFrontage and LotAreanumeric_data = data.loc[:, ['LotFrontage', 'LotArea', 'GrLivArea', 'TotalSF']]numeric_data_standardized = (numeric_data - numeric_data.mean())/numeric_data.std()# Log transformation of labels, GrLivArea and TotalSFtrain_labels = np.log(train_labels)# data['GrLivArea'] = np.log(data['GrLivArea'])# data['TotalSF'] = np.log(data['TotalSF'])# Getting Dummies from Condition1 and Condition2conditions = set([x for x in data['Condition1']] + [x for x in data['Condition2']])dummies = pd.DataFrame(data=np.zeros((len(data.index), len(conditions))), index=data.index, columns=conditions)for i, cond in enumerate(zip(data['Condition1'], data['Condition2'])): dummies.ix[i, cond] = 1data = pd.concat([data, dummies.add_prefix('Condition_')], axis=1)data.drop(['Condition1', 'Condition2'], axis=1, inplace=True)# Getting Dummies from Exterior1st and Exterior2ndexteriors = set([x for x in data['Exterior1st']] + [x for x in data['Exterior2nd']])dummies = pd.DataFrame(data=np.zeros((len(data.index), len(exteriors))), index=data.index, columns=exteriors)for i, ext in enumerate(zip(data['Exterior1st'], data['Exterior2nd'])): dummies.ix[i, ext] = 1data = pd.concat([data, dummies.add_prefix('Exterior_')], axis=1)data.drop(['Exterior1st', 'Exterior2nd'], axis=1, inplace=True)# Getting Dummies from all other categorical varsfor col in data.dtypes[data.dtypes == 'object'].index: for_dummy = data.pop(col) data = pd.concat([data, pd.get_dummies(for_dummy, prefix=col)], axis=1)# copy datadata_standardized = data.copy()# Replacing numeric feature by standardized valuesdata_standardized.update(numeric_data_standardized)# Splitting dataset to train and testtrain_data = data.loc['train'].select_dtypes(include=[np.number]).valuestest_data = data.loc['test'].select_dtypes(include=[np.number]).values# Splitting standardized featurestrain_data_st = data_standardized.loc['train'].select_dtypes(include=[np.number]).valuestest_data_st = data_standardized.loc['test'].select_dtypes(include=[np.number]).values I also explored others methods and make a midification to improve the result to 0.116. I will describe it in the future. Reference Kaggle Kernels of House Prices Comprehensive Data Exploration using Python How to get to TOP 25% with Simple Model (sklearn) PCA and Regression]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>python</tag>
        <tag>data analysis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark Installation on Mac OS X]]></title>
    <url>%2F2017%2F06%2F28%2FSpark-Installation-on-Mac-OS-X%2F</url>
    <content type="text"><![CDATA[It is an introduction of Spark installation under localhost mode. Generally, Spark is available for multiple models: local, clustered–Spark Standalone, clustered–Spark on Apache Mesos and so forth, more details here: link. Install Spark Since Spark requires Hadoop environment, so we need to install and configure Hadoop first. In this case, referring my another article: Hadoop Installation on Max OS X to see how to install Hadoop as well as Homebrew, Java installation and SSH configuration. After the Hadoop is installed and configured successfully, we need to install Scala (&gt;2.9.3 version). First, go to Scala official website: link and download the newest version of Scala, then unzip and move it to /usr/local/: 123$ sudo tar -zxvf ~/downloads/scala-2.12.2.tgz -C /usr/local/$ cd /usr/local/$ sudo mv scala-2.12.2/ scala/ Open /etc/profile, input and save: 12export SCALA_HOME=/usr/local/scalaexport PATH=$PATH:$SCALA_HOME/bin Then source profile to activate the path declaration. (Simply, you can install Scala easily by Homebrew brew install scala, it will install Scala under /usr/local/Cellar directory). After all things are done properly above, we can start to install Spark, go to Apach Spark official website: link, and download it (you need to select the spark release and package type to ensure the spark you download fits your environment). Then, unzip and move Spark to /usr/local/: 123$ sudo tar -zxvf ~/Dowmloads/spark-2.1.0-bin-hadoop2.7.tgz -C /usr/local/$ cd /usr/local/$ sudo mv spark-2.1.0-bin-hadoop2.7/ spark/ Configure Spark Open /etc/profile and add: 12export SPARK_HOME=/usr/local/sparkexport PATH=$PATH:$SPARK_HOME/bin and we also need to configure spark-env.sh: 123$ cd /usr/local/spark/conf$ sudo cp spark-env.sh.template spark-env.sh$ sudo vim spark-env.sh then add: 123export SCALA_HOME=/usr/local/scalaexport SPARK_MASTER_IP=localhostexport SPARK_WORKER_MEMORY=4g Here we finish the configuration of Spark environment variables. Then we need to test the Spark by executing spark-shell in the terminal. If it returns the error like: 1234567Mon Jul 03 11:19:17 SGT 2017 Thread[main,5,main] Cleanup action startingERROR XBM0H: Directory /usr/local/spark/conf/metastore_db cannot be created. at org.apache.derby.iapi.error.StandardException.newException(Unknown Source) at org.apache.derby.iapi.error.StandardException.newException(Unknown Source) at org.apache.derby.impl.services.monitor.StorageFactoryService$10.run(Unknown Source) at java.security.AccessController.doPrivileged(Native Method) ... It means that you do not have permissions to write in that directory, try sudo spark-shell. Normally, it will return: 123456789101112131415Spark context Web UI available at http://192.168.68.237:4040Spark context available as 'sc' (master = local[*], app id = local-1499055411427).Spark session available as 'spark'.Welcome to ____ __ / __/__ ___ _____/ /__ _\ \/ _ \/ _ `/ __/ '_/ /___/ .__/\_,_/_/ /_/\_\ version 2.1.0 /_/ Using Scala version 2.11.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_131)Type in expressions to have them evaluated.Type :help for more information.scala&gt; Finally, Spark is installed successfully. Use Spark with spark-shell After launching Spark (as above), we create a RDD from /usr/local/spark/README.md: 12scala&gt; val textFile = sc.textFile("file:///usr/local/spark/README.md")&gt;textFile: org.apache.spark.rdd.RDD[String] = file:///usr/local/spark/README.md MapPartitionsRDD[1] at textFile at &lt;console&gt;:24 here, file:// prefix (or ignore it) indicates to load a local file. If you want to access a HDFS file, you should indicate hdfs://remote host/Hadoop port/filename and you should make sure that the file has beed updated to HDFS. RDD supports two operation types 1. actions: manipulating on current dataset and return result. 2. transformations: transform and create a new dataset from current dataset. After the RDD is created, we can execute count() and first() method to get the following information: 12345scala&gt; textFile.count()res0: Long = 104scala&gt; textFile.first()res1: String = # Apache Spark where .count() operation returns the number of item in RDD, for text file, it denotes the total lines of this text file. While .first() operation returns the content of first line of such text file. For transformations, we can use filter transformatin to return a new RDD: 12345scala&gt; var linesWithSpark = textFile.filter(line =&gt; line.contains("Spark"))linesWithSpark: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[2] at filter at &lt;console&gt;:26scala&gt; linesWithSpark.count()res2: Long = 20 .filter() operation will filter the dataset with user-defined rules and return a new RDD (linesWithSpark). In practice, actions and transformations operations of RDD are able to handle various complex manipulations. For instance, we can find a line with highest number of words and return the number: 12345scala&gt; import java.lang.Mathimport java.lang.Mathscala&gt; textFile.map(line =&gt; line.split(" ").size).reduce((a, b) =&gt; Math.max(a, b))res3: Int = 22 It is a lambda expression, firstly, .map() operation map each line to a Integer, which will create a new RDD, then execute .reduce() operation on this RDD to obtain maximum. Besides, Spark is able to implement Hadoop MapReduce too: 12345scala&gt; val wordCounts = textFile.flatMap(line =&gt; line.split(" ")).map(word =&gt; (word, 1)).reduceByKey((a, b) =&gt; a + b)wordCounts: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[6] at reduceByKey at &lt;console&gt;:27scala&gt; wordCounts.collect()res4: Array[(String, Int)] = Array((package,1), (this,1), (Version"](http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version),1), (Because,1), (Python,2), (page](http://spark.apache.org/documentation.html).,1), (cluster.,1), (its,1), ([run,1), (general,3), (have,1), (pre-built,1), (YARN,,1), ([http://spark.apache.org/developer-tools.html](the,1), (changed,1), (locally,2), (sc.parallelize(1,1), (only,1), (locally.,1), (several,1), (This,2), (basic,1), (Configuration,1), (learning,,1), (documentation,3), (first,1), (graph,1), (Hive,2), (info,1), (["Specifying,1), ("yarn",1), ([params]`.,1), ([project,1), (prefer,1), (SparkPi,2), (&lt;http://spark.apache.org/&gt;,1), (engine,1), (version,1), (file,1), (documentation,,1), (MASTER,1), (example,3), (["Parallel,1), (are... Above only give some simple examples of spark usage through Scala, you can get more examples and information through Google and related forum. Create Spark Project in IntelliJ First create a maven project in IntelliJ and add the Spark dependency: 12345678910111213141516171819202122232425262728293031323334353637&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.isaac.spark&lt;/groupId&gt; &lt;artifactId&gt;SparkTest&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;spark.version&gt;2.1.0&lt;/spark.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;!-- https://mvnrepository.com/artifact/org.apache.spark/spark-core_2.10 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-core_2.10&lt;/artifactId&gt; &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; Then create WordCount.java file: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253package com.isaac.spark;import org.apache.spark.SparkConf;import org.apache.spark.api.java.JavaPairRDD;import org.apache.spark.api.java.JavaRDD;import org.apache.spark.api.java.JavaSparkContext;import org.apache.spark.api.java.function.FlatMapFunction;import org.apache.spark.api.java.function.Function2;import org.apache.spark.api.java.function.PairFunction;import scala.Tuple2;import java.util.Arrays;import java.util.Iterator;import java.util.List;import java.util.regex.Pattern;/** * Created by zhanghao on 3/7/17. * @author ZHANG HAO */public class WordCount &#123; public static void main (String[] args) &#123; // Create RDD object SparkConf conf = new SparkConf().setAppName("Simple Test").setMaster("local"); JavaSparkContext sc = new JavaSparkContext(conf); // load data JavaRDD&lt;String&gt; textFile = sc.textFile("file:///Users/zhanghao/Desktop/test.txt"); /* * User is able to handle different operations on the obtained DStream, * first we split the data, then use Map and ReduceByKey to calculation. */ JavaRDD&lt;String&gt; words = textFile.flatMap(new FlatMapFunction&lt;String, String&gt;() &#123; @Override public Iterator&lt;String&gt; call(String s) &#123; return Arrays.asList(Pattern.compile(" ").split(s)).iterator(); &#125; &#125;); JavaPairRDD&lt;String, Integer&gt; ones = words.mapToPair(new PairFunction&lt;String, String, Integer&gt;() &#123; @Override public Tuple2&lt;String, Integer&gt; call(String s) &#123; return new Tuple2&lt;&gt;(s, 1); &#125; &#125;); JavaPairRDD&lt;String, Integer&gt; counts = ones.reduceByKey(new Function2&lt;Integer, Integer, Integer&gt;() &#123; @Override public Integer call(Integer i1, Integer i2) &#123; return i1 + i2; &#125; &#125;); List&lt;Tuple2&lt;String, Integer&gt;&gt; output = counts.collect(); for (Tuple2&lt;?, ?&gt; tuple : output) &#123; System.out.println(tuple._1() + ": " + tuple._2()); &#125; sc.stop(); &#125;&#125; The output is shown below: 123456789101112131415161718192021Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties17/07/03 22:18:01 INFO SparkContext: Running Spark version 2.1.017/07/03 22:18:01 WARN SparkContext: Support for Scala 2.10 is deprecated as of Spark 2.1.017/07/03 22:18:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable...17/07/03 22:18:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms17/07/03 22:18:06 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1979 bytes result sent to driver17/07/03 22:18:06 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 55 ms on localhost (executor driver) (1/1)17/07/03 22:18:06 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 17/07/03 22:18:06 INFO DAGScheduler: ResultStage 1 (collect at WordCount.java:54) finished in 0.056 sHello: 1love: 1Java: 1I: 1Spark.: 1and: 1World,: 117/07/03 22:18:06 INFO DAGScheduler: Job 0 finished: collect at WordCount.java:54, took 0.561756 s17/07/03 22:18:06 INFO SparkUI: Stopped Spark web UI at http://192.168.1.4:404017/07/03 22:18:07 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!... Reference Install and Use Spark under Mac]]></content>
      <categories>
        <category>Setting</category>
      </categories>
      <tags>
        <tag>mac os x</tag>
        <tag>hadoop</tag>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop Installation on Mac OS X]]></title>
    <url>%2F2017%2F06%2F27%2FHadoop-Installation-on-Mac-OS-X%2F</url>
    <content type="text"><![CDATA[It is an introduction of Hadoop installation under pseudo-distributed model. The difference among single node, pseudo-distributed and distributed is introduced here: link. # Install Homebrew and Cask Homebrew is a free and open-source software package management system that simplifies the installation of software on Apple’s macOS operating system. Originally written by Max Howell, the package manager has gained popularity in the Ruby on Rails community and earned praise for its extensibility. Homebrew has been recommended for its ease of use as well as its integration into the command line. 12$ ruby -e "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)"$ brew install caskroom/cask/brew-cask Install Java Use Homebrew: 12$ brew update$ brew cask install java Or, download Java from offical website and install it manually: link. Configure SSH In order to keep the safety of Hadoop remote administration as well as user sharing among Hadoop nodes, Hadoop requires SSH protocol. First, go to System Preferences -&gt; Sharing, change Allow access for: All Users. Then open Terminal, input ssh localhost, if terminal returns Last login: Sun Jul 2 16:57:36 2017, which means that you have configured SSH Keys successfully before. If you suffer the problem of ssh: connect to host localhost port 22: Connection refused, it happens since the remote login is closed. 12$ sudo systemsetup -getremoteloginRemote Login: off You need to open port 22 in Mac OS X: 123$ sudo systemsetup -setremotelogin on$ ssh localhostLast login: ... And if you did not get the information of Last login: ..., then you need to create a new configuration: 1$ ssh-keygen -t rsa Executing the command above will generate a id_rsa file in .ssh directory under the current user directory, after that, input the following command: 1$ cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys which is used to authorize your public key to the local to prevent the passphrase request when you login, then, input ssh localhost again, if it will return Last login: .... Install Hadoop First, install Hadoop via Homebrew: brew install hadoop, it will install the hadoop under /usr/local/Cellar/hadoop. Then, you need to modify the configuration files: Go to usr/local/Cellar/hadoop/2.8.0/libexec/etc/hadoop, then open hadoop-env.sh 1export HADOOP_OPTS="$HADOOP_OPTS -Djava.net.preferIPv4Stack=true" change to 12export HADOOP_OPTS="$HADOOP_OPTS -Djava.net.preferIPv4Stack=true -Djava.security.krb5.realm= -Djava.security.krb5.kdc="export JAVA_HOME="/Library/Java/JavaVirtualMachines/jdk1.7.0_79.jdk/Contents/Home" Then configure HDFS address and port number, open core-site.xml, input following content in &lt;configuration&gt;&lt;/configuration&gt; tag 123456789101112&lt;!-- Put site-specific property overrides in this file. --&gt; &lt;configuration&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/usr/local/Cellar/hadoop/hdfs/tmp&lt;/value&gt; &lt;description&gt;A base for other temporary directories.&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.default.name&lt;/name&gt; &lt;value&gt;hdfs://localhost:8020&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; Configure jobtracker address and port number in map-reduce, first sudo cp mapred-site.xml.template mapred-site.xml to make a copy of mapred-site.xml, and open mapred-site.xml, add 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapred.job.tracker&lt;/name&gt; &lt;value&gt;localhost:8021&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; Set HDFS default backup, the default value is 3, we should change to 1, open hdfs-site.xml, add 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; Before running background program, we should format the installed HDFS first, executing command hdfs namenode -format, when terminal returns a long inforamtion like: 123456717/07/02 16:11:05 INFO namenode.NameNode: STARTUP_MSG: /************************************************************......17/07/02 16:11:07 INFO namenode.NameNode: SHUTDOWN_MSG: /************************************************************SHUTDOWN_MSG: Shutting down NameNode at haodemacbook-pro.local/192.168.1.4************************************************************/ It means that we finish HDFS configuration, and Hadoop is ready to launch. Besides, maybe you will get a warning 1$ ... WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable It happens since you are running on 64-bit system but Hadoop native library is based on 32-bit. This is not a big issue. If it appears, you can fixed by refering this link: here. Launch Hadoop Go to /usr/local/Cellar/hadoop/2.8.0/sbin, execute: 12$ ./start-dfs.sh # start HDFS service$ ./stop-dfs.sh # stop HDFS service Ternimal will return the following information: 1234Starting namenodes on [localhost]localhost: starting namenode, logging to /usr/local/Cellar/hadoop/2.8.0/libexec/logs/hadoop-zhanghao-namenode-HaodeMacBook-Pro.local.outlocalhost: starting datanode, logging to /usr/local/Cellar/hadoop/2.8.0/libexec/logs/hadoop-zhanghao-datanode-HaodeMacBook-Pro.local.outStarting secondary namenodes [0.0.0.0] It means the local service launched successfully, then open Resource Manager in browser through the link http://localhost:50070, you can see the following page Samely, under current diretory, you can start the JobTracker through the commands: 12$ ./start-yarn.sh # start yarn, MapReduce framework$ ./stop-yarn.sh # stop yarn Then open browser and go to the page http://localhost:8088, Specific Node Information http://localhost:8042, you will see Simply, you can execute ./start-all.sh and ./stop-all.sh to start or close all the hadoop service. Finally, open /etc/profile and add the configuration information of Hadoop environment variables. 12export HADOOP_HOME=/usr/local/Cellar/hadoop/2.8.0export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin Then you can start and close Hadoop under the user directory rather than go to /usr/local/Cellar/hadoop/2.8.0/sbin every time. Reference Homebrew Installation and Usage of Hadoop Hadoop on Mac Difference of single node, pseudo-distributed and distributed model of Hadoop What is the difference between single node &amp; pseudo-distributed mode in Hadoop?]]></content>
      <categories>
        <category>Setting</category>
      </categories>
      <tags>
        <tag>mac os x</tag>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Install XGBoost on Mac OS X]]></title>
    <url>%2F2017%2F06%2F20%2FInstall-XGBoost-on-Mac-OS-X%2F</url>
    <content type="text"><![CDATA[Generally, installing XGBoost on Mac is not a cumbersome task, but I still suffered some errors while dealing with it. So I make a note here to record the overall process to make it works. XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. Um… those words are directly copy from XGBoost GitHub page. Here is not a introduction of XGBoost, but a tutorial for how to install it on Mac. Prerequisite First of all, the Homebrew is required, if you do not install it yet, you can get it from here: [link]. Installing it is straightforward, just copy the link provided in Homebrew home page and execute it. Then you need install gcc with OpenMP: 1$ brew install gcc --without-multilib It will download and build gcc automatically. This process takes a while, around 20 minutes or more. After gcc is built successfully, we can start to install XGBoost. Install XGBoost First, get the XGBoost: 12$ cd &lt;Directory&gt; # a directory where you want to store it$ git clone --recursive https://github.com/dmlc/xgboost After cloning XGBoost to you local directory, we need to build it. Here I name the &lt;Directory&gt; as xgboost 12$ cd xgboost$ vi make/config.mk Then uncomment the following two lines: 12export CC = gccexport CXX = g++ This is not enough, if you just uncomment it and try to build the XGBoost, it will return the following error to you (system tried to build XGBoost with clang): 123...clang: error: unsupported option '-fopenmp'... Since it will use the self-contained modules of system to make and compile it, the system has the clang version accord to ~3.7, and openMP was default supported since 3.8. Here we need to use the installed gcc just now to build it. Here we should check which version of gcc we have installed first: 1$ cd /usr/local/bin Check which version of gcc was installed, mine is gcc-7 and g++-7. So I change the two umcomment lines to: 12export CC = gcc-7export CXX = g++-7 Then build XGBoost with following commands: 123$ cd ..$ cp make/config.mk .$ sudo make -j4 # sudo is important, prevent permission denied issue Python-Package Configuration Once XGBoost is built, we can use it with its command line. Since I am using Python, so I also do the following step to set it to my python environment: 12$ cd python-package$ sudo python3 setup.py install Then you will get the information: 1234567891011121314...Installed /usr/local/lib/python3.6/site-packages/xgboost-0.6-py3.6.eggProcessing dependencies for xgboost==0.6Searching for scipy==0.19.1Best match: scipy 0.19.1Adding scipy 0.19.1 to easy-install.pth fileUsing /usr/local/lib/python3.6/site-packagesSearching for numpy==1.13.0Best match: numpy 1.13.0Adding numpy 1.13.0 to easy-install.pth fileUsing /usr/local/lib/python3.6/site-packagesFinished processing dependencies for xgboost==0.6 which means you have installed XGBoost successfully, then you can restart Python (or the IDE you used), and you will be able to import xgboost. JVM-Package Configuration Currently, XGBoost4J only support installation from source. Building XGBoost4J using Maven requires Maven 3 or newer, Java 7+ and CMake 3.2+ for compiling the JNI bindings. Before you install XGBoost4J, you need to define environment variable JAVA_HOME as your JDK directory to ensure that your compiler can find jni.h correctly, since XGBoost4J relies on JNI to implement the interaction between the JVM and native libraries. After your JAVA_HOME is defined correctly, it is as simple as run mvn package under jvm-packages directory to install XGBoost4J. You can also skip the tests by running mvn -DskipTests=true package, if you are sure about the correctness of your local setup. Go to jvm-packages, then use mvn command to compile it. 12$ cd jvm-packages$ sudo mvn package # sudo is important, prevent permission denied issue If the setting is correct, we will get the following information (It will take some time to compile, be patient): 1234567891011121314151617181920212223...[INFO] [INFO] --- maven-jar-plugin:2.4:jar (default-jar) @ xgboost4j-example ---[INFO] Building jar: /usr/local/xgboost/jvm-packages/xgboost4j-example/target/xgboost4j-example-0.7.jar[INFO] [INFO] --- maven-assembly-plugin:2.6:single (make-assembly) @ xgboost4j-example ---[INFO] Building jar: /usr/local/xgboost/jvm-packages/xgboost4j-example/target/xgboost4j-example-0.7-jar-with-dependencies.jar[INFO] ------------------------------------------------------------------------[INFO] Reactor Summary:[INFO] [INFO] xgboost-jvm ........................................ SUCCESS [ 5.185 s][INFO] xgboost4j .......................................... SUCCESS [02:26 min][INFO] xgboost4j-spark .................................... SUCCESS [02:55 min][INFO] xgboost4j-flink .................................... SUCCESS [ 59.954 s][INFO] xgboost4j-example .................................. SUCCESS [ 19.563 s][INFO] ------------------------------------------------------------------------[INFO] BUILD SUCCESS[INFO] ------------------------------------------------------------------------[INFO] Total time: 06:46 min[INFO] Finished at: 2017-07-27T19:04:16+08:00[INFO] Final Memory: 434M/1551M[INFO] ------------------------------------------------------------------------IsaacChanghaus-MacBook-Pro:jvm-packages zhanghao$ Reference XGBoost GitHub Page How to install xgboost in python on Mac? Installing XGBoost on Mac OSX unsupported option ‘-fopenmp’ on Mac OS X XGBoost JVM Package databricks – Install and Use XGBoost]]></content>
      <categories>
        <category>Setting</category>
      </categories>
      <tags>
        <tag>mac os x</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mathematical Derivation of Covariance Matrix]]></title>
    <url>%2F2017%2F06%2F17%2FMathematical-Derivation-of-Covariance%2F</url>
    <content type="text"><![CDATA[In probability theory and statistics, covariance is a measure of the joint variability of two random variables. If the greater values of one variable mainly correspond with the greater values of the other variable, and the same holds for the lesser values, i.e., the variables tend to show similar behavior, the covariance is positive. In the opposite case, when the greater values of one variable mainly correspond to the lesser values of the other, i.e., the variables tend to show opposite behavior, the covariance is negative. The sign of the covariance therefore shows the tendency in the linear relationship between the variables. While the normalized version of the covariance, the correlation coefficient, however, shows by its magnitude the strength of the linear relation. Notation and Definition Suppose a set of random variables, \(X_{1},X_{2},\dots,X_{n}\), where each random variable has \(m\) samples, say, \(X_{i}=(x_{i,1},\dots,x_{i,m})\), and denote \(\mathbf{X}=(X_{1},X_{2},\dots,X_{n})^{T}\), thus, \(\mathbf{X}\) is the random sample matrix:\[\mathbf{X}=\begin{bmatrix} x_{1,1} &amp; x_{1,2} &amp; \dots &amp; x_{1,m} \\ x_{2,1} &amp; x_{2,2} &amp; \dots &amp; x_{2,m} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ x_{n,1} &amp; x_{n,2} &amp; \dots &amp; x_{n,m} \end{bmatrix}_{n\times m}\tag{1}\]where the elements in \(i\)-th row of \(\mathbf{X}\), i.e., \(x_{i,1},\dots,x_{i,m}\), represent the \(m\) samples of \(i\)-th random variable \(X_{i}\). Moreover, let’s denote\[\alpha_{i}:=(x_{i,1},x_{i,2},\dots,x_{i,m})^{T},i=1,2,\dots,n\tag{2}\]\[\beta_{j}:=(x_{1,j},x_{2,j},\dots,x_{n,j})^{T},j=1,2,\dots,m\tag{3}\]so, \(\alpha_{i}\) represents the \(i\)-th row of \(\mathbf{X}\), while \(\beta_{j}\) corresponds to the \(j\)-th column of \(\mathbf{X}\), then we will have:\[\mathbf{X}=(\beta_{1},\beta_{2},\dots,\beta_{m})=(\alpha_{1},\alpha_{2},\dots,\alpha_{n})^{T}\tag{4}\]Below gives some basic concepts which will be used to deduce the covariance matrix: Mathematical expectation, \(E[\xi]\), is the summation or integration of a possible values from a random variable \(\xi\). It is computed by the sum of the product of each posibale value \(x_{i}\) and the probability \(P(\xi=x_{i})\) of the value occurring.\[E[\xi]=\sum_{i}x_{i}P(\xi=x_{i})\tag{5}\]Here the mathematical expectation reflects the average of all possible values of the random variable. Simply, assuming all the possible values of \(\xi\) are \(x_{1},x_{2},\dots,x_{n}\), and they are equiprobable, i.e, \(P(\xi=x_{i})=\frac{1}{n}\), \(i=1,2,\dots,n\), then we have\[E[\xi]=\sum_{i=1}^{n}x_{i}P(\xi=x_{i})=\frac{1}{n}\sum_{i=1}^{n}x_{i}\tag{6}\]Besides, mathematical expectation operator is linear in the sense that\[E[a\xi+b\eta]=aE[\xi]+bE[\eta]\tag{7}\]and \(\xi-E[\xi]\) is called dispersion of random variable \(\xi\). Variance, denote as \(D[\xi]\), or \(var[\xi]\), is the expectation of the squared deviation of a random variable \(\xi\) from its mean\[var[\xi]=E\big[(\xi-E[\xi])^{2}\big]\tag{8}\]It is non-negative, and informally measures how far a set of (random) numbers are spread out from their mean. Thus, the variance can be used to infer the rate of divergence of distribution of random varibale. Covariance is a measure of the joint variability of two random variables. Considering two random variables \(X_{i}\) and \(X_{j}\), their covariance is defined as\[cov(X_{i},X_{j})=E\big[(X_{i}-E[X_{i}])\centerdot(X_{j}-E[X_{j}])\big]\tag{9}\]and we have - \(cov(X_{i},X_{j})=cov(X_{j},X_{i})\) - \(cov(X_{i},X_{i})=var(X_{i})\) Futher, accoding to the linearity of expectation, we have\[\begin{aligned}cov(X_{i},X_{j}) &amp; = E\big[(X_{i}-E[X_{i}])\centerdot(X_{j}-E[X_{j}])\big]\\ &amp; = E\big[X_{i}X_{j}-E[X_{j}]X_{i}-E[X_{i}]X_{j}+E[X_{i}]E[X_{j}]\big]\\ &amp; = E[X_{i}X_{j}]-E[X_{j}]E[X_{i}]-E[X_{i}]E[X_{j}]+E[X_{i}]E[X_{j}]\\ &amp; = E[X_{i}X_{j}]-E[X_{i}]E[X_{j}]\end{aligned}\tag{10}\]Thus, \(cov(X_{i},X_{j})=E[X_{i}X_{j}]-E[X_{i}]E[X_{j}]\). Independence: If \(X_{i}\) and \(X_{j}\) are independent, then their covariance is zero. This follows because under independence, we have\[E[X_{i},X_{j}]=E[X_{i}]E[X_{j}]\]Coherence: assume that the random variables \(X_{i}\) and \(X_{j}\) santisfy \(var[X_{i}]var[X_{j}]\neq 0\), then the correlation coefficient is defined as\[\rho_{X_{i},X_{j}}=\frac{cov[X_{i},X_{j}]}{\sqrt{var[X_{i}]}\centerdot\sqrt{var[X_{j}]}}\tag{11}\]if \(\rho_{X_{i},X_{j}}=0\), \(X_{i}\) and \(X_{j}\) areuncorrelated, otherwise, they are correlated. Correlation coefficient, also named as linearly correlation coefficient, since the correlation coefficient describes the degree of “linear” relation between \(X_{i}\) and \(X_{j}\). Meanwhile, it is worth to mention that for two random variables \(X_{i}\) and \(X_{j}\), \(X_{i}\) and \(X_{j}\) are independent is the sufficient but not necessary condition of uncorrelated, say, if \(X_{i}\) and \(X_{j}\) are independent, they must be uncorrelated, conversely, it does not always stand up. Thus, we can derive the relationship: independent \(\Rightarrow\) uncorrelated \(\Leftrightarrow\) \(cov(X_{i},X_{j})=0\). For a \(n\) dimensional random variable \(\mathbf{X}=(X_{1},X_{2},\dots,X_{n})^{T}\), its covariance matrix is defined as:\[C:=C(\mathbf{X})=(c_{i,j})_{n\times n}=\begin{bmatrix} cov(X_{1},X_{1}) &amp; cov(X_{1},X_{2}) &amp; \dots &amp; cov(X_{1},X_{n}) \\ cov(X_{2},X_{1}) &amp; cov(X_{2},X_{2}) &amp; \dots &amp; cov(X_{2},X_{n}) \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ cov(X_{n},X_{1}) &amp; cov(X_{n},X_{2}) &amp; \dots &amp; cov(X_{n},X_{n}) \end{bmatrix}_{n\times n},\tag{12}\]where \(c_{i,j}=cov(X_{i},X_{j})\), obviously, \(C\) is a symmetric matrix, and its diagonal elements represent variance, non-diagonal elements represent the covariance among different components of \(\mathbf{X}\). If the coherence of two components are small, then the corresponding value in \(C\) is small either. Specifically, if all the components are uncorrelated with each other, \(C\) becomes a diagonal matrix. Note that, we cannot derive the actual value of \(C(\mathbf{X})\), but approximately estimate its value according to the sample data provided in \(\mathbf{X}\), which means the computed covariance matrix relys on the sample data, normally, the larger the sample data (\(m\) is larger), the broader th coverage, and the covariance metrix is more reliable. Mathematical Derivation According to (6), \(E[X_{i}]\) and \(E[X_{j}]\) can be approximately written as\[ E[X_{i}]=\frac{1}{m}\sum_{k=1}^{m}x_{i,k},\qquad E[X_{j}]=\frac{1}{m}\sum_{k=1}^{m}x_{j,k}\tag{13}\]Let \(Y=(X_{i}-E[X_{i}])\centerdot(X_{j}-E[X_{j}]):=(Y_{1},Y_{2},\dots,Y_{m})\), where \(Y_{k}=(x_{i,k}-E[X_{i}])\centerdot(x_{j,k}-E[X_{j}])\), \(k=1,2,\dots,m\), with (13), we can derive:\[\begin{aligned}c_{i,j} &amp; =E[Y]=\frac{1}{m}\sum_{k=1}^{m}Y_{k}\\ &amp; =\frac{1}{m}\sum_{k=1}^{m}(x_{i,k}-E[X_{i}])\centerdot(x_{j,k}-E[X_{j}])\\ &amp; =\frac{1}{m}\sum_{k=1}^{m}\bigg[\big(x_{i,k}-\frac{1}{m}\sum_{p=1}^{m}x_{i,p}\big)\centerdot\big(x_{j,k}-\frac{1}{m}\sum_{q=1}^{m}x_{j,q}\big)\bigg] \end{aligned}\tag{14}\]Then we need to simply the formula (14), as describe below:\[\begin{aligned}c_{i,j} &amp; = \frac{1}{m}\sum_{k=1}^{m}\bigg[\big(x_{i,k}-\frac{1}{m}\sum_{p=1}^{m}x_{i,p}\big)\centerdot\big(x_{j,k}-\frac{1}{m}\sum_{q=1}^{m}x_{j,q}\big)\bigg]\\ &amp; = \frac{1}{m}\sum_{k=1}^{m}\bigg[x_{i,k}\centerdot x_{j,k}-x_{i,k}\centerdot\frac{1}{m}\sum_{q=1}^{m}x_{j,q}-x_{j,k}\centerdot\frac{1}{m}\sum_{p=1}^{m}x_{i,p}+\frac{1}{m}\sum_{p=1}^{m}x_{i,p}\centerdot\frac{1}{m}\sum_{q=1}^{m}x_{j,q}\bigg]\\ &amp; = \frac{1}{m}\sum_{k=1}^{m}x_{i,k}\centerdot x_{j,k}-\frac{1}{m}\sum_{k=1}^{m}x_{i,k}\centerdot\frac{1}{m}\sum_{q=1}^{m}x_{j,q}-\frac{1}{m}\sum_{k=1}^{m}x_{j,k}\centerdot\frac{1}{m}\sum_{p=1}^{m}x_{i,p}+\frac{1}{m}\sum_{k=1}^{m}\frac{1}{m}\sum_{p=1}^{m}x_{i,p}\centerdot\frac{1}{m}\sum_{q=1}^{m}x_{j,q}\\ &amp; =\frac{1}{m}\sum_{k=1}^{m}x_{i,k}\centerdot x_{j,k}-\frac{1}{m^{2}}\sum_{k=1}^{m}x_{i,k}\centerdot\sum_{k=1}^{m}x_{j,k}-\frac{1}{m^{2}}\sum_{k=1}^{m}x_{j,k}\centerdot\sum_{k=1}^{m}x_{i,k}+\frac{1}{m^{3}}\sum_{k=1}^{m}1\centerdot\sum_{p=1}^{m}x_{i,p}\centerdot\sum_{q=1}^{m}x_{j,q}\\ &amp; = \frac{1}{m}\sum_{k=1}^{m}x_{i,k}\centerdot x_{j,k}-\frac{1}{m^{2}}\big(\sum_{k=1}^{m}x_{i,k}\big)\centerdot\big(\sum_{k=1}^{m}x_{j,k}\big)-\frac{1}{m^{2}}\big(\sum_{k=1}^{m}x_{j,k}\big)\centerdot\big(\sum_{k=1}^{m}x_{i,k}\big)+\frac{1}{m^{2}}\big(\sum_{k=1}^{m}x_{i,k}\big)\centerdot\big(\sum_{k=1}^{m}x_{j,k}\big)\\ &amp; = \frac{1}{m}\sum_{k=1}^{m}x_{i,k}\centerdot x_{j,k}-\frac{1}{m^{2}}\big(\sum_{k=1}^{m}x_{i,k}\big)\centerdot\big(\sum_{k=1}^{m}x_{j,k}\big)\\ &amp; =\frac{1}{m}\alpha_{i}^{T}\alpha_{j}-\frac{1}{m^{2}}\big(\sum_{k=1}^{m}x_{i,k}\big)\centerdot\big(\sum_{k=1}^{m}x_{j,k}\big) \end{aligned}\tag{15}\]Here we derive the representation of element \(c_{i,j}\) in \(C\), in order to obtain a more terse expression of \(C\), we first introduce two temporary matrix \(A\) and \(B\), make \(C=A-B\), and simply \(A\) and \(B\) seperately. For matrix \(A=(a_{i,j})_{n\times n}\), where\[a_{i,j}=\frac{1}{m}\alpha_{i}^{T}\alpha_{j}\tag{16}\]according to (4), we have\[\begin{aligned}A &amp; =\frac{1}{m}\begin{pmatrix}\alpha_{1}^{T}\\\alpha_{2}^{T}\\\vdots\\\alpha_{n}^{T}\end{pmatrix}(\alpha_{1},\alpha_{2},\dots,\alpha_{n})\\ &amp; =\frac{1}{m}\mathbf{X}\mathbf{X}^{T}\\ &amp; =\frac{1}{m}(\beta_{1},\beta_{2},\dots,\beta_{m})(\beta_{1},\beta_{2},\dots,\beta_{m})^{T}\\ &amp; =\frac{1}{m}\sum_{i=1}^{m}\beta_{i}\beta_{i}^{T} \end{aligned}\tag{17}\]Then for matrix \(B=(b_{i,j})_{n\times n}\), where\[b_{i,j}=\frac{1}{m^{2}}\big(\sum_{k=1}^{m}x_{i,k}\big)\centerdot\big(\sum_{k=1}^{m}x_{j,k}\big)\tag{18}\]we have\[\begin{aligned}B &amp; =\frac{1}{m^{2}} \begin{pmatrix}\sum_{k=1}^{m}x_{1,k}\\ \sum_{k=1}^{m}x_{2,k}\\ \vdots\\ \sum_{k=1}^{m}x_{n,k} \end{pmatrix}(\sum_{k=1}^{m}x_{1,k},\sum_{k=1}^{m}x_{2,k},\dots,\sum_{k=1}^{m}x_{n,k})\\ &amp; =\frac{1}{m^{2}}\big(\sum_{i=1}^{m}\beta_{i}\big)\big(\sum_{i=1}^{m}\beta_{i}\big)^{T}\end{aligned}\tag{19}\]Since we suppose that \(C=A-B\), so according to (17) and(19), we can write \(C\) as\[ C=\frac{1}{m}\sum_{i=1}^{m}\beta_{i}\beta_{i}^{T} -\frac{1}{m^{2}}\big(\sum_{i=1}^{m}\beta_{i}\big)\big(\sum_{i=1}^{m}\beta_{i}\big)^{T}\tag{20}\]Actually, the formula (20) still can be simplified to merge the two terms as one term:\[\begin{aligned}C &amp; =\frac{1}{m}\sum_{i=1}^{m}\beta_{i}\beta_{i}^{T}-\frac{1}{m^{2}}\big(\sum_{i=1}^{m}\beta_{i}\big)\big(\sum_{k=1}^{m}\beta_{k}\big)^{T}\\ &amp; =\frac{1}{m}\sum_{i=1}^{m}\beta_{i}\beta_{i}^{T}-\frac{1}{m^{2}}\big(\sum_{i=1}^{m}\beta_{i}\big)\big(\sum_{k=1}^{m}\beta_{k}\big)^{T}-\frac{1}{m^{2}}\big(\sum_{k=1}^{m}\beta_{k}\big)\big(\sum_{i=1}^{m}\beta_{i}\big)^{T}+\frac{1}{m^{2}}\big(\sum_{k=1}^{m}\beta_{k}\big)\big(\sum_{i=k}^{m}\beta_{k}\big)^{T}\\ &amp; =\frac{1}{m}\sum_{i=1}^{m}\beta_{i}\beta_{i}^{T}-\frac{1}{m^{2}}\big(\sum_{i=1}^{m}\beta_{i}\big)\big(\sum_{k=1}^{m}\beta_{k}\big)^{T}-\frac{1}{m^{2}}\big(\sum_{k=1}^{m}\beta_{k}\big)\big(\sum_{i=1}^{m}\beta_{i}\big)^{T}+\frac{1}{m^{3}}\sum_{i=1}^{m}\big(\sum_{k=1}^{m}\beta_{k}\big)\big(\sum_{i=k}^{m}\beta_{k}\big)^{T}\\ &amp; = \frac{1}{m}\bigg\{\beta_{i}\beta_{i}^{T}-\frac{1}{m}\beta_{i}\big(\sum_{k=1}^{m}\beta_{k}\big)^{T}-\frac{1}{m}\big(\sum_{k=1}^{m}\beta_{k}\big)\big(\beta_{i}^{T}\big)+\big(\frac{1}{m}\sum_{k=1}^{m}\beta_{k}\big)\big(\frac{1}{m}\sum_{i=k}^{m}\beta_{k}\big)^{T}\bigg\}\\ &amp; =\frac{1}{m}\sum_{i=1}^{m}\big(\beta_{i}-\frac{1}{m}\sum_{k=1}^{m}\beta_{k}\big)\big(\beta_{i}^{T}-\frac{1}{m}\sum_{k=1}^{m}\beta_{k}^{T}\big)\\ &amp; =\frac{1}{m}\sum_{i=1}^{m}\big(\beta_{i}-\beta_{0}\big)\big(\beta_{i}-\beta_{0}\big)^{T} \end{aligned}\tag{21}\]where \(\beta_{0}=\frac{1}{m}\sum_{k=1}^{m}\beta_{k}\), specifically, if the average of \(m\) samples are zero for every random variable in \(\mathbf{X}\), i.e., \(\sum_{k=1}^{m}x_{i,k}=0\), then according to (18), we have \(b_{i,j}=0\), further, \(B=0\). In this case, we have \(c_{i,j}=a_{i,j}=\frac{1}{m}\alpha_{i}^{T}\alpha_{j}\),\[C=A=\frac{1}{m}\sum_{i=1}^{m}\beta_{i}\beta_{i}^{T}=\frac{1}{m}\mathbf{X}\mathbf{X}^{T}\tag{22}\]This is the overall mathematical process to deduce the covariance matrix. Reference Covariance Covariance Matrix Mathematical expectation]]></content>
      <categories>
        <category>Mathematics</category>
      </categories>
      <tags>
        <tag>mathematics</tag>
        <tag>covariance</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Understanding LSTM Networks [Reprinted]]]></title>
    <url>%2F2017%2F06%2F13%2FUnderstanding-LSTM-Networks%2F</url>
    <content type="text"><![CDATA[Declaration: This blog article is totaly not my original article. I just reproduce it from colah’s blog – Understanding LSTM Networks, since it is really an excellent article of explanation of LSTM and I am afraid that I may lose the link of this article or the author may change her blog address. So I put this article into my own blog… Moreover, the Chinese version of this article, translated by Not_GOD, are available here. Recurrent Neural Networks Humans don’t start their thinking from scratch every second. As you read this essay, you understand each word based on your understanding of previous words. You don’t throw everything away and start thinking from scratch again. Your thoughts have persistence. Traditional neural networks can’t do this, and it seems like a major shortcoming. For example, imagine you want to classify what kind of event is happening at every point in a movie. It’s unclear how a traditional neural network could use its reasoning about previous events in the film to inform later ones. Recurrent neural networks address this issue. They are networks with loops in them, allowing information to persist. In the above diagram, a chunk of neural network, \(A\), looks at some input \(x_{t}\) and outputs a value \(h_{t}\). A loop allows information to be passed from one step of the network to the next. These loops make recurrent neural networks seem kind of mysterious. However, if you think a bit more, it turns out that they aren’t all that different than a normal neural network. A recurrent neural network can be thought of as multiple copies of the same network, each passing a message to a successor. Consider what happens if we unroll the loop: This chain-like nature reveals that recurrent neural networks are intimately related to sequences and lists. They’re the natural architecture of neural network to use for such data. And they certainly are used! In the last few years, there have been incredible success applying RNNs to a variety of problems: speech recognition, language modeling, translation, image captioning… The list goes on. I’ll leave discussion of the amazing feats one can achieve with RNNs to Andrej Karpathy’s excellent blog post, The Unreasonable Effectiveness of Recurrent Neural Networks. But they really are pretty amazing. Essential to these successes is the use of “LSTMs”, a very special kind of recurrent neural network which works, for many tasks, much much better than the standard version. Almost all exciting results based on recurrent neural networks are achieved with them. It’s these LSTMs that this essay will explore. The Problem of Long-Term Dependencies One of the appeals of RNNs is the idea that they might be able to connect previous information to the present task, such as using previous video frames might inform the understanding of the present frame. If RNNs could do this, they’d be extremely useful. But can they? It depends. Sometimes, we only need to look at recent information to perform the present task. For example, consider a language model trying to predict the next word based on the previous ones. If we are trying to predict the last word in “the clouds are in the sky”, we don’t need any further context – it’s pretty obvious the next word is going to be sky. In such cases, where the gap between the relevant information and the place that it’s needed is small, RNNs can learn to use the past information. But there are also cases where we need more context. Consider trying to predict the last word in the text “I grew up in France… I speak fluent French”. Recent information suggests that the next word is probably the name of a language, but if we want to narrow down which language, we need the context of France, from further back. It’s entirely possible for the gap between the relevant information and the point where it is needed to become very large. Unfortunately, as that gap grows, RNNs become unable to learn to connect the information. In theory, RNNs are absolutely capable of handling such “long-term dependencies”. A human could carefully pick parameters for them to solve toy problems of this form. Sadly, in practice, RNNs don’t seem to be able to learn them. The problem was explored in depth by Hochreiter (1991) [German] and Bengio, et al. (1994), who found some pretty fundamental reasons why it might be difficult. Thankfully, LSTMs don’t have this problem! LSTM Networks Long Short Term Memory networks – usually just called “LSTMs” – are a special kind of RNN, capable of learning long-term dependencies. They were introduced by Hochreiter &amp; Schmidhuber (1997), and were refined and popularized by many people in following work. (In addition to the original authors, a lot of people contributed to the modern LSTM. A non-comprehensive list is: Felix Gers, Fred Cummins, Santiago Fernandez, Justin Bayer, Daan Wierstra, Julian Togelius, Faustino Gomez, Matteo Gagliolo, and Alex Graves) They work tremendously well on a large variety of problems, and are now widely used. LSTMs are explicitly designed to avoid the long-term dependency problem. Remembering information for long periods of time is practically their default behavior, not something they struggle to learn! All recurrent neural networks have the form of a chain of repeating modules of neural network. In standard RNNs, this repeating module will have a very simple structure, such as a single tanh layer. LSTMs also have this chain like structure, but the repeating module has a different structure. Instead of having a single neural network layer, there are four, interacting in a very special way. Don’t worry about the details of what’s going on. We’ll walk through the LSTM diagram step by step later. For now, let’s just try to get comfortable with the notation we’ll be using. In the above diagram, each line carries an entire vector, from the output of one node to the inputs of others. The pink circles represent pointwise operations, like vector addition, while the yellow boxes are learned neural network layers. Lines merging denote concatenation, while a line forking denote its content being copied and the copies going to different locations. The Core Idea Behind LSTMs The key to LSTMs is the cell state, the horizontal line running through the top of the diagram. The cell state is kind of like a conveyor belt. It runs straight down the entire chain, with only some minor linear interactions. It’s very easy for information to just flow along it unchanged. The LSTM does have the ability to remove or add information to the cell state, carefully regulated by structures called gates. Gates are a way to optionally let information through. They are composed out of a sigmoid neural net layer and a pointwise multiplication operation. The sigmoid layer outputs numbers between zero and one, describing how much of each component should be let through. A value of zero means “let nothing through”, while a value of one means “let everything through”! An LSTM has three of these gates, to protect and control the cell state. Step-by-Step LSTM Walk Through The first step in our LSTM is to decide what information we’re going to throw away from the cell state. This decision is made by a sigmoid layer called the “forget gate layer”. It looks at \(h_{t-1}\) and \(x_{t}\), and outputs a number between \(0\) and \(1\) for each number in the cell state \(C_{t-1}\). A \(1\) represents “completely keep this” while a \(0\) represents “completely get rid of this”. Let’s go back to our example of a language model trying to predict the next word based on all the previous ones. In such a problem, the cell state might include the gender of the present subject, so that the correct pronouns can be used. When we see a new subject, we want to forget the gender of the old subject. The next step is to decide what new information we’re going to store in the cell state. This has two parts. First, a sigmoid layer called the “input gate layer” decides which values we’ll update. Next, a tanh layer creates a vector of new candidate values, \(\tilde{C}_{t}\), that could be added to the state. In the next step, we’ll combine these two to create an update to the state. In the example of our language model, we’d want to add the gender of the new subject to the cell state, to replace the old one we’re forgetting. It’s now time to update the old cell state, \(C_{t-1}\), into the new cell state \(C_{t}\). The previous steps already decided what to do, we just need to actually do it. We multiply the old state by \(f_{t}\), forgetting the things we decided to forget earlier. Then we add \(i_{t}*\tilde{C}_{t}\). This is the new candidate values, scaled by how much we decided to update each state value. In the case of the language model, this is where we’d actually drop the information about the old subject’s gender and add the new information, as we decided in the previous steps. Finally, we need to decide what we’re going to output. This output will be based on our cell state, but will be a filtered version. First, we run a sigmoid layer which decides what parts of the cell state we’re going to output. Then, we put the cell state through \(tanh\) (to push the values to be between \(-1\) and \(1\)) and multiply it by the output of the sigmoid gate, so that we only output the parts we decided to. For the language model example, since it just saw a subject, it might want to output information relevant to a verb, in case that’s what is coming next. For example, it might output whether the subject is singular or plural, so that we know what form a verb should be conjugated into if that’s what follows next. Variants on Long Short Term Memory What I’ve described so far is a pretty normal LSTM. But not all LSTMs are the same as the above. In fact, it seems like almost every paper involving LSTMs uses a slightly different version. The differences are minor, but it’s worth mentioning some of them. One popular LSTM variant, introduced by Gers &amp; Schmidhuber (2000), is adding “peephole connections”. This means that we let the gate layers look at the cell state. The above diagram adds peepholes to all the gates, but many papers will give some peepholes and not others. Another variation is to use coupled forget and input gates. Instead of separately deciding what to forget and what we should add new information to, we make those decisions together. We only forget when we’re going to input something in its place. We only input new values to the state when we forget something older. A slightly more dramatic variation on the LSTM is the Gated Recurrent Unit, or GRU, introduced by Cho, et al. (2014). It combines the forget and input gates into a single “update gate”. It also merges the cell state and hidden state, and makes some other changes. The resulting model is simpler than standard LSTM models, and has been growing increasingly popular. These are only a few of the most notable LSTM variants. There are lots of others, like Depth Gated RNNs by Yao, et al. (2015). There’s also some completely different approach to tackling long-term dependencies, like Clockwork RNNs by Koutnik, et al. (2014). Which of these variants is best? Do the differences matter? Greff, et al. (2015) do a nice comparison of popular variants, finding that they’re all about the same. Jozefowicz, et al. (2015) tested more than ten thousand RNN architectures, finding some that worked better than LSTMs on certain tasks. Conclusion Earlier, I mentioned the remarkable results people are achieving with RNNs. Essentially all of these are achieved using LSTMs. They really work a lot better for most tasks! Written down as a set of equations, LSTMs look pretty intimidating. Hopefully, walking through them step by step in this essay has made them a bit more approachable. LSTMs were a big step in what we can accomplish with RNNs. It’s natural to wonder: is there another big step? A common opinion among researchers is: “Yes! There is a next step and it’s attention!” The idea is to let every step of an RNN pick information to look at from some larger collection of information. For example, if you are using an RNN to create a caption describing an image, it might pick a part of the image to look at for every word it outputs. In fact, Xu, et al. (2015) do exactly this – it might be a fun starting point if you want to explore attention! There’s been a number of really exciting results using attention, and it seems like a lot more are around the corner… Attention isn’t the only exciting thread in RNN research. For example, Grid LSTMs by Kalchbrenner, et al. (2015) seem extremely promising. Work using RNNs in generative models – such as Gregor, et al. (2015), Chung, et al. (2015), or Bayer &amp; Osendorfer (2015) – also seems very interesting. The last few years have been an exciting time for recurrent neural networks, and the coming ones promise to only be more so! Acknowledgments I’m grateful to a number of people for helping me better understand LSTMs, commenting on the visualizations, and providing feedback on this post. I’m very grateful to my colleagues at Google for their helpful feedback, especially Oriol Vinyals, Greg Corrado, Jon Shlens, Luke Vilnis, and Ilya Sutskever. I’m also thankful to many other friends and colleagues for taking the time to help me, including Dario Amodei, and Jacob Steinhardt. I’m especially thankful to Kyunghyun Cho for extremely thoughtful correspondence about my diagrams. Before this post, I practiced explaining LSTMs during two seminar series I taught on neural networks. Thanks to everyone who participated in those for their patience with me, and for their feedback. Reference &amp; Some LSTM Information colah’s blog – Understanding LSTM Networks Grid-LSTM Slides Grid Long Short-Term Memory Learning to Forget: Continual Prediction with LSTM LSTM in Recurrent Neural Networks Supervised Sequence Labelling with Recurrent Neural Networks Long Short-Term Memory Unsupervised Learning of Video Representations using LSTMs Unsupervised Learning of Video Representations using LSTMs – GitHub]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
        <tag>machine learning</tag>
        <tag>lstm</tag>
        <tag>gru</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Loss Functions in Artificial Neural Networks]]></title>
    <url>%2F2017%2F06%2F07%2FLoss-Functions-in-Artificial-Neural-Networks%2F</url>
    <content type="text"><![CDATA[Loss function is an important part in artificial neural networks, which is used to measure the inconsistency between predicted value (\(\hat{y}\)) and actual label (\(y\)). It is a non-negative value, where the robustness of model increases along with the decrease of the value of loss function. Loss function is the hard core of empirical risk function as well as a significant component of structural risk function. Generally, the structural risk function of a model is consist of empirical risk term and regularization term, which can be represented as\[\boldsymbol{\theta}^{*}=\arg\min_{\boldsymbol{\theta}}\boldsymbol{\mathcal{L}}(\boldsymbol{\theta})+\lambda\centerdot\Phi(\boldsymbol{\theta})=\arg\min_{\boldsymbol{\theta}}\frac{1}{n}\sum_{i=1}^{n}L\big(y^{(i)},\hat{y}^{(i)}\big)+\lambda\centerdot\Phi(\boldsymbol{\theta})\\=\arg\min_{\boldsymbol{\theta}}\frac{1}{n}\sum_{i=1}^{n}L\big(y^{(i)},f(\mathbf{x}^{(i)},\boldsymbol{\theta})\big)+\lambda\centerdot\Phi(\boldsymbol{\theta})\]where \(\Phi(\boldsymbol{\theta})\) is the regularization term or penalty term, \(\boldsymbol{\theta}\) is the parameters of model to be learned, \(f(\centerdot)\) represents the activation function and \(\mathbf{x}^{(i)}=\{x_{1}^{(i)},x_{2}^{(i)},\dots ,x_{m}^{(i)}\}\in\mathbb{R}^{m}\) denotes the a training sample. Here we only concentrate on the empirical risk term (loss function)\[\boldsymbol{\mathcal{L}}(\boldsymbol{\theta})=\frac{1}{n}\sum_{i=1}^{n}L\big(y^{(i)},f(\mathbf{x}^{(i)},\boldsymbol{\theta})\big)\]and introduce the mathematical expressions of several commonly-used loss functions as well as the corresponding expression in DeepLearning4J. Mean Squared Error Mean Squared Error (MSE), or quadratic, loss function is widely used in linear regression as the performance measure, and the method of minimizing MSE is called Ordinary Least Squares (OSL), the basic principle of OSL is that the optimized fitting line should be a line which minimizes the sum of distance of each point to the regression line, i.e., minimizes the quadratic sum. The standard form of MSE loss function is defined as\[\boldsymbol{\mathcal{L}}=\frac{1}{n}\sum_{i=1}^{n}(y^{(i)}-\hat{y}^{(i)})^{2}\]where \((y^{(i)}-\hat{y}^{(i)})\) is named as residual, and the target of MSE loss function is to minimize the residual sum of squares. In DeepLearning4J, it is LossFunctions.LossFunction.MSE or LossFunctions.LossFunction.SQUARED_LOSS (they are same in DL4J). However, if using Sigmoid as the activation function, the quadratic loss function would suffer the problem of slow convergence (learning speed), for other activation funtions, it would not have such problem. For example, by using Sigmoid, \(\hat{y}^{(i)}=\sigma(\mathbf{z}^{(i)})=\sigma(\boldsymbol{\theta}^{T}\mathbf{x}^{(i)})\), simply, we only consider one sample, say, \((y-\sigma(\mathbf{z}))^{2}\), and it derivative is computed by\[\frac{\partial\boldsymbol{\mathcal{L}}}{\partial\boldsymbol{\theta}}=-(y-\sigma(\mathbf{z}))\centerdot\sigma&#39;(\mathbf{z})\centerdot\mathbf{x}\]according to the shape and feature of Sigmoid (see my another blog: Activation Functions in Artificial Neural Networks), when \(\sigma(\mathbf{z})\) tends to 0 or 1, \(\sigma&#39;(\mathbf{z})\) is close to zero, and when \(\sigma(\mathbf{z})\) close to 0.5, \(\sigma&#39;(\mathbf{z})\) will reach it maximum. In this case, when the difference between predicted value and true label \((y-\sigma(\mathbf{z}))\) is large, \(\sigma&#39;(\mathbf{z})\) will close to 0, which decreases the convergence speed, this is improper, since we expect that the learning speed should be fast when the error is large. Mean Squared Logarithmic Error Mean Squared Logarithmic Error (MSLE) loss function is a variant of MSE, which is defined as\[\boldsymbol{\mathcal{L}}=\frac{1}{n}\sum_{i=1}^{n}\big(\log(y^{(i)}+1)-\log(\hat{y}^{(i)}+1)\big)^{2}\]MSLE is also used to measure the different between actual and predicted. By taking the log of the predictions and actual values, what changes is the variance that you are measuring. It is usually used when you do not want to penalize huge differences in the predicted and the actual values when both predicted and true values are huge numbers. Another thing is that MSLE penalizes under-estimates more than over-estimates. 1. If both predicted and actual values are small: MSE and MSLE is same. 2. If either predicted or the actual value is big: \(MSE &gt; MSLE\). 3. If both predicted and actual values are big: \(MSE &gt; MSLE\) (MSLE becomes almost negligible). In DeepLearning4J, it is expressed as LossFunctions.LossFunction.MEAN_SQUARED_LOGARITHMIC_ERROR. L2 L2 loss function is the square of the L2 norm of the difference between actual value and predicted value. It is mathematically similar to MSE, only do not have division by \(n\), it is computed by\[\boldsymbol{\mathcal{L}}=\sum_{i=1}^{n}(y^{(i)}-\hat{y}^{(i)})^{2}\]For more details, typically in mathematic, please read the paper: On Loss Functions for Deep Neural Networks in Classification, which gives comprehensive explanation about several commomly-used loss functions, including L2, L1 loss function. In DeepLearning4J, it is expressed as LossFunctions.LossFunction.L2. Mean Absolute Error Mean Absolute Error (MAE) is a quantity used to measure how close forecasts or predictions are to the eventual outcomes, which is computed by\[\boldsymbol{\mathcal{L}}=\frac{1}{n}\sum_{i=1}^{n}\big\lvert y^{(i)}-\hat{y}^{(i)}\big\rvert\]where \(\lvert\centerdot\rvert\) denotes the absolute value. Albeit, both MSE and MAE are used in predictive modeling, there are several differences between them. MSE has nice mathematical properties which makes it easier to compute the gradient. However, MAE requires more complicated tools such as linear programming to compute the gradient. Because of the square, large errors have relatively greater influence on MSE than do the smaller error. Therefore, MAE is more robust to outliers since it does not make use of square. On the other hand, MSE is more useful if concerning about large errors whose consequences are much bigger than equivalent smaller ones. MSE also corresponds to maximizing the likelihood of Gaussian random variables. In DeepLearning4J, it is expressed as LossFunctions.LossFunction.MEAN_ABSOLUTE_ERROR. Mean Absolute Percentage Error Mean Absolute Percentage Error (MAPE) is a variant of MAE, it is computed by\[\boldsymbol{\mathcal{L}}=\frac{1}{n}\sum_{i=1}^{n}\bigg\lvert\frac{y^{(i)}-\hat{y}^{(i)}}{y^{(i)}}\bigg\rvert\centerdot100\]Although the concept of MAPE sounds very simple and convincing, it has major drawbacks in practical application: 1. It cannot be used if there are zero values (which sometimes happens for example in demand data) because there would be a division by zero. 2. For forecasts which are too low the percentage error cannot exceed \(100%\), but for forecasts which are too high there is no upper limit to the percentage error. 3. When MAPE is used to compare the accuracy of prediction methods it is biased in that it will systematically select a method whose forecasts are too low. This little-known but serious issue can be overcome by using an accuracy measure based on the ratio of the predicted to actual value (called the Accuracy Ratio), this approach leads to superior statistical properties and leads to predictions which can be interpreted in terms of the geometric mean. In DeepLearning4J, it is expressed as LossFunctions.LossFunction.MEAN_ABSOLUTE_PERCENTAGE_ERROR. L1 L1 loss function is sum of absolute errors of the difference between actual value and predicted value. Similar to the relation between MSE and L2, L1 is mathematically similar to MAE, only do not have division by \(n\), and it is defined as\[\boldsymbol{\mathcal{L}}=\sum_{i=1}^{n}\big\lvert y^{(i)}-\hat{y}^{(i)}\big\rvert\]In DeepLearning4J, it is expressed as LossFunctions.LossFunction.L1. Kullback Leibler (KL) Divergence KL Divergence, also known as relative entropy, information divergence/gain, is a measure of how one probability distribution diverges from a second expected probability distribution. KL divergence loss function is computed by\[\boldsymbol{\mathcal{L}}=\frac{1}{n}\sum_{i=1}^{n}\mathcal{D}_{KL}(y^{(i)}||\hat{y}^{(i)})=\frac{1}{n}\sum_{i=1}^{n}\big[y^{(i)}\centerdot\log\big(\frac{y^{(i)}}{\hat{y}^{(i)}}\big)\big]\\=\underbrace{\frac{1}{n}\sum_{i=1}^{n}\big(y^{(i)}\centerdot\log(y^{(i)})\big)}_{\boldsymbol{entropy}}\underbrace{-\frac{1}{n}\sum_{i=1}^{n}\big(y^{(i)}\centerdot\log(\hat{y}^{(i)})\big)}_{\boldsymbol{cross-entropy}}\]where the first term is entropy and another is cross entropy (another kind of loss function which will be introduced later). KL divergence is a distribution-wise asymmetric measure and thus does not qualify as a statistical metric of spread. In the simple case, a KL divergence of 0 indicates that we can expect similar, if not the same, behavior of two different distributions, while a KL divergence of 1 indicates that the two distributions behave in such a different manner that the expectation given the first distribution approaches zero. For more details, please visit the wikipedia: [link]. In DeepLearning4J, it is expressed as LossFunctions.LossFunction.KL_DIVERGENCE. Moreover, the implementation of Reconstruction Cross Entropy in DeepLearning4J is same as Kullback Leibler (KL) Divergence, thus, you can also use LossFunctions.LossFunction.RECONSTRUCTION_CROSSENTROPY. Cross Entropy Cross Entropy is commonly-used in binary classification (labels are assumed to take values 0 or 1) as a loss function (For multi-classification, use Multi-class Cross Entropy), which is computed by\[\boldsymbol{\mathcal{L}}=-\frac{1}{n}\sum_{i=1}^{n}\big[y^{(i)}\log(\hat{y}^{(i)})+(1-y^{(i)})\log(1-\hat{y}^{(i)})\big]\]Cross entropy measures the divergence between two probability distribution, if the cross entropy is large, which means that the difference between two distribution is large, while if the cross entropy is small, which means that two distribution is similar to each other. As we have mentioned in MSE that it suffers slow divergence when using Sigmoid as activation function, here the cross entropy does not have such problem. Samely, \(\hat{y}^{(i)}=\sigma(\mathbf{z}^{(i)})=\sigma(\boldsymbol{\theta}^{T}\mathbf{x}^{(i)})\), and we only consider one training sample, by using Sigmoid, we have \(\boldsymbol{\mathcal{L}}=y\log(\sigma(\mathbf{z}))+(1-y)\log(1-\sigma(\mathbf{z}))\), and compute it derivative as\[\frac{\partial\boldsymbol{\mathcal{L}}}{\partial\boldsymbol{\theta}}=(y-\sigma(\mathbf{z}))\centerdot\mathbf{x}\]compare to the derivative in MSE, it eliminates the term \(\sigma&#39;(\mathbf{z})\), where the learning speed is only controlled by \((y-\sigma(\mathbf{z}))\). In this case, when the difference between predicted value and actual value is large, the learning speed, i.e., convergence speed, is fast, otherwise, the difference is small, the learning speed is small, this is our expectation. Generally, comparing to quadratic cost function, cross entropy cost function has the advantages that fast convergence and is more likely to reach the global optimization (like the momentum, it increases the update step). For the mathematical details, see wikipedia: [link]. In DeepLearning4J, it is expressed as LossFunctions.LossFunction.XENT. For multi-classification, it is better use LossFunctions.LossFunction.MCXENT. Negative Logarithmic Likelihood Negative Log Likelihood loss function is widely used in neural networks, it measures the accuracy of a classifier. It is used when the model outputs a probability for each class, rather than just the most likely class. It is a “soft” measurement of accuracy that incorporates the idea of probabilistic confidence. It is intimately tied to information theory. And it is similar to cross entropy (in binary classification) or multi-class cross entropy (in multi-classification) mathematically. Negative log likelihood is computed by\[\boldsymbol{\mathcal{L}}=-\frac{1}{n}\sum_{i=1}^{n}\log(\hat{y}^{(i)})\]More details about Negative Log Likelihood and the relation of KL Divergence, Cross Entropy and Negative Log Likelihood, you can visit this post: [link]. In DeepLearning4J, it is expressed as LossFunctions.LossFunction.NEGATIVELOGLIKELIHOOD. Actually, in DL4J, the implementation of MCXENT and NEGATIVELOGLIKELIHOOD is same, since they have almost the mathematically samilar expressions. Poisson Poisson loss function is a measure of how the predicted distribution diverges from the expected distribution, the poisson as loss function is a variant from Poisson Distribution, where the poisson distribution is widely used for modeling count data. It can be shown to be the limiting distribution for a normal approximation to a binomial where the number of trials goes to infinity and the probability goes to zero and both happen at such a rate that np is equal to some mean frequency for the process. In DL4J, the poisson loss function is computed by\[\boldsymbol{\mathcal{L}}=\frac{1}{n}\sum_{i=1}^{n}\big(\hat{y}^{(i)}-y^{(i)}\centerdot\log(\hat{y}^{(i)})\big)\] In DL4J, it is expressed as LossFunctions.LossFunction.POISSON. Moreover, the implementation of Exponential Log Likelihood in DeepLearning4J is same as Poisson, so you can also use LossFunctions.LossFunction.EXPLL. Cosine Proximity Cosine Proximity loss function computes the cosine proximity between predicted value and actual value, which is defined as\[\boldsymbol{\mathcal{L}}=-\frac{\mathbf{y}\centerdot\mathbf{\hat{y}}}{||\mathbf{y}||_{2}\centerdot||\mathbf{\hat{y}}||_{2}}=-\frac{\sum_{i=1}^{n}y^{(i)}\centerdot\hat{y}^{(i)}}{\sqrt{\sum_{i=1}^{n}\big(y^{(i)}\big)^{2}}\centerdot\sqrt{\sum_{i=1}^{n}\big(\hat{y}^{(i)}\big)^{2}}}\]where \(\mathbf{y}=\{y^{(1)},y^{(2)},\dots,y^{(n)}\}\in\mathbb{R}^{n}\), and \(\mathbf{\hat{y}}=\{\hat{y}^{(1)},\hat{y}^{(2)},\dots,\hat{y}^{(n)}\}\in\mathbb{R}^{n}\). It is same as Cosine Similarity, which is a measure of similarity between two non-zero vectors of an inner product space that measures the cosine of the angle between them. In this case, note that unit vectors are maximally “similar” if they’re parallel and maximally “dissimilar” if they’re orthogonal (perpendicular). This is analogous to the cosine, which is unity (maximum value) when the segments subtend a zero angle and zero (uncorrelated) when the segments are perpendicular. In DeepLearning4J, it is expressed as LossFunctions.LossFunction.COSINE_PROXIMITY. Hinge Hinge Loss, also known as max-margin objective, is a loss function used for training classifiers. The hinge loss is used for “maximum-margin” classification, most notably for support vector machines (SVMs). For an intended output \(y^{(i)}=\pm 1\), i.e., binary classification and a classifier score \(\hat{y}^{(i)}\), the hinge loss of the prediction y is defined as\[\boldsymbol{\mathcal{L}}=\frac{1}{n}\sum_{i=1}^{n}\max(0,1-y^{(i)}\centerdot\hat{y}^{(i)})\]Note that \(\hat{y}^{(i)}\) should be the “raw” output of the classifier’s decision function, not the predicted class label. It can be seen that when \(y^{(i)}\) and \(\hat{y}^{(i)}\) have the same sign (meaning \(\hat{y}^{(i)}\) predicts the right class) and \(|\hat{y}^{(i)}|&gt;1\), the hinge loss equals to zero, but when they have opposite sign, hinge loss increases linearly with \(\hat{y}^{(i)}\) (one-sided error). And in DeepLearning4J, this formula is expressed as LossFunctions.LossFunction.HINGE (in ND4J codes, the HINGE loss function is implemented by the formula above). However, there is a more general expression\[\boldsymbol{\mathcal{L}}=\frac{1}{n}\sum_{i=1}^{n}\max(0,m-y^{(i)}\centerdot\hat{y}^{(i)})\]where \(m\) (margin) is a customized value. More details about extending to multi-classification, optimization, you can visit Hinge loss’s wikipedia: [link]. Squared Hinge Squared Hinge Loss function is a variant of Hinge Loss, it solves the problem in hinge loss that the derivative of hinge loss has a discontinuity at \(y^{(i)}\centerdot\hat{y}^{(i)}=1\). Squared Hinge Loss is computed by\[\boldsymbol{\mathcal{L}}=\frac{1}{n}\sum_{i=1}^{n}\big(\max(0,1-y^{(i)}\centerdot\hat{y}^{(i)})\big)^{2}\] as the definition in DL4J. In DeepLearning4J, it is expressed as LossFunctions.LossFunction.SQUARED_HINGE. Reference On Loss Functions for Deep Neural Networks in Classification Loss functions ND4J Loss Functions Losses - Keras Documentation What is the difference between an RMSE and RMSLE Machine Learning-Loss Function Neural Network-Loss Function Cross Entropy Cost Function What is the difference between squared error and absolute error? Mean Absolute Percentage Error KL-divergence as an objective function Poisson regression A Study on L2-Loss (Squared Hinge-Loss) Multi-Class SVM Why Minimize Negative Log Likelihood?]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
        <tag>machine learning</tag>
        <tag>deeplearning4j</tag>
        <tag>loss functions</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Parameter Update Algorithms in Artificial Neural Networks]]></title>
    <url>%2F2017%2F05%2F29%2Fparameter-update-methods%2F</url>
    <content type="text"><![CDATA[Gradient descent optimization algorithms are very popular to perform optimization and by far the most common way to optimize neural networks. However, there are also many other algorithms, like Hessian Free, Conjugate Gradient, BFGS, L-BFGS and etc., are proposed to deal with optimization tasks, here we only take those gradient descent methods into account. And we will discuss the drawbacks among those gradient algorithms and the methods to solve these blemishes. Gradient Descent Optimization Algorithm Vanilla Gradient Descent Consider an objective \(\mathcal{L}(\boldsymbol{\theta})\), and our goal is aiming to mimimize it. Here, the \(\boldsymbol{\theta}\in\mathbb{R}^{n}\) is the set of parameters for entire dataset. Using (vanilla/batch) gradient descent, to minimize the objective, we need to update \(\boldsymbol{\theta}\) by\[\boldsymbol{\theta}:=\boldsymbol{\theta}-\eta\centerdot\nabla_{\boldsymbol{\theta}}\mathcal{L}(\boldsymbol{\theta})\]where \(\eta\) denotes learning rate, which determines the size of the steps we take to reach a (local) minimum, \(\nabla_{\boldsymbol{\theta}}\) represents the partial derivative of \(\mathcal{L}(\boldsymbol{\theta})\) with respect to \(\boldsymbol{\theta}\), we can derive from the formula above that, for each update, the gradients for whole dataset need to be computed, which is extraordinarily time consuming and heavy memory occupation. Moreover, the batch gradient descent does not allow online training. Other drawbacks: - It is relatively slow close to the minimum: technically, its asymptotic rate of convergence is inferior to many other methods. For poorly conditioned convex problems, gradient descent increasingly ‘zigzags’ as the gradients point nearly orthogonally to the shortest direction to a minimum point\(.^{4}\) - It is only guaranteed to converge to the global minimum for convex error surfaces and to a local minimum for non-convex surfaces\(.^{1}\) In deeplearning4j, Vanilla Gradient Descent is OptimizationAlgorithm.LINE_GRADIENT_DESCENT. Stochastic Gradient Descent Stochastic gradient descent (often shortened in SGD), also known as incremental gradient descent, is a stochastic approximation of the gradient descent optimization method for minimizing an objective function\(.^{5}\) Unlike vanilla gradient descent, which performs redundant computations (since it may recompute gradients for similar examples before each parameter updates), SGD performs parameter update for each sample \((\mathbf{x}^{(i)},y^{(i)})\), i.e., it computes one update a time\[\boldsymbol{\theta}:=\boldsymbol{\theta}-\eta\centerdot\nabla_{\boldsymbol{\theta}}\mathcal{L}_{i}(\boldsymbol{\theta})\]Thus, it is much faster than vanilla gradient descent and is capable of training online. Moreover, becasue of updating by one sample each time, SGD always perform frequent updates with a high variance that cause the objective function to fluctuate heavily. For SGD’s fluctuation, it may enable it to jump from a local minimum to new and potentially better local (even global) minima, however, this ultimately complicates convergence to the exact minimum, as SGD will keep overshooting. Besides, when the learning rate is slowly decreased, SGD shows the same convergence behaviour as batch gradient descent, almost certainly converging to a local or the global minimum for non-convex and convex optimization respectively. In deeplearning4j, Stochastic Gradient Descent is OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT. Mini-batch (Stochastic) Gradient Descent Mini-batch (Stochastic) Gradient Descent performs an update for every \(n\) training sample (\(n=1\), it becomes an online method)\[\boldsymbol{\theta}:=\boldsymbol{\theta}-\eta\centerdot\nabla_{\boldsymbol{\theta}}\mathcal{L}_{i:i+n}(\boldsymbol{\theta})\]y, it reduces the variance of the parameter updates, which can lead to more stable convergence and is able to make use of highly optimized matrix optimizations that make computing the gradient with respect to a mini-batch very efficient. Mini-batch gradient descent is typically the algorithm of choice when training a neural network and the term SGD usually is employed also when mini-batches are used. In deeplearning4j, by using mini-batch, set .miniBatch(true) and the batch size is defined by data processor. Albeit improvements are made to ameliorate gradient descent methods, there are still some challanges: - Choosing a proper learning rate can be difficult. A learning rate that is too small leads to painfully slow convergence, while a learning rate that is too large can hinder convergence and cause the loss function to fluctuate around the minimum or even to diverge\(.^{1}\) - Learning rate schedules try to adjust the learning rate during training by e.g. annealing, i.e. reducing the learning rate according to a pre-defined schedule or when the change in objective between epochs falls below a threshold. These schedules and thresholds, however, have to be defined in advance and are thus unable to adapt to a dataset’s characteristics\(.^{1}\) - Additionally, the same learning rate applies to all parameter updates. If our data is sparse and our features have very different frequencies, we might not want to update all of them to the same extent, but perform a larger update for rarely occurring features\(.^{1}\) - Another key challenge of minimizing highly non-convex error functions common for neural networks is avoiding getting trapped in their numerous suboptimal local minima. some argue that the difficulty arises in fact not from local minima but from saddle points, i.e. points where one dimension slopes up and another slopes down. These saddle points are usually surrounded by a plateau of the same error, which makes it notoriously hard for SGD to escape, as the gradient is close to zero in all dimensions\(.^{1}\) Parameter Update Methods Here I summarize some methods that are widely used in neural networks to deal with the aforementioned challanges. Momentum Since SGD has trouble navigating ravines, which are common around local optima. In these scenarios, SGD oscillates across the slopes of the ravine while only making hesitant progress along the bottom towards the local optimum. Momentum is a method, which appeared in Rumelhart, Hinton and Williams’ seminal paper on backpropagation learning, that helps to acclerate SGD in the relevant direction and dampens oscillations. Momentum remembers the update \(\Delta w\) at each iteration, and determines the next update as a convex combination of the gradient and the previous update\[\Delta w_{t}:=\alpha\Delta w_{t-1}+\eta\nabla_{\boldsymbol{\theta}}\mathcal{L}(\boldsymbol{\theta})\\\boldsymbol{\theta}:=\boldsymbol{\theta}-\Delta w_{t}\]where, \(\alpha\) is a hyperparameter, which controls the contribution of the update vector of the past time step to the current update vector. The momentum term increases for dimensions whose gradients point in the same directions and reduces updates for dimensions whose gradients change directions. As a result, we gain faster convergence and reduced oscillation. Normally, the momentum term \(\alpha\) is usually set to 0.9 or a similar value. In deeplearning4j, using momentum by setting .momentum(0.9). Nesterov Momentum Nesterov momentum, also know as Nesterov accelerated gradient (NAG), is a way to give the momentum term a kind of prescience that prevents the gradient blindly following the slope and let it knows where to go and then to slow down before the hill slopes up again.\[\Delta w_{t}:=\alpha\Delta w_{t-1}+\eta\nabla_{\boldsymbol{\theta}}\mathcal{L}(\boldsymbol{\theta}-\alpha\Delta w_{t-1})\\\boldsymbol{\theta}:=\boldsymbol{\theta}-\Delta w_{t}\]where the \(\eta\) denotes the learning rate, \(\alpha\) denotes the momentum, computing \(\boldsymbol{\theta}-\alpha\Delta w_{t-1}\) is to obtain an approximation of the next position of the parameters (the gradient is missing for the full update), a rough idea where the parameters are going to be. See from the graph, while Momentum first computes the current gradient (small blue vector) and then takes a big jump in the direction of the updated accumulated gradient (big blue vector), NAG first makes a big jump in the direction of the previous accumulated gradient (brown vector), measures the gradient and then makes a correction (green vector). This anticipatory update prevents us from going too fast and results in increased responsiveness, which has significantly increased the performance of RNNs on a number of tasks\(.^{[1]}\) In deeplearning4j, Nesterov is .updater(Updater.NESTEROVS).momentum(0.9). AdaGrad AdaGrad (also know as adaptive gradient algorithm) is a modified stochastic gradient descent with per-parameter learning rate, it increases the learning rate (lager updates) for more sparse parameters and decreases the learning rate (smaller updates) for less sparse ones. This strategy often improves convergence performance over standard stochastic gradient descent for dealing with sparse data. In practice, AdaGrad greatly improved the robustness of SGD and it is often used for training large-scale neural nets, besides, GloVe model also use the AdaGrad. Different from the above methods, who updates all parameters \(\boldsymbol{\theta}\) at once and use the same learning rate for every parameter, however, AdaGrad uses a different learning rate for every parameter \(\theta_{i}\) at every time step \(t\). Denote \(g_{t,i}\) to be the gradient of the objective function with respect to the parameter \(\theta_{i}\) at time step \(t\), then we have \(g_{t,i}=\nabla_{\boldsymbol{\theta}}\mathcal{L}(\theta_{i})\), AdaGrad modifies the general learning rate \(\eta\) at each time step \(t\) for every parameter \(\theta_{i}\) based on the previous gradients that have been computed for \(\theta_{i}\):\[\theta_{t,i}:=\theta_{t-1,i}-\frac{\eta\centerdot g_{t-1,i}}{\sqrt{G_{t-1,ii}+\epsilon}}:=\theta_{t-1,i}-\frac{\eta\centerdot\nabla_{\boldsymbol{\theta}}\mathcal{L}(\theta_{i-1})}{\sqrt{G_{t-1,ii}+\epsilon}}\]where \(G_{t}\in\mathbb{R}^{n\times n}\) is a diagonal matrix where each diagonal element is the sum of the squares of the gradients with respect to \(\theta_{i}\) up to time step \(t\), while \(\epsilon\) is a smoothing term that avoids division by zero (usually takes \(1e-8\)). Thus, for \(G_{t}\), which contains the sum of the squares of the past gradients with respect to all parameters \(\boldsymbol{\theta}\) along its diagonal, so we have\[\boldsymbol{\theta}_{t}:=\boldsymbol{\theta}_{t-1}-\frac{\eta\centerdot\nabla_{\boldsymbol{\theta}}\mathcal{L}(\theta)}{\sqrt{G_{t-1}+\epsilon}}\]Adagrad’s main benefits is that it eliminates the need to manually tune the learning rate. While, its main weakness is its accumulation of the squared gradients in the denominator, since every added term is positive, the accumulated sum keeps growing during training. This in turn causes the learning rate to shrink and eventually become infinitesimally small, at which point the algorithm is no longer able to acquire additional knowledge. In deeplearning4j, AdaGrad is Updater.ADAGRAD. AdaDelta AdaDelta is an extension of AdaGrad that seeks to reduce the aggressive, monotonically decreasing learning rate of AdaGrad. Instead of accumulating all past squared gradients, Adadelta restricts the window of accumulated past gradients to some fixed size \(win\). Instead of inefficiently storing \(win\) previous squared gradients, the sum of gradients is recursively defined as a decaying average of all past squared gradients. The running average \(E[g^{2}]_{t}\) at time step \(t\) then depends (as a fraction \(\alpha\) similarly to the Momentum term) only on the previous average and the current gradient:\[E[g^{2}]_{t}=\alpha E[g^{2}]_{t-1}+(1-\alpha^{2})g_{t}^{2}\]From AdaGrad, we have\[\Delta w_{t}=-\frac{\eta\centerdot g_{t}}{\sqrt{G_{t}+\epsilon}}\]Now, simply replace the diagonal matrix \(G_{t}\) with the decaying average over past squared gradients \(E[g^{2}]_{t}\):\[\Delta w_{t}=-\frac{\eta\centerdot g_{t}}{\sqrt{E[g^{2}]_{t}+\epsilon}}=-\frac{\eta\centerdot g_{t}}{RMS[g]_{t}}\]The authors note that the units in this update (as well as in SGD, Momentum, or Adagrad) do not match, i.e. the update should have the same hypothetical units as the parameter. To realize this, they first define another exponentially decaying average, this time not of squared gradients but of squared parameter updates:\[E[\Delta w^{2}]_{t}=\alpha\centerdot E[\Delta w^{2}]_{t-1}+(1-\alpha)\Delta w_{t}^{2}\]The root mean squared error of parameter updates is thus:\[RMS[\Delta w]_{t}=\sqrt{E[\Delta w^{2}]_{t}+\epsilon}\]Since \(RMS[\Delta w]_{t}\) is unknown, it is approximated with the RMS of parameter updates until the previous time step, replacing the learning rate \(\eta\) in the previous update rule with \(RMS[\Delta w]_{t-1}\), yields the Adadelta update rule:\[\Delta w_{t}=-\frac{RMS[\Delta w]_{t-1}}{RMS[g]_{t}}\centerdot g_{t}\\\boldsymbol{\theta}_{t+1}=\boldsymbol{\theta}_{t}+\Delta w_{t}\]With Adadelta, we do not even need to set a default learning rate, as it has been eliminated from the update rule. In deeplearning4j, AdaDelta is Updater.ADADELTA. RMSProp RMSProp is another alternative to resolve AdaGrad’s radically diminishing learning rates, it is identical to the first update vector of AdaDelta\[E[g^{2}]_{t}:=0.9\centerdot E[g^{2}]_{t-1}+0.1\centerdot g_{t}^{2}\\\boldsymbol{\theta}_{t+1}:=\boldsymbol{\theta}_{t}-\frac{\eta\centerdot g_{t}}{\sqrt{E[g^{2}]_{t}+\epsilon}}\]RMSprop as well divides the learning rate by an exponentially decaying average of squared gradients. In practice, \(\alpha\) equals to 0.9, and \(\eta\) equals to 0.001 are good default values. In deeplearning4j, RMSProp is Updater.RMSPROP. Adam Adaptive Moment Estimation (Adam) is another method that computes adaptive learning rates for each parameter. In addition to storing an exponentially decaying average of past squared gradients \(v_{t}\), Adam also keeps an exponentially decaying average of past gradients \(m_{t}\):\[m_{t}=\beta_{1}m_{t-1}+(1-\beta_{1})g_{t}\\v_{t}=\beta_{2}v_{t-1}+(1-\beta_{2})g_{t}^{2}\]where \(m_{t}\) and \(v_{t}\) are estimates of the first moment (the mean) and the second moment (the uncentered variance) of the gradients respectively, then correcting the biaes by computing bias-corrected first and second moment estimates:\[\hat{m}_{t}=\frac{m_{t}}{1-\beta_{1}^{t}}\\\hat{v}_{t}=\frac{v_{t}}{1-\beta_{2}^{t}}\]then use these to update the parameter yield the Adam update rule:\[\boldsymbol{\theta}_{t+1}=\boldsymbol{\theta}_{t}-\frac{\eta\centerdot\hat{m}_{t}}{\sqrt{\hat{v}_{t}+\epsilon}}\]Normally, in practice, \(\beta_{1}=0.9\), \(\beta_{2}=0.999\) and \(\epsilon=10^{-8}\). In deeplearning4j, Adam is Updater.ADAM. kSGD Kalman-based Stochastic Gradient Descent (kSGD) is an online and offline algorithm for learning parameters from statistical problems from quasi-likelihood models, which include linear models, non-linear models, generalized linear models, and neural networks with squared error loss as special cases. The benefits of kSGD is not sensitive to the condition number of the problem and as a robust choice of hyperparameters as well as has a stopping condition. The drawbacks of kSGD is that the algorithm requires storing a dense covariance matrix between iterations, and requires a matrix-vector product at each iteration. More details please visit its Wikipedia – [link] Conclusion Generally, SGD is totally depends on the gradient of current batch, so it is hard to choose a reasonable learning rate, and it uses the same learning rate for all parameters, which is not suitable for sparse data/features. Moreover, SGD can be captured by local minimum easily or be trapped in saddle point. Momentum is a way to speed up SGD, reduce oscillation and gain faster convergence. In the initial period, it uses the update parameter of last stage to keep the same descent diretion, and speed up by multiple a large \(\alpha\) (momentum factor); In the middle and later periods, when the value osillates around the local minimum, and gradient tends to zero, it enlarges the update amplitude to jump out of the trap; What’s more, when the gradient changes the direction, it inhibits the update. Nesterov plays a role of correction function when the gradient updates, which avoids the gradient goes forward too fast and increases the sensitivity. AdaGrad acts as a constraint for learning rate. when the gradient is small in the initial period, the constraint term is large, which is able to magnify the gradient, and when the gradient is large in the later period, the constraint term is small, which restrains the gradient. And it is suitable for handling sparse gradient. However, it relys on the pre-set learning rate \(\eta\), if the \(\eta\) is too large, which makes contraint term hypersensitive, overamplifies the gradient in the initial period, and makes the gradient tends to zero in the later period and causes the training process terminates early. AdaDelta is an improvement of AdaGrad, which has good acceleration effect in the prelimilary and middle periods, while in the later period, it ossilates at local minimum. RMSProp can be treated as a particular case of AdaDelta, but it relys on the global learning rate. RMSProp is suitable for non-stationary target and performs well on RNNs. Adam combines the merit of AdaGrad, which is good at dealing with sparse gradient, and the merit of RMSProp, which is good at handling non-stationary target, it uses different adaptive learning rates for different parameters. So it is suitable in most non-convex optimization and also appropriate for big dataset and high-dimensional space. In summary, RMSprop is an extension of Adagrad that deals with its radically diminishing learning rates. It is identical to Adadelta, except that Adadelta uses the RMS of parameter updates in the numinator update rule. Adam, finally, adds bias-correction and momentum to RMSprop. Insofar, RMSprop, Adadelta, and Adam are very similar algorithms that do well in similar circumstances. Some researchers show that its bias-correction helps Adam slightly outperform RMSprop towards the end of optimization as gradients become sparser. Insofar, Adam might be the best overall choice (AdaMax and NAdam are two variants of Adam, which asserts that the performance is better than Adam). Reference An overview of gradient descent optimization algorithms CS231n – Neural Networks Part 3: Learning and Evaluation Sebastian Ruder’s Blog Gradient Descent – Wikipedia Stochastic Gradient Descent – Wikipedia 深度学习优化方法总结比较]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
        <tag>machine learning</tag>
        <tag>deeplearning4j</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Weight Initialization in Artificial Neural Networks]]></title>
    <url>%2F2017%2F05%2F24%2FWeight-Initialization-in-Artificial-Neural-Networks%2F</url>
    <content type="text"><![CDATA[This is a summary of weight initialization in aritifical neural networks. All Zero Initialization (Pitfall): Note that we do not know what the final value of every weight should be in the trained network, but with proper data normalization it is reasonable to assume that approximately half of the weights will be positive and half of them will be negative. A reasonable-sounding idea then might be to set all the initial weights to zero, which we expect to be the “best guess” in expectation. This turns out to be a mistake, because if every neuron in the network computes the same output, then they will also all compute the same gradients during backpropagation and undergo the exact same parameter updates. In other words, there is no source of asymmetry between neurons if their weights are initialized to be the same\(.^{[1]}\) In deeplearning4j, it corresponds to WeightInit.ZERO. (Not recommended) Small random numbers: Therefore, we still want the weights to be very close to zero, but as we have argued above, not identically zero. As a solution, it is common to initialize the weights of the neurons to small numbers and refer to doing so as symmetry breaking. The idea is that the neurons are all random and unique in the beginning, so they will compute distinct updates and integrate themselves as diverse parts of the full network. For example, \(W\sim\mathcal{N}(0,\sigma)\). With this formulation, every neuron’s weight vector is initialized as a random vector sampled from a multi-dimensional gaussian, so the neurons point in random direction in the input space. It is also possible to use small numbers drawn from a uniform distribution, but this seems to have relatively little impact on the final performance in practice\(.^{[1]}\) Warning that it is not necessarily the case that smaller numbers will work strictly better. For example, a Neural Network layer that has very small weights will during backpropagation compute very small gradients on its data (since this gradient is proportional to the value of the weights). This could greatly diminish the “gradient signal” flowing backward through a network, and could become a concern for deep networks. Below introduces several weight initialization techniques that are commonly used in neural networks and perform well. It is worth mentioning that if you do not know which technique should be chosen as weight initilalizaion method, Xaiver is often choosed as a initial try. Uniform Distribution Initialization It is a commonly used heuristic, which is defined as\[W\sim U\big[-\frac{1}{\sqrt{n_{in}}},\frac{1}{\sqrt{n_{in}}}\big]\]where \(U[−a,a]\) is the uniform distribution in the interval \((−a,a)\) and \(n\) is the size of the previous layer. Corresponds to WeightInit.UNIFORM in deeplearning4j. Xavier Initialization Assigning the network weights before training sometimes can be treated as a random process, since we do not know anything about the data, so we are not sure how to assign the weights that would work in that particular case. And there are some problems when initializing the weights: 1. If the weights in a network start too small, then the signal shrinks as it passes through each layer until it’s too tiny to be useful. 2. If the weights in a network start too large, then the signal grows as it passes through each layer until it’s too massive to be useful. In this case, assigning the weights follow a Gaussian distribution with zero mean and a finite variance is a good choice. Here, the Xavier initialization, proposed by Xavier Glorot and Yoshua Bengio, is a Gaussian distribution with zero mean and a suitable variance, which makes sure the weights are “just right”, keeping the signal in a reasonable range of values through many layers. Consider a layer of neural networks with \(n_{in}=n\) inputs and \(n_{out}=m\) outputs, and suppose we have an initialized random weights \(\mathbf{W}\in\mathbb{R}^{n\centerdot m}\). For an input \(\mathbf{x}=(x_{1},\dots ,x_{n})\), we have\[\mathbf{y}=\mathbf{W}^{T}\mathbf{x}\]where \(\mathbf{y}\) is the output vector with dimension \(m\). To simplify this, let’s assume that \(m=1\), then it degrades to a linear neuron where\[y=\mathbf{W}^{T}\mathbf{x}=w_{1}x_{1}+w_{2}x_{2}+\dots +w_{n}x_{n}\]Since we want the variance to remain the same after passing the layer to help to keep the signal from exploding to a high value or a vanishing to zero, so we need the generated weights to satisfy\[var(y)=var(w_{1}x_{1}+\dots +w_{n}x_{n})=var(w_{1}x_{1})+\dots +var(w_{n}x_{n})\]assume that \(\mathbf{W}\) and \(\mathbf{x}\) are independent and both of them have zero mean, then for \(var(w_{i}x_{i})\):\[var(w_{i}x_{i})=E[x_{i}]^{2}var(w_{i})+E[w_{i}]^{2}var(x_{i})+var(w_{i})var(x_{i})=var(w_{i})var(x_{i})\]Then if we make a further assumption that the \(w_{i}\) and \(x_{i}\) are all independent and identically distributed, we can work out that the variance of \(y\) is\[var(y)=n\centerdot var(w_{i})var(x_{i})\]So if we want to make sure the variance of \(y\) to be the same as \(\mathbf{x}\), then we need \(n\centerdot var(w_{i})=1\). Hence,\[var(w_{i})=\frac{1}{n}=\frac{1}{n_{in}}\]and using this, the weights can be initialized by the Gaussian distribution with zero mean and variance \(\frac{1}{n_{in}}\). Actually, in deeplearning4j, the weight initialization function WeightInit.XAVIER_FAN_IN is using the formula above. Samely, if you go through the same steps for the backpropagated signal, you find that you need\[var(w_{i})=\frac{1}{n_{out}}\]to keep he variance of the input gradient and the output gradient the same. These two constraints can only be satisfied simultaneously if \(n_{in}=n_{out}\), so as a compromise, the author takes the average of the two:\[var(w_{i})=\frac{2}{n_{in}+n_{out}}\]So the weight is initialized by the Gaussian distribution with zero mean and variance \(\frac{2}{n_{in}+n_{out}}\). This is corresponding to the function WeightInit.XAVIER in deeplearning4j. Moreover, the author also introduced a normalized initialization version follows uniform distribution, which is corresponding to the WeightInit.XAVIER_UNIFORM in deeplearning4j.\[\mathbf{W}\sim U\big[-\frac{\sqrt{6}}{\sqrt{n_{in}+n_{out}}},\frac{\sqrt{6}}{\sqrt{n_{in}+n_{out}}}\big]\]In the paper, the author describes that “The normalization factor may therefore be important when initializing deep networks because of the multiplicative effect through layers, and we suggest the following initialization procedure to approximately satisfy our objectives of maintaining activation variances and back-propagated gradients variance as one moves up or down the network”. More details you can get from the paper: Understanding the difficulty of training deep feedforward neural networks. (Note that, since Xavier is aimed to deal with gradient vanishing or exploding problems, however, if you use ReLU as the activation function, which would not lead to vanishing or exploding gradients, so normally, Xavier initialization would not be use with ReLU activation). What’s more, here is another version of Xavier with uniform distribution in deeplearning4j weight initialization function list: WeightInit.SIGMOID_UNIFORM, which is defined as\[\mathbf{W}\sim U\big[-\frac{4\centerdot\sqrt{6}}{\sqrt{n_{in}+n_{out}}},\frac{4\centerdot\sqrt{6}}{\sqrt{n_{in}+n_{out}}}\big]\] ReLU Initialization It is proposed by He et al. to handle the issue that for some very deep CNNs initialized by random weights drawn from Gaussian distributions with fixed standard deviations, have difficulties to converge, (Typically, used for the layers with ReLU activation function). Consider a layer in neural networks with initialized weights \(\mathbf{W}_{l}\), where \(l\) denotes a layer, elements in \(\mathbf{W}_{l}\in\mathbb{R}^{d\centerdot n}\) are independent and identical destributed (i.i.d.), and ignore \(d_{l}\). Suppose that we have an input \(\mathbf{x}_{l}=f(\mathbf{y}_{l-1})\in\mathbb{R}^{n}\), where \(\mathbf{x}_{l}\) is the input of current layer and also the output of previous layer, so we have \(\mathbf{x}_{l}=f(\mathbf{y}_{l-1})\), \(f(\centerdot)\) is the activaion function. Assume the elements in \(\mathbf{x}_{l}\) are also i.i.d., and \(\mathbf{W}_{l}\) and \(\mathbf{x}_{l}\) are independent of each other. Then,\[var(y_{l})=n_{l}\centerdot var(w_{l}x_{l})\]where \(y_{l}\), \(x_{l}\) and \(w_{l}\) represent the elements in \(\mathbf{y}_{l}\), \(\mathbf{x}_{l}\) and \(\mathbf{W}_{l}\) respectively. Let \(w_{l}\) has zero mean, then the variance of the product of independent variables gives\[var(y_{l})=n_{l}\centerdot var(w_{l})\centerdot E\big[x_{l}^{2}\big]\]where the \(E\big[x_{l}^{2}\big]\) denotes the expectation of the square of \(x_{l}\). Note that \(E\big[x_{l}^{2}\big]\neq var(x_{l})\) unless \(x_{l}\) has zero mean. However, for ReLU, \(x_{l}=max(0,y_{l-1})\) and thus it does not have zero mean. If let \(w_{l-1}\) has a symmetric distribution around zero, then \(y_{l-1}\) has zero mean and has a symmetric distribution around zero. This leads to \(E\big[x_{l}^{2}\big]=\frac{1}{2}var(y_{l-1})\) when \(f\) is ReLU, then we will obtain\[var(y_{l})=\frac{1}{2}n_{l}\centerdot var(w_{l})\centerdot var(y_{l-1})\]With \(L\) layers put together, we have\[var(y_{L})=var(y_{1})\centerdot\big(\prod_{l=2}^{L}\frac{1}{2}n_{l}\centerdot var(w_{l})\big)\]This product is the key to the initialization design. A proper initialization method should avoid reducing or magnifying the magnitudes of input signals exponentially. So the author expects the above product to take a proper scalar (e.g., 1). A sufficient condition is\[\frac{1}{2}n_{l}\centerdot var(w_{l})=1, \forall l.\]This leads to a zero-mean Gaussian distribution whose standard deviation is \(\sqrt{\frac{2}{n_{l}}}\). And this is a general idea of ReLU initilization, which is corresponding to WeightInit.RELU in deeplearning4j. Similar to Xavier, the author also provides a unifrom destribution version\[W\sim U\big[-\sqrt{\frac{6}{n_{l}}},\sqrt{\frac{6}{n_{l}}}\big]\]which is correspoinding to the initialization function WeightInit.RELU_UNIFORM in deeplearning4j. More details you can get from the paper: Delving Deep into Rectifiers Reference CS231n Convolutional Neural Networks for Visual Recognition Understanding the difficulty of training deep feedforward neural networks An Explanation of Xavier Initialization Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
        <tag>machine learning</tag>
        <tag>deeplearning4j</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Activation Functions in Artificial Neural Networks]]></title>
    <url>%2F2017%2F05%2F22%2FActivation-Functions-in-Artificial-Neural-Networks%2F</url>
    <content type="text"><![CDATA[By definition, activation function is a function used to transform the activation level of a unit (neuron) into an output signal. Typically, activation function has a “squashing” effect. An activation function serves as a threshold, alternatively called classification or a partition. Bengio et al. refers to this as “Space Folding”. It essentially divides the original space into typically two partitions. Activation functions are usually introduced as requiring to be a non-linear function, that is, the role of activation function is made neural networks non-linear. The purpose of an activation function in a Deep Learning context is to ensure that the representation in the input space is mapped to a different space in the output. In all cases a similarity function between the input and the weights are performed by a neural network. This can be an inner product, a correlation function or a convolution function. In all cases it is a measure of similarity between the learned weights and the input. This is then followed by a activation function that performs a threshold on the calculated similarity measure. In its most general sense, a neural network layer performs a projection that is followed by a selection. Both projection and selection are necessary for the dynamics learning. Without selection and only projection, a network will thus remain in the same space and be unable to create higher levels of abstraction between the layers. The projection operation may in fact be non-linear, but without the threshold function, there will be no mechanism to consolidate information. The selection operation is enforces information irreversibility, an necessary criteria for learning\(.^{[1]}\) There have been many kinds of activation functions (over 640 different activation function proposals) that have been proposed over the years. However, best practice confines the use to only a limited kind of activation functions. Here I summarize several common-used activation functions, like Sigmoid, Tanh, ReLU, Softmax and so forth, as well as their merits and drawbacks. Sigmoid Units A Sigmoid function (used for hidden layer neuron output) is a special case of the logistic function having a characteristic “S”-shaped curve. The logistic function is defined by the formula\[\sigma (x)=\frac{L}{1+e^{-k(x-x_{0})}}\]Where \(e\) is the natural logarithm base (also known as Euler’s number), \(x_{0}\) is the x-value of the Sigmoid’s midpoint, \(L\) is the curve’s maximum value, and \(k\) is the steepness of the curve. By setting \(L=1\), \(k=1\), \(x_{0}=0\), we derive\[\sigma (x)=\frac{1}{1+e^{-x}}\]This is so called Sigmoid function and it is shown in the image below. Sigmoid function takes a real-valued number and “squashes” it into range between 0 and 1, i.e., \(\sigma (x)\in (0,1)\). In particular, large negative numbers become 0 and large positive numbers become 1. Moreover, the sigmoid function has a nice interpretation as the firing rate of a neuron: from not firing at all (0) to fully-saturated firing at an assumed maximum frequency (1). But it has two major drawbacks: 1. Sigmoids saturate and kill gradients: The Sigmoid neuron has a property that when the neuron’s activation saturates at either tail of 0 or 1, the gradient (where \(\sigma&#39;(x)=\sigma (x)\centerdot (1-\sigma (x))\), see the red dotted line above) at these regions is almost zero. During backpropagation, this gradient will be multiplied to the gradient of this gate’s output for the whole objective. Therefore, if the local gradient is very small, it will effectively “kill” the gradient and almost no signal will flow through the neuron to its weights and recursively to its data. So, it is critically important to initialize the weights of sigmoid neurons to prevent saturation. For instance, if the initial weights are too large then most neurons would become saturated and the network will barely learn\(.^{[2]}\) 2. Sigmoid outputs are not zero-centered: It is undesirable since neurons in later layers of processing in a Neural Network would be receiving data that is not zero-centered. This has implications on the dynamics during gradient descent, because if the data coming into a neuron is always positive (e.g. \(x&gt;0\) elementwise in \(f(x)=w^{T}x+b\)), then the gradient on the weights \(w\) will during backpropagation become either all be positive, or all negative (depending on the gradient of the whole expression \(f\)). This could introduce undesirable zig-zagging dynamics in the gradient updates for the weights. However, notice that once these gradients are added up across a batch of data the final update for the weights can have variable signs, somewhat mitigating this issue. Therefore, this is an inconvenience but it has less severe consequences compared to the saturated activation problem\(.^{[2]}\) Tanh Units The hyperbolic tangent (tanh) function (used for hidden layer neuron output) is an alternative to Sigmoid function. It is defined by the formula\[tanh(x)=\frac{1-e^{-2x}}{1+e^{-2x}}\]See the following image, tanh function is similar to Sigmoid function (Mathematically, \(tanh(x)=2\sigma (2x)-1\)), which is also sigmoidal (“S”-shaped). It squashes real-valued number to the range between -1 and 1, i.e., \(tanh(x)\in (-1, 1)\). Like the Sigmoid units, its activations saturate, but its output is zero-centered (means tanh solves the second drawback of Sigmoid). Therefore, in practice the tanh units is always preferred to the sigmoid units. The derivative of tanh function is defined as\[tanh&#39;(x)=1-tanh^{2}(x)\]See the red dotted line in the above image, it interprets that tanh also saturate and kill gradient, since tanh’s derivative has similar shape as compare to Sigmoid’s derivative. What’s more, tanh has stronger gradients, since data is centered around 0, the derivatives are higher, and tanh avoids bias in the gradients. Rectified Linear Units (ReLU) In the context of artificial neural networks, the ReLU (used for hidden layer neuron output) is defined as\[f(x)=max(0,x)\]Where x is the input to a neuron. In other words, the activation is simply thresholded at zero. The range of ReLU is betweem 0 to \(\infty\). See the image below (red dotted line is the derivative) The ReLU function is more effectively than the widely used logistic sigmoid and its more practical counterpart, the hyperbolic tangent, since it efficaciously reduce the computation cost as well as some other merits: 1. It was found to greatly accelerate (Krizhevsky et al.) the convergence of stochastic gradient descent compared to the sigmoid/tanh functions. It is argued that this is due to its linear, non-saturating form\(.^{[2]}\) 2. Compared to tanh/sigmoid neurons that involve expensive operations (exponentials, etc.), the ReLU can be implemented by simply thresholding a matrix of activations at zero\(.^{[2]}\) Unfortunately, ReLU also suffers several drawbacks, for instance, - ReLU units can be fragile during training and can “die”\(.^{[2]}\) For example, a large gradient flowing through a ReLU neuron could cause the weights to update in such a way that the neuron will never activate on any datapoint again. If this happens, then the gradient flowing through the unit will forever be zero from that point on. That is, the ReLU units can irreversibly die during training since they can get knocked off the data manifold. You may find that as much as 40% of your network can be “dead” (i.e. neurons that never activate across the entire training dataset) if the learning rate is set too high. With a proper setting of the learning rate this is less frequently an issue\(.^{[2]}\) Plus, here is a smooth approximation to the rectifier, which is called the softplus function (see the green line in the above image). It is defined as\[f(x)=\ln (1+e^{x})\]And its derivative is\[f&#39;(x)=\frac{e^{x}}{1+e^{x}}=\frac{1}{1+e^{-x}}\]Interestingly, the derivative of Softplus is the logistic function. We can see that both the ReLU and Softplus are largely similar, except near 0 where the softplus is enticingly smooth and differentiable. But it is much easier and efficient to compute ReLU and its derivative than for the softplus function which has \(log(\centerdot)\) and \(exp(\centerdot)\) in its formulation. In deep learning, computing the activation function and its derivative is as frequent as addition and subtraction in arithmetic. By using ReLU, the forward and backward passes are much faster while retaining the non-linear nature of the activation function required for deep neural networks to be useful. Leaky and Parametric ReLU Leaky ReLU is one attempt to fix the “dying ReLU” problem. Instead of the function being zero when \(x&lt;0\), a leaky ReLU will instead have a small negative slope (of 0.01, or so). That is, the function computes\[f(x)=0.01x, (x&lt;0)\]\[f(x)=x, (x\geq 0)\]This form of activation function achieves some success, but the results are not always consistent. The slope in the negative region can also be made into a parameter of each neuron, in this case, it is a Parametric ReLU (introduced in Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification), which take this idea further by making the coefficient of leakage into a parameter that is learned along with the other neural network parameters.\[f(x)=\alpha x, (x&lt;0)\]\[f(x)=x, (x\geq 0)\] Where \(\alpha\) is a small constant (smaller than 1). However, the consistency of the benefit across tasks is presently unclear. Moreover, the formula of Parametric ReLU is equivalent to\[f(x)=max(x, \alpha x)\]Which has a relation to “maxout” networks. Randomized ReLU Randomized ReLU (RReLU) is a randomized version of Leaky ReLU, where the \(\alpha\) is a random number. In RReLU, the slopes of negative parts are randomized in a given range in the training, and then fixed in the testing. It is reported that RReLU could reduce overfitting due to its randomized nature in the Kaggle National Data Science Bowl (NDSB) competition. Mathematically, RReLU computes\[f(x_{ji})=x_{ji}, if x_{ji} \geq 0\]\[f(x_{ji})=\alpha_{ji}x_{ji}, if x_{ji}&lt;0\]Where \(x_{ji}\) denotes the input of \(i\)th channel in \(j\)th example, \(f(x_{ji})\) denotes the corresponding output after passing the activation function, \(\alpha_{ji}\sim U(l,u), l &lt; u\) and \(l,u\in[0,1)\). The highlight of RReLU is that in training process, \(\alpha_{ji}\) is a random number sampled from a uniform distribution \(U(l,u)\), while in the test phase, we take average of all the \(\alpha_{ji}\) in training as in the method of dropout, and thus set \(\alpha_{ji}\) to \(\frac{l+u}{2}\) to get a deterministic result (In NDSB, \(\alpha_{ji}\) is sampled from \(U(3,8)\)). So in the test time, we have\[f(x_{ji})=\frac{x_{ji}}{\frac{l+u}{2}}\]Here gives the comparing graph of different ReLUs For Parametric ReLU, \(\alpha_{i}\) is learned and for Leaky ReLU \(\alpha_{i}\) is fixed. For RReLU, \(\alpha_{ji}\) is a random variable keeps sampling in a given range, and remains fixed in testing. Generally, we summarize the advantages and potential problems of ReLUs: - (+) Biological plausibility: One-sided, compared to the antisymmetry of tanh. - (+) Sparse activation: For example, in a randomly initialized network, only about 50% of hidden units are activated (having a non-zero output). - (+) Efficient gradient propagation: No vanishing or exploding gradient problems. - (+) Efficient computation: Only comparison, addition and multiplication. - (+) Scale-invariant: \(max(0,\alpha x)=\alpha\centerdot max(0,x)\). - (-) Non-differentiable at zero: however it is differentiable anywhere else, including points arbitrarily close to (but not equal to) zero. - (-) Non-zero centered. - (-) Unbounded: Could potentially blow up. - (-) Dying Relu problem: Relu neurons can sometimes be pushed into states in which they become inactive for essentially all inputs. In this state, no gradients flow backward through the neuron, and so the neuron becomes stuck in a perpetually inactive state and “dies.” In some cases, large numbers of neurons in a network can become stuck in dead states, effectively decreasing the model capacity. This problem typically arises when the learning rate is set too high. Maxout Some other types of units that do not have the functional form \(f(w^{T}x+b)\) where a non-linearity is applied on the dot product between the weights and the data. One relatively popular choice is the Maxout neuron that generalizes the ReLU and its leaky version. The Maxout neuron computes the function\[max(w^{T}_{1}+b_{1},w^{T}_{2}+b_{2})\]Notice that both ReLU and Leaky ReLU are a special case of this form (for example, for ReLU we have \(w_{1},b_{1}=0\)). The Maxout neuron therefore enjoys all the benefits of a ReLU unit (linear regime of operation, no saturation) and does not have its drawbacks (dying ReLU). However, unlike the ReLU neurons it doubles the number of parameters for every single neuron, leading to a high total number of parameters. Softmax The Softmax function (Used for multi-classification neural network output), or normalized exponential function, in mathematics, is a generalization of the logistic function that “squashes” a \(K\)-dimensional vector \(\mathbf{z}\) from arbitrary real values to a \(K\)-dimensional vector \(\sigma (\mathbf{z})\) of real values in the range \([0,1]\) that add up to 1. The function is given by\[\sigma (\mathbf{z})_{j}=\frac{e^{z_{j}}}{\sum_{k=1}^{K}e^{z_{k}}}, j=1, 2, \dots, K\]In probability theory, the output of the Softmax function can be used to represent a categorical distribution, that is, a probability distribution over \(K\) different possible outcomes. In fact, it is the gradient-log-normalizer of the categorical probability distribution. Here is an example of Softmax application The softmax function is used in various multiclass classification methods, such as multinomial logistic regression, multiclass linear discriminant analysis, naive Bayes classifiers, and artificial neural networks. Specifically, in multinomial logistic regression and linear discriminant analysis, the input to the function is the result of K distinct linear functions, and the predicted probability for the \(j\)th class given a sample vector \(\mathbf{x}\) and a weighting vector \(\mathbf{w}\) is\[P(y=j|\mathbf{x})=\frac{e^{x^{T}w_{j}}}{\sum_{k=1}^{K}e^{x^{T}w_{k}}}\]This can be seen as the composition of \(K\) linear functions \(\mathbf{x}\mapsto x^{T}w_{1},\dots ,\mathbf{x}\mapsto x^{T}w_{K}\) and the softmax function (where \(x^{T}w\) denotes the inner product of \(\mathbf{x}\) and \(\mathbf{w}\)). The operation is equivalent to applying a linear operator defined by \(\mathbf{w}\) to vectors \(\mathbf{x}\), thus transforming the original, probably highly-dimensional, input to vectors in a \(K\)-dimensional space \(R^{K}\). More details, see the link: Softmax Function. Other Activation Functions Like, Identity, Binary Step, ArcTan, SoftSign, Exponential linear unit (ELU), S-shaped rectified linear activation unit (SReLU), Adaptive piecewise linear (APL), Bent identity, SoftExponential, Sinusoid, Sinc, Gaussian and so forth. See the Wikipedia link: Activation Function. Choose Activation Functions Finally, to choose an activation function, we can follow a simple rule that: “Use the ReLU non-linearity, be careful with your learning rates and possibly monitor the fraction of ‘dead’ units in a network. If this concerns you, give Leaky ReLU or Maxout a try. Never use sigmoid. Try tanh, but expect it to work worse than ReLU/Maxout.” Reference What is the role of the activation function in a neural network? – Quora CS231n Convolutional Neural Networks for Visual Recognition Derivation: Derivatives for Common Neural Network Activation Functions Why use activation functions? What are the benefits of using ReLU over softplus as activation functions? – Quora Empirical Evaluation of Rectified Activations in Convolution Network [Rectifier (neural networks)](https://en.wikipedia.org/wiki/Rectifier_(neural_networks) Maxout Networks Softmax Function]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>[object Object]</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Word2Vecf -- Dependency-Based Word Embeddings and Lexical Substitute]]></title>
    <url>%2F2017%2F05%2F18%2Fword2vecf%2F</url>
    <content type="text"><![CDATA[It is a summary of Dependency-based Word Embeddings and A Simple Word Embedding Model for Lexical Substitution proposed by Omer Levy on ACL 2014 and VSM 2015 respectively. After studying the two papers, I implement the methods intorduced in the two papers with Java to enhance my comprehension and deal with some practical tasks. Introduction The method proposed in “Dependency-based Word Embeddings” is a generalized skip-gram model with negative sampling, which is capable of dealing with the arbitrary contexts, and its generated embeddings are less topical and exhibit more functional similarity than the original skip-gram embeddings (For more details of skip-gram and negative sampling, you can see my another blog: Word2Vec Summary). While the methods introduced in “A Simple Word Embedding Model for Lexical Substitution” are using the results generated by the generalized skip-gram model to handle the lexical substitution task. The novelty of our approach is in leveraging explicitly the context embeddings generated within the skip-gram model, which were so far considered only as an internal component of the learning process. Dependency-based Word Embeddings In this paper, the author proposed an improved skip-gram model with negative sampling, which is capable of dealing with the arbitrary contexts. Skip-Gram Model Here I will introduce the skip-gram model in brief, more details can refer my another blog – Word2Vec Summary. Before start, let’s summarize the notations will be used later: \(\boldsymbol{\mathcal{W}}\) represents the words vocabulary, \(\boldsymbol{\mathcal{C}}\) represents the contexts vocabulary, \(d\) denotes the vector dimensionality, \(w\) denotes a word in words vocabulary, \(c\) denotes a context in contexts vocabulary, \(\mathbf{v}_{w}\) and \(\mathbf{v}_{c}\), respectively, represents word vector and context vector, and \(\boldsymbol{\mathcal{D}}\) represents a dataset of observed \((w,c)\) pairs of words \(w\) and contexts \(c\), which appeared in a large body of text (in this case, a word \(w\) in \(\boldsymbol{\mathcal{D}}\) only have one context \(c\), so it can be treated as unigram). In the skip-gram model, each word \(w\in\boldsymbol{\mathcal{W}}\) is associated with a vector \(\mathbf{v}_{w}\in\mathbb{R}^{d}\) and similarly each context \(c\in\boldsymbol{\mathcal{C}}\) is represented as a vector \(\mathbf{v}_{c}\in\mathbb{R}^{d}\). The entries in the vectors are latent, and treated as parameters to be learned, the author aims to seek the vector representations for both words and contexts such that the dot product \(\mathbf{v}_{w}\centerdot\mathbf{v}_{c}\) associated with “good” word-context pairs is maximized. For example, consider a word-context pair \((w,c)\), the probability that \((w,c)\) is from the dataset denotes by \(P(1|w,c)\), while the probability that \((w,c)\) did not is given by \(P(0|w,c)=1-P(1|w,c)\), this is the idea of negative sampling, and it’s also a binary classification problem. Note that \(P(1|w,c)=\frac{1}{1+exp(-\mathbf{v}_{w}^{T}\centerdot\mathbf{v}_{c})}\), and \(\mathbf{v}_{w}\), \(\mathbf{v}_{c}\) are the model parameters to be learned. Thus, the goal is to maximize the log-probability of the observed pairs belonging to the dataset, leadig to the objective:\[\arg\max_{\mathbf{v}_{w},\mathbf{v}_{c}}\sum_{(w,c)\in\boldsymbol{\mathcal{D}}}\log\frac{1}{1+e^{-\mathbf{v}_{w}^{T}\centerdot\mathbf{v}_{c}}}\]However, this objective admits a trivial solution where \(P(1|w,c)=1\) for every pair \((w,c)\) if \(\mathbf{v}_{w}=\mathbf{v}_{c}\) or \(\mathbf{v}_{w}^{T}\centerdot\mathbf{v}_{c}=K\) for all \(w,c\), where \(K\) is large enough number. To avoid the trivial solution, the objective is extended with \((w,c)\) pairs for which \(P(1|w,c)\) must be low, say, pairs which are not in the dataset, by generating the set \(\boldsymbol{\mathcal{D}}&#39;\) of random \((w,c)\) pairs yielding the negative-sampling training objective:\[\arg\max_{\mathbf{v}_{w},\mathbf{v}_{c}}\big(\prod_{(w,c)\in\boldsymbol{\mathcal{D}}}P(1|w,c)\centerdot\prod_{(w,c)\in\boldsymbol{\mathcal{D}}&#39;}P(0|w,c)\big)\]and its logarithmic form is\[\arg\max_{\mathbf{v}_{w},\mathbf{v}_{c}}\big(\sum_{(w,c)\in\boldsymbol{\mathcal{D}}}\log\sigma(\mathbf{v}_{w}^{T}\mathbf{v}_{c})+\sum_{(w,c)\in\boldsymbol{\mathcal{D}}&#39;}\log\sigma(-\mathbf{v}_{w}^{T}\mathbf{v}_{c})\big)\]The objective is trained in the online fashion using stochastic gradient updates over the corpus \(\boldsymbol{\mathcal{D}}\cup\boldsymbol{\mathcal{D}}&#39;\). Optimizing this objective makes observed word-context pairs have similar embeddings, while scattering unobserved pairs. Intuitively, words that appear in similar contexts should have similar embeddings, but the author has not yet found a formal proof that skip-gram model does indeed maximize the dot product of similar words. Dependency-based Context In the original skip-gram model, the contexts of a word \(w\) are the words surrounding it in the text, thus, the context vocabulary \(\boldsymbol{\mathcal{C}}\) is identical to the word vocabulary \(\boldsymbol{\mathcal{W}}\). However, contexts need to corresponding to words and the number of context-types can be substantially larger than the number of word-types, so the author generalize the skip-gram by replacing the bag-of-words contexts with arbitrary contexts, i.e., dependency-based syntactic contexts, which is capable of capturing different information than bag-of-word contexts. Using the sentence below as an example: &gt; Australian scientist discovers star with telescope. For bag-of-words contexts, for example, by setting the window equals to 2, the contexts of “discovers” are “Australian”, “scientist”, “star” and “with”. the window with size 2 will miss some important contexts ,like “telescope”, and include some accidental ones, like “Australian”. Moreover, the contexts are unmarked, resulting in “discovers” being a context of both “stars” and “scientist”, which may result in “stars” and “scientists” ending up as neighbours in the embedded space. By setting the window equals to 5, it is able to capture broad topicalcontent, but may weaken the importance of focused information about the target word. For dependency-based contexts, an alternative to the bag-of-words approach is to derive contexts based on the syntactic relations the word participates in. This type of contexts can be derived by some parsing technology. After parsing sentence, the derived word contexts is: for a target word \(w\) with modifier \(m_{1},\dots ,m_{k}\) and a head \(h\), consider the contexts \((m_{1},lbl_{1}),\dots ,(m_{k},lbl_{k}),(h,lbl_{h}^{-1})\), where \(lbl\) is the type of the dependency relation between the head and the modifier (e.g., nsubj, dobj, amod and etc., see link to get more information) and \(lbl^{-1}\) is used to mark the inverse-relation. Relations that include a preposition are “collapsed” prior to context extraction, by directly connecting the head and the object of the preposition, and subsuming the preposition itself into the dependency label. The dependency-based contexts are able to capture relations to words that are far apart and thus “out-of-reach” with small window bag-of-words (like discovers and telescope) and also filter out coincidental contexts which are within the window but not directly related to the target word (like Australian and discovers). Thus, this syntactic contexts may yield more focused embeddings and capture more functional and less topical similarity. So, this is the general idea of dependency-based word embeddings, to build this improved skip-gram model, the author keeps the initial parameters setting in the original skip-gram model, and before using the corpus to train word vectors, the pre-process is needed, say, the dependency-based contexts extraction, and also construct the word and context vocabularies. All of those details are given in the author Python codes except the derivation of dependency-based contexts, however, in my Java implementation, I already cover all of them in my repository and make the model more flexiable. Lexical Substitute Lexical substitution tasks are used for evaluating context-sensitive lexical inference models since the introduction of the original task in SemEval-2007 and additional later variants (Kremer et al., Biemann). In these tasks, systems are required to predict substitutes for a target word instance, which preserve its meaning in a given sentential context. To address this challenge, several models, like sparse syntax-based vector models, probabilistic graphical models, LDA topic models, were proposed recent years, these models typically generated a word instance representation, which is biased towards its given context, and then identified substitute words based on their similarity to this biased representation. In this paper, the author directly utilize the skip-gram model with dependency-based context for the context-sensitive lexical substitution by make use of the learned context embeddings in conjunction with the target word embeddings to model target word instances, instead of discarding them. The suitable substitute is identified via its combined similarity to the embeddings of both the target and its given context. The model supposes that a good lexical substitute for a target word instance under a given context needs to be both semantically similar to the target word and compatible with the given context. Above is an example of identifying substitutes for target word acquire under the syntactic context dobj_company, visualized in a 2-dimensional embedded space. Even though learn is the closest word to acquire, the word buy is both reasonably close to acquire as well as to the context dobj_company and is therefore considered a better substitute. In order to satisfy the assumptions, the model estimates the semantic similarity between a substitute word and the target word using a second-order target-to-target similarity measure, and the compatibility of a substitute word with the given context using a first-order target-to-context similarity measure. Mathematically, the model contains four methods, and they are defined as\[Add=\frac{\cos (s,t)+\sum_{c\in\mathcal{C}}\cos (s,c)}{|\mathcal{C}|+1}\\ BalAdd=\frac{|\mathcal{C}|\centerdot\cos (s,t)+\sum_{c\in\mathcal{C}}\cos (s,c)}{2\centerdot |\mathcal{C}|}\\Mult=\sqrt[|\mathcal{C}|+1]{pcos(s,t)\centerdot\prod_{c\in\mathcal{C}}pcos(s,c)}\\BalMult=\sqrt[2\centerdot |\mathcal{C}|]{pcos(s,t)^{|\mathcal{C}|}\centerdot\prod_{c\in\mathcal{C}}pcos(s,c)}\]where \(\mathcal{C}\) is the set of the target word’s context elements in the context sentence and \(|\mathcal{C}|\) denotes the number of context element, \(c\) denotes an individual context element, \(pcos(v,v&#39;)=\frac{\cos(v,v&#39;)+1}{2}\) is used to avoid negative values, \(s\) is a lexical substitute and \(t\) is the target word. Actually, we can derive from the formula that both target-to-target and target-to-context similarities are estimated by the vector Cosine distance. These four methods are the context-sensitive substitutability metric for estimating the suitability of a lexical substitute for a target word in a given sentential context. Besides, the \(Add\) and \(BalAdd\) are called arithmetic mean, \(Mult\) and \(BalMult\) are named as geometrical mean. Below give two snippets of Java implementation to help you have a better understanding. For \(Add\) and \(BalAdd\): 123456789101112131415161718public INDArray Represent(String target, List&lt;String&gt; deps, boolean avgFlag) &#123; INDArray targetVec = wordVecs.Represent(target); INDArray depVec = contextVecs.Zeros(); int depsFound = 0; for (String dep : deps) &#123; if (!contextVecs.Contains(dep)) continue; depsFound++; depVec.addi(contextVecs.Represent(dep).dup()); // each element add with each other &#125; INDArray retVec = targetVec.dup(); if (depsFound &gt; 0) &#123; if (avgFlag) depVec.divi(Nd4j.scalar(depsFound)); retVec.addi(depVec); &#125; double norm = Math.pow(retVec.mmul(retVec.transpose()).getDouble(0), 0.5); if (norm != 0.0) retVec.divi(Nd4j.scalar(norm)); return retVec;&#125; For \(Mult\) and \(BalMult\): 123456789101112public List&lt;Map.Entry&lt;String, Float&gt;&gt; Mult(String target, List&lt;String&gt; deps, boolean geoMeanFlag) &#123; INDArray targetVec = wordVecs.Represent(target); INDArray scores = wordVecs.PosScores(targetVec); for (String dep : deps) &#123; if (!contextVecs.Contains(dep)) continue; INDArray depVec = contextVecs.Represent(dep); INDArray multScores = wordVecs.PosScores(depVec); if (geoMeanFlag) multScores = Transforms.pow(multScores, 1.0 / deps.size()); scores.muli(multScores); &#125; return wordVecs.TopScores(scores, -1);&#125; Java Implementation For “Dependency-based Word Embeddings”, by referring the Python source codes provided by the author on Bitbucket, I wrote a Java version to achieve its functions and make this algorithm more flexiable. In addition, the source provided by the author only implement the core algorithm of dependency-based skip-gram with negative sampling, it does not give any implementation details about how to parse sentences in corpus to the dependency-based context (the author only introduce the parsing technology in brief). So after studying the structure of dependency-based context (CoNLL-U), and search the related information online, I find that Stanford CoreNLP toolkit (Java based) powered by the Stanford Natural Language Processing Group provides tools to analyze and parse sentences to generate the dependency-based context (details of CoNLL-U format: [link]). So I add this function into my Word2Vecf Java repository. Moreover, I also implement the four lexical substitution methods in this repository as well as some similarity functions and measuring tasks, like WS353, Analogy, TOEFL tasks, introduced in Mikolov’s Word2Vec paper. Here is the GitHub link of my java codes: Word2VecfJava. For “A Simple Word Embedding Model for Lexical Substitution”, the author also provide the Python source codes, this source is used to implement the four methods proposed by the author to deal with two datasets, LS-SE and LS-CIC, and measure the results by “Generalized Average Precision (GAP)”, “BEST” and “OOT” scores to validate the accuracy and reliability of those methods. Here is the GitHub link of my java implementation: LexicalSubstitute. Reference Dependency-based Word Embeddings A Simple Word Embedding Model for Lexical Substitution]]></content>
      <categories>
        <category>Natural Language Processing</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
        <tag>java</tag>
        <tag>natural language processing</tag>
        <tag>word embeddings</tag>
        <tag>skip-gram</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Word2Vec Summary -- Mathematical Principles and Java Implementation]]></title>
    <url>%2F2017%2F05%2F13%2FWord2Vec%2F</url>
    <content type="text"><![CDATA[I began to get to know the natural language process area when I started to work as a research engineer around one year ago, and the first technique I was asked to learn is the Word2Vec. Since the Word2Vec was proposed amost four years ago, there are already amount of research reports, papers, tutorials and even applications are available online. In order to understand this technique well, I explored and studied many of those online resources at that time. Now, one year later, I find that the details of Word2Vec have disappeared from my mind when I reviewed it a few days ago, and consider that the knowledge I gained from Word2Vec helps me a lot in my futher work since I studied it, thus, it is a good choice for me to write something about Word2Vec down and cement the impression of it. Note that Word2Vec does not belong to deep learning methods strictly, it is only a two-layer, shallow neural networks, I also tag this article to Deep Learning for convenience. Introduction Word2Vec was created by a team of researchers led by Tomas Mikolov at Google. It is a shallow, two-layer neural networks that are trained to reconstruct linguistic contexts of words. Word2Vec takes as its input a large corpus of text and produces a vector space, typically of several hundred dimensions, with each unique word in the corpus being assigned a corresponding vector in the space. Prerequisites Here I will introduce the knowledge used in Word2Vec, including Sigmoid, Softmax, Logistic Regression, Huffman Coding, and so forth. Sigmoid A Sigmoid function is a special case of the logistic function having a characteristic “S”-shaped curve. It is defined by the formula\[\sigma (x)=\frac{1}{1+e^{-x}}\]And its derivaitve is \(\sigma&#39;(x)=\sigma (x)\centerdot (1-\sigma (x))\), the graph of Sigmoid and its derivative are shown below Sigmoid function takes a real-valued number and “squashes” it into range between 0 and 1, i.e., \(\sigma (x)\in (0,1)\). Approximately, we can derive from the graph that large negative numbers (e.g. \(x&lt;-6, \sigma (-6)\approx 0.0025\)) become 0 and large positive numbers (e.g. \(x&gt;6, \sigma (6)\approx 0.9975\)) become 1. Thus, in practice, considering that we need to compute a mass of \(\sigma (x)\) of various \(x\) and the required precision is not strict, thus we can apply an approximate computation method (as Mikolov did). Here we assume that \(\sigma (x)=0, x&lt;-6\) and \(\sigma (x)=1, x&gt;6\), then, for \(x\in [-6,6]\), we isometrically cut into 1000 parts, as shown in the graph, compute \(\sigma (x)\) for each \(x_{i}\), and store them into an array, which can be expediently used for further process. Below is the Java implementation. 12345678910111213/** Boundary for maximum exponent allowed */static final int MAX_EXP = 6;/** Size of the pre-cached exponent table */static final int EXP_TABLE_SIZE = 1_000;static final double[] EXP_TABLE = new double[EXP_TABLE_SIZE];static &#123; for (int i = 0; i &lt; EXP_TABLE_SIZE; i++) &#123; // Precompute the exp() table EXP_TABLE[i] = Math.exp((i / (double) EXP_TABLE_SIZE * 2 - 1) * MAX_EXP); // Precompute f(x) = x / (x + 1) EXP_TABLE[i] /= EXP_TABLE[i] + 1; &#125;&#125; Softmax The Softmax function is a generalization of the logistic function that “squashes” a K-dimensional vector \(\mathbf{z}\) from arbitrary real values to a K-dimensional vector \(\sigma (\mathbf{z})\) of real values in the range \([0,1]\) that add up to 1. For more details about Softmax, you can read my another blog: Activation Functions in Artificial Neural Networks. Logistic Regression Logistic regression is the appropriate regression analysis to conduct when the dependent variable is dichotomous. In logistic regression, the dependent variable is binary or dichotomous, i.e. it only contains data coded as 1 (true, success, etc.) or 0 (false, failure, etc.). The goal of logistic regression is to find the best fitting model to describe the relationship between the dichotomous characteristic of interest and a set of independent variables. Note that there is a related method named Multinomial Logistic Regression, which is a classification method that generalizes logistic regression to multiclass problems, i.e. with more than two possible discrete outcomes. Here we only discuss the binary logistic regression. For instance, considering a binary classification problem, like spam classification, suppose that \(\{\mathbf{x}_{i},y_{i}\}_{i=1}^{m}\) is the sample data of a binary classification problem, where \(\mathbf{x}_{i}\in \mathbb{R}^{n}\), \(y_{i}\in \{0,1\}\). Using Sigmoid function, for an arbitrary sample \(\mathbf{x}=(x_{1},x_{2},\dots ,x_{n})^{T}\), we can write the hypothesis function of such binary classification problem as\[h_{\theta}(\mathbf{x})=\sigma (\theta_{0}+\theta_{1}x_{1}+\theta_{2}x_{2}+\dots +\theta_{n}x_{n})\]where \(\theta=(\theta_{0},\theta_{1},\dots ,\theta_{n})^{T}\) is the coefficient. Briefly, we introduce \(x_{0}=1\) and extend \(\mathbf{x}\) to \((x_{0},x_{1},x_{2},\dots ,x_{n})^{T}\), thus, \(h_{\theta}\) can be simplified as\[h_{\theta}(\mathbf{x})=\sigma (\theta^{T}\mathbf{x})=\frac{1}{1+e^{-\theta^{T}\mathbf{x}}}\]The Sigmoid function maps any values of a real number to a value from 0 to 1, therefore, the output can be regarded as the posterior probability for each class. Assume the sample data follows Bernoulli distribution, then we have\[P(Y=y_{i}|\mathbf{x}_{i})=\begin{cases} h_{\theta}(\mathbf{x}_{i}),&amp;y_{i}=1; \\ 1-h_{\theta}(\mathbf{x}_{i}),&amp;y_{i}=0. \end{cases}\]Combining the two equations, it can written as \[P(Y=y_{i}|\mathbf{x}_{i})=\big(h_{\theta}(\mathbf{x}_{i}) \big)^{y_{i}}+\big( 1-h_{\theta}(\mathbf{x}_{i}) \big)^{1-y_{i}}\]Next step is to compute \(\theta\), and the key point of this task is focusing on measuring the performance of model prediction, thus, here we introduce the likelihood function, which estimates the maximum likelihood of the coefficients, can be expressed as\[L(\theta)=\prod_{i=1}^{m}\bigg(\big( h_{\theta}(\mathbf{x}_{i}) \big)^{y_{i}}+\big( 1-h_{\theta}(\mathbf{x}_{i}) \big)^{1-y_{i}}\bigg)\]In order to derive the optimized coefficients \(\theta\), we need to maximize this likelihood function, but the calculation is complex because of the mathematical product. To make it easier, we take the logarithm of the likelihood function. Additionally, we insert a minus sign to turn the object to minimize the negative log-likelihood function. The equation is defined as\[J(\theta)=-\log \big(L(\theta)\big)=-\frac{1}{m}\sum_{i=1}^{m}\bigg(y_{i}\centerdot\log(h_{\theta}(\mathbf{x}_{i}))+(1-y_{i})\centerdot\log(1-h_{\theta}(\mathbf{x}_{i}))\bigg)\]which is so called logarithmic loss function or log-likelihood loss function, and \(\theta\) is computed by optimized this function. For more datails about Logistic Regression: Wikipedia, UFLDL Tutorial. To help you have a better understanding of how to implement logistic regression. You can get codes of logistic regression from my GitHub repository: [Full Java Implementation], [Java Implementation with ND4J]. Since the codes are fairly long, I do not show them here in order to save space. Bayes’ Formula Bayes’ formula is an important method for computing conditional probabilities. It is often used to compute posterior probabilities (as opposed to priorior probabilities) given observations. Suppose that \(P(A)\) and \(P(B)\) represent the probability of event \(A\) happens and the probability of event \(B\) happens respectively, \(P(A|B)\) denotes the probability of event \(A\) happens given event \(B\), \(P(A,B)\) denotes the probability that event \(A\) and \(B\) happen simultaneously, so we have\[P(A|B)=\frac{P(A,B)}{P(B)},\quad P(B|A)=\frac{P(A,B)}{P(A)},\\ P(A,B)=P(B)\centerdot P(A|B)=P(A)\centerdot P(B|A)\quad and\quad P(A|B)=P(A)\centerdot\frac{P(B|A)}{P(B)}\]For four variables, \(A_{1}\), \(A_{2}\), \(A_{3}\), \(A_{4}\), using the knowledge above, it is able to compute that\[P(A_{4},A_{3},A_{2},A_{1})=P(A_{4}|A_{3},A_{2},A_{1})\centerdot P(A_{3}|A_{2},A_{1})\centerdot P(A_{2}|A_{1})\centerdot P(A_{1})\]Generally, consider an indexed set of sets \(A_{1},\dots,A_{n}\), we can derive that\[P\bigg( \bigcap_{k=1}^{n}A_{k}\bigg) = \prod_{k=1}^{n}P\bigg( A_{k}|\bigcap_{j=1}^{k-1}A_{j}\bigg)\]This is so called Bayes’ Chain Rule. Huffman Coding In computer science and information theory, a Huffman code is a particular type of optimal prefix code that is commonly used for lossless data compression. The process of finding and/or using such a code proceeds by means of Huffman coding, a lossless data encoding algorithm, developed by David A. Huffman and published in the 1952 paper – A Method for the Construction of Minimum-Redundancy Codes. Huffman Tree Tree is an important non-linear data structure that simulates a hierarchical tree structure, with a root value and subtrees of children with a parent node, represented as a set of linked nodes. A tree data structure can be defined recursively (locally) as a collection of nodes (starting at a root node), where each node is a data structure consisting of a value, together with a list of references to nodes (the “children”), with the constraints that no reference is duplicated, and none points to the root. Alternatively, a tree can be defined abstractly as a whole (globally) as an ordered tree, with a value assigned to each node. Both these perspectives are useful: while a tree can be analyzed mathematically as a whole, when actually represented as a data structure it is usually represented and worked with separately by node. And here is some common-used concepts in tree: - Path and Path Length: the path is a sequence of nodes and edges connecting a node with a descendant, while the number of edges is path length. For example, denote that the level number of root node as 1, then the path length from root node to the node at \(L\) level is \(L-1\). - Node Weight and Weight Path Length (WPL): give a (non-negative) value with a certain meaning to the nodes in the tree, this value is called weight. The WPL is defined as the product of the node weight and the path length from root node to this node. - WPL of Tree: the WPL of Tree is defined as the sum of WPL of all the leaf nodes. Binary tree is a typical tree data structure in which each node has at most two children, which are referred to as the left child and the right child. Particularly, binary search tree (BST), also called ordered or sorted binary tree, is a particular type of binary tree that keep its nodes in sorted order, where the left subtree and right subtree can not interchange of position. Given \(n\) weighted nodes as \(n\) leaf nodes to construct a binary tree, if WPL of the built binary tree reaches the minimum, such built binary tree is an optimal binary tree, also named as Huffman Tree. Construction of Huffman Tree Given \(n\) weighted nodes \(\{w_{1},w_{2},\dots ,w_{n}\}\) as the leaf nodes of a binary tree, the Huffman tree can be constructed as follow: 1. Regard \(\{w_{1},w_{2},\dots ,w_{n}\}\) as a forest of \(n\) trees (each tree has only one node); 2. Combine two trees with minimal root node weight in the forest to build a new tree, each of these two tree becomes a subtree, and the root node weight of the new tree is the sum of the root nodes weight of its left and right subtrees; 3. Remove the two selected trees in step (2), and add the new tree into the forest; 4. Repeat step (2) and (3), till only one tree left in the forest, and this tree is the computed Huffman tree. For instance, suppose that we have a list of weighted nodes \(\{15,8,6,5,3,1\}\), the Huffman tree can be built via this way From the graph, we can derive that the node with larger weight is more closer to root node. In the construction process, the additional node generated through merge process is marked as red. Since every two nodes need to be merged one time, thus, if the number of leaf nodes is \(n\), then \(n-1\) additional nodes will be generated while constructing Huffman tree. In this case, \(n=6\), so 5 additional nodes are generated. Note that in the above case, we stipulate that the node with higher weight is the left child node, while the node with lower weight is the right child node. Certainly, if you reverse the stipulation, it is also true. Coding Process The binary prefix code built by Huffman tree is called Huffman Code, which satisfy the condition of prefix code as well as guarantee the length of message codes is shortest. Given a message “AFTER DATA EAR ARE ART AREA” to transmit, the character set used here is “\(A\), \(E\), \(R\), \(T\), \(F\), \(D\)”, their frequency are 8, 4, 5, 3, 1, 1, respectively. To transmit the message, we always want to keep the length of such message as short as possible, since the frequency of each character is different, so we try to use short codes for character with high frequency and long codes for character with low frequency in order to optimize whole message codes, then the Huffman coding is used. To construct the Huffman tree, we can derive then each character can be represent by a binary code, e.g., \(A\) is represented by “11”, \(F\) is represented by “0101”, etc. The Word2Vec toolkit also use the technology of Huffman coding, it treats the word in the corpus as leaf node, and the frequency of the word in corpus as weight, by constructing Huffman tree to coding each word. There are two stipulations in Word2Vec, one is that the node with large weight is treated as left child node, and another is the left child node is coded as 1. To keep the same, we also follow these stipulations. Here is the Java Implementation of Huffman Coding in Word2Vec: [link]. Natural Language Model A statistical language model is a probability distribution over sequences of words. Given such a sequence, say of length \(m\), it assigns a probability \(P(w_{1},\dots ,w_{m})\) to the whole sequence. Having a way to estimate the relative likelihood of different phrases is useful in many natural language processing applications, especially ones that generate text as an output. Language modeling is used in speech recognition, machine translation, part-of-speech tagging, parsing, hand-writing recognition, information retrieval and other applications. For example, in speech recognition, the computer tries to match sounds with word sequences. The language model provides context to distinguish between words and phrases that sound similar. For example, in American English, the phrases “recognize speech” and “wreck a nice beach” are pronounced almost the same but mean very different things. These ambiguities are easier to resolve when evidence from the language model is incorporated with the pronunciation model and the acoustic model. Although there are many language models, like N-gram model, decision tree, maximum entropy model, conditional random field, neural network and etc. Here we only discuss N-gram model and neural network based model. N-gram Model Given a corpus \(\boldsymbol{\mathcal{C}}\) and a sentence consist of \(N\) words, where \(\mathbf{w}=(w_{1},w_{2},\dots ,w_{N})\), then the joint probability of \(w_{1},w_{2},\dots ,w_{N}\) is\[P(\mathbf{w})=P(w_{1},w_{2},\dots ,w_{N})\]Using Bayes’ Formula, it can be decomposed to\[P(\mathbf{w})=P(w_{1})\centerdot P(w_{2}|w_{1})\centerdot P(w_{3}|w_{1},w_{2})\centerdot\centerdot\centerdot P(w_{N}|w_{1},w_{2},\dots ,w_{N-1})\]where the conditional probabilities \(P(w_{1})\), \(P(w_{2}|w_{1})\), \(\dots\), \(P(w_{N}|w_{1},w_{2},\dots ,w_{N-1})\) is the parameters of language model. Once these parameters are determined, then we can derive the probability of \(P(\mathbf{w})\). Take the approximate computation of \(P(w_{k}|w_{1}^{k-1})\) first, where \(w_{1}^{k-1}=(w_{1},\dots ,w_{k-1})\). By Bayes’ formula, we have\[P(w_{k}|w_{1}^{k-1})=\frac{P(w_{1}^{k})}{P(w_{1}^{k-1})}\]According to the law of large numbers, when the corpus \(\boldsymbol{\mathcal{C}}\) is large enough, then \(P(w_{k}|w_{1}^{k-1})\) can be approximately written as\[P(w_{k}|w_{1}^{k-1})\approx\frac{counter(w_{1}^{k})}{counter(w_{1}^{k-1})}\]where \(counter(w_{1}^{k})\) and \(counter(w_{1}^{k-1})\) represent the appearance frequency of \(w_{1}^{k}\) and \(w_{1}^{k-1}\) in corpus respectively. One can imagine, once \(k\) goes larger, the statistics of \(counter(w_{1}^{k})\) and \(counter(w_{1}^{k-1})\) will become extremely time consuming. The N-gram model is used to solve this problem, in the previous case, we can see that the appearance probability of a word is related to all the words in front of it. How about assuming that such probability is only related to a certain number of words in front of it. This is the basic idea of N-gram model, who makes a \(n-1\) order Markov Hypothesis to restrict that the appearance probability of a word is only related to \(n-1\) words in front of it, say\[P(w_{k}|w_{1}^{k-1})\approx P(w_{k}|w_{k-n+1}^{k-1})\approx\frac{counter(w_{k-n+1}^{k})}{counter(w_{k-n+1}^{k-1})}\]For example, by setting \(n=2\), we will have\[P(w_{k}|w_{1}^{k-1})\approx\frac{counter(w_{k-1},w_{k})}{counter(w_{k-1})}\]After simplification, it bacomes easier to compute the single parameter and reduce the total amount of parameters. Below gives the relationship between \(n\) and parameter number (suppose that the size of dictionary \(\boldsymbol{\mathcal{D}}\) is 200000): From the table, we can derive that the computational complexity increase with the growth of \(n\), say, \(\big(\boldsymbol{\mathcal{O}}(|\boldsymbol{\mathcal{D}}|^{n})\big)\), it is an exponentially incremental magnitute, so \(n\) can not set too large, normally, \(n=3\) (trigram) is enough for most cases. For the model performance, theoretically, \(n\) is bigger, the performance will be better. However, when \(n\) goes up to some extent, the hoist scope of model performance will decrease. Another thing is that when the number of parameters goes up, the distinguishability of model bacomes better, the reliability goes down, becasue of the instances of the single parameter decrease. Thus, it is needed to make a compromise between distinguishability and reliability. Moreover, smoothing is needed in N-gram model, because of these reasons: 1. if \(counter(w_{k-n+1}^{k})=0\), we can not say that \(P(w_{k}|w_{1}^{k-1})\) is equal to 0; 2. if \(counter(w_{k-n+1}^{k})=counter(w_{k-n+1}^{k-1})\), we can not say that \(P(w_{k}|w_{1}^{k-1})\) is equal to 1. Thus, the smoothing techniques is used to deal with this issue. In practice it is necessary to smooth the probability distributions by also assigning non-zero probabilities to unseen words or n-grams. The reason is that models derived directly from the n-gram frequency counts have severe problems when confronted with any n-grams that have not explicitly been seen before – the zero-frequency problem. Various smoothing methods are used, from simple “add-one” (Laplace) smoothing (assign a count of 1 to unseen n-grams) to more sophisticated models, such as Good–Turing discounting or back-off models. Some of these methods are equivalent to assigning a prior distribution to the probabilities of the n-grams and using Bayesian inference to compute the resulting posterior n-gram probabilities. However, the more sophisticated smoothing models were typically not derived in this fashion, but instead through independent considerations. However, in machine learning field, there is a commonly used method: after constructing a model of the task, then building a objective function for it, trying to optimize this function to got an optimized parameters, and using the model with this optimized parameters to do the further predict task. For statistical language model, using maximal likelihood, the objective function can be set as\[\prod_{w\in\boldsymbol{\mathcal{C}}}P(w|context(w))\]where \(\boldsymbol{\mathcal{C}}\) represents the corpus, \(context(w)\) denotes the context of word \(w\), i.e., the set of circumjacent words of \(w\). And when \(context(w)=\emptyset\), we have \(P(w|context(w))=P(w)\). Particularly, for N-gram model, \(context(w_{i})=w_{i-n+1}^{i-1}\). In practice, the maximal logarithm likelihood is often used, i.e., setting the objective function as\[\mathcal{L}=\sum_{w\in\boldsymbol{\mathcal{C}}}\log P(w|context(w))=\sum_{w\in\boldsymbol{\mathcal{C}}}\log F(w,context(w),\boldsymbol{\theta})\]and maximizing this function, where \(\boldsymbol{\theta}\) is the parameter set to be determined and \(P(w|context(w))\) here is treated as the function of \(w\) and \(context(w)\). Once we obtain the optimized parameter set \(\boldsymbol{\theta}^{*}\) by optimizing the objective function above, \(F\) is well-determined too, and further any probability \(P(w|context(w))\) is able to be computed by \(F(w,context(w),\boldsymbol{\theta}^{*})\). Compare to N-gram model, this method do not need to compute and store all the probability values in advance, it computes the probability directly, and by choosing a suitable model will make the number of parameters in \(\boldsymbol{\theta}\) much less than that in N-gram model. Actually, this method is the base of Word2Vec algorithm framework, and we will intruduce it in details in the following part. Neural Probabilistic based Model In natural language processing task, we need to use machine learning algorithms to deal with natural language, however, machines can not understand human language directly, thus, we have to make the laguage mathematicization. In neural probabilistic language model, there is an important concept named distributed representation (word vector). It is a good way to digitalize natural language, unlike the traditional on-hot representation, which just signify the words, do not contain any semantic information and easily suffer dimensionality curse in deep learning task. The distributed representation method overcomes those drawbacks of on-hot representation, its basic idea is mapping each word of a certain language to a short vector with fixed length through training, all of those vectors consist a word vector space, and each vector can be treated as point in this space, then introducing “distance” concept, and the “distance” among words can be used to determine their (grammatical, semantic, and etc.) similarities. Mathematically, for an arbitrary word \(w\) the dictionary \(\boldsymbol{\mathcal{D}}\), assign a real number vector with fixed length \(e(w)\in\mathbb{R}^{m}\), \(e(w)\) is called the word vector of \(w\), and \(m\) is the length of word vector. In practice, there are many different methods can be used to estimate the word vectors, like Latent Semantic Analysis (LSA), Latent Dirichlet Allocation (LDA), Vector Space Models (VSMs), Neural Probabilistic based Model and etc. Since the Word2Vec is kind of the neural probabilistic based language model to obtain the word vectors with distributed representation, so here we only introduce the neural probabilistic model. Here we use the paper, “A Neural Probabilistic Language Model”, proposed by Bengio in 2003 as an example to introduce the neural probabilistical based language model. Below gives the architecture graph of this model, it is a four layer neural networks, with input layer, projection layer, hidden layer and output layer. In this graph, \(tanh\) is the hyperbolic tangent function, which acts as the activation function of hidden layer; \(\mathbf{W}\) and \(\mathbf{p}\) are the weight matrix and bias vector between projection layer and hidden layer respectively; while \(\mathbf{U}\) and \(\mathbf{q}\) are the weight matrix and bias vector between hidden layer and output layer respectively. Note that the dimension of projection layer is \((n-1)\centerdot m\), the dimension of output layer is \(N=|\boldsymbol{\mathcal{D}}|\), where the \(|\boldsymbol{\mathcal{D}}|\) denotes the number of words in the dictionary, and the dimension of hidden layer is an adaptive parameter, which can be modified by user. The \(w_{t-n+1},\dots ,w_{t-1}\) are the \(n-1\) words in front of word \(w_{t}\) in corpus \(\boldsymbol{\mathcal{C}}\), i.e., \(context(w_{t})=\big(w_{t-n+1},\dots ,w_{t-1}\big)\). Here, we simply denotes the \(w_{t}\) as \(w\), so \(\big(w, context(w)\big)\) is a training sample. According to the architecture in the graph\[\begin{cases}\mathbf{z}_{w}=tanh\big(\mathbf{W}\mathbf{X}_{w}+\mathbf{p}\big),\\\mathbf{y}_{w}=\mathbf{U}\mathbf{z}_{w}+\mathbf{q}.\end{cases}\]Here, \(\mathbf{y}_{w}=\big(y_{w,1},y_{w,2},\dots ,y_{w,N}\big)^{T}\) is a vector with length \(N\), and its element can not represent the probability in this case. In order to make \(y_{w,i}\) represents the probability of the target word is \(i^{th}\) word in dictionary \(\boldsymbol{\mathcal{D}}\) for the given \(context(w)\), we need to do a Softmax normalization, after normalizing, \(P(w|context(w))\) can be expressed as\[P(w|context(w))=\frac{exp(y_{w,i_{w}})}{\sum_{i=1}^{N}exp(y_{w,i})}\]where \(i_{w}\) represents the index of word \(w\) in dictionary \(\boldsymbol{\mathcal{D}}\). Note that, in the last section, we said that \(P(w|context(w))\) is treated as the function of \(w\) and \(context(w)\), i.e., \(P(w|context(w))=F(w,context(w),\boldsymbol{\theta})\), so what are the parameters to be determined, i.e, what does \(\boldsymbol{\theta}\) contain? In summary, it includes two parts: 1. word vector: \(e(w)\in\mathbb{R}^{m}, w\in\boldsymbol{\mathcal{D}}\) and filled vector. 2. neural network parameters: \(\mathbf{W}\in\mathbb{R}^{n_{h}\centerdot (n-1)m}\), \(\mathbf{p}\in\mathbb{R}^{n_{h}}\), \(\mathbf{U}\in\mathbb{R}^{N\centerdot n_{h}}\), and \(\mathbf{q}\in\mathbb{R}^{N}\). All of those parameters can be obtained through training. And compare to N-gram model, neural probabilistic based model has two major advantages: 1. the similarities among words can be expressed by word vectors. 2. it does not need additional smoothing process. Word2Vec In Word2Vec, Mikolov et al. proposed two models, one is Continuous Bag-of-Words (CBOW) Model and another is Skip-gram (SG) Model. The graph of models are shown below, both of two models have three layers: Input layer, Projection layer and Output layer. From the graph, for CBOW model, it is given the contexts \(\{w_{t-2},w_{t-1},w_{t+1},w_{t+2}\}\) to predict the word \(w_{t}\), while SG model is given the word \(w_{t}\) to predict the contexts \(\{w_{t-2},w_{t-1},w_{t+1},w_{t+2}\}\). Different from other neural network language models, CBOW and SG models do not have hidden layer, by taking out hidden layer, the models convert from neural network structure to logarithmic-linear structure directly, which in accordance with Logistic Regression. Compare to three layer neural networks, the logarithmic-linear structure decreases one layer’s matrix manipulation, which significantly improve model’s training speed. The second thing is that CBOW and Skip-gram ignore the word order information of context, so they do not concatenate the word vector of each word in context as those neural network language models did, however, they directly sum the word vector of each word in context (in the later version of Word2Vec, it changes to average of word vectors). Another difference is that the output layer of the models are tree structure. Here we define some notations and symbols: \(\boldsymbol{\mathcal{C}}\) represents corpus; \(\boldsymbol{\mathcal{D}}\) represents dictionary; \(e(w)\) represents word vector of word \(w\); \(context(w)\) represents a set of words is constituted by before and after \(n\) words of word \(w\), i.e. \(context(w)\) contains a set of words \(\{w_{-n},\dots ,w_{-1},w_{1},\dots ,w_{n}\}\); \(m\) represents the dimension of word vector. Continuous Bag-of-Words Model As aforesaid, CBOW model contains three layer: input layer, projection layer and output layer. Given a training sample \((w,context(w))\), 1. Input layer: it includes \(2n\) word vectors in \(context(w)\), i.e., \(e(w_{-n}),\dots ,e(w_{n})\in\mathbb{R}^{m}\), where \(\mathbb{R}\) is the set of real numbers. 2. Projection layer: accumulate the \(2n\) vectors, i.e. \(\mathbf{x}_{w}=\sum_{i=-n,i\neq 0}^{n}\big(e(w_{i})\big)\). 3. Output layer: the output corresponds to a Huffman tree, it uses the words appear in the corpus \(\mathcal{C}\) as leaf nodes, uses the appearance frequency of word in the corpus as weight. In this Huffman tree, there are \(N (=|\boldsymbol{\mathcal{D}}|)\) leaf nodes, each of them is correspond to a word in dictionary \(\boldsymbol{\mathcal{D}}\), and there are \(N-1\) nonleaf nodes as well. For neural network based language model, most of the calculations are concentrated on the matrix manipulation between hidden layer and output layer, as well as the softmax normalized operation at output layer. From the graph above, CBOW makes specially change on these high computational conplexity field, it takes out hidden layer, and uses Huffman tree at output layer, which lay the foundation of the utilization of Hierarchical Softmax technique. Since the CBOW does not have hidden layer, so its input layer is the representation of context, and it can predict target word directly. And similar to other neural network based language model, the optimization objective of CBOW is to maximize the following log-likelihood function\[\mathcal{L}=\sum_{w\in\boldsymbol{\mathcal{C}}}\log P\big(w|context(w)\big)\]and the key point is to construct and calculate the conditional probability function \(P\big(w|context(w)\big)\). Hierarchical Softmax Hierarchical softmax is an efficient way of computing softmax, a key technique to improve performance in Word2Vec. It uses a binary tree to represent all words in the vocabulary. If there are \(V\) words, all of them must be leaf units of the tree. It can be proved that there are \(V-1\) inner units. For each leaf unit, there exists a unique path from the root to the unit; and this path is used to estimate the probability of the word represented by the leaf unit. Before talking about this technique, we introduce some related symbols. Considering a leaf node in the Huffman tree, assume this leaf node corresponding to the word \(w\) in dictionary \(\boldsymbol{\mathcal{D}}\), note that 1. \(\mathbf{p}\): the path from root node to the leaf node corresponding to \(w\); 2. \(l\): the number of nodes within the path \(\mathbf{p}\); 3. \(p_{1},p_{2},\dots ,p_{l}\): \(l\) nodes in the path \(\mathbf{p}\), where \(p_{1}\) represents root node, \(p_{l}\) represents the leaf node corresponding to \(w\). 4. \(d_{2},d_{3},\dots ,d_{l}\in\{0,1\}\): the Huffman codes of word \(w\), which is consist of \(l-1\) codes, where \(d_{j}\) represents the code of \(j^{th}\) node in the path \(\mathbf{p}\) (root node does not have corresponding code); 5. \(\boldsymbol{\theta}_{1}^{w},\boldsymbol{\theta}_{2}^{w},\dots, \boldsymbol{\theta}_{l-1}^{w}\in\mathbb{R}^{m}\): the corresponding vectors (auxiliary vectors) of nonleaf nodes in path \(\mathbf{p}\), where \(\boldsymbol{\theta}_{j}^{w}\) represents the corresponding vector of \(j^{th}\) nonleaf node in path \(\mathbf{p}\). Then give a training sample “I like this girl very much”, and each word here has a certain frequency, it is coded into Huffman tree as shown in the graph below, and consider the situation that \(w=girl\). In the graph, the five nodes are linked by four red thick lines consist the path \(\mathbf{p}\), and its length \(l=5\). \(p_{1},p_{2},p_{3},p_{4},p_{5}\) are the five nodes in the path, while \(p_{1}\) corresponding to root node. \(d_{2},d_{3},d_{4},d_{5}\) are 1, 0, 0, 1, respectively, which means that the Huffman codes of word “girl” is 1001. Besides, \(\boldsymbol{\theta}_{1}^{w},\boldsymbol{\theta}_{2}^{w},\boldsymbol{\theta}_{3}^{w},\boldsymbol{\theta}_{4}^{w}\) denotes the corresponding vectors of four nonleaf nodes in path \(\mathbf{p}\). Next step is to use vector \(\mathbf{x}_{w}\in\mathbb{R}^{m}\) and Huffman tree to define the conditional probability function \(P\big(w|context(w)\big)\). Using \(w=girl\) as example, from root node to the leaf node “girl”, it goes through four branches, each branch can be treated as a dichotomy. Here we stipulate that the node with Huffman code “0” is defined as positive class, while the node with Huffman code “1” is defined as negative class (same as Word2Vec does), i.e., \(Label(p_{i})=1-d_{i}\), where \(i=2,3,\dots ,l\). In brief, when classifying a node, if it is classified to left, it is a negative class, otherwise, it is a positive class. According to logistic regression, the probability of a node to be classified as positive class is\[\sigma\big(\mathbf{x}_{w}^{T}\boldsymbol{\theta}\big)=\frac{1}{1+e^{-\mathbf{x}_{w}^{T}\boldsymbol{\theta}}}\]Otherwise, the probability to be classified as negative class is \(1-\sigma\big(\mathbf{x}_{w}^{T}\boldsymbol{\theta}\big)\), note that the auxiliary vectors \(\boldsymbol{\theta}_{j}^{w}\) we introduce above performs the same role as the undetermined parameter vector \(\boldsymbol{\theta}\) here. Since four dichotomies happen from root node to the leaf node “girl”, the probabbility of each calssification is - \(1^{st}\) time: \(P(d_{2}|\mathbf{x}_{w},\boldsymbol{\theta}_{1}^{w})=1-\sigma\big(\mathbf{x}_{w}^{T}\boldsymbol{\theta}_{1}^{w}\big)\) - \(2^{nd}\) time: \(P(d_{3}|\mathbf{x}_{w},\boldsymbol{\theta}_{2}^{w})=\sigma\big(\mathbf{x}_{w}^{T}\boldsymbol{\theta}_{2}^{w}\big)\) - \(3^{rd}\) time: \(P(d_{4}|\mathbf{x}_{w},\boldsymbol{\theta}_{3}^{w})=\sigma\big(\mathbf{x}_{w}^{T}\boldsymbol{\theta}_{3}^{w}\big)\) - \(4^{th}\) time: \(P(d_{5}|\mathbf{x}_{w},\boldsymbol{\theta}_{4}^{w})=1-\sigma\big(\mathbf{x}_{w}^{T}\boldsymbol{\theta}_{4}^{w}\big)\) For \(w=girl\), the probability of \(P(w|context(w))\) can be written as\[P(w|context(w))=\prod_{j=2}^{5}P\big(d_{j}|\mathbf{x}_{w},\boldsymbol{\theta}_{j-1}^{w}\big)\]So this is the basic idea of Hierarchical Softmax. Generally, for an arbbitrarily word \(w\) in dictionary \(\boldsymbol{\mathcal{D}}\), there must be one, and only one, path \(\mathbf{p}\) from root node to the leaf node corresponding to the word \(w\). \(l-1\) branches exist on this path, each breanch is treated as a dichotomy, and each dichotomy generates a probability, then the \(P(w|context(w))\) is defined as the product of those probabilities.\[p(w|context(w))=\prod_{j=2}^{l}P\big(d_{j}|\mathbf{x}_{w},\boldsymbol{\theta}_{j-1}^{w}\big)\]where,\[P\big(d_{j}|\mathbf{x}_{w},\boldsymbol{\theta}_{j-1}^{w}\big)=\begin{cases} \sigma\big(\mathbf{x}_{w}^{T}\boldsymbol{\theta}_{j-1}^{w}\big),&amp;d_{j}=0; \\ 1-\sigma\big(\mathbf{x}_{w}^{T}\boldsymbol{\theta}_{j-1}^{w}\big),&amp;d_{j}=1. \end{cases}\]or it can be wirtten as\[P\big(d_{j}|\mathbf{x}_{w},\boldsymbol{\theta}_{j-1}^{w}\big)=\bigg(\sigma\big(\mathbf{x}_{w}^{T}\boldsymbol{\theta}_{j-1}^{w}\big)\bigg)^{1-d_{j}}\centerdot\bigg(1-\sigma\big(\mathbf{x}_{w}^{T}\boldsymbol{\theta}_{j-1}^{w}\big)\bigg)^{d_{j}}\]Then substituting the formula above to the log-likelihood, i.e., the optimization objective of CBOW, then we have\[\mathcal{L}=\sum_{w\in\boldsymbol{\mathcal{C}}}\log\prod_{j=2}^{l}\bigg\{\bigg(\sigma\big(\mathbf{x}_{w}^{T}\boldsymbol{\theta}_{j-1}^{w}\big)\bigg)^{1-d_{j}}\centerdot\bigg(1-\sigma\big(\mathbf{x}_{w}^{T}\boldsymbol{\theta}_{j-1}^{w}\big)\bigg)^{d_{j}}\bigg\}\\=\sum_{w\in\boldsymbol{\mathcal{C}}}\sum_{j=2}^{l}\bigg\{(1-d_{j})\centerdot\log\big(\sigma\big(\mathbf{x}_{w}^{T}\boldsymbol{\theta}_{j-1}^{w}\big)+d_{j}\centerdot\log\big(1-\sigma\big(\mathbf{x}_{w}^{T}\boldsymbol{\theta}_{j-1}^{w}\big)\bigg\}\]Hereto, the function above is the target function of CBOW, then our task is to optimize it, i.e., maximize this function. In Word2Vec, the author use the Stochastic Gradient Ascent method. And the key point of gradient descent/ascent methods are to find the gradient computing formula. To make further process simple, we denote that\[\mathcal{L}(w,j)=(1-d_{j})\centerdot\log\bigg(\sigma\big(\mathbf{x}_{w}^{T}\boldsymbol{\theta}_{j-1}^{w}\big)\bigg)+d_{j}\centerdot\log\bigg(1-\sigma\big(\mathbf{x}_{w}^{T}\boldsymbol{\theta}_{j-1}^{w}\big)\bigg)\]The procedure of stochastic gradient ascent is: for every sample \((w, context(w))\), update all the related parameters in objective function. Observe the target function \(\mathcal{L}\), we can derive that the parameters in this objective function are \(\mathbf{x}_{w}\) and \(\boldsymbol{\theta}_{j-1}^{w}\). Here we give the gradients of \(\mathcal{L}(w,j)\) with regrad to these parameter vectors. First, consider the gradient computation of \(\mathcal{L}(w,j)\) with regard to \(\boldsymbol{\theta}_{j-1}^{w}\):\[\frac{\partial\mathcal{L}(w,j)}{\partial\boldsymbol{\theta}_{j-1}^{w}}=\frac{\partial}{\partial\boldsymbol{\theta}_{j-1}^{w}}\bigg\{(1-d_{j})\centerdot\log\bigg(\sigma\big(\mathbf{x}_{w}^{T}\boldsymbol{\theta}_{j-1}^{w}\big)\bigg)+d_{j}\centerdot\log\bigg(1-\sigma\big(\mathbf{x}_{w}^{T}\boldsymbol{\theta}_{j-1}^{w}\big)\bigg)\bigg\}\\=(1-d_{j})\bigg(1-\sigma\big(\mathbf{x}_{w}^{T}\boldsymbol{\theta}_{j-1}^{w}\big)\bigg)\mathbf{x}_{w}-d_{j}\sigma\big(\mathbf{x}_{w}^{T}\boldsymbol{\theta}_{j-1}^{w}\big)\mathbf{x}_{w}\\=\bigg\{(1-d_{j})\bigg(1-\sigma\big(\mathbf{x}_{w}^{T}\boldsymbol{\theta}_{j-1}^{w}\big)\bigg)-d_{j}\sigma\big(\mathbf{x}_{w}^{T}\boldsymbol{\theta}_{j-1}^{w}\big)\bigg\}\mathbf{x}_{w}\\=\big[1-d_{j}-\sigma\big(\mathbf{x}_{w}^{T}\boldsymbol{\theta}_{j-1}^{w}\big)\big]\mathbf{x}_{w}\] Thus the update formula of \(\boldsymbol{\theta}_{j-1}^{w}\) can be written as (\(\eta\) is learning rate)\[\boldsymbol{\theta}_{j-1}^{w}:=\boldsymbol{\theta}_{j-1}^{w}+\eta\big[1-d_{j}-\sigma\big(\mathbf{x}_{w}^{T}\boldsymbol{\theta}_{j-1}^{w}\big)\big]\mathbf{x}_{w}\]Then consider the gradient of \(\mathcal{L}(w,j)\) with regard to \(\mathbf{x}_{w}\), since the variable \(\mathbf{x}_{w}\) and \(\boldsymbol{\theta}_{j-1}^{w}\) are symmetrical, which means that they can change position, so we can derive that\[\frac{\partial\mathcal{L}(w,j)}{\partial\mathbf{x}_{w}}=\big[1-d_{j}-\sigma\big(\mathbf{x}_{w}^{T}\boldsymbol{\theta}_{j-1}^{w}\big)\big]\boldsymbol{\theta}_{j-1}^{w}\]So far we have got the gradients of \(\mathcal{L}(w,j)\) with regard to \(\boldsymbol{\theta}_{j-1}^{w}\) and \(\mathbf{x}_{w}\), but our ultimate goal is to compute the word vector for every word in the dictionary \(\boldsymbol{\mathcal{D}}\). In order to use \(\frac{\partial\mathcal{L}(w,j)}{\partial\mathbf{x}_{w}}\) to update \(e(\tilde{w}),\tilde{w}\in context(w)\), in Word2Vec, the author directly use\[e(\tilde{w}):=e(\tilde{w})+\eta\sum_{j=2}^{l}\frac{\partial\mathcal{L}(w,j)}{\partial\mathbf{x}_{w}},\tilde{w}\in context(w)\]it contributes \(\sum_{j=2}^{l}\frac{\partial\mathcal{L}(w,j)}{\partial\mathbf{x}_{w}}\) to every word vector of word in \(context(w)\) (think about if it is more reasonable that change \(\eta\) to \(\frac{\eta}{|context(w)|}\), where \(|context(w)|\) denotes the number of words in \(context(w)\), i.e., use average contribution). It is easy to understand, since \(\mathbf{x}_{w}\) is the summation of all the word vectors in \(context(w)\), it should be contributed to every word after gradient calculation. Here is the pseudocode about using stochastic gradient ascent method to update the parameters in CBOW: &gt; 1. \(\mathbf{v}=\boldsymbol{0}\) &gt; 2. \(\mathbf{x}_{w}=\sum_{w\in context(w)}e(w)\) &gt; 3. For \(j=2:l\) do : &gt; 3.1 \(q=\sigma\big(\mathbf{x}_{w}^{T}\boldsymbol{\theta}_{j-1}^{w}\big)\) &gt; 3.2 \(g=\eta\centerdot (1-d_{j}-q)\) &gt; 3.3 \(\mathbf{v}:=\mathbf{v}+g\centerdot\boldsymbol{\theta}_{j-1}^{w}\) &gt; 3.4 \(\boldsymbol{\theta}_{j-1}^{w}:=\boldsymbol{\theta}_{j-1}^{w}+g\centerdot\mathbf{x}_{w}\) &gt; 4. For \(w\in context(w)\) do : &gt; \(e(w):=e(w)+\mathbf{v}\) Note that \(e(w)\) is corresponding to the “syn0” in Word2Vec source codes, \(\boldsymbol{\theta}_{j-1}^{w}\) is corresponding to “syn1”, \(\mathbf{x}_{w}\) is corresponding to “neu1” and \(\mathbf{v}\) is corresponding to “neu1e”. Here shows some java codes of the procedure. For step 1, the vector \(\mathbf{v}\) is initialized to zero (“layer1_size” is dimension of vector, i.e., \(m\) in this article): 12for (int c = 0; c &lt; layer1_size; c++) neu1e[c] = 0; For step 2: 1234567891011for (int c = 0; c &lt; layer1_size; c++) neu1[c] = 0;...for (int a = b; a &lt; window * 2 + 1 - b; a++) &#123; if (a == window) continue; int c = sentencePosition - window + a; if (c &lt; 0 || c &gt;= sentenceLength) continue; int idx = huffmanNodes.get(sentence.get(c)).idx; for (int d = 0; d &lt; layer1_size; d++) &#123; neu1[d] += syn0[idx][d]; &#125; ...&#125; For step 3: 1234567891011121314151617181920for (int d = 0; d &lt; huffmanNode.code.length; d++) &#123; double f = 0; int l2 = huffmanNode.point[d]; /**------------------------------3.1--------------------------------------*/ // Propagate hidden -&gt; output for (int c = 0; c &lt; layer1_size; c++) f += neu1[c] * syn1[l2][c]; if (f &lt;= -MAX_EXP || f &gt;= MAX_EXP) continue; else f = EXP_TABLE[(int)((f + MAX_EXP) * (EXP_TABLE_SIZE / MAX_EXP / 2))]; /**------------------------------3.2--------------------------------------*/ // 'g' is the gradient multiplied by the learning rate double g = (1 - huffmanNode.code[d] - f) * alpha; /**------------------------------3.3--------------------------------------*/ // Propagate errors output -&gt; hidden for (int c = 0; c &lt; layer1_size; c++) neu1e[c] += g * syn1[l2][c]; /**------------------------------3.4--------------------------------------*/ // Learn weights hidden -&gt; output for (int c = 0; c &lt; layer1_size; c++) syn1[l2][c] += g * neu1[c];&#125; For step 4: 1234567for (int a = b; a &lt; window * 2 + 1 - b; a++) &#123; if (a == window) continue; int c = sentencePosition - window + a; if (c &lt; 0 || c &gt;= sentenceLength) continue; int idx = huffmanNodes.get(sentence.get(c)).idx; for (int d = 0; d &lt; layer1_size; d++) syn0[idx][d] += neu1e[d]; &#125; Negative Sampling The idea of negative sampling is more straightforward than hierarchical softmax: in order to deal with the difficulty of having too many output vectors that need to be updated per iteration, we only update a sample of them. Apparently the output word (i.e., the ground truth, or positive sample) should be kept in our sample and gets updated, and we need to sample a few words as negative samples (hence “negative sampling”). A probabilistic distribution is needed for the sampling process, and it can be arbitrarily chosen. For instance, the words in dictionary \(\boldsymbol{\mathcal{D}}\) have different appearance frequency in corpus \(\boldsymbol{\mathcal{C}}\), some are higher, some are lower. For those words with high frequency, the probability to be chosen as negative sample is high, and vice versa. So the concrete ways in Word2Vec to do negative sampling is: define \(len_{0}=0, len_{k}=\sum_{j=1}^{k}ratio(w_{j}), k=1,2,\dots ,N\), where \(w_{j}\) means the \(j^{th}\) words in dictionary \(\boldsymbol{\mathcal{D}}\), \(ratio(w)=\frac{counter(w)}{\sum_{w&#39;\in\boldsymbol{\mathcal{D}}}counter(w&#39;)}\), and \(counter(\centerdot)\) represents the frequency of a word appears in corpus \(\boldsymbol{\mathcal{C}}\). Then using \(\big\{len_{j}\big\}_{j=0}^{N}\) as the subdivision nodes, we can derive an unequidistant subdivision in the \([0,1]\) interval. And \(I_{i}=(len_{i-1},len_{i}], i=1,2,\dots ,N\) is the \(N\) sub-intervals. Further, introducing an equidistant subdivision in this \([0,1]\) interval, and the subdivision nodes are \(\big\{m_{j}\big\}_{j=0}^{M}\), where \(M\gg N\), see the graph below Then project the equidistant subdivision nodes into the unequidistant subdivision (as red dot lines), we can derive the mapping relation between \(\big\{m_{j}\big\}_{j=0}^{M}\) and \(\big\{I_{j}\big\}_{j=1}^{N}\)(or \(\big\{w_{j}\big\}_{j=1}^{N}\)):\[Table(i)=w_{k}, m_{i}\in I_{k}, i=1,2,\dots ,M-1\]Once we have this table, it is easy to do sampling: generating a random integer \(r\) within \([1,M-1]\), \(Table(r)\) is a sample. One thing to mention here is that when doing negative sampling for \(w_{j}\), if the random integer index to the \(w_{j}\) itself, then skip. It is also worth mentioning that, in Word2Vec source codes, the authors did not use \(counter(w)\) directly to set the weight for word in dictionary \(\boldsymbol{\mathcal{D}}\), but powered \(counter(w)\) with \(\alpha =0.75\). In addition, \(M\) (corresponding to “table_size” in codes) is set to \(10^{8}\). See the java codes below: 12345678910111213141516171819private static final int TABLE_SIZE = (int) 1e8;private void initializeUnigramTable() &#123; long trainWordsPow = 0; double power = 0.75; for (HuffmanNode node : huffmanNodes.values()) &#123; trainWordsPow += Math.pow(node.count, power); &#125; Iterator&lt;HuffmanNode&gt; nodeIter = huffmanNodes.values().iterator(); HuffmanNode last = nodeIter.next(); double d1 = Math.pow(last.count, power) / trainWordsPow; int i = 0; for (int a = 0; a &lt; TABLE_SIZE; a++) &#123; table[a] = i; if (a / (double) TABLE_SIZE &gt; d1) &#123; i++; HuffmanNode next = nodeIter.hasNext() ? nodeIter.next() : last; d1 += Math.pow(next.count, power) / trainWordsPow; last = next; &#125; &#125;&#125; In CBOW model, given the \(context(w)\), and we need to predict \(w\), thus, for the given \(context(w)\), \(w\) is a positive sample, then other words are negative samples. Suppose that we have selected a negative sample subset \(NEG(w)\neq\emptyset\), and for \(\forall\tilde{w}\in\boldsymbol{\mathcal{D}}\), define\[L^{w}(\tilde{w})=\begin{cases}1, &amp; \tilde{w}=w;\\ 0, &amp; \tilde{w}\neq w.\end{cases}\]to represents the label of \(\tilde{w}\), i.e., the label of positive sample is 1, negative sample is 0. For a given positive sample \(\big(w, context(w)\big)\), we want to maximize\[g(w)=\prod_{u\in\{w\}\cup NEG(w)}P(u|context(w))\]where\[P(u|context(w))=\big[\sigma(\mathbf{x}_{w}^{T}\boldsymbol{\theta}^{u})\big]^{L^{w}(u)}\centerdot\big[1-\sigma(\mathbf{x}_{w}^{T}\boldsymbol{\theta}^{u})\big]^{1-L^{w}(u)}\]here the \(\mathbf{x}_{w}\) also denotes the sum of vectors of all words in \(context(w)\), \(\boldsymbol{\theta}^{u}\in\mathbb{R}^{m}\) represents an auxiliary vector, which is corresponding to word \(u\). Combine the tow formula above, we have\[g(w)=\sigma(\mathbf{x}_{w}^{T}\boldsymbol{\theta}^{w})\prod_{u\in NEG(w)}\big[1-\sigma(\mathbf{x}_{w}^{T}\boldsymbol{\theta}^{u})\big]\]the \(\sigma(\mathbf{x}_{w}^{T}\boldsymbol{\theta}^{w})\) represents the probability when predicted word is \(w\), while \(\sigma(\mathbf{x}_{w}^{T}\boldsymbol{\theta}^{u}), u\in NEG(w)\) represents the probability when predicted word is \(u\). Thus, maximizing \(g(w)\) is equal to maximizing \(\sigma(\mathbf{x}_{w}^{T}\boldsymbol{\theta}^{w})\) and minimizing \(\sigma(\mathbf{x}_{w}^{T}\boldsymbol{\theta}^{u})\) simultaneously. Actually, this is what we want, say, increase the probability of positive sample while decrease the probability of negative samples. So, for a given \(\boldsymbol{\mathcal{C}}\), the overall optimization target function is\[\mathcal{L}=\log G=\log\prod_{w\in\boldsymbol{\mathcal{C}}}g(w)=\sum_{w\in\boldsymbol{\mathcal{C}}}\log g(w) \\=\sum_{w\in\boldsymbol{\mathcal{C}}}\sum_{u\in\{w\}\cup NEG(w)}\big\{L^{w}(u)\centerdot\log\big[\sigma(\mathbf{x}_{w}^{T}\boldsymbol{\theta}^{u})\big]+\big[1-L^{w}(u)\big]\centerdot\big[1-\sigma(\mathbf{x}_{w}^{T}\boldsymbol{\theta}^{u})\big]\} \\=\sum_{w\in\boldsymbol{\mathcal{C}}}\big\{\log\big[\sigma(\mathbf{x}_{w}^{T}\boldsymbol{\theta}^{u})\big]+\sum_{u\in NEG(w)}\log\big[\sigma(-\mathbf{x}_{w}^{T}\boldsymbol{\theta}^{u})\big]\big\}\]Same as before, we denote\[\mathcal{L}(w,u)=L^{w}(u)\centerdot\log\big[\sigma(\mathbf{x}_{w}^{T}\boldsymbol{\theta}^{u})\big]+\big[1-L^{w}(u)\big]\centerdot\big[1-\sigma(\mathbf{x}_{w}^{T}\boldsymbol{\theta}^{u})\big]\]and before using stochastic gradient ascent method to optimize this target function, we need to get the gradients of \(\mathcal{L}(w,u)\) with regard to \(\boldsymbol{\theta}^{u}\) and \(\mathbf{x}_{w}\) respectively. Samely, considering the gradient computation of \(\mathcal{L}(w,u)\) with regard to \(\boldsymbol{\theta}^{u}\):\[\frac{\partial\mathcal{L}(w,u)}{\partial\boldsymbol{\theta}^{u}}=\big[L^{w}(u)-\sigma(\mathbf{x}_{w}^{T}\boldsymbol{\theta}^{u})\big]\mathbf{x}_{w}\]Hence, the update formula of \(\boldsymbol{\theta}^{u}\) can be written as\[\boldsymbol{\theta}^{u}:=\boldsymbol{\theta}^{u}+\eta\big[L^{w}(u)-\sigma(\mathbf{x}_{w}^{T}\boldsymbol{\theta}^{u})\big]\mathbf{x}_{w}\]Then, using the symmetry of \(\boldsymbol{\theta}^{u}\) and \(\mathbf{x}_{w}\) in \(\mathcal{L}(w,u)\), we have\[\frac{\partial\mathcal{L}(w,u)}{\partial\mathbf{x}_{w}}=\big[L^{w}(u)-\sigma(\mathbf{x}_{w}^{T}\boldsymbol{\theta}^{u})\big]\boldsymbol{\theta}^{u}\]Hence, using \(\frac{\partial\mathcal{L}(w,u)}{\partial\mathbf{x}_{w}}\), we can derive the update formula of \(e(\tilde{w}),\tilde{w}\in context(w)\):\[e(\tilde{w}):=e(\tilde{w})+\eta\sum_{u\in\{w\}\cup NEG(w)}\frac{\partial\mathcal{L}(w,u)}{\partial\mathbf{x}_{w}}, \tilde{w}\in context(w)\]Below is the pseudocode of negative sampling in CBOW: &gt; 1. \(\mathbf{v}=\boldsymbol{0}\) &gt; 2. \(\mathbf{x}_{w}=\sum_{u\in context(w)}e(u)\) &gt; 3. For \(u=\{w\}\cup NEG(w)\) do : &gt; 3.1 \(q=\sigma\big(\mathbf{x}_{w}^{T}\boldsymbol{\theta}^{u}\big)\) &gt; 3.2 \(g=\eta\centerdot (L^{w}(u)-q)\) &gt; 3.3 \(\mathbf{v}:=\mathbf{v}+g\centerdot\boldsymbol{\theta}^{u}\) &gt; 3.4 \(\boldsymbol{\theta}^{u}:=\boldsymbol{\theta}^{u}+g\centerdot\mathbf{x}_{w}\) &gt; 4. For \(u\in context(w)\) do : &gt; \(e(u):=e(u)+\mathbf{v}\) And here is the java codes of negative sampling (step 1,2,4 is same as hierarchical softmax above): 12345678910111213141516171819202122232425262728/**-------------------------------3---------------------------------------*/for (int d = 0; d &lt;= config.negativeSamples; d++) &#123; int target; final int label; if (d == 0) &#123; target = huffmanNode.idx; label = 1; &#125; else &#123; nextRandom = incrementRandom(nextRandom); target = table[(int) (((nextRandom &gt;&gt; 16) % TABLE_SIZE) + TABLE_SIZE) % TABLE_SIZE]; if (target == 0) target = (int) (((nextRandom % (vocabSize - 1)) + vocabSize - 1) % (vocabSize - 1)) + 1; if (target == huffmanNode.idx) continue; label = 0; &#125; int l2 = target; /**------------------------------3.1--------------------------------------*/ double f = 0; for (int c = 0; c &lt; layer1_size; c++) f += neu1[c] * syn1neg[l2][c]; /**------------------------------3.2--------------------------------------*/ final double g; if (f &gt; MAX_EXP) g = (label - 1) * alpha; else if (f &lt; -MAX_EXP) g = (label - 0) * alpha; else g = (label - EXP_TABLE[(int) ((f + MAX_EXP) * (EXP_TABLE_SIZE / MAX_EXP / 2))]) * alpha; /**------------------------------3.3--------------------------------------*/ for (int c = 0; c &lt; layer1_size; c++) neu1e[c] += g * syn1neg[l2][c]; /**------------------------------3.4--------------------------------------*/ for (int c = 0; c &lt; layer1_size; c++) syn1neg[l2][c] += g * neu1[c];&#125; Skip-gram Model Similar to CBOW model, Skip-gram model contains three layers: input layer, projection layer and output layer. However, given a training sample \(\big(w,context(w)\big)\): 1. Input Layer: it only contain to vector \(e(w)\in\mathbb{R}^{m}\) of central word \(w\) in current sample. 2. Projection Layer: it is a identitical projection, which is redundant in Skip-gram model, since it maps \(e(w)\) from input layer to this layer directly. 3. Output Layer: Same as CBOW model. Compare to CBOW, the optimization objective of Skip-gram is to maximize the following log-likelihood function\[\mathcal{L}=\sum_{w\in\boldsymbol{\mathcal{C}}}\log P\big(context(w)|w\big)\]Samely, the key point is to construct and calculate the conditional probability function \(P\big(context(w)|w\big)\). Hierarchical Softmax For Skip-gram model, given the central word \(w\), we need to predict the words in \(context(w)\). Since Skip-gram is similar to CBOW in inference process, here we use the same notations and symbols in hierarchical softmax of CBOW. In Skip-gram, the conditional probability function \(P\big(context(w)|w\big)\) is defineed as\[P\big(context(w)|w\big)=\prod_{u\in context(w)}P(u|w)\]and using the idea of hierarchical softmax introduced in CBOW, we can rewrite the \(P(u|w)\) as\[P(u|w)=\prod_{j=2}^{l}P(d_{j}|e(w),\boldsymbol{\theta}_{j-1}^{u})\]where\[P(d_{j}|e(w),\boldsymbol{\theta}_{j-1}^{u})=\big[\sigma\big(e(w)^{T}\boldsymbol{\theta}_{j-1}^{u}\big)\big]^{1-d_{j}}\centerdot\big[1-\sigma\big(e(w)^{T}\boldsymbol{\theta}_{j-1}^{u}\big)\big]^{d_{j}}\]So we can derive the complete expression of log-likelihood target function of Skip-gram model:\[\mathcal{L}=\sum_{w\in\boldsymbol{\mathcal{C}}}\log\prod_{u\in context(w)}\prod_{j=2}^{l}\big\{\big[\sigma\big(e(w)^{T}\boldsymbol{\theta}_{j-1}^{u}\big)\big]^{1-d_{j}}\centerdot\big[1-\sigma\big(e(w)^{T}\boldsymbol{\theta}_{j-1}^{u}\big)\big]^{d_{j}}\big\}\\=\sum_{w\in\boldsymbol{\mathcal{C}}}\sum_{u\in context(w)}\sum_{j=2}^{l}\big\{(1-d_{j})\centerdot\log\big[\sigma\big(e(w)^{T}\boldsymbol{\theta}_{j-1}^{u}\big)\big]+d_{j}\centerdot\log\big[1-\sigma\big(e(w)^{T}\boldsymbol{\theta}_{j-1}^{u}\big)\big]\big\}\]Hereto, the function above is the target function of Skip-gram, then our task is to optimize it, i.e., maximize this function. In Word2Vec, the author use the Stochastic Gradient Ascent method. And the key point of gradient descent/ascent methods are to find the gradient computing formula. To make further process simple, we denote that\[\mathcal{L}(w,u,j)=(1-d_{j})\centerdot\log\big[\sigma\big(e(w)^{T}\boldsymbol{\theta}_{j-1}^{u}\big)\big]+d_{j}\centerdot\log\big[1-\sigma\big(e(w)^{T}\boldsymbol{\theta}_{j-1}^{u}\big)\big]\]First, same as CBOW, consider the gradient computation of \(\mathcal{L}(w,u,j)\) with regard to \(\boldsymbol{\theta}_{j-1}^{u}\) (the computing process is same as the related part in CBOW):\[\frac{\partial\mathcal{L}(w,u,j)}{\partial\boldsymbol{\theta}_{j-1}^{u}}=\big[1-d_{j}-\sigma\big(e(w)^{T}\boldsymbol{\theta}_{j-1}^{u}\big)\big]\centerdot e(w)\]So, the update formula of \(\boldsymbol{\theta}_{j-1}^{u}\) is\[\boldsymbol{\theta}_{j-1}^{u}:=\boldsymbol{\theta}_{j-1}^{u}+\eta\big[1-d_{j}-\sigma\big(e(w)^{T}\boldsymbol{\theta}_{j-1}^{u}\big)\big]\centerdot e(w)\]Next, we consider the gradient of \(\mathcal{L}(w,u,j)\) with regard to \(e(w)\), using the symmetry of \(e(w)\) and \(\boldsymbol{\theta}_{j-1}^{u}\) in \(\mathcal{L}(w,u,j)\), we can derive that\[\frac{\partial\mathcal{L}(w,u,j)}{\partial e(w)}=\big[1-d_{j}-\sigma\big(e(w)^{T}\boldsymbol{\theta}_{j-1}^{u}\big)\big]\centerdot \boldsymbol{\theta}_{j-1}^{u}\]Thus, the update function of \(e(w)\) is\[e(w):=e(w)+\eta\sum_{u\in context(w)}\sum_{j=2}^{l}\frac{\partial\mathcal{L}(w,u,j)}{\partial e(w)}\]Here is the pseudocode about using stochastic gradient ascent method to update the parameters in Skip-gram (note that in Word2Vec source codes, the \(e(w)\) will be updated each time when a word \(u\) in \(context(w)\) is processed, not just updated after all the words in \(context(w)\) are processed): &gt;For \(u\in context(w)\) do: &gt; 1. \(\mathbf{v}=\boldsymbol{0}\) &gt; 2. For \(j=2:l\) do: &gt; 2.1 \(q=\sigma\big(e(w)^{T}\boldsymbol{\theta}_{j-1}^{u}\big)\) &gt; 2.2 \(g=\eta (1-d_{j}-q)\) &gt; 2.3 \(\mathbf{v}:=\mathbf{v}+g\centerdot\boldsymbol{\theta}_{j-1}^{u}\) &gt; 2.4 \(\boldsymbol{\theta}_{j-1}^{u}:=\boldsymbol{\theta}_{j-1}^{u}+g\centerdot e(w)\) &gt; 3. \(e(w):=e(w)+\mathbf{v}\) Here is the corresponding relationship between pseudocode and Word2Vec source codes: \(e(w)\) is corresponding to the “syn0” in Word2Vec source codes, \(\boldsymbol{\theta}_{j-1}^{u}\) is corresponding to “syn1”, while \(\mathbf{v}\) is corresponding to “neu1e”. Below the java codes of this procedure: 12345678910111213141516171819202122232425262728for (int a = b; a &lt; window * 2 + 1 - b; a++) &#123; /**--------------------------------1--------------------------------------*/ for (int d = 0; d &lt; layer1_size; d++) neu1e[d] = 0; ... /**--------------------------------2--------------------------------------*/ for (int d = 0; d &lt; huffmanNode.code.length; d++) &#123; double f = 0; int l2 = huffmanNode.point[d]; /**------------------------------2.1--------------------------------------*/ // Propagate hidden -&gt; output for (int e = 0; e &lt; layer1_size; e++) f += syn0[l1][e] * syn1[l2][e]; if (f &lt;= -MAX_EXP || f &gt;= MAX_EXP) continue; else f = EXP_TABLE[(int)((f + MAX_EXP) * (EXP_TABLE_SIZE / MAX_EXP / 2))]; /**------------------------------2.2--------------------------------------*/ // 'g' is the gradient multiplied by the learning rate double g = (1 - huffmanNode.code[d] - f) * alpha; /**------------------------------2.3--------------------------------------*/ // Propagate errors output -&gt; hidden for (int e = 0; e &lt; layer1_size; e++) neu1e[e] += g * syn1[l2][e]; /**------------------------------2.4--------------------------------------*/ // Learn weights hidden -&gt; output for (int e = 0; e &lt; layer1_size; e++) syn1[l2][e] += g * syn0[l1][e]; &#125; ... /**--------------------------------3--------------------------------------*/ // Learn weights input -&gt; hidden for (int d = 0; d &lt; layer1_size; d++) &#123; syn0[l1][d] += neu1e[d]; &#125;&#125; Negative Sampling Since we already have the experience in negative sampling of CBOW model, for skip-gram model, the derivation process is similar. First of all, we rewrite the optimization target function \(G=\prod_{w\in\boldsymbol{\mathcal{C}}}g(w)\) to\[G=\prod_{w\in\boldsymbol{\mathcal{C}}}\prod_{u\in context(w)}g(u)\]Here, \(\prod_{u\in context(w)}g(u)\) represents a target that we want to maximize for a given sample \(\big(w, context(w)\big)\), and \(g(u)\) is similar to the \(g(w)\) in the corresponding part of CBOW, and \(g(u)\) is defined as\[g(u) =\prod_{z\in\{u\}\cup NEG(u)}P(z|w)\]where \(NEG(u)\) denotes the generated subset of negative sample when deal with word \(u\), the conditional probability \(P(z|w)\) is\[P(z|w)=\begin{cases} \sigma\big(e(w)^{T}\boldsymbol{\theta}^{z}\big), &amp; L^{u}(z)=1;\\1-\sigma\big(e(w)^{T}\boldsymbol{\theta}^{z}\big), &amp; L^{u}(z)=0.\end{cases}\]or simply, we can merge this two expressions:\[P(z|w)=\big[\sigma\big(e(w)^{T}\boldsymbol{\theta}^{z}\big)\big]^{L^{u}(z)}\centerdot\big[1-\sigma\big(e(w)^{T}\boldsymbol{\theta}^{z}\big)\big]^{1-L^{u}(z)}\]Hence, taking the logarithm of \(G\), our ultimate target function is\[\mathcal{L}=\log G=\log\prod_{w\in\boldsymbol{\mathcal{C}}}\prod_{u\in context(w)}g(u)=\sum_{w\in\boldsymbol{\mathcal{C}}}\sum_{u\in context(w)}\log g(u)\\=\sum_{w\in\boldsymbol{\mathcal{C}}}\sum_{u\in context(w)}\log\prod_{z\in\{u\}\cup NEG(u)}P(z|w)=\sum_{w\in\boldsymbol{\mathcal{C}}}\sum_{u\in context(w)}\sum_{z\in\{u\}\cup NEG(u)}\log P(z|w)\\=\sum_{w\in\boldsymbol{\mathcal{C}}}\sum_{u\in context(w)}\sum_{z\in\{u\}\cup NEG(u)}\big\{L^{u}(z)\centerdot\log\big[\sigma\big(e(w)^{T}\boldsymbol{\theta}^{z}\big)\big]+\big[1-L^{u}(z)\big]\centerdot\log\big[1-\sigma\big(e(w)^{T}\boldsymbol{\theta}^{z}\big)\big]\big\}\]It is worth to mention that the negative sampling based Skip-gram model in Word2Vec source codes is not coding based on the formula above. Since, if it is based on the formula above, then for every sample \(\big(w, context(w)\big)\), the Word2Vec needs to do the negative sampling for every word in \(context(w)\), however, it only do \(|context(w)|\) times negative sampling for \(w\). Here gives some thinking about this issue: the essence of Skip-gram is still using the CBOW model, it just change the summation of vectors of \(context(w)\) to take them into consideration one by one. In this case, for a given sample \(\big(w, context(w)\big)\), we want to maximize\[g(w)=\prod_{\tilde{w}\in context(w)}\prod_{u\in\{w\}\cup NEG^{\tilde{w}}(w)}P(u|\tilde{w})\]where\[P(u|\tilde{w})=\begin{cases}\sigma\big(e(\tilde{w})^{T}\boldsymbol{\theta}^{u}\big), &amp; L^{w}(u)=1;\\1-\sigma\big(e(\tilde{w})^{T}\boldsymbol{\theta}^{u}\big), &amp; L^{w}(u)=0.\end{cases}\]or it can be written as\[P(u|\tilde{w})=\big[\sigma\big(e(\tilde{w})^{T}\boldsymbol{\theta}^{u}\big)\big]^{L^{w}(u)}\centerdot\big[1-\sigma\big(e(\tilde{w})^{T}\boldsymbol{\theta}^{u}\big)\big]^{1-L^{w}(u)}\]here \(NEG^{\tilde{w}}(w)\) represents the subset of negative sample when dealing with word \(\tilde{w}\). Thereoupon, for a given corpus \(\boldsymbol{\mathcal{C}}\), function \(G=\prod_{w\in\boldsymbol{\mathcal{C}}}g(w)\) can be treated as an overall optimization objective. Similarily, taking the logarithm of \(G\), our ultimate objective function is\[\mathcal{L}=\log G=\log\prod_{w\in\boldsymbol{\mathcal{C}}}g(w)=\sum_{w\in\boldsymbol{\mathcal{C}}}g(w)\\=\sum_{w\in\boldsymbol{\mathcal{C}}}\sum_{\tilde{w}\in context(w)}\sum_{u\in\{w\}\cup NEG^{\tilde{w}}(w)}\big\{L^{w}(u)\centerdot\log\big[\sigma\big(e(\tilde{w})^{T}\boldsymbol{\theta}^{u}\big)\big]+\big[1-L^{w}(u)\big]\centerdot\log\big[1-\sigma\big(e(\tilde{w})^{T}\boldsymbol{\theta}^{u}\big)\big]\big\}\]To make it simple, we denotes \(\mathcal{L}(w,\tilde{w},u)\):\[\mathcal{L}(w,\tilde{w},u)=L^{w}(u)\centerdot\log\big[\sigma\big(e(\tilde{w})^{T}\boldsymbol{\theta}^{u}\big)\big]+\big[1-L^{w}(u)\big]\centerdot\log\big[1-\sigma\big(e(\tilde{w})^{T}\boldsymbol{\theta}^{u}\big)\big]\]Then we use Stochastic Gradient Ascent method to optimize this objective function. First, consider the gradient computation of \(\mathcal{L}(w,\tilde{w},u)\) with regard to \(\boldsymbol{\theta}^{u}\):\[\frac{\partial\mathcal{L}(w,\tilde{w},u)}{\partial\boldsymbol{\theta}^{u}}=\big[L^{w}(u)-\sigma\big(e(\tilde{w})^{T}\boldsymbol{\theta}^{u}\big)\big]e(\tilde{w})\]thus, the update formula of \(\boldsymbol{\theta}^{u}\) can be written as\[\boldsymbol{\theta}^{u}:=\boldsymbol{\theta}^{u}+\eta\big[L^{w}(u)-\sigma\big(e(\tilde{w})^{T}\boldsymbol{\theta}^{u}\big)\big]e(\tilde{w})\]Then, consider the gradient of \(\mathcal{L}(w,\tilde{w},u)\) with regard to \(e(\tilde{w})\), using the symmetry of \(\boldsymbol{\theta}^{u}\) and \(e(\tilde{w})\) in \(\mathcal{L}(w,\tilde{w},u)\), then we have\[\frac{\partial\mathcal{L}(w,\tilde{w},u)}{\partial e(\tilde{w})}=\big[L^{w}(u)-\sigma\big(e(\tilde{w})^{T}\boldsymbol{\theta}^{u}\big)\big]\boldsymbol{\theta}^{u}\]hence, the update formula of \(e(\tilde{w})\) can be written as\[e(\tilde{w}):=e(\tilde{w})+\eta\sum_{u\in\{w\}\cup NEG^{\tilde{w}}(w)}\frac{\partial\mathcal{L}(w,\tilde{w},u)}{\partial e(\tilde{w})}\]Here we give the pseudocode of using stochastic gradient ascent model to update the parameters in negative sampling based Skip-gram model. &gt;For \(\tilde{w}\in context(w)\) do: &gt; 1. \(\mathbf{v}=\boldsymbol{0}\) &gt; 2. For \(u=\{w\}\cup NEG^{\tilde{w}}(w)\) do: &gt; 2.1 \(q=\sigma\big(e(\tilde{w})^{T}\boldsymbol{\theta}^{u}\big)\) &gt; 2.2 \(g=\eta (L^{w}(u)-q)\) &gt; 2.3 \(\mathbf{v}:=\mathbf{v}+g\centerdot\boldsymbol{\theta}^{u}\) &gt; 2.4 \(\boldsymbol{\theta}^{u}:=\boldsymbol{\theta}^{u}+g\centerdot e(\tilde{w})\) &gt; 3. \(e(\tilde{w}):=e(\tilde{w})+\mathbf{v}\) Below the java codes of this procedure: 12345678910111213141516171819202122232425262728/**--------------------------------3--------------------------------------*/for (int d = 0; d &lt;= config.negativeSamples; d++) &#123; int target; final int label; if (d == 0) &#123; target = huffmanNode.idx; label = 1; &#125; else &#123; nextRandom = incrementRandom(nextRandom); target = table[(int) (((nextRandom &gt;&gt; 16) % TABLE_SIZE) + TABLE_SIZE) % TABLE_SIZE]; if (target == 0) target = (int) (((nextRandom % (vocabSize - 1)) + vocabSize - 1) % (vocabSize - 1)) + 1; if (target == huffmanNode.idx) continue; label = 0; &#125; int l2 = target; /**------------------------------3.1--------------------------------------*/ double f = 0; for (int c = 0; c &lt; layer1_size; c++) f += neu1[c] * syn1neg[l2][c]; final double g; /**------------------------------3.2--------------------------------------*/ if (f &gt; MAX_EXP) g = (label - 1) * alpha; else if (f &lt; -MAX_EXP) g = (label - 0) * alpha; else g = (label - EXP_TABLE[(int) ((f + MAX_EXP) * (EXP_TABLE_SIZE / MAX_EXP / 2))]) * alpha; /**------------------------------3.3--------------------------------------*/ for (int c = 0; c &lt; layer1_size; c++) neu1e[c] += g * syn1neg[l2][c]; /**------------------------------3.4--------------------------------------*/ for (int c = 0; c &lt; layer1_size; c++) syn1neg[l2][c] += g * neu1[c];&#125; DL4J Implementation Nowadays, there are lots of toolkit can be used to construct Word2Vec framework, like gensim for Python, TensorFlow for Python, google Word2Vec open source for C(++), deeplearning4j for Scala and Java and etc. In this case, you may not need to implement Word2Vec by writting all the codes youself, what you need to do is using these packages to build your Word2Vec framework effectively. Here is an sample codes of using deeplearning4j to implement Word2Vec 123456789101112131415161718192021// wikinotitle is dumped from wikipedia, which is around 13GB, here I just put a 66.6MB sample dataString filePath = new ClassPathResource("wikinotitle").getFile().getAbsolutePath();log.info("Load &amp; Vectorize Sentences....");// Strip white space before and after for each lineSentenceIterator iter = new BasicLineIterator(filePath);// Split on white spaces in the line to get wordsTokenizerFactory t = new DefaultTokenizerFactory();// CommonPreprocessor will apply the following regex to each token: [\d\.:,"'\(\)\[\]|/?!;]+// So, effectively all numbers, punctuation symbols and some special symbols are stripped off.// Additionally it forces lower case for all tokens.t.setTokenPreProcessor(new CommonPreprocessor());log.info("Building model....");// for small corpus, load --&gt; w2vBuilder4SmallCorpus(iter, t);Word2Vec vec = w2vBuilder(iter, t);log.info("Fitting Word2Vec model....");vec.fit();log.info("done.");// Write word vectors to filelog.info("Writing word vectors to file....");WordVectorSerializer.writeWord2VecModel(vec, "src/main/resources/wiki-model");log.info("done."); The full codes you can get from my GitHub repository: Word2VecExample, or directly go to the Deeplearning4j offical website: [link] Reference Word2Vec – Wikipedia Distributed Representations of Words and Phrases and their Compositionality word2vec Explained: Deriving Mikolov et al.’s Negative-Sampling Word-Embedding Method word2vec Parameter Learning Explained Efficient Estimation of Word Representations in Vector Space Explanation of Mathematical Principles in Word2Vec (Chinese) Word and Document Embeddings based on Neural Network Approaches (Chinese) Word2VecJava Word2Vec Source]]></content>
      <categories>
        <category>Natural Language Processing</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
        <tag>java</tag>
        <tag>deeplearning4j</tag>
        <tag>natural language processing</tag>
        <tag>word embeddings</tag>
        <tag>skip-gram</tag>
        <tag>logistic regression</tag>
        <tag>bag-of-words</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习(周志华)--学习笔记(二) 线性回归(Linear Regression)]]></title>
    <url>%2F2017%2F05%2F04%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%91%A8%E5%BF%97%E5%8D%8E-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-2%2F</url>
    <content type="text"><![CDATA[第三章–线性模型 (linear regression) 线性模型 给定由\(d\)个属性描述的示例\(\mathbf{x}=(x_{1};x_{2};\dots ;x_{d})\)，其中\(x_{i}\)是\(\mathbf{x}\)在第\(i\)个属性上的取值，线性模型(linear model)试图学的一个通过属性的线性组合来进行预测的函数，即\[f(\mathbf{x})=\mathbf{w}^{T}\mathbf{x}+b\]其中\(\mathbf{w}=(w_{1};w_{2};\dots ;w_{d})\)。\(\mathbf{w}\)和\(b\)学得之后，模型就可以确定。 线性回归 给定数据集\(D={(\mathbf{x}_{1},y_{1}),(\mathbf{x}_{2},y_{2}),\dots ,(\mathbf{x}_{m},y_{m})}\)，其中\(\mathbf{x}_{i}=(x_{i1};x_{i2};\dots ;x_{id}), y_{i}\in \mathbb{R}\)。“线性回归” (linear regression)试图学得\[f(\mathbf{x}_{i})=\mathbf{w}^{T}\mathbf{x}_{i}+b, 使得f(\mathbf{x}_{i})\simeq y_{i}\]接下来的任务是确定\(\mathbf{w}\)和\(b\)，其关键在于衡量\(f(x)\)和\(y\)之间的差别，而均方误差是回归任务中最常用的性能度量，它对应了常用的“欧式距离” (Euclidean distance)，因此可以试图让均方误差最小化。均方误差最小化进行模型求解的方法称为“最小二乘法” (least square method)。在线性回归中，最小二乘法就是试图找到一条直线，使所有样本到直线上的欧式距离之和最小。这里，我们将\(\mathbf{w}\)和\(b\)吸收入向量形式\(\mathbf{\hat{w}}=(b;\mathbf{w})=(b;w_{1};w_{2};\dots ;w_{d})\)，相应的，将数据集\(D\)中的\(\mathbf{x}_{i}\)表示为\(\mathbf{x}_{i}=(1;x_{i1};x_{i2};\dots ;x_{id})\)，因此数据集\(D\)可以表示为一个\(m\times (d+1)\)的矩阵\(\mathbf{X}=(\mathbf{x}_{1};\mathbf{x}_{2};\dots ;\mathbf{x}_{m})^{T}\)，再把标记也写成向量形式\(\mathbf{y}=(y_{1};y_{2};\dots ;y_{m})\)。于是有\[f(\mathbf{X})=\mathbf{X}\mathbf{\hat{w}}, 使得f(\mathbf{X})\simeq\mathbf{y}\]最小化均方误差，有\[\mathbf{\hat{w}}^{*}=\arg\min_{\mathbf{\hat{w}}}(f(\mathbf{X})-\mathbf{y})^{2}=\arg\min_{\mathbf{\hat{w}}}(\mathbf{y}-\mathbf{X}\mathbf{\hat{w}})^{T}(\mathbf{y}-\mathbf{X}\mathbf{\hat{w}})\]令\(E_{\mathbf{\hat{w}}}=(\mathbf{y}-\mathbf{X}\mathbf{\hat{w}})^{T}(\mathbf{y}-\mathbf{X}\mathbf{\hat{w}})\)，对\(\mathbf{\hat{w}}\)求导得到\[\frac{\partial E_{\mathbf{\hat{w}}}}{\partial \mathbf{\hat{w}}}=2\mathbf{X}^{T}(\mathbf{X}\mathbf{\hat{w}}-\mathbf{y})\]令上式为零可得\(\mathbf{\hat{w}}\)最优解的封闭式。当\(\mathbf{X}^{T}\mathbf{X}\)为满秩矩阵 (full-rank matrix)或正定矩阵 (positive definite matrix)时，可得到\[\mathbf{\hat{w}}^{*}=(\mathbf{X}^{T}\mathbf{X})^{-1}\mathbf{X}^{T}\mathbf{y}\]其中\((\mathbf{X}^{T}\mathbf{X})^{-1}\)是\(\mathbf{X}^{T}\mathbf{X}\)的逆矩阵。然而现实任务中，\(\mathbf{X}^{T}\mathbf{X}\)往往不是满秩矩阵，此时可解出多个\(\mathbf{\hat{w}}\)，都能使均方误差最小化，而选择哪一个解作为输出将由算法的归纳偏好决定，常见的做法是引入正则化(regularization)项。 线性模型预测值不仅可以逼近\(y\)，也可以使其逼近\(y\)的衍生物。比如，可将输出标记的对数作为线性模型逼近的目标，即“对数线性回归” (log-linear regression)\[\ln y=\mathbf{w}^{T}\mathbf{x}+b\]它实际上是让\(e^{\mathbf{w}^{T}\mathbf{x}+b}\)逼近\(y\)，形式上它仍是线性回归，但实质上已是在求取输入空间到输出空间的非线性函数映射，如图 更一般地，考虑单调可微函数\(g(\centerdot)\)，令\[y=g^{-1}(\mathbf{w}^{T}\mathbf{x}+b)\]这样得到的模型称为“广义线性模型” (generalized linear model)，其中函数\(g(\centerdot)\)称为“联系函数” (link function)。对数线性回归是广义线性模型在\(g(\centerdot)=\ln (\centerdot)\)时的特例。 对数几率回归 将线性回归应用到分类任务的方法: 找到一个单调可微函数将分类任务的真实标记\(y\)与线性回归模型的预测值联系起来。例如，对于二分类任务，\(y\in {0,1}\)，使用“单位阶跃函数” (unit-step/Heaviside function)可将模型预测值转换为0/1值。但是单位阶跃函数不连续，于是使用近似单位阶跃函数的替代函数，如使用“对数机率函数”\[y=\frac{1}{1+e^{-z}}\]它是一种“Sigmoid函数”，用对数机率函数作为\(g^{-1}\)得到\[y=\frac{1}{1+e^{-(\mathbf{w}^{T}\mathbf{x}+b)}}\]变换得\[\ln \frac{y}{1-y}=\mathbf{w}^{T}\mathbf{x}+b\]这种模型称为“对数机率回归” (logistic regression)。虽然它的名字是“回归”但实际上是一种分类学习方法。对数函数是任意阶可导的凸函数，许多数值优化算法均可以求解其最优解。 重写上式，有:\[\ln\frac{p(y=1|\mathbf{x})}{p(y=0|\mathbf{x})}=\boldsymbol{\mathcal{w}}^{T}\mathbf{x}+b\]其中:\[p(y=1|\mathbf{x})=\frac{e^{(\mathbf{w}^{T}\mathbf{x}+b)}}{1+e^{(\mathbf{w}^{T}\mathbf{x}+b)}}\]\[p(y=0|\mathbf{x})=\frac{1}{1+e^{(\mathbf{w}^{T}\mathbf{x}+b)}}\]于是，可通过“极大似然法”(maximum likelihood method)来估计\(\boldsymbol{\mathcal{w}}\)和\(b\)。给定数据集\(\{(\mathbf{x}_{i},y_{i})\}_{i=1}^{m}\)，对数回归模型最大化“对数似然”(log-likelihood):\[\mathcal{l}(\boldsymbol{\mathcal{w}},b)=\sum_{i=1}^{m}\ln p(y_{i}|\mathbf{x}_{i};\boldsymbol{\mathcal{w}},b)\]即令每个样本属于其真实标记的概率越大越好。这里令\(\boldsymbol{\beta}=(\boldsymbol{\mathcal{w}};b)\)，\(\hat{\mathbf{x}}=(\mathbf{x};1)\)，则\(\boldsymbol{\mathcal{w}}^{T}\mathbf{x}+b\)简写为\(\boldsymbol{\beta}^{T}\hat{\mathbf{x}}\)。再令\(p_{1}(\hat{\mathbf{x}};\boldsymbol{\beta})=p(y=1|\hat{\mathbf{x}};\boldsymbol{\beta})\)，\(p_{0}(\hat{\mathbf{x}};\boldsymbol{\beta})=p(y=0|\hat{\mathbf{x}};\boldsymbol{\beta})=1-p_{1}(\hat{\mathbf{x}};\boldsymbol{\beta})\)，则:\[p(y_{i}|\mathbf{x}_{i};\boldsymbol{\mathcal{w}},b)=y_{i}p_{1}(\hat{\mathbf{x}}_{i};\boldsymbol{\beta})+(1-y_{i})p_{0}(\hat{\mathbf{x}}_{i};\boldsymbol{\beta})\]该问题可转化为最小化:\[\mathcal{l}(\boldsymbol{\beta})=\sum_{i=1}^{m}\bigg(-y_{i}\boldsymbol{\beta}^{T}\hat{\mathbf{x}}_{i}+\ln\big(1+e^{\boldsymbol{\beta}^{T}\hat{\mathbf{x}}_{i}}\big)\bigg)\]上式为关于\(\boldsymbol{\beta}\)的高阶可导连续凸函数，根据凸优化理论，经典的数值优化方法如“梯度下降法” (gradient descent method)、“牛顿法” (Newton method)等都可以求得其最优解，于是有:\[\boldsymbol{\beta}^{*}=\arg\min_{\boldsymbol{\beta}}\mathcal{l}(\boldsymbol{\beta})\]以牛顿法为例，其第\(t+1\)轮迭代解的更新公式为:\[\boldsymbol{\beta}^{t+1}=\boldsymbol{\beta}^{t}-\bigg(\frac{\partial^{2}\mathcal{l}(\boldsymbol{\beta})}{\partial\boldsymbol{\beta}\partial\boldsymbol{\beta}^{T}}\bigg)^{-1}\frac{\partial\mathcal{l}(\boldsymbol{\beta})}{\partial\boldsymbol{\beta}}\]其中关于\(\boldsymbol{\beta}\)的一阶、二阶导数分别为:\[\frac{\partial\mathcal{l}(\boldsymbol{\beta})}{\partial\boldsymbol{\beta}}=-\sum_{i=1}^{m}\hat{\mathbf{x}}_{i}(y_{i}-p_{1}(\hat{\mathbf{x}}_{i};\boldsymbol{\beta}))\]\[\frac{\partial^{2}\mathcal{l}(\boldsymbol{\beta})}{\partial\boldsymbol{\beta}\partial\boldsymbol{\beta}^{T}}=\sum_{i=1}^{m}\hat{\mathbf{x}}_{i}\hat{\mathbf{x}}_{i}^{T}p_{1}(\hat{\mathbf{x}}_{i};\boldsymbol{\beta})(1-p_{1}(\hat{\mathbf{x}}_{i};\boldsymbol{\beta}))\] 线性判别分析 (LDA) 线性判别分析 (Linear Discriminant Analysis, LDA)是一种经典的线性学习方法，其思想非常朴素: 给定训练样例集，设法将样例投影到一条直线上，使得同类样例的投影点尽可能接近、异类样例的投影点尽可能的远离；在对新样本进行分类时，将其投影到同样的这条直线上，再根据投影点的位置来判断新样本的类别。如图为二分类例子 其中“+”、“-”分别表示正例和反例，椭圆表示数据簇的外轮廓，虚线表示投影，红色实心圆盒实心三角形分别表示两类样本投影后的中心点。(略) 多分类学习任务 对于多分类学习，一些二分类学习方法可以直接应用于多分类任务，但更多情况下，需要将多分类任务进行“拆解”，再使用二分类学习器来解决。考虑\(N\)个类别\(C_{1},C_{2},\dots,C_{N}\)，“拆解法”将多分类任务拆为若干个二分类任务， 然后为每个二分类任务训练一个分类器，测试时将这些分类器预测结果进行集成从而获得最终的结果。最经典的拆分策略有三种: “一对一”(One v.s. One, OvO)、“一对其余”(One v.s. Rest, OvR)、“多对多”(Many v.s. Many, MvM)。 给定数据集\(D=\{(x_{1},y_{1}),(x_{2},y_{2}),\dots,(x_{m},y_{m})\},y_{i}\in\{C_{1},C_{2},\dots,C_{N}\}\): - OvO将\(N\)个类别两两配对产生\(\frac{N\centerdot(N-1)}{2}\)个二分类任务，例如，OvO中区分\(C_{i}\)和\(C_{j}\)的一个分类器，该分类器把\(D\)中的\(C_{i}\)类样例作为正例，\(C_{j}\)类样例作为负例。测试阶段，新样本同时提交给所有分类器，然后得到\(\frac{N\centerdot(N-1)}{2}\)个分类结果，最终结果投票产生 (多者胜)。 - OvR则是每次将一个类的样例作为正例，其余样例作为负例来训练\(N\)个分类器。测试时，若仅有一个分类器预测为正类，则对应的类别标记作为最终分类结果；若多个分类器预测为正类，则根据分类器的预测置信度，选择置信度最大的类别标记为分类结果。 - MvM是每次将若干个类作为正类，其余作为反类。显然OvO和OvR是MvM的特例。MvM的正、负类构造需要有特殊设计，不能随意选择，“纠错输出码”(Error Correcting Output Codes, ECOC)是最常用的MvM技术。 比较OvO和OvR，OvR只需要\(N\)个分类器，而OvO需要\(\frac{N\centerdot(N-1)}{2}\)个分类器，因此OvO的存储开销和测试时间开销通常比OvR更大。但训练时，OvR的每个分类器都需要使用全部的样例，而OvO的每个分类器之需要两个类的样例，因此OvO的训练时间开销通常小于OvR。而预测性能， 则取决于具体的数据分布，多数情况下两者性能相近。 这里介绍ECOC技术，ECOC是将编码的思想引入类别拆分，并尽可能在解码过程中具有容错性。其主要分两步: 1. 编码: 对\(N\)个类别做\(M\)次划分，每次划分将一部分类别划为正类，一部分划为负类，从而形成一个二分类训练集，这样一共产生\(M\)个训练集，可训练出\(M\)个分类器。 2. 解码: \(M\)个分类器分别对测试样本进行预测，这些预测标记组成一个编码。将这个预测编码与每个列别各自的编码进行比较，返回其中距离最小的类别作为最终预测结果。 类别划分通过“编码矩阵”(coding matrix)指定。常见的编码矩阵有二元编码和三元编码。前者将每个类别划分为正类的负类，后者在正、负类之外，还可指定“停用类”。如图所示 (a)中分类器\(f_{2}\)将\(C_{1}\)类和\(C_{3}\)类的样例作为正例，\(C_{2}\)类和\(C_{4}\)类的样例作为负例。(b)中分类器\(f_{4}\)将\(C_{1}\)类和\(C_{4}\)类作为正例，\(C_{3}\)类作为负例。在解码阶段，各分类器的预测结果联合起来形成了测试示例的编码，该编码与各类对应的编码进行比较，将距离最小的编码所对应的类别作为预测结果。距离计算一般采用采用欧式距离(Euclidean distance)或海明距离(Hamming distance)。如(a)中，若基于欧式距离，预测结果为\(C_{3}\)。 该方法之所以称为“纠错输出码”，是因为在测试阶段，ECOC编码对分类的错误有一定的容忍和修正能力。例如在(a)中对测试示例的正确预测编码是\((-1,+1,+1,-1,+1)\)，假设在预测时某个分类器出错了，例如\(f_{2}\)出错从而导致了错误编码\((-1,-1,+1,-1,+1)\)，但居于这个编码仍能产生正确的最终分类结果\(C_{3}\)。一般来说，对同一个学习任务，ECOCO编码越长，纠错能力越强。但是，编码越长，意味着需要训练的分类器越多，计算和存储开销都会增大。此外，对有限类别数，可能的组合数目有限，码长超过一定范围后就失去了意义。 对同等长度的编码，任意两个类别间的编码距离越远，则纠错能力越强。因此，在码长较小时可根据这个原则计算出理论最优编码。然而，码长稍大一些就难以有效地确定最优编码，事实上这是一个NP难问题。不过，通常我们不需获得理论最优编码，因为非最优编码在实践中往往已能产生足够好的分类器。另一方面，并不是编码的理论性质越好，分类器性能就越好，因为机器学习问题涉及很多因素，例如将多个类拆解为两个“类别子集”，不同拆解方法所形成的连个类别自己的区分难度往往不同，及其导致的二分类问题的难度不同。于是，一个理论纠错性质好，但导致二分类问题较难的编码，与另一个理论纠错性质差一些，但导致的二分类问题较简单的编码，最终产生的模型性能孰强孰弱很难说。 类别不平衡问题 类别不平衡(class-imbalance)指分类任务中不同类别的训练样例数目差别很大的情况。这样的情况很常见，例如用拆分解决多分类问题时，即使原数据集中不同类别的训练样例数相当，在使用OvR、MvM策略后产生的二分类任务仍有可能出现类别不平衡现象。从线性分类器的角度，用\(y=\boldsymbol{\mathcal{w}}^{T}\mathbf{x}+b\)对新样本\(\mathbf{x}\)进行分类时，事实上是在用预测出的\(y\)值与一个阈值进行比较，通常\(y&gt;0.5\)判别为正例，否则为负例。\(y\)实际上表达了正例的可能性，几率\(\frac{y}{1-y}\)反映了正例和负例可能性的比值，阈值设置\(0.5\)表明分类器认为正、负；例可能性高相同，即分类器决策规则为：若\(\frac{y}{1-y}&gt;1\)则预测为正例。 但是，当正反例数目不同时，令\(m^{+}\)表示正例数目，\(m^{-}\)表示负例数目，则观测几率为\(\frac{m^{+}}{m^{-}}\)，假设训练集时真实样本总体的无偏采样，因为观测几率代表例真实几率，所以只要分类器预测几率高于观测几率就因该判定为正例：若\(\frac{y}{1-y}&gt;\frac{m^{+}}{m^{-}}\)则预测为正例。然而，分类器通常基于阈值\(0.5\)进行决策，因此需要对预测值进行调整，令\(\frac{y&#39;}{1-y&#39;}=\frac{y}{1-y}\times\frac{m^{-}}{m^{+}}\)，这是类别不平衡学习的一个基本策略——“再缩放”(rescaling, or rebalance)。 再缩放思想虽然简单，但实际操作中，之前假设的“训练集是真实样本总体的无偏采样”往往不成立，即我们未必能有效地基于训练集观测几率来推断真实几率。现有技术大体上有三类做法 (假设负例样本数目大于正例样本数目): 1. 直接对训练集例的反例样本进行“欠采样”(undersampling)，即去除部分反例，使得正、负例数目接近，然后再进行学习。 2. 对训练集例正例样本进行“过采样”(oversampling)，即增加一些正例，使得正、负例数目接近，然后再进行学习。 3. 直接给予原始训练集进行学习，但在用训练好的分类器进行预测时，将\(\frac{y&#39;}{1-y&#39;}=\frac{y}{1-y}\times\frac{m^{-}}{m^{+}}\)嵌入到决策过程中，称为“阈值移动”(threshold-moving)。 欠采样的时间开销远小于过采样，因为前者丢弃了很多负例，使得分类器徐连击远小于初始训练集，而过采样法增加了很多正例，其训练集大于初始训练集。需要注意的是: - 过采样法不能简单的对初始正例样本进行重复采样，否则会导致严重的过拟合。过采样法的代表性算法SMOTE是通过对训练集里的正例进行插值来产生额外的正例。 - 欠采样法若随机丢弃负例，可能会丢失一些重要信息。欠采样法的代表性算法EasyEnsemble则是利用集成学习机制，将负例划分为若干个集合供不同的学习器使用，这样对每个学习器都进行了欠采样，但在全局来看却不会丢失重要信息。 值得一提的是，“再缩放”也是“代价敏感学习”(cost-sensitive learning)的基础。在代价敏感学习中将\(\frac{y&#39;}{1-y&#39;}=\frac{y}{1-y}\times\frac{m^{-}}{m^{+}}\)中的\(\frac{m^{-}}{m^{+}}\)用\(\frac{cost^{+}}{cost^{-}}\)代替即可，其中\(cost^{+}\)是将正例误分为负例的代价，\(cost^{-}\)是将负例误分为正例的代价。 习题 3.1 试分析在什么情况下\(y=w^{T}x+b\)中不必考虑偏置项\(b\)。 Ans: 对于线性模型\(y=w^{T}x+b\)，第\(i\)个实例减去第一个实例可得\(y_{i}-y_{0}=w^{T}(x_{i}-x_{0})\)，这里偏置项被消除，因此，可以对于每一个样本数据，均和第一个样本相减，然后对新的样本进行线性回归，就只需要使用模型\(y=w^{T}x\)。 3.2 试证明，对于参数\(\boldsymbol{\mathcal{w}}\)，对数几率回归的目标函数\[y=\frac{1}{1+e^{-(w^{T}x+b)}}\tag{1}\]是非凸的，但是对数似然函数\[\mathcal{l}(\beta)=\sum_{i=1}^{m}\bigg(-y_{i}\beta^{T}\hat{x}_{i}+\ln\big(1+e^{\beta^{T}\hat{x}_{i}}\big)\bigg)\tag{2}\]是凸的。 Ans: 如果一个多元函数是凸的，那么它的Hessian矩阵是半正定的。对(1)关于\(w\)求偏导:\[\frac{\partial y}{\partial w}=x(y-y^{2})\]并对上式求关于\(w^{T}\)的偏导有：\[\frac{\partial}{\partial w^{T}}\big(\frac{\partial y}{\partial w}\big)=x(1-2y)\big(\frac{\partial y}{\partial w}\big)^{T}=xx^{T}y(y-1)(1-2y)\]这里，\(xx^{T}\)合同为单位矩阵，所以\(xx^{T}\)是半正定的。而\(y\in(0,1)\)，当\(y\in{0.5,1}\)时，\(y(y-1)(1-2y)&lt;0\)，使得\(\frac{\partial}{\partial w^{T}}\big(\frac{\partial y}{\partial w}\big)\)为半负定的，因此\(y=\frac{1}{1+e^{-(w^{T}x+b)}}\)为非凸的。 对于(2)，有\[\frac{\partial}{\partial\beta^{T}}\big(\frac{\partial\mathcal{l}(\beta)}{\partial\beta}\big)=\sum_{i=1}^{m}\hat{x}_{i}\hat{x}_{i}^{T}\mathcal{p}_{1}(\hat{x}_{i};\beta)(1-\mathcal{p}_{1}(\hat{x}_{i};\beta))\]显然\(\mathcal{p}_{1}\in(0,1)\)，且\(\mathcal{p}_{1}(\hat{x}_{i};\beta)(1-\mathcal{p}_{1}(\hat{x}_{i};\beta))\geq0\)，因此\(\mathcal{l}(\beta)\)是凸的。 3.3 编程实现对率回归，并给出西瓜数据集\(3.0\alpha\)上的数据结果。 Id Density Sugar Content Good Melon 1 0.697 0.460 yes 2 0.774 0.376 yes 3 0.634 0.264 yes 4 0.608 0.318 yes 5 0.556 0.215 yes 6 0.403 0.237 yes 7 0.481 0.149 yes 8 0.437 0.211 yes 9 0.666 0.091 no 10 0.243 0.267 no 11 0.245 0.057 no 12 0.343 0.099 no 13 0.639 0.161 no 14 0.657 0.198 no 15 0.360 0.370 no 16 0.593 0.042 no 17 0.719 0.103 no Ans: 西瓜数据集\(3.0\alpha\)的数据量太小，这里我没用采用，而是使用UCI数据集的Iris数据集作为对率回归的训练和测试集。算法包使用Spark MLlib，代码由Scala实现: 123456// data preview5.1,3.5,1.4,0.2,Iris-setosa4.9,3.0,1.4,0.2,Iris-setosa4.7,3.2,1.3,0.2,Iris-setosa4.6,3.1,1.5,0.2,Iris-setosa5.0,3.6,1.4,0.2,Iris-setosa Spark MLlib实现对率回归: 1234567891011121314151617181920212223242526272829303132333435object LogisticRegressionIris &#123; case class Iris (features: Vector, classes: Double) def parsingIris (str: String): Iris = &#123; val fields = str.split(",") assert(fields.size == 5) val label = fields(4) match &#123; case "Iris-setosa" =&gt; 0.0 case "Iris-versicolor" =&gt; 1.0 case "Iris-virginica" =&gt; 2.0 &#125; Iris(Vectors.dense(fields(0).toDouble, fields(1).toDouble, fields(2).toDouble, fields(3).toDouble), label) &#125; def main (args: Array[String]): Unit = &#123; Logger.getLogger("org").setLevel(Level.OFF) // close logger /** create spark session */ val spark = SparkSession.builder().master("local").appName("LogisticRegressionIris").getOrCreate() import spark.implicits._ val iris = spark.read.textFile(new ClassPathResource("iris.txt").getFile.getAbsolutePath) // load data and transform to dataframe .map(parsingIris).toDF().select("features", "classes") val Array(training, test) = iris.randomSplit(Array(0.8, 0.2)) // 0.8 for training, 0.2 for testing val lr = new LogisticRegression() // logistic regression model .setMaxIter(100) .setRegParam(0.01) .setFamily("multinomial") .setFeaturesCol("features") .setLabelCol("classes") val model = lr.fit(training) // fitting dataset val predictions = model.transform(test) // predict predictions.select("classes", "prediction").collect().foreach(e =&gt; println(e.get(0) + "\t" + e.get(1))) val evaluator = new RegressionEvaluator().setMetricName("rmse").setLabelCol("classes").setPredictionCol("prediction") val rmse = evaluator.evaluate(predictions) println(s"Root-mean-square error = $rmse") spark.stop() // stop spark &#125;&#125; 3.4 选择两个UCI数据集，比较10折交叉验证法和留一法所估计出的对率回归的错误率。 Ans: 这里只选用一个UCI数据集Iris进行实验。留一法思想在3.3题中已经体现(80% for training, 20% for testing)，这里只做10折交叉验证: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748object LogisticRegressionIrisCV &#123; case class Iris (features: Vector, classes: Double) def parsingIris (str: String): Iris = &#123; val fields = str.split(",") assert(fields.size == 5) val label = fields(4) match &#123; case "Iris-setosa" =&gt; 0.0 case "Iris-versicolor" =&gt; 1.0 case "Iris-virginica" =&gt; 2.0 &#125; Iris(Vectors.dense(fields(0).toDouble, fields(1).toDouble, fields(2).toDouble, fields(3).toDouble), label) &#125; def main (args: Array[String]): Unit = &#123; Logger.getLogger("org").setLevel(Level.OFF) // close logger /** create spark session */ val spark = SparkSession.builder().master("local").appName("LogisticRegressionIris").getOrCreate() import spark.implicits._ /** load data and transform to dataframe */ val iris = spark.read.textFile(new ClassPathResource("iris.txt").getFile.getAbsolutePath) .map(parsingIris).toDF().select("features", "classes") val Array(training, test) = iris.randomSplit(Array(0.9, 0.1)) // 90% for training, 10% for testing val lr = new LogisticRegression() // build logistic regression model .setMaxIter(20) .setFamily("multinomial") .setFeaturesCol("features") .setLabelCol("classes") val pipeline = new Pipeline().setStages(Array(lr)) // build pipeline val paramGrid = new ParamGridBuilder() // set parameters map .addGrid(lr.regParam, Array(0.01, 0.02, 0.05, 0.1, 0.2)) .build() val cv = new CrossValidator() // build cross validator for logistic regression .setEstimator(pipeline) .setEvaluator(new RegressionEvaluator() .setLabelCol("classes") .setPredictionCol("prediction") .setMetricName("rmse")) .setEstimatorParamMaps(paramGrid) .setNumFolds(10) val cvModel = cv.fit(training) // training val predictions = cvModel.transform(test) // prediction val evaluator = new RegressionEvaluator() // evaluating .setMetricName("rmse") .setLabelCol("classes") .setPredictionCol("prediction") val rmse = evaluator.evaluate(predictions) println(s"Root-mean-square error = $rmse") &#125;&#125; 3.5 编程实现线性判别分析，并给出西瓜数据集\(3.0\alpha\)上的结果。 略 3.6 线性判别分析仅在线性可分的数据上能获得理想结果，试设计一个改进的方法能使其较好的应用于非线性可分数据。 Ans: 在当前维度线性不可分，可以使用适当的映射方法，使其在更高一维上可分，典型的方法有\(KLDA\)，可以很好的划分数据。 3.7 令码长为9，类别数位4，试给出海明距离意义下理论最优的ECOC二元码并证明之。 Ans: 对于ECOC二元码，当码长为\(2^{n}\)时，至少可以使\(2n\)个类别达到最优间隔，他们的海明距离为\(2^{n-1}\)，比如长度为8时，可以的序列为 Column 1 Column 2 Column 3 Column 4 Column 5 Column 6 Column 7 Column 8 1 1 1 1 -1 -1 -1 -1 1 1 -1 -1 1 1 -1 -1 1 -1 1 -1 1 -1 1 -1 -1 -1 -1 -1 1 1 1 1 -1 -1 1 1 -1 -1 1 1 -1 1 -1 1 -1 1 -1 1 其中4,5,6行是对1,2,3行的取反。若分类数为4，一共可能的分类器共有\(2^{4}−2\)种(排除了全1和全0)，在码长为8的最优分类器后添加一列没有出现过的分类器，就是码长为9的最优分类器。 3.8 ECOC编码能起到理想纠错作用的重要条件是：在每一位编码上出错的概率相当且独立。试分析多分类任务经ECOC编码后产生的二分类器满足该条件的可能性及由此产生的影响。 Ans: 理论上的ECOC码能理想纠错的重要条件是每个码位出错的概率相当，因为如果某个码位的错误率很高，会导致这位始终保持相同的结果，不再有分类作用，这就相当于全0或者全 1的分类器，这点和NFL的前提很像。但由于事实的样本并不一定满足这些条件，所以书中提到了有多种问题依赖的ECOC被提出。 3.9 使用OvR和MvM将多分类任务分解为二分类任务求解时，试描述为何无需专门针对类别不平衡性进行处理。 Ans: 对于OvR，MvM来说，由于对每个类进行了相同的处理，其拆解出的二分类任务中类别不平衡的影响会相互抵消，因此通常不需要专门处理。以ECOC编码为例，每个生成的二分类器会将所有样本分成较为均衡的二类，使类别不平衡的影响减小。当然拆解后仍然可能出现明显的类别不平衡现象，比如一个超级大类和一群小类。 3.10 试推到出多分类代价敏感学习(仅考虑基于类别的误分类代价)使用“再缩放”能获得理论最优解的条件。 Ans: 仅考虑类别分类的误分类代价，那么就默认正确分类的代价为0。于是得到分类表，(假设为3类) Column 1 Column 2 Column 3 0 \(c_{12}\) \(c_{13}\) \(c_{21}\) 0 \(c_{23}\) \(c_{31}\) \(c_{32}\) 0 对于二分类而言，将样本为正例的后验概率设为是p,那么预测为正的代价是\((1-p)\centerdot\mathcal{c}_{12}\)。预测为负的代价是\(p\centerdot\mathcal{c}_{21}\)。当\((1-p)\centerdot\mathcal{c}_{12}\leq p\centerdot\mathcal{c}_{21}\)样本就会被预测成正例，因为他的代价更小。当不等式取等号时，得到了最优划分，这个阀值\(p_{r}=\frac{\mathcal{c}_{12}}{\mathcal{c}_{12}+\mathcal{c}_{21}}\)，这表示正例与反例的划分比例应该是初始的\(\frac{\mathcal{c}_{12}}{\mathcal{c}_{21}}\)倍。假设分类器预设的阀值是\(p_{0}\)，不考虑代价敏感时，当\(\frac{y}{1-y}&gt;\frac{p_{0}}{1-p_{0}}\)时取正例。当考虑代价敏感，则应该是\(\frac{y}{1-y}&gt;\frac{p_{r}}{1-p_{r}}\centerdot\frac{p_{0}}{1-p_{0}}=\frac{\mathcal{c}_{12}}{\mathcal{c}_{21}}\centerdot\frac{p_{0}}{1-p_{0}}\)。 推广到对于多分类，任意两类的最优再缩放系数\(t_{ij}=\frac{c_{ij}}{c_{ji}}\)，然而所有类别的最优缩放系数并不一定能同时满足。当代价表满足下面条件时，能通过再缩放得到最优解。设\(t_{ij}=\frac{w_{i}}{w_{j}}\)，则\(\frac{w_{i}}{w_{j}}=\frac{c_{ij}}{c_{ji}}\)对所有\(i,j\)成立，假设有k类，共\(C_{2}^{k}\)个等式，此时代价表中\(k(k−1)\)个数，最少只要知道\(2(k−1)\)就能推出整张表。 基于神经网络的逻辑回归分类 DL4J实现 以下是使用DeepLearning4J实现的基于神经网络的逻辑回归分类算法。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465/** * @author ZHANG HAO * link: https://www.kaggle.com/uciml/iris */public class IrisClassification &#123; private static Logger log = LoggerFactory.getLogger(IrisClassification.class); public static void main(String[] args) throws Exception &#123; int numLinesToSkip = 0; String delimiter = ","; RecordReader recordReader = new CSVRecordReader(numLinesToSkip,delimiter); recordReader.initialize(new FileSplit(new ClassPathResource("iris.txt").getFile())); int labelIndex = 4; int numClasses = 3; int batchSize = 150; DataSetIterator iterator = new RecordReaderDataSetIterator(recordReader,batchSize,labelIndex,numClasses); DataSet allData = iterator.next(); allData.shuffle(); SplitTestAndTrain testAndTrain = allData.splitTestAndTrain(0.65); DataSet trainingData = testAndTrain.getTrain(); DataSet testData = testAndTrain.getTest(); //We need to normalize our data. We'll use NormalizeStandardize (which gives us mean 0, unit variance): DataNormalization normalizer = new NormalizerStandardize(); normalizer.fit(trainingData);//Collect the statistics (mean/stdev) from the training data. This does not modify the input data normalizer.transform(trainingData); //Apply normalization to the training data normalizer.transform(testData); //Apply normalization to the test data. This is using statistics calculated from the *training* set final int numInputs = 4; int outputNum = 3; int iterations = 1000; long seed = 6; int epochs = 100; log.info("Build model...."); MultiLayerConfiguration conf = new NeuralNetConfiguration.Builder() .seed(seed) .iterations(iterations) .weightInit(WeightInit.XAVIER) .learningRate(0.1) .regularization(true) .l2(1e-4) .list() .layer(0, new DenseLayer.Builder().nIn(numInputs).nOut(20).activation(Activation.TANH).build()) .layer(1, new DenseLayer.Builder().nOut(10).activation(Activation.TANH).build()) .layer(2, new OutputLayer.Builder().lossFunction(LossFunctions.LossFunction.NEGATIVELOGLIKELIHOOD).activation(Activation.SOFTMAX).nOut(outputNum).build()) .backprop(true) .pretrain(false) .build(); //run the model MultiLayerNetwork model = new MultiLayerNetwork(conf); model.init(); model.setListeners(new ScoreIterationListener(100)); for (int epoch = 0; epoch &lt; epochs; epoch++) &#123; model.fit(trainingData); &#125; //evaluate the model on the test set Evaluation eval = new Evaluation(numClasses); INDArray output = model.output(testData.getFeatureMatrix()); eval.eval(testData.getLabels(), output); log.info(eval.stats()); &#125;&#125;/*==========================Scores======================================== Accuracy: 0.9811 Precision: 0.9815 Recall: 0.9792 F1 Score: 0.9803======================================================================== */ 详细代码请浏览我的GitHub repository: [link]。 Pure-Java实现 以下是一个Java demo实现利用逻辑机率回归(logistic regression)处理分类问题(3个类别)。这3个类别的数据是我随机生成的，每个类别的数据均服从特定的uu和σσ的高斯分布。首先定义三组数据，每组数组400个用于训练，60个用于测试，每个输入数据均为2维的向量。输出结果为3维向量: \((1,0,0)\)表示class-1，\((0,1,0)\)表示class-2，\((0,0,1)\)表示class-3。每类数据均服从一个特定的高斯分布: 123456789101112131415161718192021222324252627282930313233final int patterns = 3; // number of classesfinal int train_N = 400 * patterns;final int test_N = 60 * patterns;final int nIn = 2; // input dim.final int nOut = patterns; // output dimdouble[][] train_X = new double[train_N][nIn]; // training matrix --&gt; Xint[][] train_T = new int[train_N][nOut]; // training label --&gt; ydouble[][] test_X = new double[test_N][nIn];Integer[][] test_T = new Integer[test_N][nOut];Integer[][] predicted_T = new Integer[test_N][nOut];...// Training data for demo// class 1 : x1 ~ N(-2.0, 1.0), y1 ~ N(+2.0, 1.0)// class 2 : x2 ~ N(+2.0, 1.0), y2 ~ N(-2.0, 1.0)// class 3 : x3 ~ N( 0.0, 1.0), y3 ~ N( 0.0, 1.0)GaussianDistribution g1 = new GaussianDistribution(-2.0, 1.0, rng);GaussianDistribution g2 = new GaussianDistribution(2.0, 1.0, rng);GaussianDistribution g3 = new GaussianDistribution(0.0, 1.0, rng);// data set in class 1for (int i = 0; i &lt; train_N / patterns - 1; i++) &#123; train_X[i][0] = g1.random(); train_X[i][1] = g2.random(); train_T[i] = new int[] &#123; 1, 0, 0 &#125;;&#125;for (int i = 0; i &lt; test_N / patterns - 1; i++) &#123; test_X[i][0] = g1.random(); test_X[i][1] = g2.random(); test_T[i] = new Integer[] &#123; 1, 0, 0 &#125;;&#125;// data set in class 2for (int i = train_N / patterns - 1; i &lt; train_N / patterns * 2 - 1; i++) &#123; ... &#125;for (int i = test_N / patterns - 1; i &lt; test_N / patterns * 2 - 1; i++) &#123; ... &#125;... 其中上面代码使用的GaussianDistribution定义如下: 12345678910111213141516171819public final class GaussianDistribution &#123; private final double mean; private final double var; private final Random rng; public GaussianDistribution(double mean, double var, Random rng) &#123; if (var &lt; 0.0) &#123; throw new IllegalArgumentException("Variance must be non-negative value."); &#125; this.mean = mean; this.var = var; if (rng == null) &#123; rng = new Random(); &#125; this.rng = rng; &#125; public double random() &#123; double r = 0.0; while (r == 0.0) &#123; r = rng.nextDouble(); &#125; double c = Math.sqrt(-2.0 * Math.log(r)); if (rng.nextDouble() &lt; 0.5) &#123; return c * Math.sin(2.0 * Math.PI * rng.nextDouble()) * var + mean; &#125; return c * Math.cos(2.0 * Math.PI * rng.nextDouble()) * var + mean; &#125;&#125; 回归的数值优化使用了“随机梯度下降法” (Stochastic Gradient Descent)，为了降低每次循环的梯度的计算复杂度(特别是在数据集很大的时候)，这里将数据拆分成若干batch，每个batch的大小为50。关于mini-batch的trade-off请参照:[link]。代码中的具体做法是: 1234567891011121314int minibatchSize = 50; // number of data in each minibatchint minibatch_N = train_N / minibatchSize; // number of minibatchesdouble[][][] train_X_minibatch = new double[minibatch_N][minibatchSize][nIn]; // minibatches of train dataint[][][] train_T_minibatch = new int[minibatch_N][minibatchSize][nOut]; // minibatches of output data for trainingList&lt;Integer&gt; minibatchIndex = new ArrayList&lt;Integer&gt;(); // data index for minibatch to apply SGDfor (int i = 0; i &lt; train_N; i++) &#123; minibatchIndex.add(i); &#125;Collections.shuffle(minibatchIndex, rng); // shuffle data index for SGD// create minibatches with training datafor (int i = 0; i &lt; minibatch_N; i++) &#123; for (int j = 0; j &lt; minibatchSize; j++) &#123; train_X_minibatch[i][j] = train_X[minibatchIndex.get(i * minibatchSize + j)]; train_T_minibatch[i][j] = train_T[minibatchIndex.get(i * minibatchSize + j)]; &#125;&#125; 前面介绍的逻辑机率回归使用的是Sigmoid函数，对于多分类任务，我们使用Softmax函数，两者区别和关系请参照: [link]。使用Sigmoid能得到每个类别的一个值，而使用softmax能得到每个类的概率，其Java实现如下 12345678910111213public final class ActivationFunction &#123; public static double sigmoid(double x) &#123; return 1.0 / (1.0 + Math.pow(Math.E, -x)); &#125; public static double dsigmoid(double y) &#123; return y * (1.0 - y); &#125; public static double[] softmax(double[] x, int n) &#123; double[] y = new double[n]; double max = 0.0; double sum = 0.0; for (int i = 0; i &lt; n; i++) &#123; if (max &lt; x[i]) &#123; max = x[i]; &#125; &#125; // prevent overflow for (int i = 0; i &lt; n; i++) &#123; y[i] = Math.exp(x[i] - max); sum += y[i]; &#125; for (int i = 0; i &lt; n; i++) &#123; y[i] /= sum; &#125; return y; &#125;&#125; 接下来，逻辑机率回归函数用于学习特征向量，数值优化使用基于Mini-batch的随机梯度下降法。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253public class LogisticRegression &#123; public int nIn; public int nOut; public double[][] W; public double[] b; public LogisticRegression(int nIn, int nOut) &#123; this.nIn = nIn; this.nOut = nOut; W = new double[nOut][nIn]; b = new double[nOut]; &#125; public double[][] train(double[][] X, int T[][], int minibatchSize, double learningRate) &#123; double[][] grad_W = new double[nOut][nIn]; double[] grad_b = new double[nOut]; double[][] dY = new double[minibatchSize][nOut]; // train with SGD for (int n = 0; n &lt; minibatchSize; n++) &#123; // 1. calculate gradient of W, b double[] predicted_Y_ = output(X[n]); for (int j = 0; j &lt; nOut; j++) &#123; dY[n][j] = predicted_Y_[j] - T[n][j]; for (int i = 0; i &lt; nIn; i++) &#123; grad_W[j][i] += dY[n][j] * X[n][i]; &#125; grad_b[j] += dY[n][j]; &#125; &#125; for (int j = 0; j &lt; nOut; j++) &#123; // 2. update params for (int i = 0; i &lt; nIn; i++) &#123; W[j][i] -= learningRate * grad_W[j][i] / minibatchSize; &#125; b[j] -= learningRate * grad_b[j] / minibatchSize; &#125; return dY; &#125; public double[] output(double[] x) &#123; double[] preActivation = new double[nOut]; for (int j = 0; j &lt; nOut; j++) &#123; for (int i = 0; i &lt; nIn; i++) &#123; preActivation[j] += W[j][i] * x[i]; &#125; preActivation[j] += b[j]; // linear output &#125; return ActivationFunction.softmax(preActivation, nOut); &#125; public Integer[] predict(double[] x) &#123; double[] y = output(x); // activate input data through learned networks Integer[] t = new Integer[nOut]; // output is the probability, so cast it to label int argmax = -1; double max = 0.0; for (int i = 0; i &lt; nOut; i++) &#123; if (max &lt; y[i]) &#123; max = y[i]; argmax = i; &#125; &#125; for (int i = 0; i &lt; nOut; i++) &#123; if (i == argmax) &#123; t[i] = 1; &#125; else &#123; t[i] = 0; &#125; &#125; return t; &#125;&#125; 构建完logistic regression后就可将数据读入进行训练: 12345678int epochs = 2000; // number of epochesdouble learningRate = 0.2; // initial learning rate// Build Logistic Regression modelLogisticRegression classifier = new LogisticRegression(nIn, nOut); // construct logistic regressionfor (int epoch = 0; epoch &lt; epochs; epoch++) &#123; // train for (int batch = 0; batch &lt; minibatch_N; batch++) classifier.train(train_X_minibatch[batch], train_T_minibatch[batch], minibatchSize, learningRate); learningRate *= 0.95;&#125; 训练结束后进行测试和评估: 1234567891011121314151617181920212223242526for (int i = 0; i &lt; test_N; i++) &#123; predicted_T[i] = classifier.predict(test_X[i]); &#125;int[][] confusionMatrix = new int[patterns][patterns]; // define confusion matrixdouble accuracy = 0.0;double[] precision = new double[patterns];double[] recall = new double[patterns];for (int i = 0; i &lt; test_N; i++) &#123; int predicted_ = Arrays.asList(predicted_T[i]).indexOf(1); int actual_ = Arrays.asList(test_T[i]).indexOf(1); confusionMatrix[actual_][predicted_] += 1;&#125;for (int i = 0; i &lt; patterns; i++) &#123; double col_ = 0.; double row_ = 0.; for (int j = 0; j &lt; patterns; j++) &#123; if (i == j) &#123; accuracy += confusionMatrix[i][j]; precision[i] += confusionMatrix[j][i]; recall[i] += confusionMatrix[i][j]; &#125; col_ += confusionMatrix[j][i]; row_ += confusionMatrix[i][j]; &#125; precision[i] /= col_; recall[i] /= row_;&#125;accuracy /= test_N; 最终得到评估结果为 1234567891011Logistic Regression model evaluation------------------------------------Accuracy: 91.1Precision:class 1: 95.0class 2: 91.5class 3: 86.9Recall:class 1: 96.6class 2: 90.0class 3: 86.9 以上是Logistic Regression的Java实现，该代码仅仅实现了其功能，并没有过多的考虑优化使其准确度达到更高，由于测试数据相对简单，稍作修改很容易就达到更高的准确度。详细代码请参照我的GitHub Repository: Logistic Regression]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>java</tag>
        <tag>deeplearning4j</tag>
        <tag>spark</tag>
        <tag>scala</tag>
        <tag>linear regression</tag>
        <tag>logistic regression</tag>
        <tag>linear discriminant analysis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习(周志华)--学习笔记(一) 模型评估与选择(Model Evaluation and Selection)]]></title>
    <url>%2F2017%2F04%2F28%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%91%A8%E5%BF%97%E5%8D%8E-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-1%2F</url>
    <content type="text"><![CDATA[第二章–模型评估与选择 (Model Evaluation and Selection) 根据训练数据是否拥有标记信息，学习任务可大致分为两大类: “监督学习” (supervised learning) 和“无监督学习” (unsupervised learning)，分类 (classification) 和回归 (regression) 是前者的代表，而聚类 (clustering) 是后者的代表。 机器学习的目标是使学得的模型能很好的适用于“新样本”，而不是仅仅在训练样本上工作得很好；即便对聚类这样的无监督学习任务，我们也希望学得的簇划分能适用于没在训练集中出现的样本。学得模型适用于新样本的能力，称为泛化 (generalization) 能力。具有强泛化能力的模型能很好的适用于整个样本空间。 经验误差与过拟合 错误率 (error rate) 为分类错误的样本数占样本总数的比例，即在\(m\)个样本中有\(a\)个样本分类错误，则错误率\(E=\frac{a}{m}\)，相应的\(1-\frac{a}{m}\)称为精度 (accuracy)，即“精度=1-错误率”。 “误差” (error): 学习器实际预测输出与样本的真实输出之间的差异。 “训练误差“ (training error)或“经验误差”(empirical error): 学习器在训练集上的误差。 “泛化误差” (generalization error): 学习器在新样本上的误差。 “过拟合” (overfitting): 指在拟合一个统计模型时，使用过多参数。对比于可获取的数据总量来说，一个荒谬的模型只要足够复杂，是可以完美地适应数据。过拟合一般可以视为违反奥卡姆剃刀原则。当可选择的参数的自由度超过数据所包含信息内容时，这会导致最后（拟合后）模型使用任意的参数，这会减少或破坏模型一般化的能力更甚于适应数据。过拟合的可能性不只取决于参数个数和数据，也跟模型架构与数据的一致性有关。此外对比于数据中预期的噪声或错误数量，跟模型错误的数量也有关。 “欠拟合” (underfitting): 指模型没有很好地捕捉到数据特征，不能够很好地拟合数据，与过拟合相反。 评估方法 “留出法” (hold-out): 直接将数据集D划分为两个互斥的集合，其中一个集合作为训练集\(S\)，另一个作为测试集\(T\)，即\(D=S\cup T\)，\(S\cap T=\varnothing\)。在\(S\)上训练出模型后，用\(T\)来评估器测试误差，作为泛化误差的估计。拆分数据集\(D\)一般采用“分层采样” (stratified sampling)。 “交叉验证法” (cross validation): 先将数据集\(D\)划分为\(k\)个大小相似的互斥子集，即\(D=D_{1}\cup D_{2} \cup ...\cup D_{k}\)，\(D_{i}\cap D_{j}=\varnothing (i\neq j)\)。每个子集\(D_{i}\)都尽可能保持数据分布的一致性，即从\(D\)中通过分层采样得到。然后，每次用k-1个子集的并集作为训练集，余下的那个子集作为测试集，这样就可以获得k组训练/测试集，从而可进行k次训练和测试，最终返回k次测试结果的均值。这种方法的稳定性 (stability) 和保真性 (fidelity) 很大程度上取决于k的取值，通常这种方法又称为“k折交叉验证” (k-fold cross validation)。通常k的取值为10。 “自助法” (bootstrapping): 以自助采样 (bootstrap sampling) 为基础，给定包含\(m\)个样本的数据集\(D\)，对它进行采样产生数据集\(D^{,}\)，每次随机从\(D\)中挑选一个样本，将其拷贝到\(D^{,}\)，然后再将该样本放回初始数据集\(D\)中，使得该样本在下次采样时仍有可能被采到；这个过程重复执行\(m\)次后，就得到包含\(m\)个样本的数据集\(D^{,}\)，这就是自助采样的结果。显然\(D\)中有一部分样本会在\(D^{,}\)中多次出现，而另一部分样本不出现。粗略估计，样本在\(m\)次采样中始终不被采到的概率是\((1-\frac{1}{m})^{m}\)，取极限得到\[\lim_{m \to \infty}(1-\frac{1}{m})^{m}\longmapsto\frac{1}{e}\approx0.368\]即通过自助采样，初始数据集\(D\)中约有36.8%的样本未出现在采样数据集\(D^{,}\)中，于是可以将\(D^{,}\)用作训练集，\(D-D^{,}\)用作测试集。这种方式的测试结果也称为“包外估计” (out-of-bag estimate)。 性能度量 在预测任务中，给定样例集\(D=\lbrace (x_{1},y_{1}),(x_{2},y_{2}), \dots ,(x_{m},y_{m})\rbrace\)，其中\(y_{i}\)是示例\(x_{i}\)的真实标记 (label)。要评估学习器\(f\)的性能，就要把学习器预测结果\(f(x)\)与真实标记\(y\)进行比较。 例如，回归(regression)任务最常用的性能度量是“均方误差” (mean squared error)\[E(f;D)=\frac{1}{m}\sum_{i=1}^m (f(x_{i})-y_{i})^{2}\]更一般的，对于数据分布\(\mathcal{D}\)和概率密度分布函数\(p(\centerdot)\)，均方误差可描述为\[E(f;\mathcal{D})=\int_{x \sim \mathcal{D}}(f(x)-y)^{2}p(x)dx\] 错误率与精度 对样例集\(D\)，分类错误率定义为\[E(f;D)=\frac{1}{m}\sum_{i=1}^m(f(x_{i})\neq y_{i})\]精度则定义为\[acc(f;D)=\frac{1}{m}\sum_{i=1}^m(f(x_{i})=y_{i})=1-E(f;D)\] 查准率(percision)、查全率(recall)与F1 真正例(true positive): \(TP\)，假正例(false positive): \(FP\)，真反例(true negative): \(TN\)，假反例(false negative): \(FN\)。显然，\(TP+FP+TN+FN=样例总数\)。如下是分类结果的“混淆矩阵”(confusion matrix): 查准率\(P\)和查全率\(R\)分别定义为\[P=\frac{TP}{TP+FP}, R=\frac{TP}{TP+FN}\]查准率和查全率是一对矛盾的度量，一般来说，查准率高时，查全率往往偏低，二查全率高时，查准率往往偏低。以查准率为纵轴，查全率为横轴作图，可以得到“P-R曲线” 图中“平衡点” (Break-Event Point, BER)是“查准率=查全率”时的取值。BER度量相对简单，更常用的是\(F1\)度量:\[F1=\frac{2\times P\times R}{P+R}=\frac{2\times TP}{样例总数+TP-TN}\]F1度量的一般形式为\(F_{\beta}\)，能表达出对查准率／查全率的不同偏好\[F_{\beta}=\frac{(1+\beta^{2})\times P\times R}{(\beta^{2}\times P)+R}\]其中\(\beta&gt;0\)度量了查全率对查准率的相对重要性。 关于“宏查准率”(macro-P)、“微查准率” (micro-P)等知识点参照书P-32页。 ROC和AUC “真正例率” (True Positive Rate, TPR): \(TPR=\frac{TP}{TP+FN}\) “假正例率” (False Positive Rate, FPR): \(FPR=\frac{FP}{TN+FP}\) 图中对角线对应于“随即猜测”模型。进行学习器比较时，若一个学习器的ROC曲线被另一个学习器的曲线完全“包住”，则后者的性能优于前者；若两个学习器的ROC曲线发生交叉，则很难断定孰优孰劣。若一定要进行比较，则比较ROC曲线下的面积，即AUC (Area Under ROC Curve)。如上图(b)，AUC可通过对ROC曲线下各部分面积求和而得。假定ROC曲线是由坐标为\(\lbrace (x_{1},y_{1}),(x_{2},y_{2}),\dots ,(x_{m},y_{m})\rbrace\)的按序连接而形成\((x_{1}=0,x_{m}=1)\)，则AUC可估算为\[AUC=\frac{1}{2}\sum_{i=1}^{m-1}(x_{i+1}-x_{i})\centerdot (y_{i}+y_{i+1})\]形式化的看，AUC考虑的是样本预测的排序质量，因此它与排序误差有紧密联系。给定\(m^{+}\)个正例和\(m^{-}\)个反例，令\(D^{+}\)和\(D^{-}\)分别表示正、反例集合，则排序“损失” (loss)定义为\[l_{rank}=\frac{1}{m^{+}m^{-}}\sum_{x^{+}\in D^{+}}\sum_{x^{-}\in D^{-}}\lbrace (f(x^{+}\leq f(x^{-}))+\frac{1}{2}(f(x^{+})=f(x^{-}))\rbrace\]即考虑每一对正、反例，若正例的预测值小于反例，则记一个“罚分”，若相等，则记0.5个“罚分”。\(l\-{rank}\)对应的是ROC曲线上的面积，若一个正例在ROC曲线上对应标记点坐标为\((x,y)\)，则\(x\)恰是排序在其之前的反例所占的比例，即假正例率。因此\[AUC=1-l_{rank}\] 偏差与方差 偏差-方差分解试图对学习算法的期望泛化错误率进行拆解。对测试样本\(x\)，令\(y_{D}\)为\(x\)在数据集中的标记，\(y\)为\(x\)的真实标记，\(f(x;D)\)为训练集\(D\)上学得模型\(f\)在\(x\)上的预测输出。以回归任务为例，学习算法的期望预测为\[\bar{f}(x)=\mathbb{E}_{D}[f(x;D)]\]使用样本数相同的不同训练集产生的方差为\[var(x)=\mathbb{E}_{D}\lbrack (f(x:D)-\bar{f}(x))^{2}\rbrack\]噪声为\[\varepsilon^{2}=\mathbb{E}_{D}[(y_{D}-y)^{2}]\]期望输出与真实标记的差别称为偏差(bias)，即\[bias^{2}(x)=(\bar{f}(x)-y)^{2}\]为便于讨论，假定噪声期望为0，即\(\mathbb{E}_{D}[y_{D}-y]=0\)。通过简单的多项式展开合并，可对算法的期望泛化误差进行分解: 于是，\[E(f;D)=bias^{2}(x)+var(x)+\varepsilon^{2}\]也就是说，泛化误差可分解为偏差、方差和噪声之和。偏差度量了学习算法的期望预测与真实结果的偏离程度，即刻画了学习算法本身的拟合能力；方差度量了同样大小的训练集变动所导致的学习性能变化，即刻画了数据扰动所造成的影响；噪声则表达了当前任务上任何学习算法所能达到的期望泛化误差的下届，即刻画了学习问题本身的难度。偏差-方差分解说明，泛化性能是由学习算法的能力、数据的充分性以及学习任务本身难度所共同决定的。给定学习任务，为了取得好的泛化性能，则需要使偏差较小，即能够充分拟合数据，并且使方差较小，即使得数据扰动产生的影响小。 一般来说，方差与偏差时有冲突的，这称为偏差-方差窘境(bias-variance dilemma)。如图 给定学习任务，假设我们能控制学习算法的训练程度，在训练不足时，学习器的拟合能力不够强，训练数据的扰动不足以时学习器发生显著变化，此时偏差主导了泛化错误率；随着训练程度的加深，学习器的拟合能力逐渐增强，训练数据发生的扰动渐渐能被学习器学到，方差逐渐主导了泛化错误率；在训练程度充足后，学习器的拟合能力已非常强，训练数据发生的轻微扰动都会导致学习器发生显著变化，若训练数据自身的、非全局的特性被学习器学到了，则将发生过拟合。]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Generating New SSH Key, Adding It to the ssh-agent and GitHub Account [Reprinted]]]></title>
    <url>%2F2017%2F04%2F24%2FGenerating-New-SSH-Key-Adding-It-to-the-ssh-agent-and-GitHub-Account%2F</url>
    <content type="text"><![CDATA[This post is reproduced from GitHub Help, and merge several solutions in GitHub Help. It is a tutorial of generating SSH key, and adding to ssh-agent as well as GitHub account on Mac OS X. Generating new SSH key If you’re unsure whether you already have an SSH key, check it in the terminal to see if existing SSH keys are present: 12$ ls -al ~/.ssh# Lists the files in your .ssh directory, if they exist Or check the directory listing to see if you already have a public SSH key. By default, the filenames of the public keys are one of the following: - id_dsa.pub - id_ecdsa.pub - id_ed25519.pub - id_rsa.pub If you do not have the SSH key, then 1$ ssh-keygen -t rsa -b 4096 -C "your_email@example.com" Substituting in your own GitHub email address, this creates a new SSH key, using the provided email as a label 1Generating public/private rsa key pair. When you’re prompted to “Enter a file in which to save the key,” press Enter. This accepts the default file location. 1Enter a file in which to save the key (/Users/you/.ssh/id_rsa): [Press enter] At the prompt, type a secure passphrase. 12Enter passphrase (empty for no passphrase): [Type a passphrase]Enter same passphrase again: [Type passphrase again] Working with SSH key passphrases With SSH keys, if someone gains access to your computer, they also gain access to every system that uses that key. To add an extra layer of security, you can add a passphrase to your SSH key. You can use ssh-agent to securely save your passphrase so you don’t have to reenter it. Adding or changing a passphrase You can change the passphrase for an existing private key without regenerating the keypair by typing the following command: 1234567$ ssh-keygen -p# Start the SSH key creation processEnter file in which the key is (/Users/you/.ssh/id_rsa): [Hit enter]Key has comment '/Users/you/.ssh/id_rsa'Enter new passphrase (empty for no passphrase): [Type new passphrase]Enter same passphrase again: [One more time for luck]Your identification has been saved with the new passphrase. If your key already has a passphrase, you will be prompted to enter it before you can change to a new passphrase. Saving your passphrase in the keychain On Mac OS X, these default private key files are handled automatically: - .ssh/id_rsa - .ssh/id_dsa - .ssh/identity The first time you use your key, you will be prompted to enter your passphrase. If you choose to save the passphrase with your keychain, you won’t have to enter it again. Otherwise, you can store your passphrase in the keychain when you add your key to the ssh-agent. Adding your SSH key to the ssh-agent When adding your SSH key to the agent, use the default macOS ssh-add command, and not one installed by macports, homebrew, or some other external source. Start the ssh-agent in the background. 12$ eval "$(ssh-agent -s)"Agent pid 59566 If you’re using macOS Sierra 10.12.2 or later, you will need to modify your ~/.ssh/config file to automatically load keys into the ssh-agent and store passphrases in your keychain. 1234Host * AddKeysToAgent yes UseKeychain yes IdentityFile ~/.ssh/id_rsa Add your SSH private key to the ssh-agent and store your passphrase in the keychain. If you created your key with a different name, or if you are adding an existing key that has a different name, replace id_rsa in the command with the name of your private key file. 1$ ssh-add -K ~/.ssh/id_rsa Adding a new SSH key to your GitHub account Copy the SSH key to your clipboard. If your SSH key file has a different name than the example code, modify the filename to match your current setup. When copying your key, don’t add any newlines or whitespace. 12$ pbcopy &lt; ~/.ssh/id_rsa.pub# Copies the contents of the id_rsa.pub file to your clipboard Login to your GitHub account, in the upper-right corner of any page, click your profile photo, then click Settings In the user settings sidebar, click SSH and GPG keys Click New SSH key or Add SSH key In the “Title” field, add a descriptive label for the new key. For example, if you’re using a personal Mac, you might call this key “Personal MacBook Air”. Then Paste your key into the “Key” field. Click Add SSH key If prompted, confirm your GitHub password. Reference Generating a new SSH key and adding it to the ssh-agent Working with SSH key passphrases Adding a new SSH key to your GitHub account]]></content>
      <categories>
        <category>Setting</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Removing Backscatter to Enhance the Visibility of Underwater Object]]></title>
    <url>%2F2017%2F04%2F20%2FRemoving-Backscatter-to-Enhance-the-Visibility-of-Underwater-Object%2F</url>
    <content type="text"><![CDATA[This is a novel method based on underwater optical model for underwater object visibility enhancement based on single image. General Schema For an underwater distorted image, it is firstly decomposed to reflectance and illuminance. Then color correction and dehazing methods are utilized to process each component separately, according to their specific features. Finally, we calculate the weights of two components and apply an efficient image fusion method to obtain the enhanced image. Image Decomposition Here we introduce a weighted image decomposition method, which separates original image into two parts, one expresses the illumination component of image while another expresses the reflectance component of image. Since the process of camera captures scenes is based on the luminance of light which is reflected from objects or scenes to camera. For a clear environment, the radiance intensity of scenes spread widely in the display range, which makes captured image shows good contrast information and mean value is closed to middle gray according to gray world assumption. However, for the situations with hazy case, the reflectance would be distorted. In order to obtain accurate luminance and reflectance, we should derive the reflectance component and illumination component by decomposing original image. Firstly, an image can be expressed as:\[I_{\lambda}(x)=I_{\lambda}^{R}(x)+I_{\lambda}^{I}(x) \tag{1}\]where \(\lambda\in\{r,g,b\}\) represents each color component of image, \(I_{\lambda}^{R}(x)\) is the reflectance component and \(I_{\lambda}^{I}(x)\) is the illumination component:\[I_{\lambda}^{R}(x)=\gamma\centerdot I_{\lambda}(x) \tag{2}\]\[I_{\lambda}^{I}(x)=(1-\gamma)\centerdot I_{\lambda}(x) \tag{3}\]where \(\gamma\) is a weighted parameter, which maintains bright areas remains brighter than dark areas and enhances the contrast of reflectance component to remove the backscatter effect from it as much as possible. So \(\gamma\) can be derived by\[\gamma=\zeta\centerdot\frac{I_{\lambda}(x)}{I_{\lambda}^{\max}} \tag{4}\]where \(I_{\lambda}^{\max}\) is the maximal pixel value of \(\lambda\) color channel, and \(\zeta\) is a control parameter to determine the weight of reflectance component, and \(\zeta\in[0,1]\). If \(\zeta=0\), the whole image is treated as illuminance component, while \(\zeta=1\) , the whole image is otherwise treated as reflectance component. For further image process, we consider that the backscatter effect only exists in the illuminance component, while reflectance component only suffers from color distortion. Above is an example of image decomposition, from left to right: original image and its corresponding histogram, illuminance component and its corresponding histogram, reflectance component and its corresponding histogram. Dehazing and Color Correction for Illuminance Component Global Underwater Background Light Estimation In order to estimate the background light, an ideal way is to pick up a pixel or an area lies as the maximum depth with regard to the camera, since color distortion and contrast degradation are distance dependent. With distance increases, the haze is denser due to the scattering of turbid medium, which causes relatively brighter color. However, in this scheme, objects or scenes, which are brighter than the background light, may lead to an undesirable selection result. In order to obtain accurate result, this scheme should be eliminated. Since the variance of objects and scenes pixel values are lower with denser haze, we utilize a hierarchical searching method based on the quad-tree subdivision to execute this process. Firstly, the image is separated into four equal rectangular regions, then for each region, we compute the average value subtract the standard deviation values as shown below:\[Score_{l}=\frac{1}{3N}\sum_{\lambda\in\{r,g,b\}}\sum_{x=1}^{N}I_{l}^{\lambda}(x)-\frac{1}{3}\sum_{\lambda\in\{r,g,b\}}\sqrt{\frac{\sum_{x=1}^{N}(I_{l}^{\lambda}(x)-\bar{I}_{l}^{\lambda})^{2}}{N}} \tag{5}\]where \(l=1,2,3,4\) represents to the four image regions, \(N\) is the pixel number within the region, \(I_{l}^{\lambda}(x)\) is the pixel value of \(x\) point of \(\lambda\) component of \(l\) region, \(\bar{I}_{l}^{\lambda}\) is the average pixel value of \(\lambda\) component of \(l\) region. After that, we select the region with the lowest variance, and divide it into four regions as done before. These processes are repeated till the size is less than the threshold, and normally we set this threshold to 100. Within the determined region, we calculate mean value vector as the final obtained background light and this vector can be considered as the approximately brightest value with the full image. Above image is choosing background light from proper image block. Transmission Map Estimation Coarse Estimation Assuming the background light is given, according to the underwater optical model formation\[I_{\lambda}(x)=\big(J_{\lambda}(x)\centerdot T_{\lambda}(x)+L_{\lambda}(x)\centerdot t_{\lambda}(x)\big)\centerdot t_{\lambda}(x)+A(\lambda)\centerdot(1-t_{\lambda}(x))\tag{6}\]where \(I_{\lambda}(x)\) represent each color component of captured image, \(J_{\lambda}(x)\) is the scene radiance, \(L_{\lambda}(x)\) is the possible existence of artificial light, \(A(\lambda)\) is the background light, \(T_{\lambda}(x)=e^{-c(\lambda)\centerdot D(x)}\) expresses the light attenuation in the vertical direction, i.e., vertical transmission and \(t_{\lambda}(x)=e^{-c(\lambda)\centerdot d(x)}\) is transmission map in the horizontal direction \(c(\lambda)\) is attenuation coefficient for \(\lambda\) component, \(D(x)\) is vertical depth and \(d(x)\) is horizontal depth. In order to recover scenes or objects radiance from the captured image, we still need to estimate the \(T_{\lambda}(x)\), \(t_{\lambda}(x)\) and the effect of artificial light. However, for most case, especially in the swallow underwater environment, there is little effect of artificial light. Simply, we eliminate the impact of artificial light and derive the simplified model as:\[I_{\lambda}(x)=\big(J_{\lambda}(x)\centerdot T_{\lambda}(x)\big)\centerdot t_{\lambda}(x)+A(\lambda)\centerdot(1-t_{\lambda}(x))\tag{7}\]where \(A(\lambda)\) is already known after background light estimation, we firstly consider \(J_{\lambda}(x)\centerdot T_{\lambda}(x)\) to be one part, denotes as \(J_{\lambda}^{T}(x)\), then the formation can be written as:\[I_{\lambda}(x)=J_{\lambda}^{T}(x)\centerdot t_{\lambda}(x)+A(\lambda)\centerdot(1-t_{\lambda}(x))\tag{8}\]It has the similar format as hazy formation model and we can derive \(J_{\lambda}^{T}(x)\) by estimating the transmission map in the horizontal direction, \(t_{\lambda}(x)\). In such scheme, the transmission map estimation for dehazing in the atmospheric environment can be utilized to compute the transmission map of underwater case after some modifications, since both of these two environments are similar to each other. To estimate the transmission map of each color channel more accurate, we choose an optimal transmission estimation (OTS) method to prevent the over-enhancement and obtain optimized estimation. This method is a generalized dark channel prior (DCP), and in the DCP, scene depth is considered to be local similar, and some pixels within the local area of at least one color channel is nearly to be zero. Since the backscatters due to dust-like particles tend to reduce the contrast of local area, inversely, the contrast information of a degraded area also seems to implicit the effect of backscatters. So the mean square error (MSE) is utilized to measure the contrast of local scene area. The MSE contrast represents the variance of pixel values, which is given by:\[\mathcal{C}_{MSE}=\sum_{x=1}^{N}\frac{\big(J_{\lambda}^{T}(x)-\bar{J}_{\lambda}^{T}\big)^{2}}{N} \tag{9}\]where \(\bar{J}_{\lambda}^{T}\) is the average pixel value of \(J_{\lambda}^{T}(x)\), \(N\) is the pixel amount within the local area. By transforming the \((8)\), we can derive the expression of \(J_{\lambda}^{T}(x)\):\[J_{\lambda}^{T}(x)=\frac{1}{t_{\lambda}(x)}\centerdot\big(I_{\lambda}(x)-A(\lambda)\big)+A(\lambda) \tag{10}\]then, \(\mathcal{C}_{MSE}\) can be written as\[\mathcal{C}_{MSE}=\sum_{x=1}^{N}\frac{\big(I_{\lambda}(x)-\bar{I}_{\lambda}\big)^{2}}{N\centerdot t_{\lambda}(x)^{2}} \tag{11}\]Considering the transmission value is locally same and MSE contrast is inversely proportional to the transmission value \(t_{\lambda}(x)\), which means the contrast of local area is greater with smaller \(t_{\lambda}(x)\). However, \(t_{\lambda}(x)\) can not be arbitrarily small because it may cause some pixel values of restored image out of the full dynamic range, and further lead information loss, as shown in the figure below, only pixel values within \([\alpha,\beta]\) can be enhanced after mapping process, other pixels will be truncated. In general, choosing a larger transmission value is able to reduce the information loss, but contrast is enhanced by choose smaller transmission value. The red regions represent the information loss due to the truncation of output pixel values, and input pixel values are mapped to output pixel values according to a transformation function. Thus, the transmission value of \(t_{\lambda}(x)\) can not be chosen arbitrarily, contrast enhancement and information loss reduction should be taken into consideration at the same time. First, the contrast enhancement cost function, \(E_{c}\) and information loss cost function, \(E_{i}\) are designed and then minimize the two functions simultaneously, where contrast enhancement cost is defined as the negative sum of \(\mathcal{C}_{MSE}\) of all color channels and information loss cost is defined as the sum of square value of truncated pixel values.\[E_{c}=-\sum_{\lambda\in\{r,g,b\}}\sum_{x=1}^{N}\frac{\big(J_{\lambda}^{T}(x)-\bar{J}_{\lambda}^{T}\big)^{2}}{N}=-\sum_{\lambda\in\{r,g,b\}}\sum_{x=1}^{N}\frac{\big(I_{\lambda}(x)-\bar{I}_{\lambda}\big)^{2}}{N\centerdot t_{\lambda}(x)^{2}} \tag{12}\]\[E_{i}=-\sum_{\lambda\in\{r,g,b\}}\sum_{x=1}^{N}\bigg\{\big(\min\{0,J_{\lambda}^{T}(x)\}\big)^{2}+\big(\max\{0,J_{\lambda}^{T}(x)-255\}\big)^{2}\bigg\} \tag{13}\]Finally, for each local area, the optimal transmission value \(t_{\lambda}(x)\) is estimated by minimizing the following function\[E=E_{c}+\kappa\centerdot E_{i} \tag{14}\]where \(\kappa\) is a weighted parameter to control the influence of information loss cost. The graph above is coarse transmission map generated by general dark channel prior, left: illuminance component, right: corresponding coarse transmission map. Transmission Map Refinement In the previous sub-section, the transmission value is treated to be local constant of a block. However, the scene depths within each local area are vary spatially and block-based local constant transmission value is likely to yield block artifact, further to weaken the contrast of restoring image. In order to solve this problem, different methods have been proposed to refine the transmission map. Here, image guided filtering is used to refine the transmission map, which also uses an input image as a guidance. The transmission map refinement can be executed by solving a sparse linear system\[(\mathbf{L}+\lambda\centerdot\mathbf{U})\centerdot t=\lambda\centerdot\tilde{t} \tag{15}\]where \(\mathbf{L}\) is matting Laplacian matrix, and \(\mathbf{U}\) is the identity matrix with the same size as \(\mathbf{L}\), and \(\lambda\) is a regularization parameter. Since in image guided filter, it kernel has the similar form of the elements of matting Laplacian matrix, thus its elements can be represented by:\[L_{ij}=|\omega|\centerdot(\delta_{ij}-W_{ij}) \tag{16}\]where \(|\omega|\) is the number of pixels in a block, \(\delta_{ij}\) is Kronecker delta, and \(W_{ij}\) is the guided filter kernel weight, which is defined as:\[W_{ij}=\frac{1}{|\omega|^{2}}\centerdot\sum_{k:(i,j)\in\omega_{k}}\bigg(1+\frac{(I_{i}-\mu_{k})\centerdot(I_{j}-\mu_{k})}{\delta_{k}^{2}+\tau}\bigg) \tag{17}\]where \(\mu_{k}\) is mean of \(I\), \(\delta_{k}^{2}\) is variance of \(I\), \(\tau\) is the regularization parameter and \(I\) the guidance image. Thus, the transmission map is refined by computing \((15)\). The graph above is the transmission map refinement via image guided filtering, left: coarse transmission map, right: refined transmission map. Transmission Map Enhancement After refinement, the transmission is relatively smoothed. In order to obtain a more accurate transmission map and extract more details from it, the refined transmission map should further be enhanced to improve its texture and details. Since an image can be separated into two parts, one is smooth component, another is detailed component, the transmission map also can be rewritten as this type.\[t=t_{smooth}+t_{detail} \tag{18}\]where \(t_{smooth}\) is the smooth component of transmission map while \(t_{detail}\) is the detailed component. For smooth component, we can derive it by using a blur filter. Gaussian low-pass filter (GLPF) is an effective smoothing filter, the idea of Gaussian blur is computing the mean value of center pixel and its surround pixels by utilizing a convolution kernel, and larger the kernel is, smoother the image is. The Gaussian convolution kernel can be represented as:\[G(x,y)=\frac{1}{2\pi\sigma^{2}}\centerdot e^{-\frac{(x^{2}+y^{2})}{2\sigma^{2}}} \tag{19}\]where \(\sigma\) is the scale parameter of Gaussian blur. Therefore, the smooth component can be obtained by smoothing the refined transmission map with Gaussian low-pass filter.\[t_{smooth}=t*G \tag{20}\]Then the detailed component can be derived as the difference between \(t\) and \(t_{smooth}\),\[t_{detail}=t-t_{smooth} \tag{21}\]After that, the enhanced transmission map is calculated by:\[t_{enhanced}=t_{smooth}+\alpha\centerdot t_{detail} \tag{22}\]where \(\alpha\) is the enhancement parameter to control the amplified degree of detailed component. The graph above is the enhanced transmission map, left: refined transmission map, right: enhanced transmission map. Enhanced Transmission Map for Each Color Component The transmission maps are various among different color components due to different light absorption abilities for light beams with different wavelength. After obtaining the enhanced transmission map, each color component’s transmission map can be derived by exploring the intrinsic relationship and difference among color components. Since underwater images always dominated by one color, i.e., greenish or bluish, and other color components are attenuated, the average pixel value of each color component, although not very accurate, can reflect the attenuation ratio of each color component in water. And it is known that the vertical depth of a given underwater image is hard to estimate due to insufficient prior knowledge and information provided by the image, so the average pixel value can further be utilized to estimate the transmission map for each color component. As in the graph above, using a reference line, we can derive the intensity of each color component, since underwater images often show bluish or greenish, the average intensity of the image, green channel is highest, following by blue channel and red channel is lowest. We are able to get the intrinsic information that the average pixel intensities also contain the attenuation coefficient of each color component. In the underwater optical model, the formation of transmission map can be simplified as \(t(x)=e^{-c\centerdot d(x)}\), where \(c\) is the total attenuation coefficient and \(d(x)\) is the distance from camera to the objects. Due to different light absorption abilities of water for different wavelength light beams, the total attenuation coefficients of different color components are diverse. Meanwhile, the average pixel value generally reflects the relative attenuation ratio of each color component, and the lower value of transmission map is, the higher contrast of restored image is. We firstly define the enhanced transmission map is transmission map of the color component with highest average pixel value. In order to define simply, we suppose that blue component has the highest average pixel value.\[t_{b}(x)=t_{enhanced}(x)=e^{-c_{b}\centerdot d(x)} \tag{23}\]At the same time, the transmission map of other color components can be written as\[t_{r}(x)=e^{-c_{r}\centerdot d(x)},\quad t_{g}(x)=e^{-c_{g}\centerdot d(x)} \tag{24}\]then using \(t_{b}(x)\) to express \(t_{r}(x)\) and \(t_{g}(x)\)\[t_{r}(x)=\big(t_{b}(x)\big)^{\frac{c_{r}}{c_{b}}}=\big(t_{b}(x)\big)^{\beta_{r}},\quad t_{g}(x)=\big(t_{b}(x)\big)^{\frac{c_{g}}{c_{b}}}=\big(t_{b}(x)\big)^{\beta_{g}} \tag{25}\]where \(\beta_{r}\) and \(\beta_{g}\) are the relative attenuation ratios and can be estimated by average pixel value. To simplify the computation, we define that \(\beta_{r}=\frac{mean(I_{b})}{mean(I_{r})}\), \(\beta_{g}=\frac{mean(I_{b})}{mean(I_{g})}\). The graph above is the transmission map of each color component, from left to right: red channel, green channel and blue channel. Dehazing and Color Correction After estimating the background light and transmission map of each color component, we can restore the underwater image via the simplified physical model formation \((10)\), say, \(J_{\lambda}^{T}(x)=\frac{1}{t_{\lambda}(x)}\centerdot\big(I_{\lambda}(x)-A(\lambda)\big)+A(\lambda)\). However, the result of this formation can not guarantee the intensities of restored image lie in the display area \([0,1]\) or \([0,255]\), so a simple minimum-maximum normalization of intensity values is utilized to map them to the display interval. Meanwhile, although normalization method shows some effect on color correction, \(J_{\lambda}^{T}(x)\) still suffers from part of light attenuation in the vertical direction and cause color distortion. Thus, color correction method should be introduced to solve this problem. For this case, a white balance method is used to execute the vertical direction compensation to obtain the final enhanced illuminance component \(J_{\lambda}(x)\), which can achieve good results. The graph above is the processed result of illuminance component, left: input image and its histogram, right: output image and its histogram. Color Correction for Reflectance Component The reflectance component reflects the texture and details of underwater scenes, and we have mentioned that it is considered to be free of backscatter. Thus, the reflectance component only suffers from color distortion caused by energy absorption of water. In order to deal with this issue, the color correction method should be introduced. Color constancy based color correction methods are excellent way to handle this process, which shows great balance between the correction performance and information loss, since the general procedures of such methods include darkest and brightest pixels’ truncation and histogram stretch, which may cause some undesired phenomena of texture information loss. By comparison, the simplest color balance (SCB) method is a better method for this case. The algorithm is fast and efficient, since it only simply stretches the pixel values of the three color channels while preserves the information of image as much as possible by manually setting different truncation ratio of different color channel, so that their histograms are able to occupy the maximal display range \([0,255]\) (or \([0,1]\)). In order to execute fast stretch process, an affine transform function \(ax+b\) is applied on each color channel to map pixel values from minimum 0 to maximum 255 (or 1) by computing proper \(a\) and \(b\). However, few aberrant pixels of many images already map the maximum and minimum values, so truncation is used to improve color performance by “clipping” a small percentage of pixels with highest values and lowest values before applying affine transform function. Actually, this process will cause more or less white and black regions in images, which may look unnatural. Thus, the number of truncated pixels must be as less as possible. In general, although this algorithm is not real white balance algorithm since it does not focus on correcting the color distributions, it can provide white balance effect and contrast enhancement to some degree. The graph above is the processed result of reflectance component, left: input image and its histogram, right: output image and its histogram. Fusion Process Weights of the Fusion Process After deriving the enhanced illuminance and reflectance components, a fusion-based method is introduced to combine these two images and generate the final free backscatter and color distortion image. For fusion techniques, one of the crucial steps is computing the weight maps of input images. In order to represent different features of input images well, we use several weight map methods to measure the features of input images and then combine them together to derive the normalized weighted maps. In practice, we choose three weight maps, luminance weight map, saliency weight map and exposedness weight map. Luminance weight map, \(W_{L}\), which represents the luminance parameter of each image component. This weight map is generated by calculating STD between \(r\), \(g\) and \(b\) color channels and the luminance value \(l\), where the luminance value \(l\) is derived by:\[l=\alpha\centerdot r+\beta\centerdot g+\gamma\centerdot b \tag{26}\]where \(\alpha+\beta+\gamma=1\), each represents the weight parameter of each color component, and normally we set \(\alpha=0.299\), \(\beta=0.587\) and \(\gamma=0.114\). It generates high values correlated with the preservation degree of each input region, while the multi-scale blending ensures a seamless transition between the inputs. However, this weight map is able to correctly reflect the luminance degree of image and show greater enhancement for degraded image, it shows negative effects on contrast and colorfulness. In order to compensate the drawbacks, following weight maps are introduced. Saliency weight map, \(W_{S}\), which reflects the salient objects and points in an image, and it aims to emphasize these discriminating objects of underwater scenes. In order to obtain the saliency map, the algorithm of Achanta et al. based on biological concept of center-surround contrast is applied due to its computationally efficient and time saving. One of the drawbacks of applying saliency map is over-estimation of highlighted areas, thus exposedness weight map is utilized to guarantee the accuracy and protect the mid tones of image. Exposedness weight map, \(W_{E}\), which evaluates the status of exposed pixels. It provides an operator to protect local contrast to be non-exaggerated or non-understated. Generally, pixel values close to mean value is likely to have higher exposed appearance. The map is written as Gaussian-modeled distance to the mean value:\[W_{E}=e^{-\frac{(I(x)-\bar{I})^{2}}{2\centerdot\sigma^{2}}} \tag{27}\]where \(\sigma\) is the standard deviation, \(I(x)\) denotes pixel value locates at position \(x\) and \(\bar{I}\) represents mean value. From the formation, pixels close to mean value have higher weight while pixels with larger distances are associated with over-exposed and under-exposed regions. Consequently, these three weight maps are able to produce well preserved appearance of fused images. Multi-scale Fusion Process Practically, in order to prevent undesirable halos and improve the performance, we utilize a multi-scale Gaussian and Laplacian pyramid decomposition technology to execute fusion process. In this method, each input is decomposed to several layers with different scales by Laplacian operator and Gaussian kernel. Meanwhile, higher layers are generated by differentiating the original image and filtered image of lower layer in Gaussian pyramid. Thus, the Laplacian pyramid is a set of quasi-bandpass versions of image. At the same time, the Gaussian pyramid of normalized weight map \(W_{norm}\) is calculated, so that both Laplacian and Gaussian pyramids have same levels, and fusion process can be written as:\[\mathcal{R}_{\lambda}^{l}=\sum_{n=1}^{N}G^{l}\{W_{norm}^{n}\}\centerdot L^{l}\{J_{\lambda}^{n}\} \tag{28}\]where \(L^{l}\{J_{\lambda}^{n}\}\) is Laplacian pyramid of the \(\lambda\) component of \(n^{th}\) input image, \(G^{l}\{W_{norm}^{n}\}\) is \(n^{th}\) normalized weight map and \(l\) is pyramid levels. Since Laplacian multi-scale strategy performs relatively fast and balances a good trade-off between speed and accuracy, the restored output image can achieve excellent result. The graph above is the final output image via multi-scale Gaussian and Laplacian fusion process. The Java Implementation of this method and the experiment results as well as evaluation and analysis is available on my GitHub repository: RemoveBackScatter, the Matlab codes are available here: [link] Reference Underwater Image Enhancement by Wavelength Compensation and Dehazing Review on Underwater Image Restoration and Enhancement Algorithms Underwater Image Processing: State of the Art of Restoration and Image Enhancement Methods Single Image Haze Removal Using Dark Channel Prior Automatic Red-Channel Underwater Image Restoration Region-specialized Underwater Image Restoration in Inhomogeneous Optical Environments Underwater Image Enhancement Using Guided Trigonometric Bilateral Filter and Fast Automatic Color Correction Deriving Inherent Optical Properties from Background Color and Underwater Image Enhancement Edge-based Color Constancy A Spatial Processor Model for Object Colour Perception Automatic Color Enhancement (ACE) and Its Fast Implementation Simplest Color Balance Adaptive Histogram Equalization and Its Variations Contrast Limited Adaptive Histogram Equalization Fusion-based Restoration of the Underwater Images Enhancing Underwater Images and Videos by Fusion A Retinex-based Enhancing Approach for Single Underwater Image Lightness and Retinex Theory Underwater Image Quality Enhancement through Integrated Color Model with Rayleigh Distribution Enhancement of Low Quality Underwater Image through Integrated Global and Local Contrast Correction Optimized Contrast Enhancement for Real-time Image and Video Dehazing Contrast in Complex Images Single Image Dehazing Guided Image Filtering Frequency-tuned Salient Region Detection The Statistics of Visual Representation Human Visual System Inspired Underwater Image Quality Measures Alpha-trimmed Means and Their Relationship to Median Filters]]></content>
      <categories>
        <category>Image Processing</category>
      </categories>
      <tags>
        <tag>image processing</tag>
        <tag>java</tag>
        <tag>opencv</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Underwater Image Enhance via Fusion and Its Java Implementation]]></title>
    <url>%2F2017%2F04%2F15%2FUnderwater-Image-Enhance-via-Fusion%2F</url>
    <content type="text"><![CDATA[This paper “Enhancing Underwater Images and Videos by Fusion”, published by Ancuti et al. on CVPR, describes a novel strategy, built on the fusion principles, to enhance underwater videos and images. Here I implement this algorithm by Java, and summerize the main idea in this paper. General Schema The enhancing approach starts from a single distorted underwater image, first of all, applying white balance to this image to generate the first input of fusion process, denotes as \(img_{1}\), and applying a temporal coherent noise reduction method to this \(img_{1}\) to derive another input of fusion process, denotes as \(img_{2}\); then, obtaining the weights of these two inputs, where Laplacian contrast weight (\(W_{L}\)), local contrast weight (\(W_{LC}\)), Saliency weight (\(W_{S}\)) and exposedness weight (\(W_{E}\)) are used in this process; finally, the multi-scale fusion process is applied to generate the restored image. Fusion Materials Generation The general idea of this fusion process is that the processed result, combines several input images by preserving only the most significant features of them. Here, the author choose to use two inputs, and the enhancing solution does not search to derive the inputs based on the physical model of the scene, only aims for a fast and simple technique that works generally. The first derived input is represented by the color corrected version of the image while the second is computed as a contrast enhanced version of the underwater image after a noise reduction operation is performed. White Balance White balancing aims to enhance the image appearance by discarding unwanted color casts, due to various illuminants. Since, for underwater images, different components of light suffer from different degree of attenuation. In water deeper than 30 ft, white balancing suffers from noticeable effects since the absorbed colors are difficult to be restored. Additionally, underwater scenes present significant lack of contrast due to the poor light propagation in this type of medium. To deal with this problem, the author intorduces a modified balancing method according to Shades-of-Grey hypothesis, where the illumination is estimated by the value \(\mu_{I}\) that is computed from the average of the scene \(\mu_{ref}\) and adjusted by the parameter \(\lambda\):\[\mu_{I}=0.5+\lambda\centerdot\mu_{ref}\]The average color \(\mu_{ref}\) is used to estimate the illuminant color and can be obtained based on Minkowski norm when \(p=1\). Practically, the first input is computed by this straightforward white balancing operation. Since the white balancing solely is not able to solve the problem of visibility, thus, an additional input is required in order to enhance the contrast of the degraded image. To implement the color balance, I found that among different methods (including the method provided by the author), the Simplest Color Balance, which performs color balancing via histogram bormalization, is a better way to handle such process, since it is fast and the restoring performance is better. Below is the Java codes I implemented for the Simplest Color Balance: 12345678910111213141516171819202122232425262728293031323334353637383940/** * Simplest Color Balance. Performs color balancing via histogram normalization. * @param img input color or gray scale image * @param percent controls the percentage of pixels to clip to white and black. (normally, choose 1~10) * @return Balanced image in CvType.CV_32F */public static Mat SimplestColorBalance(Mat img, int percent) &#123; if (percent &lt;= 0) percent = 5; img.convertTo(img, CvType.CV_32F); List&lt;Mat&gt; channels = new ArrayList&lt;Mat&gt;(); int rows = img.rows(); // number of rows of image int cols = img.cols(); // number of columns of image int chnls = img.channels(); // number of channels of image double halfPercent = percent / 200.0; if (chnls == 3) &#123; Core.split(img, channels); &#125; else &#123; channels.add(img); &#125; List&lt;Mat&gt; results = new ArrayList&lt;Mat&gt;(); for (int i = 0; i &lt; chnls; i++) &#123; // find the low and high precentile values (based on the input percentile) Mat flat = new Mat(); channels.get(i).reshape(1, 1).copyTo(flat); Core.sort(flat, flat, Core.SORT_ASCENDING); double lowVal = flat.get(0, (int) Math.floor(flat.cols() * halfPercent))[0]; double topVal = flat.get(0, (int) Math.ceil(flat.cols() * (1.0 - halfPercent)))[0]; // saturate below the low percentile and above the high percentile Mat channel = channels.get(i); for (int m = 0; m &lt; rows; m++) &#123; for (int n = 0; n &lt; cols; n++) &#123; if (channel.get(m, n)[0] &lt; lowVal) channel.put(m, n, lowVal); if (channel.get(m, n)[0] &gt; topVal) channel.put(m, n, topVal); &#125; &#125; Core.normalize(channel, channel, 0, 255, Core.NORM_MINMAX); channel.convertTo(channel, CvType.CV_32F); results.add(channel); &#125; Mat outval = new Mat(); Core.merge(results, outval); return outval;&#125; Temporal Coherent Noise Reduction The temporal coherent noise reduction process is introduced, since underwater images are noisy due to the impurities and the special illumination conditions. To remove noise while preserve edges (for videos, also need to take both spatial and temporal coherence into consideration), The bilateral filter is commonly used, by considering the domain \(\Omega\) of the spatial filter kernel \(f\) (Gaussian with standard deviation \(\sigma_{f}\)), the bilateral filter blends the center pixel \(s\) of the kernel with the neighboring pixels \(p\) that are similar to \(s\):\[J_{s}=\frac{1}{k(s)}\sum_{p\in\Omega}f\big(p-s,\sigma_{f}\big)\centerdot g\big(D(p,s),\sigma_{g}\big)\centerdot I_{p}\]where \(D(p,s)=I_{p}-I_{s}\) is the difference in intensities, the normailization term\[k(s)=\sum_{p\in\Omega}f\big(p-s,\sigma_{f}\big)\centerdot g\big(D(p,s),\sigma_{g}\big)\]\(g\) is the range kernel that is a Gaussian with standard deviation \(\sigma_{g}\) that penalizes pixels across edges that have large intensity differences. However, the bilateral filter does not guarantee the preservation of the temporal coherence for videos, thus, the author proposed a temporal bilateral filter,which instead of comparing the intensities as \(D(p,s)=I_{p}-I_{s}\) directly, they compute the sum of squared differences between small spatial neighborhood \(\Psi\) around \(s\) and \(p\) weighted by a Gaussian \(\Gamma(x,y)\):\[D(p,s)=\sum_{x}^{\Psi}\sum_{y}^{\Psi}\Gamma(x,y)(I_{p}-I_{s})^{2}\]Typically, the size of neighborhood \(\Psi\) is \(3\times 3\) or \(5\times 5\). Practically, the second input is computed from the noise-free and color corrected version of the original image. This input is designed in order to reduce the degradation due to volume scattering. To achieve an optimal contrast level of the image, the second input is obtained by applying the classical contrast local adaptive histogram equalization. And the Java Implementation is shown below: 123456789101112// color balanceMat img1 = ColorBalance.SimplestColorBalance(image, 5);img1.convertTo(img1, CvType.CV_8UC1);// Perform sRGB to CIE Lab color space conversionMat LabIm1 = new Mat();Imgproc.cvtColor(img1, LabIm1, Imgproc.COLOR_BGR2Lab);Mat L1 = new Mat();Core.extractChannel(LabIm1, L1, 0);// apply CLAHEMat[] result = applyCLAHE(LabIm1, L1);Mat img2 = result[0];Mat L2 = result[1]; where applyCLAHE is implemented as: 12345678910111213141516public static Mat[] applyCLAHE(Mat img, Mat L) &#123; Mat[] result = new Mat[2]; CLAHE clahe = Imgproc.createCLAHE(); clahe.setClipLimit(2.0); Mat L2 = new Mat(); clahe.apply(L, L2); Mat LabIm2 = new Mat(); List&lt;Mat&gt; lab = new ArrayList&lt;Mat&gt;(); Core.split(img, lab); Core.merge(new ArrayList&lt;Mat&gt;(Arrays.asList(L2, lab.get(1), lab.get(2))), LabIm2); Mat img2 = new Mat(); Imgproc.cvtColor(LabIm2, img2, Imgproc.COLOR_Lab2BGR); result[0] = img2; result[1] = L2; return result;&#125; Weights of Fusion Process The author mentioned that the design of the weight measures needs to consider the desired appearance of the restored output. Image restoration is tightly correlated with the color appearance, and as a result the measurable values such as salient features, local and global contrast or exposedness are difficult to integrate by naive per pixel blending, without risking to introduce artifacts, thus, the fusion technique is used, which will be introduced later. Laplacian Contrast Weight Laplacian contrst weight deals with global contrast by applying a Laplacian filter on each input luminance channel and computing the absolute value of the filter result. 123456public static Mat LaplacianContrast(Mat img) &#123; Mat laplacian = new Mat(); Imgproc.Laplacian(img, laplacian, img.depth()); Core.convertScaleAbs(laplacian, laplacian); return laplacian;&#125; It assigns high values to edges and texture. For the underwater restoration task, however, this weight is not sufficient to recover the contrast, mainly because it can not distinguish between a ramp and flat regions. To handle this problem, we searched for an additional contrast measurement that independently assess the local distribution. Local Contrast Weight Local contrast weight comprises the relation between each pixel and its neighborhoods average. It is computed as the standard deviation between pixel luminance level and the local average of its surrounding region:\[W_{LC}(x,y)=\Vert I^{k}-I_{\omega_{hc}}^{k}\Vert\]where \(I^{k}\) represents the luminance channel of the input and \(I_{\omega_{hc}}^{k}\) denotes the low-passed version of it. This filtered version is obtained by employing a small \(5\times 5\) (\(\frac{1}{16}[1,4,6,4,1]\)) separable binomial kernel with the high frequency cut-off value \(\omega_{hc}=\frac{\pi}{2.75}\). For small kernels the binomial kernel is a good approximation of its Gaussian counterpart, and it can be computed more effectively. 12345678910111213141516public static Mat LocalContrast(Mat img) &#123; double[] h = &#123; 1.0 / 16.0, 4.0 / 16.0, 6.0 / 16.0, 4.0 / 16.0, 1.0 / 16.0 &#125;; Mat mask = new Mat(h.length, h.length, img.type()); for (int i = 0; i &lt; h.length; i++) &#123; for (int j = 0; j &lt; h.length; j++) &#123; mask.put(i, j, h[i] * h[j]); &#125; &#125; Mat localContrast = new Mat(); Imgproc.filter2D(img, localContrast, img.depth(), mask); for (int i = 0; i &lt; localContrast.rows(); i++) &#123; for (int j = 0; j &lt; localContrast.cols(); j++) &#123; if (localContrast.get(i, j)[0] &gt; Math.PI / 2.75) localContrast.put(i, j, Math.PI / 2.75); &#125; &#125; Core.subtract(img, localContrast, localContrast); return localContrast.mul(localContrast);&#125; The impact of this measure is to strengthen the local contrast appearance since it advantages the transitions mainly in the highlighted and shadowed parts of the second input. Saliency Weight Saliency weight aims to emphasize the discriminating objects that lose their prominence in the underwater scene. The saliency detection method used in this paper is Frequency-tuned Salient Region Detection proposed by Achanta et al., which is a computationally efficient saliency algorithm: 1234567891011121314151617181920212223242526272829public static Mat Saliency(Mat img) &#123; // blur image with a 3x3 or 5x5 Gaussian filter Mat gfbgr = new Mat(); Imgproc.GaussianBlur(img, gfbgr, new Size(3, 3), 3); // Perform sRGB to CIE Lab color space conversion Mat LabIm = new Mat(); Imgproc.cvtColor(gfbgr, LabIm, Imgproc.COLOR_BGR2Lab); // Compute Lab average values (note that in the paper this average is found from the un-blurred original image, but the results are quite similar) List&lt;Mat&gt; lab = new ArrayList&lt;Mat&gt;(); Core.split(LabIm, lab); Mat l = lab.get(0); l.convertTo(l, CvType.CV_32F); Mat a = lab.get(1); a.convertTo(a, CvType.CV_32F); Mat b = lab.get(2); b.convertTo(b, CvType.CV_32F); double lm = Core.mean(l).val[0]; double am = Core.mean(a).val[0]; double bm = Core.mean(b).val[0]; // Finally compute the saliency map Mat sm = Mat.zeros(l.rows(), l.cols(), l.type()); Core.subtract(l, new Scalar(lm), l); Core.subtract(a, new Scalar(am), a); Core.subtract(b, new Scalar(bm), b); Core.add(sm, l.mul(l), sm); Core.add(sm, a.mul(a), sm); Core.add(sm, b.mul(b), sm); return sm;&#125; However, the saliency map tends to favor highlighted areas. To increase the accuracy of results, the author introduces the exposedness map to protect the mid tones that might be altered in some specific cases. Exposedness Weight Exposedness weight evaluates how well a pixel is exposed. This assessed quality provides an estimator to preserve a constant appearance of the local contrast, that ideally is neither exaggerated nor understated. Commonly, the pixels tend to have a higher exposed appearance when their normalized values are close to the average value of 0.5. This weight map is expressed as a Gaussian-modeled distance to the average normalized range value (0.5):\[W_{E}(x,y)=exp\bigg(-\frac{(I^{k}(x,y)-0.5)^{2}}{2\sigma^{2}}\bigg)\]where \(I^{k}(x,y)\) represents the value of the pixel location \((x,y)\) of input image \(I^{k}\), while the standard deviation is set to \(\sigma=0.25\), the Java implementation is shown below: 123456789101112131415public static Mat Exposedness(Mat img) &#123; double sigma = 0.25; double average = 0.5; int rows = img.rows(); int cols = img.cols(); Mat exposedness = Mat.zeros(rows, cols, img.type()); // W = exp(-(img - aver).^2 / (2*sigma^2)); for (int i = 0; i &lt; rows; i++) &#123; for (int j = 0; j &lt; cols; j++) &#123; double value = Math.exp(-1.0 * Math.pow(img.get(i, j)[0] - average, 2.0) / (2 * Math.pow(sigma, 2.0))); exposedness.put(i, j, value); &#125; &#125; return exposedness;&#125; This weight map will assign higher values to those tones with a distance close to zero, while pixels that are characterized by larger distances, are associated with the over-exposed and under-exposed regions. In consequence, this weight tempers the result of the saliency map and produces a well preserved appearance of the fused image. Multi-scale Fusion To yield consistent results, the author employs the normalized weight values \(\bar{W}\) by constraining taht the sum at ecach pixel location of weight maps \(W\) equals to one:\[\bar{W}^{k}=\frac{W^{k}}{\sum_{k=1}^{K}W^{k}}\]Then, generally, the enhanced image version \(\mathcal{R}(x,y)\) is computed by fusing the defined inputs with the weight measures at every pixel location \((x,y)\):\[\mathcal{R}(x,y)=\sum_{k=1}^{K}\bar{W}^{k}(x,y)\centerdot I^{k}(x,y)\]where \(k\) is the index of the inputs, and \(K=2\) in this paper. However, the naive approach to directly fuse the inputs and the weights introduces undesirable halos. To deal with this issue, the author introduces the Gaussian pyramid decomposition for weight maps and Laplacian pyramid decomposition for inputs. Considering that both Gaussian and Laplacian pyramids have same levels, the mixing process is performed at each level independently yielding the fused pyramid:\[\mathcal{R}^{l}(x,y)=\sum_{k=1}^{K}G^{l}\big\{\bar{W}^{k}(x,y)\big\}\centerdot L^{l}\big\{I^{k}(x,y)\big\}\]where \(l\) denotes the number of the pyramid levels (\(l=5\) in this paper), \(L^{l}\) is the Laplacian version while \(G^{l}\) is the Gaussian version. Below is the Java implementation of these two pyramids and the reconstruction method: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354public class Pyramid &#123; public static Mat[] GaussianPyramid(Mat img, int level) &#123; Mat[] gaussPyr = new Mat[level]; Mat mask = filterMask(img); Mat tmp = new Mat(); Imgproc.filter2D(img, tmp, -1, mask); gaussPyr[0] = tmp.clone(); Mat tmpImg = img.clone(); for (int i = 1; i &lt; level; i++) &#123; // resize image Imgproc.resize(tmpImg, tmpImg, new Size(), 0.5, 0.5, Imgproc.INTER_LINEAR); // blur image tmp = new Mat(); Imgproc.filter2D(tmpImg, tmp, -1, mask); gaussPyr[i] = tmp.clone(); &#125; return gaussPyr; &#125; public static Mat[] LaplacianPyramid(Mat img, int level) &#123; Mat[] lapPyr = new Mat[level]; //Mat mask = filterMask(img); lapPyr[0] = img.clone(); Mat tmpImg = img.clone(); for (int i = 1; i &lt; level; i++) &#123; // resize image Imgproc.resize(tmpImg, tmpImg, new Size(), 0.5, 0.5, Imgproc.INTER_LINEAR); lapPyr[i] = tmpImg.clone(); &#125; // calculate the DoG for (int i = 0; i &lt; level - 1; i++) &#123; Mat tmpPyr = new Mat(); Imgproc.resize(lapPyr[i + 1], tmpPyr, lapPyr[i].size(), 0, 0, Imgproc.INTER_LINEAR); Core.subtract(lapPyr[i], tmpPyr, lapPyr[i]); &#125; return lapPyr; &#125; public static Mat PyramidReconstruct(Mat[] pyramid) &#123; int level = pyramid.length; for (int i = level - 1; i &gt; 0; i--) &#123; Mat tmpPyr = new Mat(); Imgproc.resize(pyramid[i], tmpPyr, pyramid[i - 1].size(), 0, 0, Imgproc.INTER_LINEAR); Core.add(pyramid[i - 1], tmpPyr, pyramid[i - 1]); &#125; return pyramid[0]; &#125; private static Mat filterMask(Mat img) &#123; double[] h = &#123; 1.0 / 16.0, 4.0 / 16.0, 6.0 / 16.0, 4.0 / 16.0, 1.0 / 16.0 &#125;; Mat mask = new Mat(h.length, h.length, img.type()); for (int i = 0; i &lt; h.length; i++) &#123; for (int j = 0; j &lt; h.length; j++) &#123; mask.put(i, j, h[i] * h[j]); &#125; &#125; return mask; &#125;&#125; And the fusion process codes is shown here: 123456789101112131415161718192021222324252627282930313233343536// construct the gaussian pyramid for weightint level = 5;Mat[] weight1 = Pyramid.GaussianPyramid(w1, level);Mat[] weight2 = Pyramid.GaussianPyramid(w2, level);// construct the laplacian pyramid for input image channelimg1.convertTo(img1, CvType.CV_32F);img2.convertTo(img2, CvType.CV_32F);List&lt;Mat&gt; bgr = new ArrayList&lt;Mat&gt;();Core.split(img1, bgr);Mat[] bCnl1 = Pyramid.LaplacianPyramid(bgr.get(0), level);Mat[] gCnl1 = Pyramid.LaplacianPyramid(bgr.get(1), level);Mat[] rCnl1 = Pyramid.LaplacianPyramid(bgr.get(2), level);Core.split(img2, bgr);Mat[] bCnl2 = Pyramid.LaplacianPyramid(bgr.get(0), level);Mat[] gCnl2 = Pyramid.LaplacianPyramid(bgr.get(1), level);Mat[] rCnl2 = Pyramid.LaplacianPyramid(bgr.get(2), level);// fusion processMat[] bCnl = new Mat[level];Mat[] gCnl = new Mat[level];Mat[] rCnl = new Mat[level];for (int i = 0; i &lt; level; i++) &#123; Mat cn = new Mat(); Core.add(bCnl1[i].mul(weight1[i]), bCnl2[i].mul(weight2[i]), cn); bCnl[i] = cn.clone(); Core.add(gCnl1[i].mul(weight1[i]), gCnl2[i].mul(weight2[i]), cn); gCnl[i] = cn.clone(); Core.add(rCnl1[i].mul(weight1[i]), rCnl2[i].mul(weight2[i]), cn); rCnl[i] = cn.clone();&#125;// reconstruct &amp; outputMat bChannel = Pyramid.PyramidReconstruct(bCnl);Mat gChannel = Pyramid.PyramidReconstruct(gCnl);Mat rChannel = Pyramid.PyramidReconstruct(rCnl);Mat fusion = new Mat();Core.merge(new ArrayList&lt;Mat&gt;(Arrays.asList(bChannel, gChannel, rChannel)), fusion);fusion.convertTo(fusion, CvType.CV_8UC1); Moreover, you can obtain the full java codes (also Matlab codes) in my GitHub repository: ImageEnhanceViaFusion and the Matlab codes are available here: [link]. Besides, for more details about this paper, you can read about the paper: Enhancing Underwater Images and Videos by Fusion. Experiment Result Heavy Distorted Fish Example Enhancement Examples with Normalized Weight Maps of Two Divers Enhancement Examples with Normalized Weight Maps of One Diver Other Results Reference Enhancing Underwater Images and Videos by Fusion Frequency-tuned Salient Region Detection]]></content>
      <categories>
        <category>Image Processing</category>
      </categories>
      <tags>
        <tag>image processing</tag>
        <tag>java</tag>
        <tag>opencv</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Adaptive Local Tone Mapping Technique for HDR Image and Java Implementation]]></title>
    <url>%2F2017%2F04%2F11%2FALTM%2F</url>
    <content type="text"><![CDATA[This short paper, published by Ahn et al. on ICCE, introduces a new tone mapping technique for HDR images based on the retinex theory. Here I summarize this paper and show the Java implementation with OpenCV to realize the proposed algorithm. Center/Surround Retinex The retinex theory, initially defined by Land et al., explains how the realiable color information from the real world is extracted by human visual system, and Jobson et al. introduced the single-scale retinex (SSR) and multi-scale retinex (MSR) based on Land’s surround retinex theory. Generally, SSR is computed by\[R_{i}(x,y)=\log\big(I_{i}(x,y)\big)-\log\big(F(x,y)*I_{i}(x,y)\big)\]where \(x\), \(y\) are the pixel coordinates in the image, \(R_{i}(x,y)\) denotes the retinex output of \(i^{th}\) spectral band, \(I_{i}(x,y)\) is the image distribution in the \(i^{th}\) spectral band, \(*\) represents convolution operation, and \(F(x,y)\) is the Gaussian surround function, defined by\[F(x,y)=K\centerdot e^{-\frac{x^{2}+y^{2}}{c^{2}}}\]where \(c\) is Gaussian surround space constant, \(K\) is normalization factor. In Addition, MSR is described as\[R_{MSR_{i}}(x,y)=\sum_{n=1}^{N}\omega_{n}\centerdot R_{n_{i}}(x,y)\]where \(N\) is the number of scales, \(R_{n_{i}}(x,y)\) denotes the \(i^{th}\) component of the \(n^{th}\) scale, \(R_{MSR_{i}}(x,y)\) denotes \(i^{th}\) spectral component of the MSR output, and \(\omega_{n}\) is the weight associated with the \(n^{th}\) scale. The purpose of MSR is to reduce halo artifacts around high contrast edges and to keep balance with the dynamic range compression and the color rendition. MSR produces good dynamic range compression, but still suffer from halo artifacts. In addition, SSR with small space constant makes large uniform regions graying out and flat-looking in images. Adaptive Local Tone Mapping For ALTM, there are two processes, one is applying a global tone mapping, and another is a local tone mapping based on retinex algorithm. Global Adaptation Global adaptation simulates the early stage of human visual system, which senses brightness as an approximate logarithmic function according to the Weber-Fechner law. The global adaptation is defined by\[L_{g}(x,y)=\frac{\log\big(\frac{L_{w}(x,y)}{\bar{L}_{w}}+1\big)}{\log\big(\frac{L_{w\max}}{\bar{L}_{w}}+1\big)}\]where \(L_{g}(x,y)\) is global adaptation output, \(L_{w}(x,y)\) is input luminance values, \(L_{w\max}\) is maximum luminance value, \(\bar{L}_{w}\) is the logarithmic average luminance and given as\[\bar{L}_{w}=e^{\frac{1}{m}\sum_{x,y}\log\big(\delta+L_{w}(x,y)\big)}\]where \(m\) is the total number of pixels of one channel in the image and \(\delta\) is a small value to prevent the singularity that occurs if black pixels are present. Moreover, the input world luminance values \(L_{w}(x,y)\) is derived by\[L_{w}(x,y)=0.299\centerdot Red(x,y)+0.587\centerdot Green(x,y)+0.114\centerdot Blue(x,y)\]according to the paper, and \(Red\), \(Green\) and \(Blue\) denote the three color channels of the input image respectivaly. Below shows the Java implementation with OpenCV to derive the global adaptation: 123456789101112131415161718192021222324252627282930313233/** * Global Adaptation * @param b blue channel of input image * @param g green channel of input image * @param r red channel of input image * @param rows number of rows of the input image * @param cols number of cloumns of the input image * @return a Mat list contains global adaptation output (Lg) and input world luminance values (Lw) */private static List&lt;Mat&gt; globalAdaptation(Mat b, Mat g, Mat r, int rows, int cols) &#123; // Calculate Lw &amp; maximum of Lw Mat Lw = new Mat(rows, cols, r.type()); Core.multiply(r, new Scalar(rParam), r); // rParam = 0.299 Core.multiply(g, new Scalar(gParam), g); // gParam = 0.587 Core.multiply(b, new Scalar(bParam), b); // bParam = 0.114 Core.add(r, g, Lw); Core.add(Lw, b, Lw); double LwMax = Core.minMaxLoc(Lw).maxVal; // the maximum luminance value // Calculate log-average luminance and get global adaptation result Mat Lw_ = Lw.clone(); Core.add(Lw_, new Scalar(0.001), Lw_); Core.log(Lw_, Lw_); double LwAver = Math.exp(Core.sumElems(Lw_).val[0] / (rows * cols)); Mat Lg = Lw.clone(); Core.divide(Lg, new Scalar(LwAver), Lg); Core.add(Lg, new Scalar(1.0), Lg); Core.log(Lg, Lg); Core.divide(Lg, new Scalar(Math.log(LwMax / LwAver + 1.0)), Lg); // Lg is the global adaptation List&lt;Mat&gt; list = new ArrayList&lt;&gt;(); list.add(Lw); list.add(Lg); return list;&#125; Local Adaptation Local adaptation based on the retinex theory is applied after the global adaptation process. The output of SSR (and MSR) suffers strong halo artifacts and looks unnatural. In order to deal with these drawbacks, an edge-preserving filter, guided filter is used in this paper, is intorduced to substitute the Gaussian filter of the retinex algorithm to reduce the halo artifact effects. The guided filter is an edge-preserving filter like the bilateral filter whose weights depend not only on the Euclidean distances but also on the luminance differences. These filters behave similar, but the guided filter has better performance near the edges. Also, its computational complexity is linear-time without approximation and independent of the kernel size. The local adaptation equation can be written as\[L_{l}(x,y)=\log L_{g}(x,y)-\log H_{g}(x,y)\]where \(L_{l}(x,y)\) denotes the local adaptation output, and \(H_{g}(x,y)\) denotes the output of the guided filter global adaptation result. Mathematically, the guided filter is defined as\[H_{g}(x,y)=\frac{1}{|\omega|}\sum_{(\zeta_{x},\zeta_{y})\in\omega(x,y)}\big(a(\zeta_{x},\zeta_{y})\centerdot L_{g}(x,y)+b(\zeta_{x},\zeta_{y})\big)\]where \(\zeta_{x}\), \(\zeta_{y}\) are the neighborhood pixel coordinates, \(\omega(x,y)\) is a local square window of a radius \(r\) centered at pixel \((x,y)\), \(|\omega|\) represents the number of pixels in \(\omega(x,y)\), while \(a(\zeta_{x},\zeta_{y})\) and \(b(\zeta_{x},\zeta_{y})\) are linear coefficients, which are computed by\[a(\zeta_{x},\zeta_{y})=\frac{\mu_{2}(\zeta_{x},\zeta_{y})-\mu^{2}(\zeta_{x},\zeta_{y})}{\sigma^{2}(\zeta_{x},\zeta_{y})+\varepsilon}\\b(\zeta_{x},\zeta_{y})=\mu(\zeta_{x},\zeta_{y})-a(\zeta_{x},\zeta_{y})\centerdot\mu(\zeta_{x},\zeta_{y})\]where \(\mu(\zeta_{x},\zeta_{y})\) and \(\sigma^{2}(\zeta_{x},\zeta_{y})\) are the mean and variance of \(L_{g}\) in \(\omega(\zeta_{x},\zeta_{y})\), \(\mu_{2}(\zeta_{x},\zeta_{y})\) is the mean of \(L_{g}^{2}\) in \(\omega(\zeta_{x},\zeta_{y})\), and \(\varepsilon\) is a regularization parameter. Below is the Java codes to obtain the \(H_{g}\), and the implementation details of guided filter can be found in my GitHub repository: [link]. 123456789101112131415161718/** * Local Adaptation (Guided Image Filter Result) * @param Lg global adaptation output * @param rows number of rows of the input image * @param cols number of cloumns of the input image * @return local adaptation result */private static Mat localAdaptation(Mat Lg, int rows, int cols) &#123; int krnlSz = Arrays.asList(3.0, rows * krnlRatio, cols * krnlRatio).stream().max(Double::compare).get().intValue(); // maximum filter: using dilate to extract the local maximum of a image block, which acts as the maximum filter // Meanwhile, minimum filter can be achieved by using erode function Mat Lg_ = new Mat(); Mat kernel = Imgproc.getStructuringElement(Imgproc.MORPH_RECT, new Size(krnlSz, krnlSz), new Point(-1, -1)); Imgproc.dilate(Lg, Lg_, kernel); // guided image filter Mat Hg = Filters.GuidedImageFilter(Lg, Lg_, r, eps); return Hg;&#125; After obtaining the result of global aadaptation filtered by edge-preserving filter, instead of applying the local adaptation equation directly, two factors are intorduced to prevent the flat-looking appearance caused by the filter and improve the performance of the algorithm. One is the contrast enhancement factor and another is adaptive nonlinearity offset. The contrast enhancement factor is computed by\[\alpha(x,y)=1+\eta\centerdot\frac{L_{g}(x,y)}{L_{g\max}}\]where \(\eta\) is the contrast control parameter, and \(L_{g\max}\) is the maximum luminance value of global adaptation output. And the adaptive nonlinearity offset is written as\[\beta=\lambda\centerdot\bar{L}_{g}\]which varies in accordance with the scene contents, and \(\lambda\) is the nonlinearity control parameter, \(\bar{L}_{g}\) is the logarithmic average luminance of the global adaptation output. After deriving these two factors, the final local adaptation equation is established as\[L_{out}(x,y)=\alpha(x,y)\centerdot\log\bigg(\frac{L_{g}(x,y)}{H_{g}(x,y)}+\beta\bigg)\]where \(L_{out}(x,y)\) is the final local adaptation output. Below is the Java Codes to obtain the final local adaptation output: 123456789101112Mat alpha = new Mat(m, n, rChannel.type());Core.divide(Lg, new Scalar(Core.minMaxLoc(Lg).maxVal / eta), alpha);Core.add(alpha, new Scalar(1.0), alpha);Mat Lg_ = new Mat(m, n, rChannel.type());Core.add(Lg, new Scalar(1.0 / 255.0), Lg_);Core.log(Lg_, Lg_);double beta = Math.exp(Core.sumElems(Lg_).val[0] / (m * n)) * lambda;Mat Lout = new Mat(m, n, rChannel.type());Core.divide(Lg, Hg, Lout);Core.add(Lout, new Scalar(beta), Lout);Core.log(Lout, Lout);Core.normalize(alpha.mul(Lout), Lout, 0, 255, Core.NORM_MINMAX); After the local adaptation, the processed luminance values are rescaled from 0 to 1. Finally, the tone mapped image is obtained from the luminance values of the local adaptation output and the input HDR image. Here is the Java codes to do the mapping process: 123456789Mat gain = obtainGain(Lout, Lw, m, n);// outputCore.divide(rChannel.mul(gain), new Scalar(Core.minMaxLoc(rChannel).maxVal / 255.0), rChannel); // Red ChannelCore.divide(gChannel.mul(gain), new Scalar(Core.minMaxLoc(gChannel).maxVal / 255.0), gChannel); // Green ChannelCore.divide(bChannel.mul(gain), new Scalar(Core.minMaxLoc(bChannel).maxVal / 255.0), bChannel); // Blue Channel// merge three color channels to a imageMat outval = new Mat();Core.merge(new ArrayList&lt;&gt;(Arrays.asList(bChannel, gChannel, rChannel)), outval);outval.convertTo(outval, CvType.CV_8UC1); Where the gain is derived by 123456789101112private static Mat obtainGain(Mat Lout, Mat Lw, int rows, int cols) &#123; Mat gain = new Mat(rows, cols, Lout.type()); for (int i = 0; i &lt; rows; i++) &#123; for (int j = 0; j &lt; cols; j++) &#123; if (Lw.get(i, j)[0] == 0) gain.put(i, j, Lout.get(i, j)[0]); else gain.put(i, j, Lout.get(i, j)[0] / Lw.get(i, j)[0]); &#125; &#125; return gain;&#125; The full Java Codes are available in my GitHub repository: ALTMRetinex, while Matlab codes are here: [link] Experiment Result Here we exhibit some results generated by ALTM methods. And the image display method used is provided by ImShow-Java-OpenCV. Reference Adaptive Local Tone Mapping Based on Retinex for High Dynamic Range Images Multiscale Retinex ImShow-Java-OpenCV]]></content>
      <categories>
        <category>Image Processing</category>
      </categories>
      <tags>
        <tag>image processing</tag>
        <tag>java</tag>
        <tag>opencv</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Installing OpenCV for Java [Reprinted]]]></title>
    <url>%2F2017%2F04%2F10%2FInstalling-OpenCV-for-Java%2F</url>
    <content type="text"><![CDATA[As of OpenCV 2.4.4, OpenCV supports desktop Java development. Note that this post is transported from OpenCV Java Tutorials. This tutorial will help you install OpenCV on your desktop operating system. Install the latest Java version Download the latest Java JDK from the Oracle website. Now you should be able to install the last Java JDK by open the file just downloaded. Install the latest Eclipse version Download the latest Eclipse version at the Eclipse Download page choosing the Eclipse IDE for Java Developers version (suggested). Extract the downloaded compressed file and put the resulting folder wherever you want to. You don’t need to install anything. Alternatively, you can try the Eclipse installer. Install OpenCV 3.x under Windows First of all you should download the OpenCV library (version 3.x) from here. Then, extract the downloaded OpenCV file in a location of your choice. Once you get the folder opencv put in wherever you prefer. Now the only two things that you will need are: the opencv-3xx.jar file located at \opencv\build\java and the opencv_java3xx.dll library located at \opencv\build\java\x64 (for 64-bit systems) or \opencv\build\java\x86 (for 32-bit systems). The 3xx suffix of each file is a shortcut for the current OpenCV version, e.g., it will be 300 for OpenCV 3.0 and 310 for OpenCV 3.1. Install OpenCV 3.x under Mac OS X The quickest way to obtain OpenCV under macOS is to use Homebrew. After installing Homebrew, you have to check whether the XCode Command Line Tools are already installed on your system. To do so, open the Terminal and execute: xcode-select --install If macOS asks for installing such tools, proceed with the download and installation. Otherwise, continue with the OpenCV installation. To install OpenCV (with Java support) through Homebrew, you need to add the science tap to Homebrew: 1$ brew tap homebrew/science and effectively install OpenCV: 1$ brew install opencv3 --HEAD --with-contrib --with-java After the installation of OpenCV, the needed jar file and the dylib library will be located at /usr/local/opt/opencv3/share/OpenCV/java/. Install OpenCV 3.x under Linux Please, note: the following instructions are also useful if you want to compile OpenCV under Windows or Mac OS X. Linux package management systems (apt-get, yum and etc.) may provide the needed version of the OpenCV library. As first step, download and install CMake and Apache Ant, if you don’t have any of these. Download the OpenCV library from the its website. Extract the downloaded OpenCV file in a location of your choice and open CMake ( cmake-gui ). Put the location of the extracted OpenCV library in the Where is the source code field (e.g., /opencv/) and insert the destination directory of your build in the Where to build the binaries field (e.g., /opencv/build). At last, check the Grouped and Advanced checkboxes. Now press Configure and use the default compilers for Unix Makefiles. Please, be sure to have installed a C/C++ compiler. In the Ungrouped Entries group, insert the path to the Apache Ant executable (e.g., /apache-ant-1.9.6/bin/ant). In the BUILD group, unselect: - BUILD_PERF_TESTS - BUILD_SHARED_LIBRARY to make the Java bindings dynamic library all-sufficient - BUILD_TESTS - BUILD_opencv_python In the CMAKE group, set to Debug (or Release) the CMAKE_BUILD_TYPE. In the JAVA group: - insert the Java AWT include path (e.g., /usr/lib/jvm/java-1.8.0/include/) - insert the Java AWT library path (e.g., /usr/lib/jvm/java-1.8.0/include/jawt.h) - insert the Java include path (e.g., /usr/lib/jvm/java-1.8.0/include/) - insert the alternative Java include path (e.g., /usr/lib/jvm/java-1.8.0/include/linux) - insert the JVM library path (e.g., /usr/lib/jvm/java-1.8.0/include/jni.h) Press Configure twice, and the CMake window should appear with a white background. Now, press Generate and close CMake. Now open the terminal, go to the build folder of OpenCV and compile everything with the command: 1$ make -j Notice that the -j flag tells make to run in parallel with the maximum number of allowed job threads, which makes the build theoretically faster. Wait for the process to be completed… If everything went well you should have opencv-3xx.jar in the /opencv/build/bin directory and libopencv_java3xx.so in the /opencv/build/lib directory. The 3xx suffix of each file is a shortcut for the current OpenCV version, e.g., it will be 300 for OpenCV 3.0 and 310 for OpenCV 3.1. This is everything you need. Set up OpenCV for Java in Eclipse Open Eclipse and select a workspace of your choice. Create a User Library, ready to be used on all your next projects: go to Window &gt; Preferences.... From the menu navigate under Java &gt; Build Path &gt; User Libraries and choose New.... Enter a name for the library (e.g., opencv) and select the newly created user library. Choose Add External JARs..., browse to select opencv-3xx.jar from your computer. After adding the jar, extend it, select Native library location and press Edit.... Select External Folder... and browse to select the folder containing the OpenCV libraries (e.g., C:\opencv\build\java\x64 under Windows). In case of MacOS, if you installed OpenCV without Homebrew, you need to create a soft link with .dylib extension for the .so file. E.g., from the terminal, type: 1$ ln -s libopencv_java300.so libopencv_java300.dylib Set up OpenCV for Java in other IDEs (experimental) If you are using IntelliJ, you can specify the location of the library with the VM argument -Djava.library.path=/opencv/build/lib. Reference OpenCV Java Tutorials]]></content>
      <categories>
        <category>Setting</category>
      </categories>
      <tags>
        <tag>opencv</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MacOS Dictionary Application Extension]]></title>
    <url>%2F2017%2F04%2F03%2FMacOS-Dictionary-Application-Extension%2F</url>
    <content type="text"><![CDATA[Here are two ways to build your MacOS Discionary to make it powerful. ## Use DictUnifier Toolkit ### Dictionary Conversion Tool Download DictUnifier through this link, uncompressing it after downloeded, since it is a .app software, so we can use it on Mac directly. The Dictionary packages are available here. Convert Dict After downloaded your preferred dictionary sources, we need to convert them to the format can be recognized by MacOS Dictionary Application. This process is easy, open DictUnifier, drag the .ifo file in your downloaded dictionary folder to DictUnifier. Then all you need to do is waiting for the conversion and installation complete. (It may spend a few time, depending on the size of dictionary, do not think of the app broke down) Set MacOS Dictionary After the custom dictionary is successfully installed, the dictionary application will open automaticlly, click All, you will find the name of the new installed dictionary source. If not, then go to the Dictionary Preferences, check your needed dictionary sources. Unselect and Delete Dictionary Source If you do not want to use any dictionary source, just go to the Dictionary Preferences and uncheck the related sources. If you want to permanently delete any dictionary source. Then find the source under those two folder and delete: &lt;YOUR_DISK_NAME&gt; &gt; Library &gt; Dictionaries &gt; &lt;YOUR_DISK_NAME&gt; &gt; Users &gt; YOUR_USERNAME &gt; Library &gt; Dictionaries &gt; Use PyGlossary Toolkit Please go to the GitHub repository: pyglossary, here gives all the requirements and usage to install and play with PyGlossary Toolkit. Dictionaries resource StarDict Dictionaries: Here includes amount of different kinds of dictionaries Longman 5 &amp; extras Collins Cobuild English-English Macmillan English Dictionary, secret key: ve73 Oxford, Longman, Combridge, Collins 4in1 Japanese-Chinese Dict 汉典 Mdic Share Web Reference Links https://www.douban.com/group/topic/9591106/ https://www.zhihu.com/question/20428599]]></content>
      <categories>
        <category>Setting</category>
      </categories>
      <tags>
        <tag>mac os x</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Configure BitBucket Git Repository in Eclipse]]></title>
    <url>%2F2017%2F04%2F01%2FConfigure-BitBucket-Git-Repository-in-Eclipse%2F</url>
    <content type="text"><![CDATA[Bitbucket is a distributed version control system (DVCS) code hosting site that supports Mercurial and Git. It provides a fully-featured environment for managing development project, including code repository, wiki, powerful issue tracker and easy collaboration with others. The most important thing is that it is free to create a Private repository compare to GitHub, so it is a cheap and convenient way for individuals or a small team to manage their private projects. Here assume that you already have a BitBucket account. (If not, register here) Create repository on BitBucket Create a private/public repository, then on the overview page, you can get the repository information you created. Click the repository you created, then you will access the overview of the repository, see below The URL marked by a red line is repository URL which we will use later to import project into Eclipse. Make Eclipse ready for Git Open Eclipse and click on Help menu, then click Install New Softwares After that, click add button, on the pop-up window, input eGit to name and the URL http://download.eclipse.org/egit/updates to Location. Select Git integration for Eclipse and Java implementation for Git options and click next and finish install. Add Git to Perspective Open Perspective (red zone of below image) and choose Git from list. Clone Repository from BitBucket Enter Git Perspective, and click clone repository button. Then enter your Bitbucket URL and User Information as mentioned in below diagram. Click Next and Finish. No need to change other configuration in next window. Now, You should see your Bitbucket repository now in eclipse. Import project to Eclipse Import the project so you can work on the source code. Click: File &gt; Import &gt; Git &gt; Projects from Git &gt; Existing local repository &gt; Select a Git Repository &gt; Import as General Project &gt; Next &gt; Finish. The code should then appear in your ‘Project Explorer’ window as a normal project. Then you can make changes to your file as you want, and you can look at Git Staging view to see your changed files. Reference Link: http://crunchify.com/how-to-configure-bitbucket-git-repository-in-you-eclipse/]]></content>
      <categories>
        <category>Setting</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Skiing In Singapore]]></title>
    <url>%2F2017%2F03%2F30%2FSkiing-In-Singapore%2F</url>
    <content type="text"><![CDATA[It is an online test provided by RedMart, who asks to solve a problem by using DFS algorithm. More details you can find here. Discription Sometimes it’s nice to take a break and code up a solution to a small, fun problem. Here is one some of our engineers enjoyed recently called Skiing In Singapore. Well you can’t really ski in Singapore. But let’s say you hopped on a flight to the Niseko ski resort in Japan. Being a software engineer you can’t help but value efficiency, so naturally you want to ski as long as possible and as fast as possible without having to ride back up on the ski lift. So you take a look at the map of the mountain and try to find the longest ski run down. In digital form the map looks like the number grid below. 123454 44 8 7 32 5 9 36 3 2 54 4 1 6 The first line (4 4) indicates that this is a 4x4 map. Each number represents the elevation of that area of the mountain. From each area (i.e. box) in the grid you can go north, south, east, west - but only if the elevation of the area you are going into is less than the one you are in. I.e. you can only ski downhill. You can start anywhere on the map and you are looking for a starting point with the longest possible path down as measured by the number of boxes you visit. And if there are several paths down of the same length, you want to take the one with the steepest vertical drop, i.e. the largest difference between your starting elevation and your ending elevation. On this particular map the longest path down is of length = 5 and it’s highlighted in bold below: 9-5-3-2-1. 123454 44 8 7 32 5 9 36 3 2 54 4 1 6 There is another path that is also length five: 8-5-3-2-1. However the tie is broken by the first path being steeper, dropping from 9 to 1, a drop of 8, rather than just 8 to 1, a drop of 7. The task is to write a program to find the longest (and then steepest) path on this map specified in the format above. It’s 1000x1000 in size, and all the numbers on it are between 0 and 1500. Solution Traverse Traversing every element of the matrix, which means traverse every location of the mountain geographically, and computing the maximal path length as well as the first maximal drop of such max length through dfs method. 12345678// update the path matrix and drop matrix using dfs searchfor (int i = 0; i &lt; row; i++) &#123; for (int j = 0; j &lt; column; j++) &#123; int[] tmp = dfsForPathAndDrop(i, j); path[i][j] = tmp[0]; drop[i][j] = map[i][j] - map[tmp[1]][tmp[2]]; &#125;&#125; DFS search For each location (point), search its four neighbors, i.e., up, down, left, right, to find the value of any neighbor is small than its value, which means it is able to ski down, then update the path length for each recursion. 12345678910111213141516171819202122232425262728293031// computing the maximal path length and its droppublic static int[] dfsForPathAndDrop(int i, int j) &#123; int[] pathAndDrop = &#123; 0, i, j &#125;; // first value is path length, follows by x,y values int[] curPathAndDrop = new int[2]; // search from the up direction if (j &gt; 0 &amp;&amp; map[i][j] &gt; map[i][j - 1]) &#123; curPathAndDrop = dfsForPathAndDrop(i, j - 1); if (curPathAndDrop[0] &gt; pathAndDrop[0]) // if the current path value is larger pathAndDrop = Arrays.copyOf(curPathAndDrop, curPathAndDrop.length); // update the path length &#125; // search from the down direction if (j &lt; (column - 1) &amp;&amp; map[i][j] &gt; map[i][j + 1]) &#123; curPathAndDrop = dfsForPathAndDrop(i, j + 1); if (curPathAndDrop[0] &gt; pathAndDrop[0]) pathAndDrop = Arrays.copyOf(curPathAndDrop, curPathAndDrop.length); &#125; // search from the left direction if (i &gt; 0 &amp;&amp; map[i][j] &gt; map[i - 1][j]) &#123; curPathAndDrop = dfsForPathAndDrop(i - 1, j); if (curPathAndDrop[0] &gt; pathAndDrop[0]) pathAndDrop = Arrays.copyOf(curPathAndDrop, curPathAndDrop.length); &#125; // search from the right direction if (i &lt; (row - 1) &amp;&amp; map[i][j] &gt; map[i + 1][j]) &#123; curPathAndDrop = dfsForPathAndDrop(i + 1, j); if (curPathAndDrop[0] &gt; pathAndDrop[0]) pathAndDrop = Arrays.copyOf(curPathAndDrop, curPathAndDrop.length); &#125; pathAndDrop[0]++; // update the path length for each recursion return pathAndDrop;&#125; Record Ski Path The above codes will generate a path matrix and drop matrix, the path matrix stores the maximal path length of each element and the drop matrix stores the maximal drop of each element, then you only need to traverse the matrix again and pick up the element with maximal path length and drop among all. However, here also gives a method to record the ski path: 1234567891011121314151617181920212223242526272829303132// record the path of the maximal path lengthpublic static List&lt;Integer&gt; dfsForPath(int x, int y) &#123; List&lt;Integer&gt; list = new ArrayList&lt;Integer&gt;(); List&lt;Integer&gt; curPathList = new ArrayList&lt;Integer&gt;(); // search from the up direction if (y &gt; 0 &amp;&amp; map[x][y] &gt; map[x][y - 1]) &#123; curPathList = dfsForPath(x, y - 1); if (curPathList.size() &gt; list.size()) list = curPathList; &#125; // search from the down direction if (y &lt; (column - 1) &amp;&amp; map[x][y] &gt; map[x][y + 1]) &#123; curPathList = dfsForPath(x, y + 1); if (curPathList.size() &gt; list.size()) list = curPathList; &#125; // search from the left direction if (x &gt; 0 &amp;&amp; map[x][y] &gt; map[x - 1][y]) &#123; curPathList = dfsForPath(x - 1, y); if (curPathList.size() &gt; list.size()) list = curPathList; &#125; // search from the right direction if (x &lt; (row - 1) &amp;&amp; map[x][y] &gt; map[x + 1][y]) &#123; curPathList = dfsForPath(x + 1, y); if (curPathList.size() &gt; list.size()) list = curPathList; &#125; list.add(y); // add y first, because we will reverse the list later list.add(x); // add x return list;&#125; Source Full codes of skiing in singapore solution can be found in my GitHub repository: [link]. For a better understanding of Depth-first search (dfs) algorithm, please visit its wikipedia page: [link].]]></content>
      <categories>
        <category>Algorithms</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>algorithms</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MacOS Useful Commands]]></title>
    <url>%2F2017%2F03%2F30%2FMacOS-Useful-Commands%2F</url>
    <content type="text"><![CDATA[Here are some commonly used commands in Max OS X… MacOS Terminal Themes &amp; VIM Follow this GitHub repository to build your own powerful and colorful Terminal: [color-theme-link], [vimrc-link] Shut down and trun on local back up shut down: 1$ sudo tmutil disablelocal trun on: 1$ sudo tmutil enablelocal Modify the icon size of Launchpad 123$ defaults write com.apple.dock springboard-rows -int 7 # change the size by modifying the number$ defaults write com.apple.dock springboard-columns -int 6 # change the size by modifying the number$ killall Dock Reset: 12$ defaults write com.apple.dock ResetLaunchPad -bool TRUE$ killall Dock Change source ‘.bash_profile’ to source ‘.profile’ 123$ cat .bash_profile &gt;&gt; .profile$ rm .bash_profile (optional)$ echo "source ~/.profile" &gt;&gt; .bash_profile open port 22 in Mac OS X Handling the problem of: 12$ ssh localhostssh: connect to host localhost port 22: Connection refused The above problem happens since the remote login is closed in Mac OS X 12$ sudo systemsetup -getremoteloginRemote Login: off Here is the solution: 1234$ sudo systemsetup -setremotelogin on$ ssh localhostLast login: ......$ _ Uninstall MySQL on Mac 123456$ cd ~/$ sudo rm /usr/local/mysql$ sudo rm -rf /usr/local/var/mysql$ sudo rm -rf /usr/local/mysql*$ sudo rm -rf /Library/StartupItems/MySQLCOM$ sudo rm -rf /Library/PreferencePanes/My* Then open hostconfig: 1$ vim /etc/hostconfig and removed the line: MYSQLCOM=-YES-rm -rf ~/Library/PreferencePanes/My*, after that, removing following: 123$ sudo rm -rf /Library/Receipts/mysql*$ sudo rm -rf /Library/Receipts/MySQL*$ sudo rm -rf /var/db/receipts/com.mysql.* Reference How do I open port 22 in OS X 10.6.7]]></content>
      <categories>
        <category>Setting</category>
      </categories>
      <tags>
        <tag>mac os x</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Submit Sitemap to Google for Hexo Blog]]></title>
    <url>%2F2017%2F03%2F28%2F%E5%90%91Google%E6%8F%90%E4%BA%A4Sitemap%2F</url>
    <content type="text"><![CDATA[Sitemap 可方便网站管理员通知搜索引擎他们网站上有哪些可供抓取的网页。最简单的 Sitemap 形式，就是XML文件，在其中列出网站中的网址以及关于每个网址的其他元数据(上次更新的时间、更改的频率以及相对于网站上其他网址的重要程度为何等)，以便搜索引擎可以更加智能地抓取网站。 提交自己的Hexo博客的sitemap到Google，有助于让别人更好地通过Google搜索到自己的博客。 生成sitemap.xml文件 安装hexo-generator-sitemap请参考: [Hexo添加tags-categories-css-sitemap-search]，然后执行下面命令将会在public文件夹生成sitemap.xml文件。 1$ hexo g 将代码部署到GitHub服务器上，在browser中输入你的域名/sitemap.xml就能看见生成的sitemap.xml文件。 例如我的博客: https://isaacchanghau.github.io/sitemap.xml 向Google提交网页 首先使用自己的Google账号登陆Google Webmaster Central: [link]。 进入后如下图: 点击ADD A PROPERTY，输入你的网址URL(e.g.: 我的网址是https://isaacchanghau.github.io/)，然后点击Continue。 Google验证网页所有权 上一步点击Continue后将进入验证所有权页面。Google提供几种不同的验证方式: Recommended method: (图中的这个网址我随便输的，用来举个例子) Alternate methods: 这里我使用的是第一种方式，直接下载页面给的HTML verification file，然后将该文件上传到我的网页来进行验证。 对于Alternate methods中的方法: HTML tag – 参考Fiona’s Blog: [link] 其余三种方式(Domain name provider，Google Analytics，Google Tag Manager)请自行Google相关资料。 Google网站站长上传sitemap 进入Google Search Console: [link]。 因为之前已经向Google提交并验证了你的网页，所以进入后能看见自己的网页缩略图: 这里点击View details，进入下一个页面: 点击第五栏Submit a Sitemap: 之后点击ADD/TEST SITEMAP，输入你的sitemap.xml的链接(e.g.: 我的sitemap是https://isaacchanghau.github.io/sitemap.xml)，然后提交即可。 至此，所有步骤完成。 参考链接 Fiona’s Blog: [link]。 百度百科: [link]。]]></content>
      <categories>
        <category>Setting</category>
      </categories>
      <tags>
        <tag>mac os x</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Setup Personal Blog by GitHub+Hexo on Mac OS X]]></title>
    <url>%2F2017%2F03%2F28%2FMac%E7%AB%AFGitHub%2BHexo%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[这是我的第一篇博客… ## 动机 一切的一切开始于前几日读到的这篇文章: 我为什么鼓励工程师写blog。本人工作是Research Engineer，这是一个非常…不知道怎么形容的职业。有时候感觉自己是研究人员，每天埋头于各种paper和文献中，了解和学习各种相关的科技和知识。有时候感觉自己就是一个程序员，看完了别人的方法，要么发现没有源码，要么发现源码是Python或Matlab(这里没有要黑Python和Matlab的意思，这两种语言开发起来快，用来研究很适合)，无奈本人最熟悉的语言是Java，所以也只能委曲求全自己动手实现。于是，从毕业到现在工作也大概有一年了，稍微回想一下，感觉好像学习了不少知识，细细想想却发现很多东西已经想不起来了。然后就读到了这篇文章，同时意识到了写blog的重要性，也就有了现在的入坑行为。本人在仔细研究了很多搭建blog的文章之后，发现GitHub和Hexo的组合是相对简单的方式，省去了自己维护的麻烦，能够专心的投入到写文章。 ## 准备 硬件: Mac电脑 软件: Homebrew，GitHub，node.js，npm(node.js集成有npm, 无需下载)，hexo 环境搭建 安装Homebrew Homebrew是一款自由及开放源代码的软件包管理系统，用Ruby语言开发，支持千余种开源软件在Mac中的部署和管理，用以简化Mac OS X系统上的软件安装过程。Homebrew项目托管在Github上，网址为: https://github.com/mxcl/homebrew。 安装Homebrew: 1$ /usr/bin/ruby -e "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)" 安装完成后，Homebrew的主程序安装在“/usr/local/bin/brew”中，在目录“/usr/local/Library/Formula/”下保存了Homebrew 支持的所有软件的安装指引文件。 安装Git 方法1. 下载GitHub Desktop进行安装。 方法2. 使用Homebrew安装git: 1$ brew install git 使用Homebrew方式安装，Git将被安装在“/usr/local/Cellar/git/version/”中，可执行程序自动在“/usr/local/bin”目录下创建符号连接，可以直接在终端程序中访问。 通过如下命令可以查询相关包的安装路径: 1$ brew list git 关于git的配置，请参照官网给出的详细资料: [link]。 安装node.js 建议直接去官网下载安装包安装。 检验是否安装成功，终端输入 -v，返回版本号则安装成功: 1234Isaac-Changhaus-MacBook-Pro:~ zhanghao$ node -vv4.5.0Isaac-Changhaus-MacBook-Pro:~ zhanghao$ npm -v4.1.2 安装hexo 使用npm安装hexo，官方网章上给出了详尽的安装方法: https://hexo.io/docs/，或使用下面给出的npm安装方式。 1$ npm install hexo --no-optional 检验是否安装成功，终端输入 -v，返回版本号则安装成功: 12345678910111213Isaac-Changhaus-MacBook-Pro:~ zhanghao$ hexo -vhexo: 3.3.5hexo-cli: 1.0.2os: Darwin 16.5.0 darwin x64http_parser: 2.7.0node: 4.5.0v8: 4.5.103.37uv: 1.9.1zlib: 1.2.8ares: 1.10.1-DEVicu: 56.1modules: 46openssl: 1.0.2h 配置 创建GitHub pages 两种pages模式: User/Organization Pages 个人或公司站点 (1). 使用自己的用户名，每个用户名下面只能建立一个。 (2). 资源命名必须符合这样的规则username/username.github.com (or, username/username.github.io)。 (3). 主干上内容被用来构建和发布页面。 Project Pages 项目站点 (1). gh-pages分支用于构建和发布。 (2). 如果user/org pages使用了独立域名，那么托管在账户下的所有project pages将使用相同的域名进行重定向，除非project pages使用了自己的独立域名。 (3). 如果没有使用独立域名，project pages将通过子路径的形式提供服务username.github.com/projectname。 (4). 自定义404页面只能在独立域名下使用，否则会使用User Pages 404。 本人使用的是第一种模式，因此只介绍第一种模式，第二中模式请自行参照官网介绍: [link]。 第一步: 创建个人站点 第二步: 将创建的仓库clone到本地自己建立的一个站点文件夹，可使用GitHub Desktop或者使用git命令(PORJECT为你创建的repository的名字) 1$ git clone https://github.com/USERNAME/PROJECT.git PROJECT 之后，你可以将内容拷贝到这个文件夹中，进行发布。 配置hexo 执行下列命令，hexo将在指定文件夹中新建所需要的文件: 123$ hexo init &lt;folder&gt;$ cd &lt;folder&gt;$ npm install 新建完成后，指定文件夹的目录如下: 123456789.├── _config.yml├── package.json├── scaffolds├── scripts├── source| ├── _drafts| └── _posts└── themes 搭建好本地站点之后，cd进站点文件夹，使用如下命令生成静态网页: 1$ hexo g 之后执行如下命令启动站点，站点默认端口为: “localhost:4000” 1$ hexo server 关于如何使用hexo撰写和发布文章，请参考官网介绍: [link]。这里给出Hexo的主题网站: [link]，在这里可以找到很多优秀的主题。 主题及相关配置 本人使用的是hexo-next主题。 以下是本人的一些简单配置 安装: 12$ npm install hexo-renderer-scss --save$ git clone https://github.com/iissnan/hexo-theme-next themes/next 修改配置文件(_config.yml)中的theme字段为next: 1234# Extensions## Plugins: http://hexo.io/plugins/## Themes: http://hexo.io/themes/theme: next 主题的一些相关配置请参照Next主题的GitHub-Issue: [link]. Hexo MathJax插件安装 安装: 1$ npm install hexo-math --save 在blog文件夹中初始化 1$ hexo math install 在_config.yml文件中添加: 12plugins:- hexo-math 使用教程参照官网: [link] &gt; [How to config hexo-math to make it work?] 更换Hexo的markdown引擎解决公式渲染问题 123$ brew install pandoc$ npm uninstall hexo-renderer-marked --save$ npm install hexo-renderer-pandoc --save [Hexo下mathjax的转义问题] [可以实现行内代码不能实现行间代码！] 部署到GitHub 安装: 1npm install hexo-deployer-git --save 在_config.yml文件中添加: 1234deploy: type: git repository: https://github.com/IsaacChanghau/IsaacChanghau.github.io branch: master 设置网页背景图像 首先在themes/next下的source\css\_custom中创建img-bg.styl，并输入： 12345678910111213//#header &#123;// background: url("../images/settings/background-header.jpg");// background-size: cover;//&#125;body &#123; background: url("../images/settings/wallpaper.jpg"); background-size: cover; // cover the whole paper background-attachment: fixed; // set background do not follow the page to move&#125;//#footer &#123;// background: url("../images/settings/background-footer.jpg");// background-size: cover;//&#125; 之后在source\css\main.styl中引入添加的.styl文件 12345// Custom Layer// --------------------------------------------------@import "_custom/custom";// add background-image@import "_variables/img-bg.styl"; 更改链接颜色 控制链接的CSS在themes/next/source/css/_variables目录下的base.styl中 12345// Global link color.$link-color = $black-light$link-hover-color = $black-deep$link-decoration-color = $grey-light$link-decoration-hover-color = $black-deep 更改对应的颜色即可。 添加标签(tags)功能 首先在站点文件夹下创建标签页面: 12$ cd your-hexo-site$ hexo new page "tags" hexo将在source/tags/index.md自动创建。进入index.md，修改内容: 1234567---title: Tagclouddate: 2017-05-02 12:39:04type: "tags"layout: "tags"comments: false--- 如果有启用多说或者Disqus评论，默认页面会带有评论。需要关闭的话，添加字段comments并将值设置为false。 之后进入scaffolds/draft.md文件，添加: 1234---title: &#123;&#123; title &#125;&#125;tags: &#123;&#123; tags &#125;&#125;--- 在scaffolds/post.md中添加: 12345---title: &#123;&#123; title &#125;&#125;date: &#123;&#123; date &#125;&#125;tags: &#123;&#123; tags &#125;&#125;--- 最后，编辑当前使用主题下的_config.yml，添加tags到menu中: 1234menu: home: / archives: /archives tags: /tags 添加分类(categories)功能 首先在站点文件夹下创建分类页面: 12$ cd your-hexo-site$ hexo new page "categories" 其余步骤与创建tags类似。 添加RSS订阅功能 效果图: 首先在站点文件夹下用npm安装hexo-generator-feed 1$ npm install hexo-generator-feed --save 安装完后，会生成如下输出: 123Isaac-Changhaus-MacBook-Pro:hexo zhanghao$ npm install hexo-generator-feed --savehexo-site@0.0.0 /Users/zhanghao/Documents/hexo└── hexo-generator-feed@1.2.0 并且在node_modules目录下生成hexo-generator-feed目录 完成hexo-generator-feed安装后，配置其到根目录下的_config.yml 12345678910# Extensions# Plugins: http://hexo.io/plugins/# RSS Subscriptionsplugin:- hexo-generator-feed# Feed Atomfeed:type: atompath: atom.xmllimit: 20 之后在你使用的主题中的_config.yml添加RSS订阅，我使用的是next主题 1234# Set rss to false to disable feed link.# Leave rss as empty to use site's feed link.# Set rss to specific value if you have burned your feed already.rss: 添加站点地图(sitemap) 过程与添加rss类似，首先下载插件，在站点文件夹下用npm安装hexo-generator-sitemap: 1$ npm install hexo-generator-sitemap --save 引用插件，同样在根目录下的_config.yml添加: 12plugin:- hexo-generator-sitemap 然后添加如下信息: 12sitemap: path: sitemap.xml 访问/sitemap.xml就能看到站点地图。 添加站内搜索(search) 同样的，首先下载插件，在站点文件下使用npm安装hexo-generator-search: 1$ npm install hexo-generator-search --save 配置_config.yml文件: 123search: path: search.xml field: post 添加字数统计 首先在博客目录下使用 npm 安装插件： 1$ npm install hexo-wordcount --save 在配置文件（站点配置文件或主题配置文件均可）中添加一个键值对: 1word_count: true 修改themes/next/layout/_macro/post.swig，在class为post-mata的div中的添加如下内容: 12345678910&#123;% if theme.word_count %&#125; &lt;span class=&quot;post-letters-count&quot;&gt; &amp;nbsp; | &amp;nbsp; &lt;i class=&quot;fa fa-file-word-o&quot;&gt;&lt;/i&gt; &lt;span&gt;Count &#123;&#123; wordcount(post.content) &#125;&#125; words&lt;/span&gt; &amp;nbsp; | &amp;nbsp; &lt;i class=&quot;fa fa-clock-o&quot;&gt;&lt;/i&gt; &lt;span&gt;Reading time &#123;&#123; min2read(post.content) &#125;&#125; min&lt;/span&gt; &lt;/span&gt;&#123;% endif %&#125; 参考链接 如何在 GitHub 上写博客 hexo你的博客 Hexo搭建Github-Pages博客填坑教程 Hexo_NexT主题-个性化配置记录 hexo next 主题文章内容美化 Font Awesome Cheatsheet 为Hexo NexT主题添加字数统计功能 给 Next 主题添加文章更新时间 打造个性超赞博客Hexo+NexT+GithubPages的超深度优化]]></content>
      <categories>
        <category>Setting</category>
      </categories>
      <tags>
        <tag>mac os x</tag>
      </tags>
  </entry>
</search>
