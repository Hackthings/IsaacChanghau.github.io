<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Isaac Changhau</title>
  
  <subtitle>Stay Hungry, Stay Foolish</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://isaacchanghau.github.io/"/>
  <updated>2017-11-15T15:24:14.000Z</updated>
  <id>https://isaacchanghau.github.io/</id>
  
  <author>
    <name>Isaac Changhau</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>A Letter to the Past October</title>
    <link href="https://isaacchanghau.github.io/2017/11/15/%E5%86%99%E7%BB%99%E5%8D%81%E6%9C%88%E7%9A%84%E4%B8%80%E5%B0%81%E4%BF%A1/"/>
    <id>https://isaacchanghau.github.io/2017/11/15/写给十月的一封信/</id>
    <published>2017-11-15T15:22:55.000Z</published>
    <updated>2017-11-15T15:24:14.000Z</updated>
    
    <content type="html"><![CDATA[<p>任何一种环境或一个人，初次见面就预感到离别的隐痛时，你必定是爱上TA了……<a id="more"></a></p><p>过去的十月，很漫长，发生了太多。</p><p>一段三年的感情走到尽头，所有的经历，所有的回忆，顷刻间，化为乌有…<br><img src="/images/life/huizi.png" alt="huizi"></p><p>养了快一年的乔巴宝贝，一场车祸… 可能是上帝觉得它太可爱了，所以起了私心，想在天堂里让它作伴，希望你能在新的“家里”也过得好…<br><img src="/images/life/choper.png" alt="Choper"></p><p>也是十月，我的生命中出现了一个意外，不！更诚实，更具体的说，那是一场命中注定…</p><p>如果…把心掏给你是我的错…我会一点一点把它赎回来…剩下的、赎不回来的，就放在你那儿吧，反正它本来也属于你，就让它慢慢失去温度、冰冷、然后死去…</p><blockquote><p>In the end, I struggled, ran out of my enthusiasm, lost my own pain at your lips, a touch of sweet, it’s just a laniated miracle.</p></blockquote><p>你出现了，像光那样，从一颗星到达另外一颗星。后来，你又离开了。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;任何一种环境或一个人，初次见面就预感到离别的隐痛时，你必定是爱上TA了……
    
    </summary>
    
      <category term="Life" scheme="https://isaacchanghau.github.io/categories/Life/"/>
    
    
      <category term="life" scheme="https://isaacchanghau.github.io/tags/life/"/>
    
  </entry>
  
  <entry>
    <title>PTransE -- Modeling Relation Paths for Representation Learning of Knowledge Bases</title>
    <link href="https://isaacchanghau.github.io/2017/09/19/PTransE/"/>
    <id>https://isaacchanghau.github.io/2017/09/19/PTransE/</id>
    <published>2017-09-19T06:33:15.000Z</published>
    <updated>2018-01-30T08:33:35.000Z</updated>
    
    <content type="html"><![CDATA[<p>It is a summary of the paper <strong>Modeling Relation Paths for Representation Learning of Knowledge Bases</strong> (<a href="https://arxiv.org/pdf/1506.00379.pdf" target="_blank" rel="noopener"><strong>PTransE</strong></a>).<a id="more"></a> PTransE is a novel extension of <a href="https://papers.nips.cc/paper/5071-translating-embeddings-for-modeling-multi-relational-data.pdf" target="_blank" rel="noopener">TransE</a>, which is a path-based representation learning model, to model relation paths for representation learning of knowledge bases. The authors argue that multiple-step relation paths also contain rich inference patterns between entities, thus, PTransE also considers relation paths as translations between entities for representation learning, and addresses two key challenges:</p><ul><li>Since not all relation paths are reliable, so a <strong>path-constraint resource allocation</strong> (PCRA) algorithm is designed to measure the reliability of relation paths.</li><li>Relation paths is represented via semantic composition of relation embeddings.</li></ul><p>Despite the success of <a href="https://papers.nips.cc/paper/5071-translating-embeddings-for-modeling-multi-relational-data.pdf" target="_blank" rel="noopener">TransE</a>, <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.486.2800&amp;rep=rep1&amp;type=pdf" target="_blank" rel="noopener">TransH</a>, <a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/view/9571/9523" target="_blank" rel="noopener">TransR</a> and <a href="http://www.aclweb.org/anthology/P15-1067" target="_blank" rel="noopener">TransD</a> in modeling relational facts, these methods only take direct relations between entities into consideration (More details in <a href="https://isaacchanghau.github.io/2017/09/14/TransX/">TransX – Embedding Entities and Relationships of Multi-relational Data</a>). Actually, there are also substantial multiple-step relation paths between entities indicating their semantic relationships. For instance:$$<br>\mathbf{h}\xrightarrow{BornInCity}\mathbf{e}_{1}\xrightarrow{CityInState}\mathbf{e}_{2}\xrightarrow{StateInCountry}\mathbf{t}<br>$$which indicates the relation <code>Nationality</code> between $\mathbf{h}$ and $\mathbf{t}$, i.e., $(\mathbf{h},\textrm{Nationality},\mathbf{t})$. However, in PTransE, in addition to direct connected relational facts, the authors also build triples from knowledge bases using entity pairs connected with relation paths. As shown below<br><img src="/images/nlp/transx/ptranse-graph.png" alt="PTransE Graph"><br>TransE only considers direct relations between entities and optimizes the objective $h+r\approx t$, while PTransE generalizes TransE by regarding multiple-step relation paths as connections between entities, where $\circ$ is an operation to join the relations $r_{1}$ and $r_{2}$ together into a unified relation path representation.</p><p><strong>PTransE</strong>:<br>Define entity set $\mathbf{E}$ and relation set $\mathbf{R}$ are in $\mathbb{R}^{k}$, $S=\{(h,r,t)\}$ represents a set of triples. For each triple $(h,r,t)$, the energy of TransE is computed by$$<br>E(h,r,t)=\Vert h+r-t\Vert\tag{1}<br>$$Since TransE only consider the direct relations, PTransE also considers the relation paths for representation. Suppose the multiple relation paths $P(h,t)=\{p_{1},\dots,p_{N}\}$ connecting two entities $h$ and $t$, where relation path $p=\{r_{1},\dots,r_{l}\}$ indicates $h\xrightarrow{r_{1}}\dots\xrightarrow{r_{l}}t$. For each triple $(h,r,t)$, the energy function of PTransE is defined as$$<br>G(h,r,t)=E(h,r,t)+E(h,P,t)\tag{2}<br>$$where $E(h,r,t)$ models correlations between relations and entities with direct relation triples, $E(h,P,t)$ models the inference correlations between relations with multiple-step relation path triples:$$<br>E(h,P,t)=\frac{1}{Z}\sum_{p\in P(h,t)}R(p|h,t)E(h,p,t)\tag{3}<br>$$ where $R(p|h,t)$ indicates the reliability of the relation path $p$ given the entity pair $(h,t)$, $Z=\sum_{p\in P(h,t)}R(p|h,t)$ is a normalization factor, and $E(h,p,t)$ is the energy function of the triple $(h,p,t)$. The component $R(p|h,t)$ concerns about relation path reliability, and $E(h,p,t)$ concerns about relation path representation.</p><p><strong>Relation Path Reliability</strong>:<br>Since not all relation paths are meaningful and reliable for learning. the authors propose a <strong>path-constraint resource allocation</strong> (PCRA) algorithm to measure the reliability of relation paths and only the reliable relation paths are selected. The basic idea is that a certain amount of resource is associated with the head entity $h$, and will flow following the given path $p$. Using the resource amount that eventually flows to the tail entity $t$ to measure the reliability of the path $p$ as a meaningful connection between $h$ and $t$.</p><p>For a triple $(h,p,t)$, the resource amount flowing from $h$ to $t$ is computed as follows. The flowing path is writen as $S_{0}\xrightarrow{r_{1}}S_{1}\xrightarrow{r_{2}}\dots\xrightarrow{r_{l}}S_{l}$, where $S_{0}=h$ and $t\in S_{l}$. For any entity $m\in S_{i}$, denote its direct predecessors along relation $r_{i}$ in $S_{i-1}$ as $S_{i-1}(\centerdot,m)$. The resource flowing to m is defined as$$<br>R_{p}(m)=\sum_{n\in S_{i-1}(\centerdot,m)}\frac{1}{\vert S_{i}(n,\centerdot)\vert}R_{p}(n)\tag{4}<br>$$where $S_{i}(n,\centerdot)$ is the direct successors of $n\in S_{i-1}$ following the relation $r_{i}$, and $R_{p}(n)$ is the resource obtained from the entity $n$. For each relation path $p$, we set the initial resource in $h$ as $R_{p}(h)=1$. By performing resource allocation recursively from $h$ through $p$, tail entity $t$ eventually obtains the resource $R_{p}(t)$ which indicates how much information of head entity $h$ can be well translated, thus, using $R_{p}(t)$ to measure the reliability of the path $p$ given $(h,t)$, i.e., $R(p\vert h,t)=R_{p}(t)$.</p><p><strong>Relation Path Representation</strong>:<br>For <strong>relation path representation</strong>, the authors argue that the semantic meaning of a relation path depends on all relations in this path, thus, given a relation path $p=(r_{1},\dots,r_{l})$, they define and learn a binary (composition) operation function ($\circ$) to obtain the path embedding $p$ by recursively composing multiple relations. Formally, for a path $p=(r_{1},\dots,r_{l})$, the path embeddingas is obtained by $p=r_{1}\circ\centerdot\centerdot\centerdot\circ r_{l}$. As shown below<br><img src="/images/nlp/transx/path-relation.png" alt="Relation Path"><br>The authors propose three types of composition operation:</p><ul><li><strong>Addition (ADD)</strong>: The addition operation obtains the vector of a path by summing up the vectors of all relations$$<br>p=r_{1}+\dots+r_{l}\tag{5}<br>$$</li><li><strong>Multiplication (MUL)</strong>: The multiplication operation obtains the vector of a path as the cumulative product of the vectors of all relations$$<br>p=r_{1}\times\dots\times r_{l}\tag{6}<br>$$</li><li><strong>Recurrent Neural Network (RNN)</strong>: RNN is a recent neural-based model for semantic composition. The composition operation is realized using a matrix $\mathbf{W}$:$$<br>c_{i}=f(\mathbf{W}[c_{i-1};r_{i}])\tag{7}<br>$$where $f$ is a non-linearity or identical function, and $[a;b]$ represents the concatenation of two vectors. By setting $c_{1}=r_{1}$ and recursively performing RNN following the relation path, finally get $p=c_{n}$.</li></ul><p>Since $E(h,p,t)=\Vert h+p-t\Vert$, and $\Vert h+r-t\Vert$ has minimized with the direct relation triple $(h,r,t)$ to make sure $r\approx t-h$. So we can derive$$<br>E(h,p,t)=\Vert h+p-t\Vert=\Vert p-(t-h)\Vert\approx\Vert p-r\Vert=E(p,r)\tag{8}<br>$$<strong>Objective Formalization</strong>:<br>The objective of PTransE is given as$$<br>\mathcal{L}(S)=\sum_{(h,r,t)\in S}\big[L(h,r,t)+\frac{1}{Z}\sum_{p\in P(h,t)}R(p\vert h,t)L(p,r)\big]\tag{9}<br>$$Here the $L(\centerdot)$ is the margin-based loss functions$$\begin{aligned}<br>L(h,r,t)&amp;=\sum_{(h’,r’,t’)\in S^{-}}[\gamma+E(h,r,t)-E(h’,r’,t’)]_{+}\\<br>L(p,r)&amp;=\sum_{(h,r’,t)\in S^{-}}[\gamma+E(p,r)-E(p,r’)]_{+}<br>\end{aligned}$$where $[x]_{+}=\max(0,x)$ returns the maximum between $0$ and $x$, $\gamma$ is the margin, $S$ is the set of valid triples, and $S^{-}$ is the set of invalid triples derived by$$<br>S^{-}=\{(h’,r,t)\}\cup\{(h,r’,t)\}\cup\{(h,r,t’)\}\tag{10}<br>$$The objective function is also under the constraints that $\forall h,r,t$, $\Vert h\Vert_{2}\leq 1$, $\Vert r\Vert_{2}\leq 1$, $\Vert t\Vert_{2}\leq 1$.</p><p><strong>Reverse Relation Addition</strong>:<br>Besides, the authors only consider the relation paths following one direction, so in order to solve the cases, like $e_{1}\xrightarrow{BornInCity}e_{2}\xleftarrow{CityOfCountry}e_{3}$, they add reverse relations for each relation, say, for each triple $(h,r,t)$, build another $(t,r^{-1},h)$, thus, the above-mentioned path can be set as $e_{1}\xrightarrow{BornInCity}e_{2}\xrightarrow{CityOfCountry^{-1}}e_{3}$.</p><p><strong>Path Selection Limitation</strong>:<br>This process normally depends on the scale of knowledge database, generally, 3-steps are quiet enough for most cases.</p><p><strong>Resources</strong>:<br><a href="https://github.com/thunlp/Fast-TransX" target="_blank" rel="noopener">Fast-TransX</a>, <a href="https://github.com/thunlp/KB2E" target="_blank" rel="noopener">KB2E</a> from Natural Language Processing Lab at Tsinghua University (<a href="https://github.com/thunlp" target="_blank" rel="noopener">THUNLP</a>).</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;It is a summary of the paper &lt;strong&gt;Modeling Relation Paths for Representation Learning of Knowledge Bases&lt;/strong&gt; (&lt;a href=&quot;https://arxiv.org/pdf/1506.00379.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;&lt;strong&gt;PTransE&lt;/strong&gt;&lt;/a&gt;).
    
    </summary>
    
      <category term="Natural Language Processing" scheme="https://isaacchanghau.github.io/categories/Natural-Language-Processing/"/>
    
    
      <category term="deep learning" scheme="https://isaacchanghau.github.io/tags/deep-learning/"/>
    
      <category term="natural language processing" scheme="https://isaacchanghau.github.io/tags/natural-language-processing/"/>
    
      <category term="c plus plus" scheme="https://isaacchanghau.github.io/tags/c-plus-plus/"/>
    
      <category term="word embeddings" scheme="https://isaacchanghau.github.io/tags/word-embeddings/"/>
    
  </entry>
  
  <entry>
    <title>TransX -- Embedding Entities and Relationships of Multi-relational Data</title>
    <link href="https://isaacchanghau.github.io/2017/09/14/TransX/"/>
    <id>https://isaacchanghau.github.io/2017/09/14/TransX/</id>
    <published>2017-09-14T14:46:18.000Z</published>
    <updated>2018-01-30T08:34:21.000Z</updated>
    
    <content type="html"><![CDATA[<p>It is a summary of several papers about embedding entities and relationships of multi-relational knowledge database, while each paper contains one kind of specific method. Generally, the set of methods are called TransX.<a id="more"></a> Here, I only describe the general idea and mathematical process of each method, for more information about those methods in parameters setting, experiment results and discussion, you can directly read the original papers through the links I provide.</p><h1 id="Multi-relational-Data-Overview"><a href="#Multi-relational-Data-Overview" class="headerlink" title="Multi-relational Data Overview"></a>Multi-relational Data Overview</h1><p>There are a lot of relational knowledge database available nowadays, like:</p><ul><li><a href="http://conceptnet.io" target="_blank" rel="noopener">ConceptNet</a>, which is a freely-available semantic network, designed to help computers understand the meanings of words that people use.</li><li><a href="https://wordnet.princeton.edu" target="_blank" rel="noopener">WordNet</a>, which is a large lexical database of English. Nouns, verbs, adjectives and adverbs are grouped into sets of cognitive synonyms (synsets), each expressing a distinct concept. WordNet’s structure makes it a useful tool for computational linguistics and natural language processing.</li><li><a href="https://verbs.colorado.edu/verbnet/" target="_blank" rel="noopener">VerbNet</a>, which is the largest on-line verb lexicon currently available for English. It is a hierarchical domain-independent, broad-coverage verb lexicon with mappings to other lexical resources such as WordNet, <a href="http://www.cis.upenn.edu/~xtag/" target="_blank" rel="noopener">Xtag</a> and <a href="https://framenet.icsi.berkeley.edu/fndrupal/" target="_blank" rel="noopener">FrameNet</a>.</li><li><a href="https://en.wikipedia.org/wiki/Freebase" target="_blank" rel="noopener">FreeBase</a>, which is a large collaborative knowledge base consisting of data composed mainly by its community members. It was an online collection of structured data harvested from many sources, including individual, user-submitted wiki contributions. Freebase aimed to create a global resource that allowed people (and machines) to access common information more effectively.</li><li><a href="https://www.mpi-inf.mpg.de/departments/databases-and-information-systems/research/yago-naga/yago/" target="_blank" rel="noopener">YaGo</a>, which is a huge semantic knowledge base, derived from Wikipedia, WordNet and GeoNames. It has knowledge of more than 10 million entities and contains more than 120 million facts about these entities.</li></ul><p>Actually, there are still a lot of different relational knowledge database exist, here we only show some representative databases above. Before we discuss those algorithms, let’s see some examples of multi-relational knowledge database to get the idea about how the database looks like.<br><img src="/images/nlp/transx/conceptnet-wordnet.png" alt="conceptnet and wordnet"><br>In the graph above, I show an example of two knowledge DB. For ConceptNet, each entity (or, node) is a concept, and entities are connected to each other through some specific relations, like, <code>oven--UsedFor--&gt;cook</code>, <code>cake--IsA--&gt;dessert</code>, here <code>oven</code>, <code>cook</code>, <code>cake</code> and <code>dessert</code> are entities, while <code>UsedFor</code> and <code>IsA</code> are relations. For WordNet, there are two different entities, one is called Synset (blue node), another is Word (green one). Each synset in WordNet is connected with other synsets through <strong>hypernym</strong> and <strong>hyponym</strong> relations, while Word only connect to its own synset and has some links with other words in the same synset, but never connected to words outside.</p><p>Although there are amount of relations and connections within a knowledge database, generally, all of the relations are in the several certain forms, like <strong>one-to-many relation</strong>, <strong>many-to-one relation</strong>, <strong>one-to-one relation</strong>, <strong>co-relation</strong>, <strong>reflexive relation</strong> and so on.<br><img src="/images/nlp/transx/relation-forms.png" alt="relation-forms"><br>For example, one-to-many relation, which means one entity with one relation, links to several different entities, say, <code>apple--is_a--&gt;fruit</code>, <code>apple--is_a--&gt;computer_brand</code>, <code>apple--is_a--&gt;computer_manufacturer</code> and so on. For many-to-one relation, it is similar. However, the reflexive relation is unique, since two entities connect to each other with same relation, like <code>plate--is_a-&gt;dish</code>, while <code>dish--is_a--&gt;plate</code> too.</p><p>Dispite the scale of relational knowledge databases and how many different relation forms they have, all of them are able to be decomposed to the <strong>basic component</strong> (triple), i.e., an entity connect to another entity with a certain relation, as shown below<br><img src="/images/nlp/transx/basic-component.png" alt="basic-component"><br>In order to convert those relational data into embeddings, which are convenient and easy to access via statistical approach, researchers proposed several methods to handle this issue, like <a href="http://machinelearning.wustl.edu/mlpapers/paper_files/ICML2011Nickel_438.pdf" target="_blank" rel="noopener">RESCAL</a>, <a href="https://pdfs.semanticscholar.org/057a/c29c84084a576da56247bdfd63bf17b5a891.pdf" target="_blank" rel="noopener">SE</a>, <a href="http://www.thespermwhale.com/jaseweston/papers/ebrm_mlj.pdf" target="_blank" rel="noopener">SME(linear)</a>, <a href="http://www.thespermwhale.com/jaseweston/papers/ebrm_mlj.pdf" target="_blank" rel="noopener">SME(bilinear)</a>, <a href="https://hal.inria.fr/hal-00776335/document" target="_blank" rel="noopener">LFM</a>, <a href="https://papers.nips.cc/paper/5071-translating-embeddings-for-modeling-multi-relational-data.pdf" target="_blank" rel="noopener">TransE</a>, <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.486.2800&amp;rep=rep1&amp;type=pdf" target="_blank" rel="noopener">TransH</a>, <a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/view/9571/9523" target="_blank" rel="noopener">TransR</a>, <a href="http://www.aclweb.org/anthology/P15-1067" target="_blank" rel="noopener">TransD</a> and so on, while the TransE, TransH, TransR, TransD are a group of similar methods, thus, we put them together and named as <strong>TransX</strong>. So, TransX is a set of methods to create embeddings for entities and relations while remembering their connection information.</p><h1 id="TransE"><a href="#TransE" class="headerlink" title="TransE"></a>TransE</h1><p>TransE – <a href="https://papers.nips.cc/paper/5071-translating-embeddings-for-modeling-multi-relational-data.pdf" target="_blank" rel="noopener">Translating Embeddings for Modeling Multi-relational Data</a>.</p><p>This paper considers the problem of embedding entities and relationships of multi-relational data in low-dimensional vector spaces, its objective is to propose a canonical model which is easy to train, contains a reduced number of parameters and can scale up to very large databases. So the TransE is proposed, which models relationships by interpreting them as translations operating on the low-dimensional embeddings of the entities.</p><p>TransE, an energy-based model for learning low-dimensional embeddings of entities.  In TransE, relationships are represented as translations in the embedding space:</p><blockquote><p>if $(h,r,t)$ holds, then the embedding of the tail entity $t$ should be <strong>close to</strong> the embedding of the head entity $h$ <strong>plus</strong> some vector that depends on the relationship $r$, while it is also the general idea of the training process of TransE.</p></blockquote><p>The main motivation behind this translation-based parameterization is that hierarchical relationships are extremely common in KBs and translations are the natural transformations for representing them. Another motivation comes from <a href="https://arxiv.org/abs/1310.4546" target="_blank" rel="noopener">Distributed Representations of Words and Phrases and Their Compositionality</a>, in which the authors learn word embeddings from free text, and some <em>1-to-1</em> relationships between entities of different types are (coincidentally rather than willingly) represented by the model as translations in the embedding space. This suggests that there may exist embedding spaces in which <em>1-to-1</em> relationships between entities of different types may, as well, be represented by translations.</p><p><strong>Translation-based model</strong>:<br>Given a training set $S$ of triplets $(h,r,t)$ composed of two entities $h,t\in E$ (the set of entities) and a relationship $r\in R$ (the set of relationships), TransE learns vector embeddings of the entities and the relationships. The embeddings take values in $\mathbb{R}^{k}$ ($k$ is a model hyperparameter) and are denoted with the same letters, in boldface characters.</p><p>The basic idea behind the model is that the functional relation induced by the $r$-labeled edges corresponds to a translation of the embeddings, i.e. <strong>$h+r\approx t$ when $(h,r,t)$ holds ($t$ should be a nearest neighbor of $h+r$), while $h+r$ should be far away from $t$ otherwise</strong>, as the graph below shows. Following an energy-based framework, the energy of a triplet is equal to $d(h+r,t)$ for some dissimilarity measure $d$, which we take to be either the $L_{1}$ or the $L_{2}$-norm.<br><img src="/images/nlp/transx/transe-translation-graph.png" alt="TransE Translation"><br>To learn such embeddings, we minimize a margin-based ranking criterion over the training set:$$<br>\mathcal{L}=\sum_{(h,r,t)\in S}\sum_{(h’,r,t’)\in S’_{(h,r,t)}}\big[\gamma+d(h+r,t)-d(h’+r,t’)\big]_{+}\tag{1}<br>$$where $[x]_{+}$ denotes the positive part of $x$, $\gamma&gt;0$ is a margin hyperparameter, and the dissimilarity measure $d$ is the squared euclidean distance, which computed by$$<br>d(h+r,t)=\lVert h\rVert^{2}_{2}+\lVert r\rVert^{2}_{2}+\lVert t\rVert^{2}_{2}-2\big(h^{T}t+r^{T}(t-h)\big)\tag{2}<br>$$with the norm constraints that $\lVert h\rVert^{2}_{2}=\lVert t\rVert^{2}_{2}=1$, and the set of corrupted triplets, constructed according to the equation (3) below, is composed of training triplets with either the head or tail replaced by a random entity (but not both at the same time)$$<br>S’_{(h,r,t)}=\big\{(h’,r,t)\vert h’\in E\big\}\bigcup\big\{(h,r,t’)\vert t’\in E\big\}\tag{3}<br>$$The loss function (1) favors lower values of the energy for training triplets than for corrupted triplets, and is thus a natural implementation of the intended criterion.</p><p>The optimization is carried out by stochastic gradient descent (in minibatch mode), over the possible $h$, $r$, and $t$, with the additional constraints that the $L_{2}$-norm of the embeddings of the entities is 1 (no regularization or norm constraints are given to the label embeddings $r$). This constraint is important, because it prevents the training process to trivially minimize $\mathcal{L}$ by artificially increasing entity embeddings norms. The detailed optimization procedure is described in the graph below.</p><blockquote><p><strong>Algorithm</strong>: Learning <strong>TransE</strong><br><strong>Input</strong>: Training set $S=\{(h,r,t)\}$, entities and relations set $E$ and $R$, margin $\gamma$, embedding dimension $k$.<br>$\quad$1: <strong>Initialize</strong>:<br>$\quad$2: $\qquad\quad$$r\gets \textrm{uniform}(-\frac{6}{\sqrt k},\frac{6}{\sqrt k})$ for each $r\in R$<br>$\quad$3: $\qquad\quad$$r\gets \frac{r}{\Vert r\Vert}$ for each $r\in R$<br>$\quad$4: $\qquad\quad$$e\gets \textrm{uniform}(-\frac{6}{\sqrt k},\frac{6}{\sqrt k})$ for each entity $e\in E$<br>$\quad$5: <strong>Loop</strong>:<br>$\quad$6: $\qquad\quad$$e\gets \frac{e}{\Vert e\Vert}$ for each entity $e\in E$<br>$\quad$7: $\qquad\quad$$S_{batch}\gets \textrm{sample}(S,b)$$\quad$// sample a mini-batch of size $b$<br>$\quad$8: $\qquad\quad$$T_{batch}\gets\emptyset$$\quad$// initialize the set of pairs of triples<br>$\quad$9: $\qquad\quad$<strong>for</strong> $(h,r,t)\in S_{batch}$ <strong>do</strong><br>$\quad$10: $\qquad\qquad$$(h’,r,t’)\gets\textrm{sample}(S’_{h,r,t})$$\quad$// sample a corrupted triple<br>$\quad$11: $\qquad\qquad$$T_{batch}\gets T_{batch}\bigcup\big\{\big((h,r,t),(h’,r,t’)\big)\big\}$<br>$\quad$12:$\qquad\quad$<strong>end for</strong><br>$\quad$13:$\qquad\quad$Update embeddings w.r.t. $\sum_{\big((h,r,t),(h’,r,t’)\big)\in T_{batch}}\nabla\big[\gamma+d(h+r,t)-d(h’+r,t’)\big]_{+}$<br>$\quad$14: <strong>End loop</strong></p></blockquote><h1 id="TransH"><a href="#TransH" class="headerlink" title="TransH"></a>TransH</h1><p>TransH – <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.486.2800&amp;rep=rep1&amp;type=pdf" target="_blank" rel="noopener">Knowledge Graph Embedding by Translating on Hyperplanes</a>.</p><p>Before talking about TransH, let’s see some problems that TransE holds. TransE models a relation $r$ as a translation vector $r\in\mathbb{R}^{k}$, and assumes the error $\Vert h+r-t\Vert_{l_{1}/l_{2}}$ is low if $(h,r,t)$ is a golden triple. It applies well to irreflexive and one-to-one relations but has problems when dealing with reflexive or many-to-one, one-to-many, many-to-many relations. Considering the ideal case of no-error embedding where $h+r-t=0$, if $(h,r,t)\in\Delta$, we can get the following consequences directly from TransE model.</p><ul><li>If $(h,r,t)\in\Delta$ and $(t,r,h)\in\Delta$, i.e., $r$ is a reflexive map, then $r=0$ and $h=t$.</li><li>If $\forall i\in\{0,1,\dots,m\}$, $(h_{i},r,t)\in\Delta$, i.e., $r$ is a many-to-one map, then $h_{0}=\dots=h_{m}$. Similarly, if $(h,r,t_{i})\in\Delta$, i.e., $r$ is a one-to-many map, then $t_{0}=\dots=t_{m}$.</li></ul><p>The reason leading to the above consequences is, in TransE, the representation of an entity is the same when involved in any relations, ignoring <em>distributed representations of entities when involved in different relations</em>. Hence, TransH is proposed to handle the problems of TransE in modeling reflexive, one-to-many, many-to-one and many-to-many relations. The general idea of TransH is shown below, which introduces a <strong>relation-specific hyperplane</strong> to project the entities to the hyperplane, and the translation process is done in such hyperplane too. For a relation $r$, the authors position the relation-specific translation vector $d_{r}$ in the relation-specific hyperplane $w_{r}$ (the normal vector) rather than in the same space of entity embeddings.<br><img src="/images/nlp/transx/transh-graph.png" alt="TransH Graph"><br>Specifically, for a triplet $(h,r,t)$, the embedding $h$ and $t$ are first projected to the hyperplane $w_{r}$. The projections are denoted as $h_{\bot}$ and $t_{\bot}$, respectively, while$$\begin{aligned}<br>h_{\bot} &amp;=h-w_{r}^{T}hw_{r}\\<br>t_{\bot} &amp;=t-w_{r}^{T}tw_{r}<br>\end{aligned}$$The authors expect $h_{\bot}$ and $t_{\bot}$ can be connected by a translation vector $d_{r}$ on the hyperplane with low error if $(h,r,t)$ is a golden triplet. And the graph below shows how the TransH sloves the problems in TransE.<br><img src="/images/nlp/transx/transh-solve-problem.png" alt="TransH Solve Problems"><br>Thus, in TransH, by introducing the mechanism of projecting to the relation-specific hyperplane, it enables different roles of an entity in different relations/triplets. Generally, the training process of TransH is similar to TransE, its cost function is also a margin-based ranking loss$$<br>\mathcal{L}=\sum_{(h,r,t)\in S}\sum_{(h’,r,t’)\in S’_{(h,r,t)}}\big[f_{r}(h,t)+\gamma-f_{r’}(h’,t’)\big]_{+}\tag{4}<br>$$where $[x]_{+}=\max(0,x)$, $S$ is a set of golden triples, $S’_{(h,r,t)}$ denotes the set of negative triplets constructed by corrupting $(h,r,t)$, $\gamma$ is the margin separating positive and negative triplets, and $f_{r}(h,t)$ is a score function and derived by$$<br>f_{r}(h,t)=\Vert h_{\bot}+d_{r}-t_{\bot}\Vert_{2}^{2}=\Vert(h-w_{r}^{T}hw_{r})+d_{r}-(t-w_{r}^{T}tw_{r})\Vert_{2}^{2}\tag{5}<br>$$When minimizing the loss $\mathcal{L}$, the following constraints are considered:</p><ul><li>$\forall e\in E$, $\Vert e\Vert_{2}\leq 1$</li><li>$\forall r\in R$, $\vert w_{r}^{T}d_{r}\vert/\Vert d_{r}\Vert_{2}\leq\epsilon$</li><li>$\forall r\in R$, $\Vert w_{r}\Vert_{2}=1$</li></ul><p><strong>Reducing False Negative Labels</strong>:<br>Here shows a new method to sample the corrupted triples to reduce the false.</p><ol><li>Give more chance to replacing the head entity if the relation is one-to-many and give more chance to replacing the tail entity if the relation is many-to-one.</li><li>Among all the triplets of a relation r, we first get the following two statistics:<br>(1) the average number of tail entities per head entity, denoted as $tph$<br>(2) the average number of head entities per tail entity, denoted as $hpt$</li></ol><p>Then defining a Bernoulli distribution with parameter $\frac{thp}{thp+hpt}$ for sampling: given a golden triplet $(h,r,t)$ of the relation $r$, with probability $\frac{thp}{thp+hpt}$ to corrupt the triplet by replacing the head, and with probability $\frac{pht}{thp+hpt}$ to corrupt the triplet by replacing the tail.</p><h1 id="TransR-and-CTransR"><a href="#TransR-and-CTransR" class="headerlink" title="TransR and CTransR"></a>TransR and CTransR</h1><p>TransR – <a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/view/9571/9523" target="_blank" rel="noopener">Learning Entity and Relation Embeddings for Knowledge Graph Completion</a>.</p><p>TransR is proposed since the authors realize that TransH still have some limitations, since both TransE and TransH assume embeddings of entities and relations being in the same space. However, an entity may have multiple aspects, and various relations focus on different aspects of entities. Hence, it is intuitive that some entities are similar and thus close to each other in the entity space, but are <strong>comparably different in some specific aspects</strong> and thus <strong>far away from each other in the corresponding relation spaces</strong>.<br><img src="/images/nlp/transx/limitation-transh.png" alt="TransH Limitations"><br>Thus, TransR models entities and relations in distinct spaces, i.e., entity space and multiple relation spaces (i.e., relation-specific entity spaces), and performs translation in the corresponding relation space. In TransR, for each triple $(h,r,t)$, entities embeddings are set as $h,t\in\mathbb{R}^{k}$, and relation embedding is set as $r\in\mathbb{R}^{d}$, while the dimensions of entity embeddings and relation embeddings are not necessarily identical. For each relation $r$, set a projection matrix $M_{r}\in\mathbb{R}^{k\times d}$ maps entities from entity space to relation space.<br><img src="/images/nlp/transx/transr.png" alt="TransR Limitations"><br>With the mapping matrix, the projected vectors of entities are computed by$$<br>h_{r}=hM_{r},\quad t_{r}=tM_{r}<br>$$And the score function is correspondingly defined as$$<br>f_{r}(h,t)=\Vert h_{r}+r-t_{r}\Vert_{2}^{2}\tag{6}<br>$$with the constraints that $\forall h,r,t$, $\Vert h\Vert_{2}\leq 1$, $\Vert r\Vert_{2}\leq 1$, $\Vert t\Vert_{2}\leq 1$, $\Vert hM_{r}\Vert_{2}\leq 1$, $\Vert tM_{r}\Vert_{2}\leq 1$. And the following margin-based score function as objective for training:$$<br>\mathcal{L}=\sum_{(h,r,t)\in S}\sum_{(h’,r,t’)\in S’_{(h,r,t)}}\max\big[0, f_{r}(h,t)+\gamma-f_{r}(h’,t’)\big]\tag{7}<br>$$where $\max(x, y)$ aims to get the maximum between $x$ and $y$, $\gamma$ is the margin, $S$ is the set of correct triples and $S’$ is the set of incorrect triples.</p><p><strong>Cluster-based TransR (CTransR)</strong>:<br>The above mentioned models, including TransE, TransH and TransR, learn a unique vector for each relation, which may be under-representative to fit all entity pairs under this relation, because these relations are usually rather diverse. In order to better model these relations, the authors incorporate the idea of piecewise linear regression to extend TransR.</p><p>The basic idea is that first segment input instances into several groups. Formally, for a specific relation $r$, all entity pairs $(h,t)$ in the training data are clustered into multiple groups, and entity pairs in each group are expected to exhibit similar $r$ relation. All entity pairs $(h,t)$ are represented with their vector offsets $(h−t)$ for clustering, where $h$ and $t$ are obtained with TransE. Afterwards, learn a separate relation vector $r_{c}$ for each cluster and matrix $M_{r}$ for each relation, respectively. The authors define the projected vectors of entities as $h_{r,c}=hM_{r}$, $t_{r,c}=tM_{r}$ and the score function is defined as$$<br>f_{r}(h,t)=\Vert h_{r,c}+r_{c}-t_{r,c}\Vert_{2}^{2}+\alpha\Vert r_{c}-r\Vert_{2}^{2}\tag{8}<br>$$where $\Vert r_{c}-r\Vert_{2}^{2}$ aims to ensure cluster-specific relation vector $r_{c}$ not too far away from the original relation vector $r$, and $\alpha$ controls the effect of this constraint. Besides, same to TransR, CTransR also enforce constraints on norm of embeddings $h$, $r$, $t$ and mapping matrices.</p><h1 id="TransD"><a href="#TransD" class="headerlink" title="TransD"></a>TransD</h1><p>TransD – <a href="http://www.aclweb.org/anthology/P15-1067" target="_blank" rel="noopener">Knowledge Graph Embedding via Dynamic Mapping Matrix</a></p><p>Despite that TransR/CTransR has significant improvements compared with previous state-of-the-art models. However, it also has several flaws:</p><ol><li>For a typical relation $r$, all entities share the same mapping matrix $M_{r}$. However, the entities linked by a relation always contains various types and attributes.</li><li>The projection operation is an interactive process between an entity and a relation, it is unreasonable that the mapping matrices are determined only by relations.</li><li>Matrix-vector multiplication makes it has large amount of calculation, and when relation number is large, it also has much more parameters than TransE and TransH.</li></ol><p>To solve these flaws, TransD is proposed and its basic idea is shown in the graph below. In TransD, <strong>two vectors</strong> are defined for each entity and relation. The first vector represents the meaning of an entity or a relation, the other one (called <strong>projection vector</strong>) represents the way that how to project a entity embedding into a relation vector space and it will be used to construct mapping matrices. Therefore, every entity-relation pair has an unique mapping matrix. In addition, TransD has no matrix-by-vector operations which can be replaced by vectors operations.<br><img src="/images/nlp/transx/transd-graph.png" alt="TransD Graph"><br>In the graph above, each shape represents an entity pair appearing in a triplet of relation $r$. $M_{rh}$ and $M_{rt}$ are mapping matrices of $h$ and $t$, respectively. $h_{ip}$, $t_{ip}$ ($i=1,2,3$) and $r_{p}$ are projection vectors. $h_{i\bot}$ and $t_{i\bot}$ ($i=1,2,3$) are projected vectors of entities. The projected vectors satisfy $h_{i\bot}+r\approx t_{i\bot}$ ($i=1,2,3$).</p><p><strong>TransD Model</strong>: each named symbol object (entities and relations) is represented by <em>two</em> vectors. The first one captures the meaning of entity (relation), the other one is used to construct mapping matrices. For example, given a triplet $(h,r,t)$, its vectors are $h$, $h_{p}$, $r$, $r_{p}$, $t$, $t_{p}$, where subscript $p$ marks the projection vectors, $h,h_{p},t,t_{p}\in\mathbb{R}^{n}$ and $r,r_{p}\in\mathbb{R}^{m}$. For each triplet $(h,r,t)$ the authors set two mapping matrices $M_{rh},M_{rt}\in\mathbb{R}^{m\times n}$ to project entities from entity space to relation space:$$\begin{aligned}<br>M_{rh}&amp;=r_{p}h_{p}^{T}+I^{m\times n}\\<br>M_{rt}&amp;=r_{p}t_{p}^{T}+I^{m\times n}<br>\end{aligned}\tag{9}$$Therefore, the mapping matrices are determined by both entities and relations, and this kind of operation makes the two projection vectors interact sufficiently because each element of them can meet every entry comes from another vector. As the authors initialize each mapping matrix with an identity matrix, thus, the $I^{m\times n}$ is added to $M_{rh}$ and $M_{rt}$. With the mapping matrices, the projected vectors are defined as follows:$$\begin{aligned}<br>h_{\bot}&amp;=M_{rh}h\\<br>t_{\bot}&amp;=M_{rt}t<br>\end{aligned}\tag{10}$$and the score function is set as$$<br>f_{r}(h,t)=-\Vert h_{\bot}+r-t_{\bot}\Vert_{2}^{2}\tag{11}<br>$$with the constraints that $\Vert h\Vert_{2}\leq 1$, $\Vert t\Vert_{2}\leq 1$, $\Vert r\Vert_{2}\leq 1$, $\Vert h_{\bot}\Vert_{2}\leq 1$ and $\Vert t_{\bot}\Vert_{2}\leq 1$.</p><p>For training process, the authors first denote that $S=\{(h_{i},r_{i},t_{i})\vert y_{i}=1\}$ as the golden triples, and $S’=\{(h_{i},r_{i},t_{i})\vert y_{i}=0\}$ as the negative triples, and the negative triples is derived by$$<br>S’=\{(h_{l},r_{k},t_{k})\vert h_{l}\neq h_{k}\land y_{k}=1\}\cup\{(h_{k},r_{k},t_{l})\vert t_{l}\neq t_{k}\land y_{k}=1\}<br>$$while the authors also use two strategies “unif” and “bern” described in TransH to replace the head or tail entity. using $\xi$ and $\xi’$ to denote a golden triplet and a corresponding negative triplet, respectively. The training objective is$$<br>\mathcal{L}=\sum_{\xi\in S}\sum_{\xi’\in S’}\big[\gamma+f_{r}(\xi’)-f_{r}(\xi)\big]_{+}\tag{12}<br>$$<strong>Connections with TransE/H/R and CTransR</strong>:</p><ul><li>TransE is a special case of TransD when the dimension of vectors satisfies $m=n$ and all projection vectors are set zero.</li><li>TransH is related to TransD when we set $m=n$. Under the setting, projected vectors of entities can be rewritten as follows:$$\begin{aligned}<br>h_{\bot}&amp;=M_{rh}h=h+h_{p}^{T}hr_{p}\\<br>t_{\bot}&amp;=M_{rt}t=t+t_{p}^{T}tr_{p}<br>\end{aligned}$$Hence, when $m = n$, the difference between TransD and TransH is that projection vectors are determinded only by relations in TransH, but TransD’s projection vectors are determinded by both entities and relations.</li><li>As to TransR/CTransR, TransD is an improvement of it. TransR/CTransR directly defines a mapping matrix for each relation, TransD consturcts two mapping matrices dynamically for each triplet by setting a projection vector for each entity and relation. In addition, TransD has no matrix-vector multiplication operation which can be replaced by vector operations. Without loss of generality, assuming $m\geq n$, the projected vectors can be computed as follows:$$\begin{aligned}<br>h_{\bot}&amp;=M_{rh}h=h_{p}^{T}hr_{p}+\big[h^{T},0^{T}\big]^{T}\\<br>t_{\bot}&amp;=M_{rt}t=t_{p}^{T}tr_{p}+\big[t^{T},0^{T}\big]^{T}<br>\end{aligned}$$Therefore, TransD has less calculation than TransR/CTransR, which makes it train faster and can be applied on large-scale knowledge graphs.</li></ul><h1 id="Python-and-C-Codes"><a href="#Python-and-C-Codes" class="headerlink" title="Python and C++ Codes"></a>Python and C++ Codes</h1><p>The Python and C++ codes of the methods above are available in the GitHub page of Natural Language Processing Lab at Tsinghua University (<a href="https://github.com/thunlp" target="_blank" rel="noopener">THUNLP</a>).<br>Python version: <a href="https://github.com/thunlp/TensorFlow-TransX" target="_blank" rel="noopener">TensorFlow-TransX</a>.<br>Converted TensorFlow-TransX for Python 2 and Tensorflow 0.12.0: <a href="https://github.com/IsaacChanghau/AmusingPythonCodes/tree/master/transx" target="_blank" rel="noopener">link</a><br>C++ version: <a href="https://github.com/thunlp/Fast-TransX" target="_blank" rel="noopener">Fast-TransX</a>, <a href="https://github.com/thunlp/KB2E" target="_blank" rel="noopener">KB2E</a>.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;It is a summary of several papers about embedding entities and relationships of multi-relational knowledge database, while each paper contains one kind of specific method. Generally, the set of methods are called TransX.
    
    </summary>
    
      <category term="Natural Language Processing" scheme="https://isaacchanghau.github.io/categories/Natural-Language-Processing/"/>
    
    
      <category term="deep learning" scheme="https://isaacchanghau.github.io/tags/deep-learning/"/>
    
      <category term="python" scheme="https://isaacchanghau.github.io/tags/python/"/>
    
      <category term="natural language processing" scheme="https://isaacchanghau.github.io/tags/natural-language-processing/"/>
    
      <category term="c plus plus" scheme="https://isaacchanghau.github.io/tags/c-plus-plus/"/>
    
      <category term="word embeddings" scheme="https://isaacchanghau.github.io/tags/word-embeddings/"/>
    
  </entry>
  
  <entry>
    <title>Python 浅析 &quot;战狼II&quot; 170000+影评数据</title>
    <link href="https://isaacchanghau.github.io/2017/09/10/Python-%E6%B5%85%E6%9E%90-%E6%88%98%E7%8B%BC2-170000-%E5%BD%B1%E8%AF%84%E6%95%B0%E6%8D%AE/"/>
    <id>https://isaacchanghau.github.io/2017/09/10/Python-浅析-战狼2-170000-影评数据/</id>
    <published>2017-09-10T07:52:58.000Z</published>
    <updated>2017-10-22T09:14:46.000Z</updated>
    
    <content type="html"><![CDATA[<p>本文参考<a href="https://zhuanlan.zhihu.com/wajuejiprince" target="_blank" rel="noopener">数据分析小王子</a>·<a href="https://zhuanlan.zhihu.com/p/28475619" target="_blank" rel="noopener">&lt;&lt;战狼Ⅱ&gt;&gt;豆瓣十二万影评浅析</a>，利用 Python 网络爬虫爬取豆瓣影评数据并进行简要分析。<a id="more"></a></p><p>想要分析“战狼II”的影评数据，首先需要获取这些数据，这里使用 Python 的 <code>requests</code> 包进行网页请求，并使用正则表达式匹配出我们需要的数据。首先使用Chrome打开豆瓣影评·战狼II的网页 (<a href="https://movie.douban.com/subject/26363254/comments?start=0" target="_blank" rel="noopener">https://movie.douban.com/subject/26363254/comments?start=0</a>)，使用 Developer Tools 对当前页面做一个简单的了解和分析，如下图：<br><img src="/images/python/douban_comments/comment-chrome.png" alt="Comment Loc"><br>我们发现页面的所有评论、评论者、投票、评价等级等信息均存储在 <code>&lt;div class=&quot;comment-item&quot; ...&gt;</code> 标签下。而转向下一页面的链接信息存储在 <code>&lt;div id=&quot;paginator&quot;&gt;</code> 标签的 <code>&lt;a href=&quot;?start=26&amp;amp;limit=20&amp;amp;sort=new_score&amp;amp;status=P&quot; ... class=&quot;next&quot;&gt;...</code> 中。因此，可以针对这部分 HTML 标签创建相应的正则表达式来获取数据。简易的爬虫代码如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests </span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">url_first = <span class="string">'https://movie.douban.com/subject/26363254/comments?start=0'</span>  <span class="comment"># start page</span></span><br><span class="line">head = &#123;<span class="string">'User-Agent'</span>:<span class="string">'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.79 Safari/537.36'</span>&#125;</span><br><span class="line">cookies = &#123;<span class="string">'cookie'</span>:<span class="string">'you own cookies&#125;  #cookie of your account</span></span><br><span class="line"><span class="string">html = requests.get(url_first, headers=head, cookies=cookies)  # get first page</span></span><br><span class="line"><span class="string">re_page = re.compile(r'</span>&lt;a href=<span class="string">"(.*?)&amp;amp;.*?class="</span>next<span class="string">"&gt;') # next page</span></span><br><span class="line">re_content = re.compile(r'&lt;span class="votes"&gt;(.*?)&lt;/span&gt;.*?comment"&gt;(.*?)&lt;/a&gt;.*?&lt;/span&gt;.*?&lt;span.*?class=""&gt;(.*?)&lt;/a&gt;.*?&lt;span&gt;(.*?)&lt;/span&gt;.*?title="(.*?)"&gt;&lt;/span&gt;.*?title="(.*?)"&gt;.*?class=""&gt; (.*?)\n', re.S)</span><br><span class="line"><span class="keyword">while</span> html.status_code==<span class="number">200</span>:</span><br><span class="line">    url_next = <span class="string">'https://movie.douban.com/subject/26363254/comments'</span> + re.findall(re_page, html.text)[<span class="number">0</span>]</span><br><span class="line">    data = re.findall(re_content, html.text)</span><br><span class="line">    print(url_next)</span><br><span class="line">    frame = pd.DataFrame(data)</span><br><span class="line">    frame.to_csv(<span class="string">'./data/comments.csv'</span>, header=<span class="keyword">False</span>, index=<span class="keyword">False</span>, mode=<span class="string">'a+'</span>, encoding=<span class="string">'utf-8'</span>)</span><br><span class="line">    frame = []</span><br><span class="line">    data = []</span><br><span class="line">    html = requests.get(url_next, cookies=cookies, headers=head)</span><br></pre></td></tr></table></figure></p><p>这里需要设置自己的 <code>User-Agent</code> 和 <code>cookie</code>，如果使用的是Chrome，可以直接进入 Developer Tools，在 <code>Network</code> 中找到当前页面，便能获得这些信息。以上，代码没有考虑豆瓣的验证码问题，因此爬取大概20000～30000条数据时便会报出 <code>443 Error</code>，由于验证码机器识别问题涉及到一些其他的领域，这里喔直接忽略这个问题，因为数据量不是很大，所以每次保存停止，只需要重新启动程序即可。这里爬取的数据为7类：<strong>赞同数</strong>，<strong>是否有用</strong>，<strong>用户名</strong>，<strong>是否看过</strong>，<strong>评分</strong>，<strong>日期</strong>以及<strong>评论内容</strong>。</p><p>完成19万+条数据爬取之后，由于存在一些格式错误等脏数据，仍需要做一些简单的数据清理，清理后的数据量为18万+。代码如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">reg_correct = re.compile(<span class="string">r'(.*),(.*),(.*),(.*),(.*),(.*),(.*)'</span>)</span><br><span class="line">reg_dirty = re.compile(<span class="string">r'.*这条短评跟影片无关.*&lt;span class=""votes""&gt;(.*?)&lt;/span&gt;.*?class=""j a_vote_comment""&gt;(.*?)&lt;/a&gt;.*?class=""""&gt;(.*?)&lt;/a&gt;&lt;span&gt;(.*?)&lt;/span&gt;.*?title=""(.*?)",(.*),(.*)'</span>)</span><br><span class="line">data = []  <span class="comment"># 格式化数据</span></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'./data/comments.csv'</span>) <span class="keyword">as</span> file:</span><br><span class="line">    dirty_data = []  <span class="comment"># 错误格式数据</span></span><br><span class="line">    dirty_lines = <span class="string">''</span></span><br><span class="line">    flag = <span class="number">0</span></span><br><span class="line">    <span class="comment"># load and clean data</span></span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> file:</span><br><span class="line">        arr = re.findall(reg_correct, line)</span><br><span class="line">        <span class="keyword">if</span> len(arr) == <span class="number">0</span>:</span><br><span class="line">            dirty_lines = <span class="string">''</span>.join([dirty_lines, line.strip()])</span><br><span class="line">            flag = <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            value = (arr[<span class="number">0</span>][<span class="number">0</span>].replace(<span class="string">'"'</span>, <span class="string">''</span>), arr[<span class="number">0</span>][<span class="number">1</span>].replace(<span class="string">'"'</span>, <span class="string">''</span>), arr[<span class="number">0</span>][<span class="number">2</span>].replace(<span class="string">'"'</span>, <span class="string">''</span>), arr[<span class="number">0</span>][<span class="number">3</span>].replace(<span class="string">'"'</span>, <span class="string">''</span>), arr[<span class="number">0</span>][<span class="number">4</span>].replace(<span class="string">'"'</span>, <span class="string">''</span>), arr[<span class="number">0</span>][<span class="number">5</span>].replace(<span class="string">'"'</span>, <span class="string">''</span>), <span class="string">'`'</span> + arr[<span class="number">0</span>][<span class="number">6</span>].replace(<span class="string">'"'</span>, <span class="string">''</span>) + <span class="string">'`'</span>)</span><br><span class="line">            data.append(value)</span><br><span class="line">            <span class="keyword">if</span> flag == <span class="number">1</span>:</span><br><span class="line">                flag = <span class="number">0</span></span><br><span class="line">                dirty_data.append(dirty_lines)</span><br><span class="line">                dirty_lines = <span class="string">''</span></span><br><span class="line">    <span class="comment"># add cleaned data to data array</span></span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> dirty_data:</span><br><span class="line">        arr = re.findall(reg_dirty, line)</span><br><span class="line">        <span class="keyword">if</span> len(arr) != <span class="number">0</span>:</span><br><span class="line">            value = (arr[<span class="number">0</span>][<span class="number">0</span>].replace(<span class="string">'"'</span>, <span class="string">''</span>), arr[<span class="number">0</span>][<span class="number">1</span>].replace(<span class="string">'"'</span>, <span class="string">''</span>), arr[<span class="number">0</span>][<span class="number">2</span>].replace(<span class="string">'"'</span>, <span class="string">''</span>), arr[<span class="number">0</span>][<span class="number">3</span>].replace(<span class="string">'"'</span>, <span class="string">''</span>), arr[<span class="number">0</span>][<span class="number">4</span>].replace(<span class="string">'"'</span>, <span class="string">''</span>), arr[<span class="number">0</span>][<span class="number">5</span>].replace(<span class="string">'"'</span>, <span class="string">''</span>), <span class="string">'`'</span> + arr[<span class="number">0</span>][<span class="number">6</span>].replace(<span class="string">'"'</span>, <span class="string">''</span>) + <span class="string">'`'</span>)</span><br><span class="line">            data.append(value)</span><br><span class="line">path = <span class="string">'./data/comments_clean.csv'</span></span><br><span class="line">pd.DataFrame(data).to_csv(path, header=<span class="keyword">False</span>, index=<span class="keyword">False</span>, encoding=<span class="string">'utf-8'</span>)</span><br><span class="line">print(<span class="string">'Done...'</span>)</span><br></pre></td></tr></table></figure></p><p>将数据保存为 <code>.csv</code> 文件，前5行数据如下：<br><img src="/images/python/douban_comments/head.png" alt="Head"><br>之后便是进行数据分析，在数据处理过程中发现，经过清理的数据中仍然有一些数据的格式错误，由于这部分数据量很小，所以直接将这部分数据删除：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="comment"># load data</span></span><br><span class="line">column_names = [<span class="string">'Votes'</span>, <span class="string">'Useful'</span>, <span class="string">'User'</span>, <span class="string">'Watched'</span>, <span class="string">'Score'</span>, <span class="string">'Date'</span>, <span class="string">'Comment'</span>]</span><br><span class="line">data = pd.read_csv(<span class="string">'./data/comments_clean.csv'</span>, header=<span class="keyword">None</span>, names=column_names, skipinitialspace = <span class="keyword">True</span>, quotechar = <span class="string">'`'</span>)</span><br><span class="line"><span class="comment"># set value as string</span></span><br><span class="line">data[<span class="string">'Votes'</span>] = data[<span class="string">'Votes'</span>].astype(str)</span><br><span class="line">data[<span class="string">'Useful'</span>] = data[<span class="string">'Useful'</span>].astype(str)</span><br><span class="line">data[<span class="string">'User'</span>] = data[<span class="string">'User'</span>].astype(str)</span><br><span class="line">data[<span class="string">'Watched'</span>] = data[<span class="string">'Watched'</span>].astype(str)</span><br><span class="line">data[<span class="string">'Score'</span>] = data[<span class="string">'Score'</span>].astype(str)</span><br><span class="line">data[<span class="string">'Date'</span>] = data[<span class="string">'Date'</span>].astype(str)</span><br><span class="line">data[<span class="string">'Comment'</span>] = data[<span class="string">'Comment'</span>].astype(str)</span><br><span class="line"><span class="comment"># clean up the data with error format</span></span><br><span class="line">data = data[data[<span class="string">'Score'</span>].map(len) == <span class="number">6</span>]</span><br><span class="line">data = data[data[<span class="string">'Score'</span>] != <span class="string">'看过'</span>]</span><br><span class="line">data = data[data[<span class="string">'Date'</span>].map(len) == <span class="number">19</span>]</span><br><span class="line">print(<span class="string">'rows:'</span>, data.shape[<span class="number">0</span>], <span class="string">', columns: '</span>, data.shape[<span class="number">1</span>]) <span class="comment"># count rows of total dataset</span></span><br><span class="line"><span class="comment"># out: ('rows:', 176875, ', columns: ', 7)</span></span><br></pre></td></tr></table></figure></p><p>然后我们对不同评分的人数做一个简单的统计并作图：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">print(data[<span class="string">'Score'</span>].value_counts())</span><br><span class="line">index = np.arange(<span class="number">5</span>)</span><br><span class="line">score_counts = data[<span class="string">'Score'</span>].value_counts()</span><br><span class="line">values = (score_counts[<span class="number">0</span>], score_counts[<span class="number">1</span>], score_counts[<span class="number">2</span>], score_counts[<span class="number">4</span>], score_counts[<span class="number">3</span>])</span><br><span class="line">bar_width = <span class="number">0.35</span></span><br><span class="line">plt.figure(figsize=(<span class="number">20</span>, <span class="number">10</span>))</span><br><span class="line">plt.bar(index, values, bar_width, alpha=<span class="number">0.6</span>, color=<span class="string">'rgbym'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Score'</span>, fontsize=<span class="number">16</span>)  </span><br><span class="line">plt.ylabel(<span class="string">'Counts'</span>, fontsize=<span class="number">16</span>)</span><br><span class="line">plt.title(<span class="string">'Comments Level'</span>, fontsize=<span class="number">18</span>)  </span><br><span class="line">plt.xticks(index, (<span class="string">'5-star'</span>, <span class="string">'4-star'</span>, <span class="string">'3-star'</span>, <span class="string">'2-star'</span>, <span class="string">'1-star'</span>), fontsize=<span class="number">14</span>, rotation=<span class="number">20</span>)</span><br><span class="line">plt.ylim(<span class="number">0</span>, <span class="number">90000</span>)</span><br><span class="line">plt.grid()</span><br><span class="line"><span class="keyword">for</span> idx, value <span class="keyword">in</span> zip(index, values):</span><br><span class="line">    plt.text(idx, value + <span class="number">0.1</span>, <span class="string">'%d'</span> % value, ha=<span class="string">'center'</span>, va=<span class="string">'bottom'</span>, fontsize=<span class="number">14</span>, color=<span class="string">'black'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p><p>输出为：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">力荐    79361</span><br><span class="line">推荐    47724</span><br><span class="line">还行    29337</span><br><span class="line">很差    10774</span><br><span class="line">较差     9679</span><br><span class="line">Name: Score, dtype: int64</span><br></pre></td></tr></table></figure></p><p><img src="/images/python/douban_comments/stars.png" alt="Stars"><br>从输出数据和上图中可以很明显的发现，对这部电影持好评 (推荐、力荐) 的人占大多数，部分人觉得还行，少数人评价差。</p><p>最后，我们对所有的评论内容进行云图展示，首先定义两个函数，一个用于对评论内容进行清理和分词，另一个进行云图生成。</p><p><strong>评论内容清理和分词</strong>：<br>这里同样使用<a href="https://github.com/fxsjy/jieba" target="_blank" rel="noopener">结巴分词</a>进行中文分词操作和 Python 内置正则表达式进行数据清洗操作。(注释掉的部分是移除中文停用词，为了加快运行速度，这里喔忽略了这个操作)<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">segment_words</span><span class="params">(stars)</span>:</span></span><br><span class="line">    comments = <span class="keyword">None</span></span><br><span class="line">    <span class="keyword">if</span> stars == <span class="string">'all'</span>:</span><br><span class="line">        comments = data[<span class="string">'Comment'</span>]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        comments = data[data[<span class="string">'Score'</span>] == stars][<span class="string">'Comment'</span>]</span><br><span class="line">    comments_list = []</span><br><span class="line">    <span class="keyword">for</span> comment <span class="keyword">in</span> comments:</span><br><span class="line">        comment = str(comment).strip().replace(<span class="string">'span'</span>, <span class="string">''</span>).replace(<span class="string">'class'</span>, <span class="string">''</span>).replace(<span class="string">'emoji'</span>, <span class="string">''</span>)</span><br><span class="line">        comment = re.compile(<span class="string">'1f\d+\w*|[&lt;&gt;/=]'</span>).sub(<span class="string">''</span>, comment)</span><br><span class="line">        <span class="keyword">if</span> (len(comment) &gt; <span class="number">0</span>):</span><br><span class="line">            comments_list.append(comment)</span><br><span class="line">    text = <span class="string">''</span>.join(comments_list)</span><br><span class="line">    word_list = jieba.cut(text, cut_all=<span class="keyword">True</span>)</span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    stopwords_list = []</span></span><br><span class="line"><span class="string">    # load chinese stop words</span></span><br><span class="line"><span class="string">    with open('./data/中文停用词表(1208个).txt') as file:</span></span><br><span class="line"><span class="string">        for line in file:</span></span><br><span class="line"><span class="string">            stopwords_list.append(line.strip())</span></span><br><span class="line"><span class="string">    print(len(stopwords_list))</span></span><br><span class="line"><span class="string">    with open('./data/停用词表.txt') as file:</span></span><br><span class="line"><span class="string">        for line in file:</span></span><br><span class="line"><span class="string">            line = line.strip()</span></span><br><span class="line"><span class="string">            if line not in stopwords_list:</span></span><br><span class="line"><span class="string">                stopwords_list.append(line)</span></span><br><span class="line"><span class="string">    print(len(stopwords_list))</span></span><br><span class="line"><span class="string">    # remove stop words from word_list</span></span><br><span class="line"><span class="string">    word_list = [word for word in word_list if word not in stopwords_list]</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    words = <span class="string">' '</span>.join(word_list)</span><br><span class="line">    <span class="keyword">return</span> words</span><br></pre></td></tr></table></figure></p><p><strong>云图生成</strong>：<br>云图生成使用 Python 的 <a href="https://github.com/amueller/word_cloud" target="_blank" rel="noopener">WordCloud</a> 包和 <a href="https://python-pillow.org" target="_blank" rel="noopener">Pillow</a> 图像处理包。代码如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> wordcloud <span class="keyword">import</span> WordCloud, ImageColorGenerator</span><br><span class="line"><span class="keyword">import</span> PIL.Image <span class="keyword">as</span> Image</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_word_cloud</span><span class="params">(words)</span>:</span></span><br><span class="line">    coloring = np.array(Image.open(<span class="string">'./data/chinese.jpg'</span>))</span><br><span class="line">    wc = WordCloud(background_color=<span class="string">'white'</span>, max_words=<span class="number">2000</span>, mask=coloring, max_font_size=<span class="number">60</span>, random_state=<span class="number">42</span>, </span><br><span class="line">                   font_path=<span class="string">'./data/DroidSansFallbackFull.ttf'</span>, scale=<span class="number">2</span>).generate(words)</span><br><span class="line">    image_color = ImageColorGenerator(coloring)</span><br><span class="line">    plt.figure(figsize=(<span class="number">32</span>, <span class="number">16</span>))</span><br><span class="line">    plt.imshow(wc.recolor(color_func=image_color))</span><br><span class="line">    plt.imshow(wc)</span><br><span class="line">    plt.axis(<span class="string">'off'</span>)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure></p><p>首先我们对所有的评论内容生成云图：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">all_words = segment_words(<span class="string">'all'</span>)</span><br><span class="line">plot_word_cloud(all_words)</span><br></pre></td></tr></table></figure></p><p><img src="/images/python/douban_comments/all.png" alt="All"><br>然后分别对力荐、推荐，以及较差、很差生成评论云图，并进行比较：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">five_start_words = segment_words(<span class="string">'力荐'</span>)</span><br><span class="line">plot_word_cloud(five_start_words)</span><br><span class="line">four_start_words = segment_words(<span class="string">'推荐'</span>)</span><br><span class="line">plot_word_cloud(four_start_words)</span><br></pre></td></tr></table></figure></p><p><img src="/images/python/douban_comments/4-5-star.png" alt="4-5"><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">two_start_words = segment_words(<span class="string">'较差'</span>)</span><br><span class="line">plot_word_cloud(two_start_words)</span><br><span class="line">one_start_words = segment_words(<span class="string">'很差'</span>)</span><br><span class="line">plot_word_cloud(one_start_words)</span><br></pre></td></tr></table></figure></p><p><img src="/images/python/douban_comments/1-2-star.png" alt="1-2"><br>以上便是影评数据的简单分析和展示，进一步的可以对影评数据进行更精确的清理和分词操作等，并且根据评分等级，可以用该数据为基础搭建一个用户正负情感的机器学习模型等。以后有时间，再对这些数据进行进一步的处理。<br>代码可在我的 GitHub Repository 中找到：<a href="https://github.com/IsaacChanghau/AmusingPythonCodes/tree/master/wolf_warriors_ii" target="_blank" rel="noopener">AmusingPythonCodes/wolf_warriors_comments</a>。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文参考&lt;a href=&quot;https://zhuanlan.zhihu.com/wajuejiprince&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;数据分析小王子&lt;/a&gt;·&lt;a href=&quot;https://zhuanlan.zhihu.com/p/28475619&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;&amp;lt;&amp;lt;战狼Ⅱ&amp;gt;&amp;gt;豆瓣十二万影评浅析&lt;/a&gt;，利用 Python 网络爬虫爬取豆瓣影评数据并进行简要分析。
    
    </summary>
    
      <category term="Python" scheme="https://isaacchanghau.github.io/categories/Python/"/>
    
    
      <category term="python" scheme="https://isaacchanghau.github.io/tags/python/"/>
    
      <category term="data analysis" scheme="https://isaacchanghau.github.io/tags/data-analysis/"/>
    
      <category term="natural language processing" scheme="https://isaacchanghau.github.io/tags/natural-language-processing/"/>
    
  </entry>
  
  <entry>
    <title>Python itchat包分析微信好友</title>
    <link href="https://isaacchanghau.github.io/2017/09/10/Python-itchat%E5%8C%85%E5%88%86%E6%9E%90%E5%BE%AE%E4%BF%A1%E6%9C%8B%E5%8F%8B/"/>
    <id>https://isaacchanghau.github.io/2017/09/10/Python-itchat包分析微信朋友/</id>
    <published>2017-09-10T06:31:51.000Z</published>
    <updated>2017-09-21T08:54:45.000Z</updated>
    
    <content type="html"><![CDATA[<p>本文参照<a href="http://mp.weixin.qq.com/s/UZw5nx3cX5BvZ9rg1zBNLg" target="_blank" rel="noopener">大数据</a>·<a href="http://mp.weixin.qq.com/s/UZw5nx3cX5BvZ9rg1zBNLg" target="_blank" rel="noopener">“一件有趣的事：我用 Python 爬了爬自己的微信朋友”</a>，利用Python的 <a href="http://itchat.readthedocs.io/zh/latest/" target="_blank" rel="noopener">itchat</a> 包简单的分析了一下自己的朋友圈。<a id="more"></a></p><p>首先需要安装 <a href="http://itchat.readthedocs.io/zh/latest/" target="_blank" rel="noopener">itchat</a> 包：<code>sudo pip3 install itchat</code> 或者 <code>sudo pip install itchat</code>。</p><p>成功安装 itchat 后，便可正式使用这个包来爬一爬自己的微信朋友了。这里，我先导入之后需要用到的Python 包：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> itchat  <span class="comment"># itchat documentation -- https://itchat.readthedocs.io/zh/latest/api/</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">from</span> wordcloud <span class="keyword">import</span> WordCloud, ImageColorGenerator</span><br><span class="line"><span class="keyword">import</span> PIL.Image <span class="keyword">as</span> Image <span class="comment"># pillow</span></span><br><span class="line"><span class="keyword">import</span> jieba  <span class="comment"># chinese word segementation tool</span></span><br><span class="line"><span class="keyword">from</span> matplotlib.font_manager <span class="keyword">import</span> FontProperties</span><br><span class="line"><span class="comment"># since matplotlib and pandas.plot cannot display chinese</span></span><br><span class="line">font = FontProperties(fname=<span class="string">'./data/DroidSansFallbackFull.ttf'</span>, size=<span class="number">14</span>)  <span class="comment"># load chinese font</span></span><br></pre></td></tr></table></figure></p><p>现在我们需要登录网页版微信并获取朋友信息：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># login, default a QR code will be generated, scan for login</span></span><br><span class="line">itchat.login()</span><br><span class="line">friends = itchat.get_friends(update=<span class="keyword">True</span>)[<span class="number">0</span>:]  <span class="comment"># get all friends</span></span><br><span class="line">print(friends[<span class="number">0</span>])  <span class="comment"># the first one is yourself</span></span><br></pre></td></tr></table></figure></p><p>这里<code>itchat.login()</code>默认将生成一个二维码图片，利用手机微信扫描登录即可，之后获取所有的朋友信息，其中第一条数据是自己的信息，信息以<code>dict</code>形式存储。打印输出如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">'UserName'</span>: u<span class="string">'...'</span>, <span class="string">'City'</span>: <span class="string">''</span>, ..., <span class="string">'Province'</span>: <span class="string">''</span>, ..., <span class="string">'Signature'</span>: u<span class="string">'\u5982\u679c\u5fc3\u4f1a\u6210\u4e3a\u963b\u788d\n\u8ba9\u5b83\u6d88\u5931\u5c31\u597d\u4e86'</span>, ..., <span class="string">'NickName'</span>: u<span class="string">'Isaac Changhau'</span>, ..., <span class="string">'Sex'</span>: 1, ...&#125;</span><br></pre></td></tr></table></figure><p>分析上面的输出可以得出，城市、省份信息存储在 key 为 <code>City</code> 和 <code>Province</code> 的记录中，用户名信息存储在 key 为 <code>NickName</code> 的记录中，签名信息存储在 key 为 <code>Signature</code> 的记录中，性别信息存储在 key 为 <code>Sex</code> 的记录中，其中 <code>1</code> 表示男性，<code>2</code> 表示女性，其他就是不明性别的 (通常是没有填写)。</p><p>所以我先对自己的微信朋友的男女分布做一个统计，首先进行数据提取和累加：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># get male-female-ratio</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_male_female_count</span><span class="params">(friends)</span>:</span></span><br><span class="line">    male = <span class="number">0</span></span><br><span class="line">    female = <span class="number">0</span></span><br><span class="line">    others = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> friend <span class="keyword">in</span> friends:</span><br><span class="line">        sex = friend[<span class="string">'Sex'</span>]</span><br><span class="line">        <span class="keyword">if</span> sex == <span class="number">1</span>:</span><br><span class="line">            male += <span class="number">1</span></span><br><span class="line">        <span class="keyword">elif</span> sex == <span class="number">2</span>:</span><br><span class="line">            female += <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            others += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> male, female, others</span><br><span class="line"></span><br><span class="line">male, female, others = get_male_female_count(friends[<span class="number">1</span>:])</span><br><span class="line">total = len(friends[<span class="number">1</span>:])</span><br><span class="line">print(<span class="string">'Male population: &#123;:d&#125;, ratio: &#123;:.4f&#125;'</span>.format(male, male / float(total)))</span><br><span class="line">print(<span class="string">'Female population: &#123;:d&#125;, ratio: &#123;:.4f&#125;'</span>.format(female, female / float(total)))</span><br><span class="line">print(<span class="string">'Others: &#123;:d&#125;, ratio: &#123;:.4f&#125;'</span>.format(others, others / float(total)))</span><br></pre></td></tr></table></figure></p><p>得到输出：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Male population: 190, ratio: 0.5352</span><br><span class="line">Female population: 151, ratio: 0.4254</span><br><span class="line">Others: 14, ratio: 0.0394</span><br></pre></td></tr></table></figure></p><p>然后，可视化上述信息：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># plot male-female-ratio</span></span><br><span class="line">index = np.arange(<span class="number">3</span>)</span><br><span class="line">genders = (male, female, others)</span><br><span class="line">bar_width = <span class="number">0.35</span></span><br><span class="line">plt.figure(figsize=(<span class="number">14</span>, <span class="number">7</span>))</span><br><span class="line">plt.bar(index, genders, bar_width, alpha=<span class="number">0.6</span>, color=<span class="string">'rgb'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Gender'</span>, fontsize=<span class="number">16</span>)  </span><br><span class="line">plt.ylabel(<span class="string">'Population'</span>, fontsize=<span class="number">16</span>)</span><br><span class="line">plt.title(<span class="string">'Male-Female Population'</span>, fontsize=<span class="number">18</span>)  </span><br><span class="line">plt.xticks(index, (<span class="string">'Male'</span>, <span class="string">'Female'</span>, <span class="string">'Others'</span>), fontsize=<span class="number">14</span>, rotation=<span class="number">20</span>)</span><br><span class="line">plt.ylim(<span class="number">0</span>,<span class="number">220</span>)</span><br><span class="line"><span class="keyword">for</span> idx, gender <span class="keyword">in</span> zip(index, genders):</span><br><span class="line">    plt.text(idx, gender + <span class="number">0.1</span>, <span class="string">'%.0f'</span> % gender, ha=<span class="string">'center'</span>, va=<span class="string">'bottom'</span>, fontsize=<span class="number">14</span>, color=<span class="string">'black'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p><p><img src="/images/python/wechatexplore/male-female.png" alt="male-female-ratio"><br>从上图可以直观的看到，我的男性朋友比女性朋友多！(是不是说明我还是比较正直～)。另外，我们也可以对好友的城市分布做一个简单的统计并作出统计图。同样，我先提取出了感兴趣的数据段，并储存为 <code>DataFrame</code>：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># extract the variables: NickName, Sex, City, Province, Signature</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_features</span><span class="params">(friends)</span>:</span></span><br><span class="line">    features = []</span><br><span class="line">    <span class="keyword">for</span> friend <span class="keyword">in</span> friends:</span><br><span class="line">        feature = &#123;<span class="string">'NickName'</span>: friend[<span class="string">'NickName'</span>], <span class="string">'Sex'</span>: friend[<span class="string">'Sex'</span>], <span class="string">'City'</span>: friend[<span class="string">'City'</span>], </span><br><span class="line">                  <span class="string">'Province'</span>: friend[<span class="string">'Province'</span>], <span class="string">'Signature'</span>: friend[<span class="string">'Signature'</span>]&#125;</span><br><span class="line">        features.append(feature)</span><br><span class="line">    <span class="keyword">return</span> pd.DataFrame(features)</span><br><span class="line">features = get_features(friends[<span class="number">1</span>:])</span><br><span class="line">print(features.columns)</span><br><span class="line">features.head()</span><br></pre></td></tr></table></figure></p><p>输出数据为：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Index([u<span class="string">'City'</span>, u<span class="string">'NickName'</span>, u<span class="string">'Province'</span>, u<span class="string">'Sex'</span>, u<span class="string">'Signature'</span>], dtype=<span class="string">'object'</span>)</span><br><span class="line"></span><br><span class="line">       City    NickName    Province    Sex                Signature</span><br><span class="line">0      成都         娇姐         四川     2      忧郁的娇姐，愤怒的小豪！</span><br><span class="line">1   乌鲁木齐 樱桃小兔子 ❤         新疆      2      路遥知马力，日久见人心</span><br><span class="line">2      北碚    waitings         重庆      1           做一个傻子多么好</span><br><span class="line">3              AlexShi                  1    A <span class="literal">true</span> procrastinator</span><br><span class="line">4      沈阳       崔智语         辽宁      1</span><br></pre></td></tr></table></figure><p>之后提取省份和城市信息 (注：部分好友在海外，因此对于省份和城市的划分和国内不同)，然后进行简单的数据清理，之后按省份和城市进行数据聚合并且统计各个城市的人数。这里我取排名前二十的省份进行堆叠直方图展示：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">locations = features.loc[:, [<span class="string">'Province'</span>, <span class="string">'City'</span>]]  <span class="comment"># get location columns</span></span><br><span class="line">locations = locations[locations[<span class="string">'Province'</span>] != <span class="string">''</span>]  <span class="comment"># clean empty city or province records</span></span><br><span class="line">data = locations.groupby([<span class="string">'Province'</span>, <span class="string">'City'</span>]).size().unstack()  <span class="comment"># group by and count</span></span><br><span class="line">count_subset = data.take(data.sum(<span class="number">1</span>).argsort())[<span class="number">-20</span>:]  <span class="comment"># obtain the 20 highest data</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># plot</span></span><br><span class="line">subset_plot = count_subset.plot(kind=<span class="string">'bar'</span>, stacked=<span class="keyword">True</span>, figsize=(<span class="number">24</span>, <span class="number">24</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># set fonts</span></span><br><span class="line">xtick_labels = subset_plot.get_xticklabels()</span><br><span class="line"><span class="keyword">for</span> label <span class="keyword">in</span> xtick_labels: </span><br><span class="line">    label.set_fontproperties(font)</span><br><span class="line">legend_labels = subset_plot.legend().texts</span><br><span class="line"><span class="keyword">for</span> label <span class="keyword">in</span> legend_labels:</span><br><span class="line">    label.set_fontproperties(font)</span><br><span class="line">    label.set_fontsize(<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">plt.xlabel(<span class="string">'Province'</span>, fontsize=<span class="number">20</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Number'</span>, fontsize=<span class="number">20</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p><p><img src="/images/python/wechatexplore/city.png" alt="male-female-ratio"><br>上面这个图画得比较丑，哎，对Python画图不是太熟练，<code>colormap</code> 也没有好好设置，不过大概意思是能表明的。</p><p>最后，我根据好友的个性签名生成自定义云图。这里需要用到 Python 的 <a href="https://github.com/fxsjy/jieba" target="_blank" rel="noopener">jieba</a> 包对中文进行分词 (之前已经引入了该包)。首先提取出所有的个性签名，并组合成一个<code>text</code>：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">sigature_list = []</span><br><span class="line"><span class="keyword">for</span> signature <span class="keyword">in</span> features[<span class="string">'Signature'</span>]:</span><br><span class="line">    signature = signature.strip().replace(<span class="string">'span'</span>, <span class="string">''</span>).replace(<span class="string">'class'</span>, <span class="string">''</span>).replace(<span class="string">'emoji'</span>, <span class="string">''</span>)</span><br><span class="line">    <span class="comment"># re.compile(ur'[^a-zA-Z0-9\u4e00-\u9fa5 ]').sub('', signature)</span></span><br><span class="line">    signature = re.compile(<span class="string">'1f\d+\w*|[&lt;&gt;/=]'</span>).sub(<span class="string">''</span>, signature)</span><br><span class="line">    <span class="keyword">if</span> (len(signature) &gt; <span class="number">0</span>):</span><br><span class="line">        sigature_list.append(signature)</span><br><span class="line"></span><br><span class="line">text = <span class="string">''</span>.join(sigature_list)</span><br><span class="line"><span class="comment"># print(text)</span></span><br></pre></td></tr></table></figure></p><p>之后，利用结巴分词，对<code>text</code> 进行划分：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">word_list = jieba.cut(text, cut_all=<span class="keyword">True</span>)</span><br><span class="line">words = <span class="string">' '</span>.join(word_list)</span><br><span class="line"><span class="comment"># print(words)</span></span><br></pre></td></tr></table></figure></p><p>最后，基于 Python 的 <a href="https://github.com/amueller/word_cloud" target="_blank" rel="noopener">WordCloud</a> 包进行云图生成，WordCloud 可以根据自己想要的图片、形状、颜色画出相似的图形。这里喔使用我自己的头像和 Wechat logo 分别生成了词云进行展示。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">coloring = np.array(Image.open(<span class="string">'./data/avatar.jpg'</span>))</span><br><span class="line">wc = WordCloud(background_color=<span class="string">'white'</span>, max_words=<span class="number">2000</span>, mask=coloring, max_font_size=<span class="number">60</span>, random_state=<span class="number">42</span>, </span><br><span class="line">               font_path=<span class="string">'./data/DroidSansFallbackFull.ttf'</span>, scale=<span class="number">2</span>).generate(words)</span><br><span class="line">image_color = ImageColorGenerator(coloring)</span><br><span class="line">plt.figure(figsize=(<span class="number">32</span>, <span class="number">16</span>))</span><br><span class="line">plt.imshow(wc.recolor(color_func=image_color))</span><br><span class="line">plt.imshow(wc)</span><br><span class="line">plt.axis(<span class="string">'off'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p><p><img src="/images/python/wechatexplore/avatar.png" alt="male-female-ratio"><br><img src="/images/python/wechatexplore/logo.png" alt="male-female-ratio"><br>最后的最后，当然，itchat并不是只有这些无聊的功能，它更强大的地方在于可以进行消息收发，自定义个人号机器人等等…… 这些都值得花时间去考究探索。<br>代码可在我的 GitHub Repository 中找到：<a href="https://github.com/IsaacChanghau/AmusingPythonCodes/tree/master/wechat_exploration" target="_blank" rel="noopener">AmusingPythonCodes/wechat_exploration</a>。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文参照&lt;a href=&quot;http://mp.weixin.qq.com/s/UZw5nx3cX5BvZ9rg1zBNLg&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;大数据&lt;/a&gt;·&lt;a href=&quot;http://mp.weixin.qq.com/s/UZw5nx3cX5BvZ9rg1zBNLg&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;“一件有趣的事：我用 Python 爬了爬自己的微信朋友”&lt;/a&gt;，利用Python的 &lt;a href=&quot;http://itchat.readthedocs.io/zh/latest/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;itchat&lt;/a&gt; 包简单的分析了一下自己的朋友圈。
    
    </summary>
    
      <category term="Python" scheme="https://isaacchanghau.github.io/categories/Python/"/>
    
    
      <category term="python" scheme="https://isaacchanghau.github.io/tags/python/"/>
    
      <category term="data analysis" scheme="https://isaacchanghau.github.io/tags/data-analysis/"/>
    
  </entry>
  
  <entry>
    <title>Autoencoder and Sparsity</title>
    <link href="https://isaacchanghau.github.io/2017/08/19/Autoencoder-and-Sparsity/"/>
    <id>https://isaacchanghau.github.io/2017/08/19/Autoencoder-and-Sparsity/</id>
    <published>2017-08-19T06:43:15.000Z</published>
    <updated>2018-01-30T08:31:43.000Z</updated>
    
    <content type="html"><![CDATA[<p>The article is excerpted from Andrew Ng’s <a href="https://web.stanford.edu/class/cs294a/" target="_blank" rel="noopener">CS294A Lecture notes</a>: <a href="https://web.stanford.edu/class/cs294a/sparseAutoencoder_2011new.pdf" target="_blank" rel="noopener">Sparse Autoencoder</a> with some personal understanding. Before going through this article, you would better get some information from the <a href="https://github.com/IsaacChanghau/2017/08/17/Backpropagation-Algorithm/" target="_blank" rel="noopener">Backpropagation Algorithm</a>.<a id="more"></a></p><p>Suppose we have only unlabeled training examples set $\{x^{(1)},x^{(2)},x^{(3)},\dots\}$, where $x^{(i)}\in\mathbb{R}^{n}$. An <strong>autoencoder neural network</strong> is an <strong>unsupervised learning algorithm</strong> that applies backpropagation, setting the target values to be equal to the inputs. i.e., it uses $y^{(i)}=x^{(i)}$. Below is an autoencoder:<br><img src="/images/deeplearning/autoencoder/1.png" alt="1.png"><br>The <strong>autoencoder</strong> tries to learn a function $h_{W,b}(x)\approx x$. In other words, it is trying to learn an approximation to the identity function, so as to output $\hat{x}$ that is similar to $x$. The identity function seems a particularly trivial function to be trying to learn; but <strong>by placing constraints on the network</strong>, such as by <strong>limiting the number of hidden units</strong>, we can <strong>discover interesting structure about the data</strong>. </p><p>As a concrete example, suppose the inputs x are the pixel intensity values from a $10\times 10$ image (100 pixels) so $n=100$, and there are $s_{2}=50$ hidden units in layer $L_{2}$. Note that we also have $y\in\mathbb{R}^{100}$. Since there are only 50 hidden units, the network is forced to <strong>learn a compressed representation of the input</strong>. That is, given only the vector of hidden unit activations $a^{(2)}\in\mathbb{R}^{50}$, it must try to <strong>reconstruct</strong> the 100-pixel input $x$.</p><p>If the input were completely random, say, each $x_{i}$ comes from an I.I.D. (Independently and Identically Distribute) Gaussian independent of the other features, then this compression task would be very difficult. But if there is structure in the data, for example, if some of the input features are correlated, then this algorithm will be able to discover some of those correlations.</p><blockquote><p>In fact, this simple autoencoder oftern ends up learning a <strong>low-dimensional representation</strong> very similar to <strong>PCA’s</strong>.</p></blockquote><p>Our argument above relied on the number of hidden units $s_{2}$ being small. But even when the number of hidden units is large (perhaps even greater than the number of input pixels), we can still discover interesting structure, by imposing other constraints on the network. In particular, if we impose a <strong>sparsity constraint</strong> on the hidden units, then the autoencoder will still discover interesting structure in the data, even if the number of hidden units is large.</p><p>Informally, we will think of a neuron as being “active” (or as “firing”) if its output value is close to 1, or as being “inactive” if its output value is close to 0. We would like to constrain the neurons to be inactive most of the time.</p><blockquote><p>This discussion assumes a <code>sigmoid</code> activation function. If you are using a <code>tanh</code> activation function, then we think of a neuron as being inactive when it outputs values close to -1.</p></blockquote><p>Recall that $a_{j}^{(2)}$ denotes the activation of hidden unit $j$ in the autoencoder. However, this notation does not make explicit what was the input $x$ that led to that activation. Thus, we will write $a_{j}^{(2)}(x)$ to denote the activation of this hidden unit when the network is given a specific input $x$. Further, let$$<br>\hat{\rho}_{j}=\frac{1}{m}\sum_{i=1}^{m}\big[a_{j}^{(2)}(x^{(i)})\big]\tag{1}<br>$$be the <strong>average activation</strong> of hidden unit $j$ (averaged over the training set). We would like to (approximately) enforce the constraint$$<br>\hat{\rho}_{j}=\rho\tag{2}<br>$$where $\rho$ is a <strong>sparsity parameter</strong>, typically, a small value close to zero (say $\rho=0.05$). In other words, we would like the average activation of each hidden neuron $j$ to be close to 0.05 (say). To satisfy this constraint, the hidden unit’s activations must mostly be near 0.</p><p>To achieve this, we will add an extra <strong>penalty term</strong> to our <strong>optimization objective</strong> that penalizes $\hat{\rho}_{j}$ deviating significantly from $\rho$. Many choices of the penalty term will give reasonable results. We will choosr the following:$$<br>\sum_{j=1}^{s_{2}}\rho\log\frac{\rho}{\hat{\rho}_{j}}+(1-\rho)\log\frac{1-\rho}{1-\hat{\rho}_{j}}\tag{3}<br>$$Here, $s_{2}$ is the number of neurons in the hidden layer, and the index $j$ is summing over the hidden units in our network. If you are familiar with the concept of <strong>KL divergence</strong>, this penalty term is based on it, and can also be written$$<br>\sum_{j=1}^{s_{2}}KL(\rho\Vert\hat{\rho}_{j})\tag{4}<br>$$where $KL(\rho\Vert\hat{\rho}_{j})=\rho\log\frac{\rho}{\hat{\rho}_{j}}+(1-\rho)\log\frac{1-\rho}{1-\hat{\rho}_{j}}$ is the <strong>Kullback-Leibler (KL) divergence</strong> between a <strong>Bernoulli random variable</strong> with mean $\rho$ and a <strong>Bernoulli random variable</strong> with mean $\hat{\rho}_{j}$. <strong>KL-divergence is a standard function for measuring how different two different distributions are</strong>.</p><p>This penalty function has the property that $KL(\rho\Vert\hat{\rho}_{j})=0$ if $\hat{\rho}_{j}=\rho$, and<br>otherwise it increases monotonically as $\hat{\rho}_{j}$ diverges from $\rho$. For example, in the figure below, we have set $\rho=0.2$, and plotted $KL(\rho\Vert\hat{\rho}_{j})$ for a range of values of $\hat{\rho}_{j}$<br><img src="/images/deeplearning/autoencoder/2.png" alt="2.png"><br>We see that the KL-divergence reaches its minimum of 0 at $\hat{\rho}_{j}=\rho$, and blows up (it actually approaches $\infty$) as $\hat{\rho}_{j}$ approaches 0 or 1. <strong>Thus, minimizing this penalty term has the effect of causing $\hat{\rho}_{j}$ to be close to $\rho$</strong>. Our overall cost function is now$$<br>J_{sparse}(W,b)=J(W,b)+\beta\sum_{j=1}^{s_{2}}KL(\rho\Vert\hat{\rho}_{j})\tag{5}<br>$$where $J(W,b)$ is as defined previously, and <strong>$\beta$ controls the weight of the sparsity penalty term</strong>. The term $\hat{\rho}_{j}$ (implicitly) depends on $W$, $b$ also, because it is the average activation of hidden unit $j$, and the activation of a hidden unit depends on the parameters $W$, $b$.</p><p>To incorporate the KL-divergence term into your derivative calculation, there is a simple-to-implement trick involving only a small change to your code. Specifically, where previously for the second layer ($l = 2$), during backpropagation you would have computed$$<br>\delta_{i}^{(2)}=\bigg(\sum_{j=1}^{s_{2}}W_{ji}^{(2)}\delta_{j}^{(3)}\bigg)f’(z_{i}^{(2)})\tag{6}<br>$$now instead compute$$<br>\delta_{i}^{(2)}=\bigg(\big(\sum_{j=1}^{s_{2}}W_{ji}^{(2)}\delta_{j}^{(3)}\big)+\beta\big(-\frac{\rho}{\hat{\rho}_{i}}+\frac{1-\rho}{1-\hat{\rho}_{i}}\big)\bigg)f’(z_{i}^{(2)})\tag{7}<br>$$</p><blockquote><p>Here is how to derive the $\frac{\partial J_{sparse}(W,b)}{\partial W_{i,j}^{(l)}}$ and $\frac{\partial J_{sparse}(W,b)}{\partial b_{i}^{(l)}}$<br>Denote $S(W,b)=\sum_{j=1}^{s_{2}}KL(\rho\Vert\hat{\rho}_{j})$, so (5) can be written as$$<br>J_{sparse}(W,b)=J(W,b)+\beta S(W,b)\tag{8}<br>$$Then the two partial derivations in gradient descent algorithm are$$\begin{cases}<br>\frac{\partial J_{sparse}(W,b)}{\partial W_{i,j}^{(l)}} &amp;=\frac{\partial J(W,b)}{\partial W_{i,j}^{(l)}}+\beta\frac{\partial S(W,b)}{\partial W_{i,j}^{(l)}}\\<br>\frac{\partial J_{sparse}(W,b)}{\partial b_{i}^{(l)}} &amp;=\frac{\partial J(W,b)}{\partial b_{i}^{(l)}}+\beta\frac{\partial S(W,b)}{\partial b_{i}^{(l)}}<br>\end{cases}\tag{9}$$The calculation of $\frac{\partial J(W,b)}{\partial W_{i,j}^{(l)}}$ and $\frac{\partial J(W,b)}{\partial b_{i}^{(l)}}$ have discussed in <a href="https://isaacchanghau.github.io/2017/08/17/Backpropagation-Algorithm/">Backpropagation Algorithm</a>, here we only discuss the computation of $\frac{\partial S(W,b)}{\partial W_{i,j}^{(l)}}$ and $\frac{\partial S(W,b)}{\partial b_{i}^{(l)}}$.<br>First, expand $S(W,b)$, we have$$<br>S(W,b)=\sum_{t=1}^{s_{2}}KL(\rho\Vert\hat{\rho}_{t})=\sum_{t=1}^{s_{2}}\big[\rho\ast\log\frac{\rho}{\hat{\rho}_{t}}+(1-\rho)\ast\log\frac{1-\rho}{1-\hat{\rho}_{t}}\big]\tag{10}<br>$$where$$<br>\hat{\rho}_{t}=\frac{1}{m}\sum_{k=1}^{m}a_{t}^{(2)}\big(x^{(k)}\big)=\frac{1}{m}\sum_{k=1}^{m}f\big(z_{t}^{(2)}\big)\tag{11}<br>$$here$$<br>z_{t}^{(2)}=z_{t}^{(2)}\big(x^{(k)}\big)=\bigg(\sum_{s=1}^{s_{1}}W_{t,s}^{(1)}x_{s}^{(k)}\bigg)+b_{t}^{(1)}\tag{12}<br>$$according to (11) and (12), we can get the following two points:</p><ol><li>when $l\neq 1$, $\frac{\partial S(W,b)}{\partial W_{i,j}^{(l)}}=0$, $\frac{\partial S(W,b)}{\partial b_{i}^{(l)}}=0$.</li><li>$\frac{\partial S(W,b)}{\partial W_{i,j}^{(l)}}=\frac{\partial\sum_{t=1}^{s_{2}}KL(\rho\Vert\hat{\rho}_{t})}{\partial W_{i,j}^{(l)}}=\frac{\partial KL(\rho\Vert\hat{\rho}_{i})}{\partial W_{i,j}^{(l)}}$; $\frac{\partial S(W,b)}{\partial b_{i}^{(l)}}=\frac{\partial\sum_{t=1}^{s_{2}}KL(\rho\Vert\hat{\rho}_{t})}{\partial b_{i}^{(l)}}=\frac{\partial KL(\rho\Vert\hat{\rho}_{i})}{\partial b_{i}^{(l)}}$.</li></ol></blockquote><blockquote><p>According to the above two points, we have$$\begin{aligned}<br>\frac{\partial S(W,b)}{\partial W_{i,j}^{(1)}} &amp;=\frac{\partial}{\partial W_{i,j}^{(1)}}\big[\rho\ast\log\frac{\rho}{\hat{\rho}_{i}}+(1-\rho)\ast\log\frac{1-\rho}{1-\hat{\rho}_{i}}\big]\\<br>&amp;= \frac{\partial}{\partial W_{i,j}^{(1)}}\big\{\rho(\log\rho-\log\hat{\rho}_{i})+(1-\rho)[\log(1-\rho)-\log(1-\hat{\rho}_{i})]\big\}\quad (\textrm{According to }\log\frac{A}{B}=\log A-\log B)\\<br>&amp;=\rho\big(0-\frac{1}{\hat{\rho}_{i}}\frac{\partial\hat{\rho}_{i}}{\partial W_{i,j}^{(1)}}\big)+(1-\rho)\big(0+\frac{1}{1-\hat{\rho}_{i}}\frac{\partial\hat{\rho}_{i}}{\partial W_{i,j}^{(1)}}\big)\quad (\rho\textrm{ is constant})\\<br>&amp;=\big(-\frac{\rho}{\hat{\rho}_{i}}+\frac{1-\rho}{1-\hat{\rho}_{i}}\big)\frac{\partial\hat{\rho}_{i}}{\partial W_{i,j}^{(1)}}<br>\end{aligned}$$Samely, we have$$<br>\frac{\partial S(W,b)}{\partial b_{i}^{(1)}}=\big(-\frac{\rho}{\hat{\rho}_{i}}+\frac{1-\rho}{1-\hat{\rho}_{i}}\big)\frac{\partial\hat{\rho}_{i}}{\partial b_{i}^{(1)}}<br>$$Then, we need to compute $\frac{\partial\hat{\rho}_{i}}{\partial W_{i,j}^{(1)}}$ and $\frac{\partial\hat{\rho}_{i}}{\partial b_{i}^{(1)}}$, according to (11), we have$$\begin{aligned}<br>\frac{\partial\hat{\rho}_{i}}{\partial W_{i,j}^{(1)}} &amp;=\frac{1}{m}\sum_{k=1}^{m}f’\big(z_{i}^{(2)}\big)\centerdot\frac{\partial z_{i}^{(2)}}{\partial W_{i,j}^{(1)}}\\<br>&amp;=\frac{1}{m}\sum_{k=1}^{m}f’\big(z_{i}^{(2)}\big)\centerdot x_{j}^{(k)}\quad (\textrm{according to (12), we have }\frac{\partial z_{i}^{(2)}}{\partial W_{i,j}^{(1)}}=x_{j}^{(k)})<br>\end{aligned}$$Samely, we have$$\begin{aligned}<br>\frac{\partial\hat{\rho}_{i}}{\partial b_{i}^{(1)}} &amp;=\frac{1}{m}\sum_{k=1}^{m}f’\big(z_{i}^{(2)}\big)\centerdot\frac{\partial z_{i}^{(2)}}{\partial b_{i}^{(1)}}\\<br>&amp;=\frac{1}{m}\sum_{k=1}^{m}f’\big(z_{i}^{(2)}\big)\quad (\textrm{according to (12), we have }\frac{\partial z_{i}^{(2)}}{\partial b_{i}^{(1)}}=1)<br>\end{aligned}$$In summary, we can obtain$$\begin{cases}<br>\frac{\partial S(W,b)}{\partial W_{i,j}^{(1)}} &amp;=\big(-\frac{\rho}{\hat{\rho}_{i}}+\frac{1-\rho}{1-\hat{\rho}_{i}}\big)\centerdot\frac{1}{m}\sum_{k=1}^{m}f’\big(z_{i}^{(2)}\big)\centerdot x_{j}^{(k)}\\<br>\frac{\partial S(W,b)}{\partial b_{i}^{(1)}}&amp;=\big(-\frac{\rho}{\hat{\rho}_{i}}+\frac{1-\rho}{1-\hat{\rho}_{i}}\big)\centerdot\frac{1}{m}\sum_{k=1}^{m}f’\big(z_{i}^{(2)}\big)<br>\end{cases}\tag{13}$$In <a href="https://isaacchanghau.github.io/2017/08/17/Backpropagation-Algorithm/">Backpropagation Algorithm</a>, we already obtain that$$\begin{cases}<br>\frac{\partial J(W,b)}{\partial W_{i,j}^{(1)}}&amp;=\big[\frac{1}{m}\sum_{k=1}^{m}a_{j}^{(1)}\delta_{i}^{(2)}\big]+\lambda W_{i,j}^{(1)}\\<br>\frac{\partial J(W,b)}{\partial b_{i}^{(1)}}&amp;=\frac{1}{m}\sum_{k=1}^{m}\delta_{i}^{(2)}<br>\end{cases}\tag{14}$$Thus, put (13), (14) into (9), we will get$$\begin{cases}<br>\frac{\partial J_{sparse}(W,b)}{\partial W_{i,j}^{(l)}} &amp;=\big[\frac{1}{m}\sum_{k=1}^{m}a_{j}^{(1)}\delta_{i}^{(2)}\big]+\lambda W_{i,j}^{(1)}+\beta\big(-\frac{\rho}{\hat{\rho}_{i}}+\frac{1-\rho}{1-\hat{\rho}_{i}}\big)\centerdot\frac{1}{m}\sum_{k=1}^{m}f’\big(z_{i}^{(2)}\big)\centerdot x_{j}^{(k)}\\<br>\frac{\partial J_{sparse}(W,b)}{\partial b_{i}^{(l)}} &amp;=\frac{1}{m}\sum_{k=1}^{m}\delta_{i}^{(2)}+\beta\big(-\frac{\rho}{\hat{\rho}_{i}}+\frac{1-\rho}{1-\hat{\rho}_{i}}\big)\centerdot\frac{1}{m}\sum_{k=1}^{m}f’\big(z_{i}^{(2)}\big)<br>\end{cases}\tag{15}$$Note that $a_{j}^{(1)}$, $\delta_{i}^{(2)}$ and $z_{i}^{(2)}$ are related to $x^{(k)}$ (or $y^{(k)}$), particularly, we have $a_{j}^{(1)}=x_{j}^{(k)}$. After simplify (15), we get$$\begin{cases}<br>\frac{\partial J_{sparse}(W,b)}{\partial W_{i,j}^{(l)}} &amp;=\frac{1}{m}\sum_{k=1}^{m}\bigg\{a_{j}^{(1)}\big[\delta_{i}^{(2)}+\beta\big(-\frac{\rho}{\hat{\rho}_{i}}+\frac{1-\rho}{1-\hat{\rho}_{i}}\big)f’\big(z_{i}^{(2)}\big)\big]\bigg\}+\lambda W_{i,j}^{(1)}\\<br>\frac{\partial J_{sparse}(W,b)}{\partial b_{i}^{(l)}} &amp;=\frac{1}{m}\sum_{k=1}^{m}\big[\delta_{i}^{(2)}+\beta\big(-\frac{\rho}{\hat{\rho}_{i}}+\frac{1-\rho}{1-\hat{\rho}_{i}}\big)f’\big(z_{i}^{(2)}\big)\big]<br>\end{cases}$$Now, we obtain how to change (6) to (7).</p></blockquote><p>One subtlety is that you’ll need to know $\hat{\rho}_{i}$ to compute this term. Thus, you’ll need to compute a forward pass on all the training examples first to compute the average activations on the training set, before computing backpropagation on any example. If your training set is small enough to fit comfortably in computer memory (this will be the case for the programming assignment), you can compute forward passes on all your examples and keep the resulting activations in memory and compute the $\hat{\rho}_{i}$s. Then you can use your precomputed activations to perform backpropagation on all your examples. <strong>If your data is too large to fit in memory, you may have to scan through your examples computing a forward pass on each to accumulate (sum up) the activations and compute $\hat{\rho}_{i}$</strong> (discarding the result of each forward pass after you have taken its activations $a_{i}^{(2)}$ into account for computing $\hat{\rho}_{i}$). Then after having computed $\hat{\rho}_{i}$, you’d have to redo the forward pass for each example so that you can do backpropagation on that example. <strong>In this latter case, you would end up computing a forward pass twice on each example in your training set, making it computationally less efficient</strong>.</p><p>The full derivation showing that the algorithm above results in gradient descent is beyond the scope of these notes. But if you implement the autoencoder using backpropagation modified this way, you will be performing gradient descent exactly on the objective $J_{sparse}(W,b)$. Using the derivative checking method, you will be able to verify this for yourself as well.</p><p>Below is the <strong>summary of notation</strong></p><style>table th:first-of-type {    width: 70px;}</style><table><thead><tr><th style="text-align:center">Expression</th><th style="text-align:left">Explanation </th></tr></thead><tbody><tr><td style="text-align:center">$x$</td><td style="text-align:left">Input features for a training example, $x\in\mathbb{R}^{n}$.</td></tr><tr><td style="text-align:center">$y$</td><td style="text-align:left">Output/target values. Here, $y$ can be vector valued. In the case of an autoencoder, $y = x$.</td></tr><tr><td style="text-align:center">$(x^{(i)},y^{(i)})$</td><td style="text-align:left">The $i$-th training example.</td></tr><tr><td style="text-align:center">$h_{W,b}(x)$</td><td style="text-align:left">Output of our hypothesis on input $x$, using parameters $W$, $b$. This should be a vector of the same dimension as the target value $y$.</td></tr><tr><td style="text-align:center">$W_{i,j}^{(l)}$</td><td style="text-align:left">The parameter associated with the connection between unit $j$ in layer $l$, and unit $i$ in layer $l+1$.</td></tr><tr><td style="text-align:center">$b_{i}^{(l)}$</td><td style="text-align:left">The bias term associated with unit $i$ in layer $l+1$. Can also be thought of as the parameter associated with the connection between the bias unit in layer $l$ and unit $i$ in layer $l+1$.</td></tr><tr><td style="text-align:center">$\theta$</td><td style="text-align:left">Our parameter vector. It is useful to think of this as the result of taking the parameters $W$, $b$ and “unrolling” them into a long column vector.</td></tr><tr><td style="text-align:center">$a_{i}^{(l)}$</td><td style="text-align:left">Activation (output) of unit $i$ in layer $l$ of the network. In addition, since layer $L_{1}$ is the input layer, we also have $a_{i}^{(1)}=x_{i}$.</td></tr><tr><td style="text-align:center">$f(\centerdot)$</td><td style="text-align:left">The activation function. Throughout these notes, we used $f(z)=tanh(z)$.</td></tr><tr><td style="text-align:center">$z_{i}^{(l)}$</td><td style="text-align:left">Total weighted sum of inputs to unit $i$ in layer $l$. Thus, $a_{i}^{(l)}=f\big(z_{i}^{(l)}\big)$</td></tr><tr><td style="text-align:center">$\alpha$</td><td style="text-align:left">Learning rate parameter.</td></tr><tr><td style="text-align:center">$s_{l}$</td><td style="text-align:left">Number of units in layer $l$ (not counting the bias unit).</td></tr><tr><td style="text-align:center">$n_{l}$</td><td style="text-align:left">Number layers in the network. Layer $L_{1}$ is usually the input layer, and layer $L_{n_{l}} the output layer.</td></tr><tr><td style="text-align:center">$\lambda$</td><td style="text-align:left">Weight decay parameter.</td></tr><tr><td style="text-align:center">$\hat{x}$</td><td style="text-align:left">For an autoencoder, its output; i.e., its reconstruction of the input $x$. Same meaning as $h_{W,b}(x)$.</td></tr><tr><td style="text-align:center">$\rho$</td><td style="text-align:left">Sparsity parameter, which specifies our desired level of sparsity.</td></tr><tr><td style="text-align:center">$\hat{\rho}_{i}$</td><td style="text-align:left">The average activation of hidden unit $i$ (in the sparse autoencoder).</td></tr><tr><td style="text-align:center">$\beta$</td><td style="text-align:left">Weight of the sparsity penalty term (in the sparse autoencoder objective).</td></tr></tbody></table>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;The article is excerpted from Andrew Ng’s &lt;a href=&quot;https://web.stanford.edu/class/cs294a/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;CS294A Lecture notes&lt;/a&gt;: &lt;a href=&quot;https://web.stanford.edu/class/cs294a/sparseAutoencoder_2011new.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Sparse Autoencoder&lt;/a&gt; with some personal understanding. Before going through this article, you would better get some information from the &lt;a href=&quot;https://github.com/IsaacChanghau/2017/08/17/Backpropagation-Algorithm/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Backpropagation Algorithm&lt;/a&gt;.
    
    </summary>
    
      <category term="Machine Learning" scheme="https://isaacchanghau.github.io/categories/Machine-Learning/"/>
    
    
      <category term="deep learning" scheme="https://isaacchanghau.github.io/tags/deep-learning/"/>
    
      <category term="machine learning" scheme="https://isaacchanghau.github.io/tags/machine-learning/"/>
    
      <category term="mathematics" scheme="https://isaacchanghau.github.io/tags/mathematics/"/>
    
      <category term="autoencoder" scheme="https://isaacchanghau.github.io/tags/autoencoder/"/>
    
  </entry>
  
  <entry>
    <title>Backpropagation Algorithm</title>
    <link href="https://isaacchanghau.github.io/2017/08/17/Backpropagation-Algorithm/"/>
    <id>https://isaacchanghau.github.io/2017/08/17/Backpropagation-Algorithm/</id>
    <published>2017-08-17T11:58:15.000Z</published>
    <updated>2018-01-30T08:31:36.000Z</updated>
    
    <content type="html"><![CDATA[<p>The article is excerpted from Andrew Ng’s <a href="https://web.stanford.edu/class/cs294a/" target="_blank" rel="noopener">CS294A Lecture notes</a>: <a href="https://web.stanford.edu/class/cs294a/sparseAutoencoder_2011new.pdf" target="_blank" rel="noopener">Sparse Autoencoder</a>, then I add some personal understanding.<a id="more"></a></p><p>In this article, we will let $n_{l}$ denote the number of layers in our network, label $l$ as $L_{l}$, so layer $L_{1}$ is the input layer, and layer $L_{n_{l}}$ the output layer. Neural network has parameters $(W,b)=(W^{(1)},b^{(1)},\dots,W^{(n_{l}-1)},b^{(n_{l}-1)})$, where we write $W_{ij}^{(l)}$ to denote the parameter (or weight) associated with the connection between unit $j$ in layer $l$ and unit $i$ in layer $l+1$. (Note the order of the indices). Also, $b_{i}^{(l)}$ is the bais associated with unit $i$ in layer $l+1$. We also let $s_{l}$ denote the number of nodes in layer $l$ (not counting the bias unit). Plus, we will write $a_{i}^{(l)}$ to denote the activation (meaning output value) of unit $i$ in layer $l$.<br><img src="/images/deeplearning/backprop/1.png" alt="1.png"><br>Suppose we have a fixed training set $\{(x^{(1)},y^{(1)}),\dots,(x^{(m)},y^{(m)})\}$ of $m$ training examples. We can train our neural network using batch gradient descent. In detail, for a single training example $(x,y)$, we define the cost function with respect to that single example to be$$<br>J(W,b;x,y)=\frac{1}{2}\Vert h_{W,b}(x)-y\Vert^{2}\tag{1}<br>$$This is a (one-half) <strong>squared-error cost function</strong>. Given a training set of $m$ examples, we then define the <strong>overall cost function</strong> to be$$\begin{aligned}<br>J(W,b) &amp; =\bigg[\frac{1}{m}\sum_{i=1}^{m}J(W,b;x^{(i)},y^{(i)})\bigg]+\frac{\lambda}{2}\sum_{l=1}^{n_{l}-1}\sum_{i=1}^{s_{l}}\sum_{j=1}^{s_{l}+1}\big(W_{ji}^{(l)}\big)^{2}\\<br>&amp; =\bigg[\frac{1}{m}\sum_{i=1}^{m}\big(\frac{1}{2}\Vert h_{W,b}(x^{(i)})-y^{(i)}\Vert^{2}\big)\bigg]+\frac{\lambda}{2}\sum_{l=1}^{n_{l}-1}\sum_{i=1}^{s_{l}}\sum_{j=1}^{s_{l}+1}\big(W_{ji}^{(l)}\big)^{2}<br>\end{aligned}\tag{2}$$The first term is an <strong>average sum-of-squares error term</strong>. The second term is a <strong>regularization term</strong> (also called a <strong>weight decay term</strong>) that tends to decrease the magnitude of the weights, and helps <strong>prevent overfitting</strong>.</p><blockquote><p>Usually weight decay is not applied to the bias terms $b_{i}^{(l)}$, as reflected in the difinition of $J(W,b)$. Applying weight decay to the bias units usually makes only a small different to the final network, however. If you took <a href="http://cs229.stanford.edu" target="_blank" rel="noopener">CS229</a>, you may also recognize weight decay this as essentially a variant of the Bayesian regularization method you saw there, where we placed a Gaussian prior on the parameters and did MAP (instead of maximum likelihood) estimation.</p></blockquote><p>The <strong>weight decay parameter</strong> $\lambda$ controls the relative importance of the two terms. Note also the slightly overloaded notation: $J(W,b;x,y)$ is the squared error cost with respect to a single example; $J(W,b)$ is the overall cost function, which includes the weight decay term.</p><p>This cost function above is often used both for <strong>classification</strong> and for <strong>regression</strong> problems. For classification, we let $y=0$ or $1$ represent the two class labels (recall that the sigmoid activation function outputs values in $[0,1]$; if we were using a tanh activation function, we would instead use $-1$ and $+1$ to denote the labels). For regression problems, we first scale our outputs to ensure that they lie in the $[0,1]$ range (or if we were using a <code>tanh</code> activation function, then the $[−1,1]$ range).</p><p>Our goal is to minimize $J(W,b)$ as a function of $W$ and $b$. To train our neural network, we will initialize each parameter $W_{ij}^{(l)}$ and each $b_{i}^{(l)}$ to a small random value near zero (say according to a $\mathfrak{N}(0,\epsilon^{2})$ distribution for some small $\epsilon$, say $0.01$), and then apply an optimization algorithm such as batch gradient descent. Since $J(W,b)$ is a <strong>non-convex function</strong>, gradient descent is susceptible to <strong>local optima</strong>; however, in practice gradient descent usually works fairly well.</p><p>Finally, note that it is important to initialize the parameters randomly, rather than to all 0’s. If all the parameters start off at identical values, then all the hidden layer units will end up learning the same function of the input (more formally, $W_{ij}^{(1)}$ will be the same for all values of $i$, so that $a_{1}^{(2)}=a_{2}^{(2)}=a_{3}^{(2)}=\dots$ for any input $x$). The random initialization serves the purpose of <strong>symmetry breaking</strong>.</p><p>One iteration of gradient descent updates the parameters $W$, $b$ as follows:$$<br>W_{ij}^{(l)}:=W_{ij}^{(l)}-\alpha\frac{\partial J(W,b)}{\partial W_{ij}^{(l)}}\tag{3}<br>$$$$<br>b_{i}^{(l)}:=b_{i}^{(l)}-\alpha\frac{\partial J(W,b)}{\partial b_{i}^{(l)}}\tag{4}<br>$$where $\alpha$ is the <strong>learning rate</strong>. The key step is computing the partial derivatives above. We will now describe the <strong>backpropagation</strong> algorithm, which gives an efficient way to compute these partial derivatives.</p><p>We will first describe how backpropagation can be used to compute $\frac{\partial}{\partial W_{ij}^{(l)}}J(W,b;x,y)$ and $\frac{\partial}{\partial b_{i}^{(l)}}J(W,b;x,y)$, the partial derivatives of the cost function $J(W,b;x,y)$ defined with respect to a single example $(x,y)$. Once we can compute these, then by referring to Equation (2), we see that the derivative of the overall cost function $J(W,b)$ can be computed as$$<br>\frac{\partial J(W,b)}{\partial W_{ij}^{(l)}}=\bigg[\frac{1}{m}\sum_{i=1}^{m}\frac{\partial}{\partial W_{ij}^{(l)}}J(W,b;x^{(i)},y^{(i)})\bigg]+\lambda W_{ij}^{(l)}\tag{5}<br>$$$$<br>\frac{\partial J(W,b)}{\partial b_{i}^{(l)}}=\frac{1}{m}\sum_{i=1}^{m}\frac{\partial}{\partial b_{i}^{(l)}}J(W,b;x^{(i)},y^{(i)})\tag{6}<br>$$The two lines above differ slightly <em>because weight decay is applied to $W$ but not $b$</em>.</p><p>The intuition behind the backpropagation algorithm is as follows. Given a training example $(x,y)$, we will first run a “forward pass” to compute all the activations throughout the network, including the output value of the hypothesis $h_{W,b}(x)$. Then, for each node i in layer l, we would like to compute an “<strong>error term</strong>” $\delta_{i}^{(l)}$ that measures how much that node was “responsible” for any errors in our output. For an output node, we can directly measure the difference between the network’s activation and the true target value, and use that to define $\delta_{i}^{(n_{l})}$ (where layer $n_{l}$ is the output layer). How about hidden units? For those, we will compute $\delta_{i}^{(l)}$ based on a weighted average of the error terms of the nodes that uses $a_{i}^{(l)}$ (the $i$-th activation value of $l$ layer) as an input.</p><blockquote><p>The reason why we need to introduce error term $\delta_{i}^{(l)}$:<br>Using the <strong>chain rule of derivation</strong>, we have$$<br>\frac{\partial J(w,b;x,y)}{\partial W_{ij}^{(l)}}=\frac{\partial J(w,b;x,y)}{\partial z_{i}^{(l+1)}}\frac{\partial z_{i}^{(l+1)}}{\partial W_{ij}^{(l)}}<br>$$$$<br>\frac{\partial J(w,b;x,y)}{\partial b_{i}^{(l)}}=\frac{\partial J(w,b;x,y)}{\partial z_{i}^{(l+1)}}\frac{\partial z_{i}^{(l+1)}}{\partial b_{i}^{(l)}}<br>$$Since $z^{(l+1)}=W^{(l)}a^{(l)}+b^{(l)}$, then$$<br>z_{i}^{(l+1)}=\sum_{k=1}^{s_{l}}W_{ik}^{(l)}a_{k}^{(l)}+b_{i}^{(l)}$$Thus we have$$\frac{\partial z_{i}^{(l+1)}}{\partial W_{ij}^{(l)}}=a_{j}^{(l)},\quad\frac{\partial z_{i}^{(l+1)}}{b_{i}^{(l)}}=1<br>$$If we let $\delta_{i}^{(l)}=\frac{\partial J(w,b;x,y)}{\partial z_{i}^{(l)}}$, then we have$$\frac{\partial J(w,b;x,y)}{\partial W_{ij}^{(l)}}=a_{i}^{(l)}\delta_{i}^{(l+1)}$$$$\frac{\partial J(w,b;x,y)}{\partial b_{i}^{(l)}}=\delta_{i}^{(l+1)}<br>$$Thus, in order to compute $\frac{\partial J(W,b)}{\partial W_{ij}^{(l)}}$ and $\frac{\partial J(W,b)}{\partial b_{i}^{(l)}}$, the key is to compute $\delta_{i}^{(l+1)}$</p></blockquote><p>In detail, here is the backpropagation algorithm:</p><ol><li>Perform a feedforward pass, computing the activations $a_{i}^{(l)}$ for layers $L_{2}$, $L_{3}$, and so on up to the output layer $L_{n_{l}}$.</li><li><p>For each output unit $i$ in layer $n_{l}$ (the output layer), set$$<br>\delta_{i}^{(n_{l})}=\frac{\partial}{\partial z_{i}^{(n_{l})}}\frac{1}{2}\Vert y-h_{W,b}(x)\Vert^{2}=-(y_{i}-a_{i}^{(n_{l})})\centerdot f’(z_{i}^{(n_{l})})\tag{7}$$</p><blockquote><p>Its computation process as follow:$$\begin{aligned}<br>\delta_{i}^{(n_{l})} &amp;=\frac{\partial J(W,b;x,y)}{\partial z_{i}^{(n_{l})}}=\frac{\partial}{\partial z_{i}^{(n_{l})}}\bigg(\frac{1}{2}\Vert y-h_{W,b}(x)\Vert^{2}\bigg)\quad (\textrm{The definition of }J(W,b;x,y))\\<br>&amp;=\frac{1}{2}\frac{\partial}{\partial z_{i}^{(n_{l})}}\bigg(\sum_{j=1}^{s_{n_{l}}}\big(y_{j}-a_{j}^{(n_{l})}\big)^{2}\bigg)\quad (\textrm{The definition of vector norm }\Vert\centerdot\Vert)\\<br>&amp;=\frac{1}{2}\frac{\partial}{\partial z_{i}^{(n_{l})}}\bigg(\sum_{j=1}^{s_{n_{l}}}\big(y_{j}-f(z_{j}^{(n_{l})})\big)^{2}\bigg)\quad (\textrm{Using }a_{j}^{(n_{l})}=f(z_{j}^{(n_{l})}))\\<br>&amp;=-(y_{i}-f(z_{i}^{(n_{l})}))\centerdot f’(z_{i}^{(n_{l})})\quad (\textrm{compute partial derivative of sum term wrt. }z_{i}^{(n_{l})})\\<br>&amp;=-(y_{i}-a_{i}^{(n_{l})})\centerdot f’(z_{i}^{(n_{l})})\quad (\textrm{Using }a_{j}^{(n_{l})}=f(z_{j}^{(n_{l})}))<br>\end{aligned}$$</p></blockquote></li><li><p>For $l=n_{l}-1, n_{l}-2,\dots,2$, each node $i$ in layer $l$, set$$<br>\delta_{i}^{(l)}=\bigg(\sum_{j=1}^{s_{l}+1}W_{ji}^{(l)}\delta_{j}^{(l+1)}\bigg)f’(z_{i}^{(l)})\tag{8}$$</p><blockquote><p>Its computation process as follow:$$\begin{aligned}<br>\delta_{i}^{(n_{l}-1)} &amp;=\frac{\partial J(W,b;x,y)}{\partial z_{i}^{(n_{l}-1)}}=\frac{\partial}{\partial z_{i}^{(n_{l}-1)}}\bigg(\frac{1}{2}\Vert y-h_{W,b}(x)\Vert^{2}\bigg)\quad (\textrm{The definition of }J(W,b;x,y))\\<br>&amp;=\frac{1}{2}\frac{\partial}{\partial z_{i}^{(n_{l}-1)}}\bigg(\sum_{j=1}^{s_{n_{l}}}\big(y_{j}-a_{j}^{(n_{l})}\big)^{2}\bigg)\quad (\textrm{The definition of vector norm }\Vert\centerdot\Vert)\\<br>&amp;=\frac{1}{2}\sum_{j=1}^{s_{n_{l}}}\frac{\partial}{\partial z_{i}^{(n_{l}-1)}}\big(y_{j}-a_{j}^{(n_{l})}\big)^{2}\\<br>&amp;=\frac{1}{2}\sum_{j=1}^{s_{n_{l}}}\frac{\partial}{\partial z_{i}^{(n_{l}-1)}}\big(y_{j}-f(z_{j}^{(n_{l})})\big)^{2}\quad (\textrm{Using }a_{j}^{(n_{l})}=f(z_{j}^{(n_{l})}))\\<br>&amp;=\sum_{j=1}^{s_{n_{l}}}-\big(y_{j}-f(z_{j}^{(n_{l})})\big)\frac{\partial f(z_{j}^{(n_{l})})}{\partial z_{i}^{(n_{l}-1)}}\quad (\textrm{Compute partial derivative})\\<br>&amp;=\sum_{j=1}^{s_{n_{l}}}-\big(y_{j}-f(z_{j}^{(n_{l})})\big)\centerdot f’(z_{j}^{(n_{l})})\centerdot\frac{\partial z_{i}^{(n_{l})}}{\partial z_{i}^{(n_{l}-1)}}\quad (\textrm{Using Chain rule to compute }\frac{\partial f(z_{j}^{(n_{l})})}{\partial z_{i}^{(n_{l}-1)}})\\<br>&amp;=\sum_{j=1}^{s_{n_{l}}}\delta_{j}^{(n_{l})}\centerdot\frac{\partial z_{i}^{(n_{l})}}{\partial z_{i}^{(n_{l}-1)}}\quad (\textrm{Using Equation (7)})\\<br>&amp;=\sum_{j=1}^{s_{n_{l}}}\delta_{j}^{(n_{l})}\centerdot\frac{\partial}{\partial z_{i}^{(n_{l}-1)}}\bigg(\sum_{k=1}^{s_{n_{l}-1}}f(z_{k}^{(n_{l}-1)})\centerdot W_{jk}^{(n_{l}-1)}+b_{j}^{(n_{l}-1)}\bigg)\quad (\textrm{According to the definition of }z_{j}^{(n_{l})})\\<br>&amp;=\sum_{j=1}^{s_{n_{l}}}\delta_{j}^{(n_{l})}\centerdot W_{ji}^{(n_{l}-1)}\centerdot f’(z_{i}^{(n_{l}-1)})\quad (\textrm{Compute partial derivative of }z_{i}^{(n_{l}-1)})\\<br>&amp;=\bigg(\sum_{j=1}^{s_{n_{l}}}W_{ji}^{(n_{l}-1)}\centerdot\delta_{j}^{(n_{l})}\bigg)\centerdot f’(z_{i}^{(n_{l}-1)})<br>\end{aligned}$$By replacing the relationship between $n_{l}-1$ and $n_{l}$ to the relationship between $l$ and $l+1$, we can derive the Equation (8). And the above derivation process from backward to forward is the essence of “Backpropagation”.</p></blockquote></li><li><p>Compute the desired partial derivatives, which are given as:$$\begin{aligned}<br>\frac{\partial}{\partial W_{ij}^{(l)}}J(W,b;x,y) &amp;=a_{j}^{(l)}\delta_{i}^{(l+1)}\\<br>\frac{\partial}{\partial b_{i}^{(l)}}J(W,b;x,y) &amp;=\delta_{i}^{(l+1)}<br>\end{aligned}$$</p></li></ol><p>Below gives a graph to illustrate the above steps, we use a sample $(x,y)$ as an example.<br><img src="/images/deeplearning/backprop/2.png" alt="2.png"><br>Finally, we can also rewrite the algorithm using matrix-vectorial notation. We will use “$\bullet$” to denote the element-wise product operator (also called the Hadamard product), so that if $a=b\bullet c$, then $a_{i}=b_{i}c_{i}$. Similar to how we extended the definition of $f(\centerdot)$ to apply element-wise to vectors, we also do the same for $f’(\centerdot)$ (so that $f’([z_{1},z_{2},z_{3}])=\big[\frac{\partial f(z_{1})}{\partial z_{1}},\frac{\partial f(z_{2})}{\partial z_{2}},\frac{\partial f(z_{3})}{\partial z_{3}}\big]$). The algorithm can then be written:</p><ol><li>Perform a feedforward pass, computing the activations for layers $L_{2}$, $L_{3}$, up to the output layer $L_{n_{l}}$, using Equations$$\begin{aligned}<br>z^{(l+1)} &amp;=W^{(l)}a^{(l)}+b^{(l)}\\<br>a^{(l+1)} &amp;=f(z^{(l+1)})<br>\end{aligned}$$</li><li>For the output layer (layer $n_{l}$), set$$<br>\delta^{(n_{l})}=-(y-a^{(n_{l})})\bullet f’(z^{(n)})<br>$$</li><li>For $l=n_{l}-1, n_{l}-2,\dots,2$, set$$<br>\delta^{(l)}=\big((W^{(l)})^{T}\delta^{(l+1)}\big)\bullet f’(z^{(l)})<br>$$</li><li>Compute the desired partial derivatives:$$\begin{aligned}<br>\nabla_{W^{(l)}}J(W,b;x,y) &amp;=\delta^{(l+1)}\big(a^{(l)}\big)^{T}\\<br>\nabla_{b^{(l)}}J(W,b;x,y) &amp;=\delta^{(l+1)}<br>\end{aligned}$$</li></ol><p><strong>Implementation note</strong>: In steps 2 and 3 above, we need to compute $f’(z^{(l)})$ for each value of $i$. Assuming $f(z)$ is the <code>sigmoid</code> activation function, we would already have $a_{i}^{(l)}$ stored away from the forward pass through the network. Thus, using the expression that we worked out earlier for $f’(z)$, we can compute this as$$<br>f’(z^{(l)})=a_{i}^{(l)}(1-a_{i}^{(l)})<br>$$Finally, we are ready to describe the full <strong>gradient descent algorithm</strong>. In the pseudo-code below, $\Delta W^{(l)}$ is a matrix (of the same dimension as $W^{(l)}$), and $\Delta b^{(l)}$ is a vector (of the same dimension as $b^{(l)}$). Note that in this notation, “$\Delta W^{(l)}$” is a matrix, and in particular it isn’t “$\Delta$ times $W^{(l)}$”. We implement one iteration of batch gradient descent as follows:</p><ol><li>Set $\Delta W^{(l)}:=0$, $\Delta b^{(l)}:=0$ (matrix/vector of zeros) for all $l$.</li><li>For $i=1$ to $m$:<br>2a. Use backpropagation to compute $\nabla_{W^{(l)}}J(W,b;x,y)$ and $\nabla_{b^{(l)}}J(W,b;x,y)$.<br>2b. Set $\Delta W^{(l)}:=\Delta W^{(l)}+\nabla_{W^{(l)}}J(W,b;x,y)$<br>2c. Set $\Delta b^{(l)}:=\Delta b^{(l)}+\nabla_{b^{(l)}}J(W,b;x,y)$</li><li>Update the parameters$$\begin{aligned}<br>W^{(l)} &amp;:= W^{(l)}-\alpha\bigg[\big(\frac{1}{m}\Delta W^{(l)}\big)+\lambda W^{(l)}\bigg]\\<br>b^{(l)} &amp;:= b^{(l)}-\alpha\bigg[\frac{1}{m}\Delta b^{(l)}\bigg]<br>\end{aligned}$$</li></ol><p>To train our neural network, we can now repeatedly take steps of gradient descent to reduce our cost function $J(W,b)$.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;The article is excerpted from Andrew Ng’s &lt;a href=&quot;https://web.stanford.edu/class/cs294a/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;CS294A Lecture notes&lt;/a&gt;: &lt;a href=&quot;https://web.stanford.edu/class/cs294a/sparseAutoencoder_2011new.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Sparse Autoencoder&lt;/a&gt;, then I add some personal understanding.
    
    </summary>
    
      <category term="Machine Learning" scheme="https://isaacchanghau.github.io/categories/Machine-Learning/"/>
    
    
      <category term="deep learning" scheme="https://isaacchanghau.github.io/tags/deep-learning/"/>
    
      <category term="machine learning" scheme="https://isaacchanghau.github.io/tags/machine-learning/"/>
    
      <category term="mathematics" scheme="https://isaacchanghau.github.io/tags/mathematics/"/>
    
      <category term="backpropagation" scheme="https://isaacchanghau.github.io/tags/backpropagation/"/>
    
  </entry>
  
  <entry>
    <title>Seq2Seq中的Beam Search算法过程 [转载]</title>
    <link href="https://isaacchanghau.github.io/2017/08/10/Seq2Seq%E4%B8%AD%E7%9A%84Beam-Search%E7%AE%97%E6%B3%95%E8%BF%87%E7%A8%8B/"/>
    <id>https://isaacchanghau.github.io/2017/08/10/Seq2Seq中的Beam-Search算法过程/</id>
    <published>2017-08-10T10:29:30.000Z</published>
    <updated>2017-09-07T15:54:16.000Z</updated>
    
    <content type="html"><![CDATA[<p>本文转载自知乎专栏·<a href="https://zhuanlan.zhihu.com/qinlibo-ml" target="_blank" rel="noopener">机器学习算法与自然语言处理</a>的<a href="https://zhuanlan.zhihu.com/p/28048246" target="_blank" rel="noopener">seq2seq中的beam search算法过程</a>。<a id="more"></a></p><p>在 Sequence2Sequence 模型中，beam search 的方法只用在测试的情况，因为在训练过程中，每一个 decoder 的输出是有正确答案的，也就不需要 beam search 去加大输出的准确率。假设现在我们用机器翻译作为例子来说明，我们需要翻译：</p><blockquote><p>“我是中国人” —&gt; “I am Chinese”</p></blockquote><p>假设我们的词表大小只有三个单词就是 I am Chinese。那么如果我们的 beam size 为 2 的话，我们现在来解释。</p><p>如下图所示，我们在 decoder 的过程中，有了beam search 方法后，在第一次的输出，我们选取概率最大的 “I” 和 “am” 两个单词，而不是只挑选一个概率最大的单词。<br><img src="/images/deeplearning/beam-search/1.jpg" alt="1.jpg"><br>然后接下来我们要做的就是，把 “I” 单词作为下一个 decoder 的输入算一遍得到 $y_{2}$ 的输出概率分布，把 “am” 单词作为下一个 decoder 的输入算一遍也得到 $y_{2}$ 的输出概率分布。</p><p>比如将 “I” 单词作为下一个 decoder 的输入算一遍得到 $y_{2}$ 的输出概率分布如下：<br><img src="/images/deeplearning/beam-search/2.jpg" alt="2.jpg"><br>比如将 “am” 单词作为下一个 decoder 的输入算一遍得到 $y_{2}$ 的输出概率分布如下：<br><img src="/images/deeplearning/beam-search/3.jpg" alt="3.jpg"><br>那么此时我们由于我们的 beam size 为 2，也就是我们只能保留概率最大的两个序列，此时我们可以计算所有的序列概率：$$\begin{aligned}<br>&amp; “I\quad I”=0.4\times 0.3=0.12,\qquad &amp; “I\quad am”=0.4\times 0.6=0.24,\\<br>&amp; “I\quad Chinese”=0.4\times 0.1=0.04,\qquad &amp; “am\quad I”=0.5\times 0.3=0.15,\\<br>&amp; “am\quad am”=0.5\times 0.3=0.15,\qquad &amp; “am\quad Chinese”=0.5\times 0.4=0.20<br>\end{aligned}$$我们很容易得出俩个最大概率的序列为 “I am” 和 “am Chinese”，然后后面会不断重复这个过程，直到遇到结束符为止。</p><p>最终输出 2 个得分最高的序列。这就是seq2seq中的 beam search 算法过程，但是可能有些同学有一个疑问，就是但 $i-1$ 时刻选择的单词不同的时候，下一时刻的输出概率分布为什么会改变？</p><p>这是由于解码的过程中，第i时刻的模型的输入，包括了第 $i-1$ 时刻模型的输出，那么很自然在第 $i-1$ 时刻模型的输出不同的时候，就会导致下一时刻模型的输出概率分布会不同，因为第 $i-1$ 时刻的输出作为参数影响了后一时刻模型的学习。</p><p>如下图用了一个 slides 的法语翻译为英文的例子，可以更容易理解上面的解释。<br><img src="/images/deeplearning/beam-search/4.jpg" alt="4.jpg"><br>参考：<a href="https://www.zhihu.com/question/54356960" target="_blank" rel="noopener">谁能解释下seq2seq中的beam search算法过程?</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文转载自知乎专栏·&lt;a href=&quot;https://zhuanlan.zhihu.com/qinlibo-ml&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;机器学习算法与自然语言处理&lt;/a&gt;的&lt;a href=&quot;https://zhuanlan.zhihu.com/p/28048246&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;seq2seq中的beam search算法过程&lt;/a&gt;。
    
    </summary>
    
      <category term="Natural Language Processing" scheme="https://isaacchanghau.github.io/categories/Natural-Language-Processing/"/>
    
    
      <category term="deep learning" scheme="https://isaacchanghau.github.io/tags/deep-learning/"/>
    
      <category term="natural language processing" scheme="https://isaacchanghau.github.io/tags/natural-language-processing/"/>
    
      <category term="seq2seq" scheme="https://isaacchanghau.github.io/tags/seq2seq/"/>
    
      <category term="beam search" scheme="https://isaacchanghau.github.io/tags/beam-search/"/>
    
  </entry>
  
  <entry>
    <title>机器学习(周志华)--学习笔记(四) 集成学习(Ensemble Learning)</title>
    <link href="https://isaacchanghau.github.io/2017/08/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%91%A8%E5%BF%97%E5%8D%8E-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-4/"/>
    <id>https://isaacchanghau.github.io/2017/08/08/机器学习-周志华-学习笔记-4/</id>
    <published>2017-08-08T07:18:38.000Z</published>
    <updated>2017-09-21T09:04:33.000Z</updated>
    
    <content type="html"><![CDATA[<p>第八章-集成学习 (Ensemble Learning)<a id="more"></a></p><h1 id="个体与集成"><a href="#个体与集成" class="headerlink" title="个体与集成"></a>个体与集成</h1><p>集成学习 (ensemble learning) 通过构建并结合多个学习器来完成学习任务，也被称为多分类器系统 (multi-classifier system)、基于委员会的学习 (committee-based learning) 等。 下图展示了集成学习的一般结构：先产生一组“个体学习器” (individual learner)，再用某种策略将它们结合。若集成中只包含同种类型的个体学习器，这样的集成是“同质的” (homogeneous)，同质集成中的个体学习器称为“基学习器” (base learner)，相应的学习算法称为“基学习算法” (base learning algorithm)。集成也可以包含不同类型的个体学习器，即“异质的” (heterogenous)。异质集成中的个体学习器由不同的学习算法生成，此时不再有基学习算法，而个体学习器也常称为“组件学习器” (component learner)。<br><img src="/images/machinelearning/ensemble/1.png" alt="1.png"><br>集成学习通过将多个学习器进行结合，通常可以获得比单一学习器显著优越的泛化性能。这对“弱学习器” (weak learner) 尤为明显。实际中，要获得好的集成，个体学习器通常应该“好二不同”，即个体学习器要有一定的“准确性”，并且要有“多样性”，即学习器间具有差异。如下图所示<br><img src="/images/machinelearning/ensemble/2.png" alt="2.png"><br>举个例子，考虑二分类问题 $y\in\{-1,+1\}$ 和真实函数 $\boldsymbol{f}$，假设基分类器的错误率为 $\epsilon$，即对每个基分类器 $h_{i}$ 有$$<br>P(h_{i}(\boldsymbol{x})\neq\boldsymbol{f}(\boldsymbol{x}))=\epsilon\tag{1}<br>$$假设集成通过简单投票法结合 $T$ 个基分类器，若有超过半数的基分类器正确，则集成分类就正确：$$<br>H(\boldsymbol{x})=sign\bigg(\sum_{i=1}^{T}h_{i}(\boldsymbol{x})\bigg)\tag{2}<br>$$假设基分类器的错误率相互独立，则由 <a href="https://en.wikipedia.org/wiki/Hoeffding%27s_inequality" target="_blank" rel="noopener">Hoeffding</a> 不等式可知，集成的错误率为$$<br>P(H(\boldsymbol{x})\neq\boldsymbol{f}(\boldsymbol{x}))=\sum_{k=0}^{\lfloor T/2\rfloor}{T\choose k}(1-\epsilon)^{k}\epsilon^{T-k}\leq\exp\big(-\frac{1}{2}T(1-2\epsilon)^{2}\big)\tag{3}<br>$$由上式可得，随着个体集成中个体分类器数目 $T$ 的增大，集成的错误率将指数级下降，最终趋向于零。上面的分析有一个关键假设：<strong><em>基学习器的误差相互独立</em></strong>。实际上，个体学习器是为解决同一个问题训练出来的，显然不能相互独立。而“准确性”和“多样性”本身就存在冲突，当准确性很高之后，增加多样性就需要牺牲准确性。<br>根据个体学习器的生成方式，集成学习大致分为两类：</p><ul><li>个体学习器间存在强依赖关系、必须串行生成的序列化方法，如 Boosting。</li><li>个体学习器不存在强依赖关系、可同时生成的并行化方法，如 Bagging 和“随即森林” (Random Forest)。</li></ul><h1 id="Boosting"><a href="#Boosting" class="headerlink" title="Boosting"></a>Boosting</h1><p>Boosting 是一族可将弱学习器提升为强学习器的算法，其工作机制类似：先从初始训练集训练出一个基学习器，再根据基学习器的表现对训练样本进行调整，使得先前基学习器做错的训练样本在后续受到更多的关注，然后基于调整后的样本训练下一个基学习器，如此重复进行，直至基学习器达到事先指定的值 $T$，最终将这 $T$ 个基学习器进行加权结合。如 AdaBoost 算法，</p><blockquote><p>AdaBoost 算法<br>输入：训练集 $D=\{(\boldsymbol{x}_{1},y_{1}),\dots,(\boldsymbol{x}_{m},y_{m})\}$，$y\in\{-1,+1\}$；基学习算法 $\mathfrak{L}$；训练轮数 $T$。<br>过程：<br>$\qquad$$\mathcal{D}_{1}(\boldsymbol{x})=\frac{1}{m}$<br>$\qquad$<strong>for</strong> $t=1,2,\dots,T$ <strong>do</strong><br>$\qquad$$\qquad$$h_{t}=\mathfrak{L}(D,\mathcal{D}_{t})$<br>$\qquad$$\qquad$$\epsilon_{t}=P_{\boldsymbol{x}\sim\mathcal{D}_{t}}\big(h_{t}(\boldsymbol{x})\neq\boldsymbol{f}(\boldsymbol{x})\big)$<br>$\qquad$$\qquad$<strong>if</strong> $\epsilon_{t}&gt;0.5$ <strong>then break</strong>$\qquad$ (检测是否优于随机猜测)<br>$\qquad$$\qquad$$\alpha_{t}=\frac{1}{2}\ln\big(\frac{1-\epsilon_{t}}{\epsilon_{t}}\big)$<br>$\qquad$$\qquad$$\mathcal{D}_{t+1}(\boldsymbol{x})=\frac{\mathcal{D}_{t}(\boldsymbol{x})}{Z_{t}}\times\begin{cases}\exp(-\alpha_{t}), &amp; h_{t}(\boldsymbol{x})=\boldsymbol{f}(\boldsymbol{x})\\\exp(\alpha_{t}), &amp; h_{t}(\boldsymbol{x})\neq\boldsymbol{f}(\boldsymbol{x})\end{cases}=\frac{\mathcal{D}_{t}(\boldsymbol{x})\exp\big(-\alpha_{t}\boldsymbol{f}(\boldsymbol{x})h_{t}(\boldsymbol{x})\big)}{Z_{t}}$ ($Z_{t}$是规范化因子，确保$\mathcal{D}_{t+1}$是一个分布)<br>$\qquad$<strong>end for</strong><br>输出：$H(\boldsymbol{x})=sign\big(\sum_{t=1}^{T}\alpha_{t}h_{t}(\boldsymbol{x})\big)$</p></blockquote><p>AdaBoost 可以理解为一个“加性模型” (additive model)，即基学习器的线性组合$$<br>H(\boldsymbol{x})=\sum_{t=1}^{T}\alpha_{t}h_{t}(\boldsymbol{x})\tag{4}<br>$$来最小化指数损失函数 (exponential loss function)$$<br>\ell_{\exp}(H|\mathcal{D})=\mathbb{E}_{\boldsymbol{x}\sim\mathcal{D}}\big[e^{-\boldsymbol{f}(\boldsymbol{x})H(\boldsymbol{x})}\big]\tag{5}<br>$$若 $H(\boldsymbol{x})$ 能令指数损失函数最小化，则考虑式(5)对 $H(\boldsymbol{x})$ 的偏导，并令导数为零，可得$$<br>H(\boldsymbol{x})=\frac{1}{2}\ln\frac{P\big(\boldsymbol{f}(\boldsymbol{x})=1|\boldsymbol{x}\big)}{P\big(\boldsymbol{f}(\boldsymbol{x})=-1|\boldsymbol{x}\big)}\tag{6}<br>$$因此有$$<br>sign\big(H(\boldsymbol{x})\big)<br>=sign\bigg(\frac{1}{2}\ln\frac{P\big(\boldsymbol{f}(\boldsymbol{x})=1|\boldsymbol{x}\big)}{P\big(\boldsymbol{f}(\boldsymbol{x})=-1|\boldsymbol{x}\big)}\bigg)<br>=\dots<br>=\arg\max_{y\in\{-1,1\}}P\big(\boldsymbol{f}(\boldsymbol{x})=y|\boldsymbol{x}\big)\tag{7}<br>$$这意味着 $sign\big(H(\boldsymbol{x})\big)$ 达到零贝叶斯最优错误率。换言之，若指数损失函数最小化，则分类错误率也将最小化；说明指数损失函数式分类任务原本 $0/1$ 损失函数的一致 (consistent) 替代损失函数。<br>在AdaBoost算法中，第一个基分类器 $h_{1}$ 是通过直接将基学习算法用于初始数据分布而得；此后迭代生成 $h_{t}$ 和 $\alpha_{t}$，当基分类器 $h_{t}$ 基于分布 $\mathcal{D}_{t}$ 产生后，该基分类器的权重 $\alpha_{t}$应使得 $\alpha_{t}h_{t}$ 最小化指数损失函数$$<br>\ell_{\exp}(\alpha_{t}h_{t}|\mathcal{D}_{t})=\mathbb{E}_{\boldsymbol{x}\sim\mathcal{D}_{t}}\big[e^{-f(\boldsymbol{x})\alpha_{t}h_{t}(\boldsymbol{x})}\big]=\dots=e^{-\alpha_{t}}(1-\epsilon_{t})+e^{\alpha_{t}}\epsilon_{t}\tag{8}<br>$$其中 $\epsilon_{t}=P_{\boldsymbol{x}\sim\mathcal{D}_{t}}\big(h_{t}(\boldsymbol{x})\neq f(\boldsymbol{x})\big)$。考虑指数损失函数关于 $\alpha_{t}$ 的导数并令导数为零可得$$<br>\alpha_{t}=\frac{1}{2}\ln\bigg(\frac{1-\epsilon_{t}}{\epsilon_{t}}\bigg)\tag{9}<br>$$这就是分类器权重更新公式，而 AdaBoost 算法在获得 $H_{t-1}$ 之后样本分布将进行调整，使下一轮的基学习器 $h_{t}$ 能纠正 $H_{t-1}$ 的一些错误，即最小化 $\ell_{\exp}(H_{t-1}+h_{t}|\mathcal{D})$。通过泰勒展开，数学期望定义等一系列变化和相关关系 (具体请参照书中推导) 可得到理想基学习器$$<br>h_{t}(\boldsymbol{x})=\arg\min_{h}\mathbb{E}_{\boldsymbol{x}\sim\mathcal{D}_{t}}\big[\Pi(f(\boldsymbol{x})\neq h(\boldsymbol{x}))\big]\tag{10}<br>$$由此可见，理想的 $h_{t}$ 将在分布 $\mathcal{D}_{t}$ 下最小化分类误差。因此，弱分类器将基于分布 $\mathcal{D}_{t}$ 来训练，且针对 $\mathcal{D}_{t}$ 的分类误差应小于0.5，这在一定程度上类似“残差逼近”的思想。考虑到 $\mathcal{D}_{t}$ 和 $\mathcal{D}_{t+1}$ 的关系，有$$<br>\mathcal{D}_{t+1}(\boldsymbol{x})=\mathcal{D}_{t}(\boldsymbol{x})\centerdot e^{-f(\boldsymbol{x})\alpha_{t}h_{t}(\boldsymbol{x})}\frac{\mathbb{E}_{\boldsymbol{x}\sim\mathcal{D}}\big[e^{-f(\boldsymbol{x})H_{t-1}(\boldsymbol{x})}\big]}{\mathbb{E}_{\boldsymbol{x}\sim\mathcal{D}}\big[e^{-f(\boldsymbol{x})H_{t}(\boldsymbol{x})}\big]}\tag{11}<br>$$这便是样本分布更新公式。以上是从基于加性模型迭代式优化指数损失函数的角度推导出了AdaBoost算法。<br>Boosting算法要求基学习器能对特定的数据分布进行学习，这可通过“重赋权法” (re-weighting) 实施，即在训练过程的每一轮中，根据样本分布为每一个训练样本重新赋予一个权重。对无法接受带权样本的基学习算法，则可通过“重采样法” (re-sampling) 来处理，即每一轮学习中，根据样本分布对训练数据重新进行采样，在用重采样而得的样本集对基学习器进行训练。两种方法没有显著优劣差别。但是，若采用重采样法，则面对学习过程停止问题 (流程图中检测是否优于随机猜测，否抛弃当前基学习器，学习过程停止)，可获得“重启动”机会以避免训练过程过早停止。<br>从偏差-方差分解角度看，Boosting主要关注降低偏差，因此Boosting能基于泛化性能相当弱的学习器构建很强的集成。</p><h1 id="Bagging与随机森林"><a href="#Bagging与随机森林" class="headerlink" title="Bagging与随机森林"></a>Bagging与随机森林</h1><p>欲得到泛化能力强的集成，集成中的个体学习器应尽可能相互独立，，虽然在实际中无法做到，但可以设法使基学习器尽可能具有较大的差异。给定一个数据集，一种方法是对样本进行采样，产生若干不同的子集，再从每个子集中训练出一个基学习器。为了避免因为采样导致每个基学习器训练数据不足，常采用相互有交叠的采样子集。</p><h2 id="Bagging"><a href="#Bagging" class="headerlink" title="Bagging"></a>Bagging</h2><p>Bagging 是并行式集成学习的代表，它直接基于自助采样法 (<a href="https://en.wikipedia.org/wiki/Bootstrapping_(statistics" target="_blank" rel="noopener">bootstrap sampling</a>))。给定包含 $m$ 个样本的数据集，先随机取出一个样本放入采样集，再把该样本放回初始数据集，使得下次采样时该样本仍有可能被选中，经过 $m$ 次随机采样，得到含 $m$ 个样本的采样集，初始训练集中有的样本在采样集多次出现，有的则从未出现。极限状态下 ($m\mapsto\infty$)，初始训练集中约有 $63.2%$ 的样本出现在采样集中。这样可采样出 $T$ 个含 $m$ 个训练样本的采样集，然后基于每个采样集训练出一个基学习器，再将这些基学习器结合。这就是 Bagging 的基本流程。在对预测输出进行结合时，Bagging 通常对分类任务使用简单投票法，对回归任务使用简单平均法。若分类预测时出现两个类收到同样票数的情形，则最简单的方法是随机选择一个，也可进一步考察学习器投票的置信度来确定最终胜者。Bagging 算法流程如下：</p><blockquote><p>输入：训练集 $D=\{(\boldsymbol{x}_{1},y_{1}),\dots,(\boldsymbol{x}_{m},y_{m})\}$，$y\in\{-1,+1\}$；基学习算法 $\mathfrak{L}$；训练轮数 $T$。<br>过程：<br>$\qquad$<strong>for</strong> $t=1,2,\dots,T$ <strong>do</strong><br>$\qquad$$\qquad$$h_{t}=\mathfrak{L}(D,\mathcal{D}_{bs})$<br>$\qquad$<strong>end for</strong><br>输出：$H(\boldsymbol{x})=\arg\max_{y\in\mathcal{Y}}\sum_{t=1}^{T}\Pi\big(h_{t}(\boldsymbol{x})=y\big)$</p></blockquote><p>假设基学习器的计算复杂度为 $\mathcal{O}(m)$，则 Bagging 的复杂度大致为 $T\big(\mathcal{O}(m)+\mathcal{O}(s)\big)$，考虑到采样与投票/平均过程的复杂度 $\mathcal{O}(s)$ 很小，而 $T$ 通常是一个不太大的常数，因此训练一个 Bagging 集成与直接使用基学习器训练一个学习器的复杂度同阶。此外，与标准 AdaBoost 只适用于二分类任务不同，Bagging能不经修改的用于多分类、回归任务。而自助采样过程还给 Bagging 带来了另一个优点：由于每个基学习器只使用了初始训练集中约 $63.2%$ 的样本，剩下的样本可用作验证集来对泛化性能进行“包外估计” (<a href="https://en.wikipedia.org/wiki/Out-of-bag_error" target="_blank" rel="noopener">out-of-bag estimate</a>)。<br>令 $D_{t}$ 表示 $h_{t}$ 实际使用的训练集，令 $H^{oob}(\boldsymbol{x})$ 表示对样本 $\boldsymbol{x}$ 的包外预测，即仅考虑那些未使用 $\boldsymbol{x}$ 训练的基学习器在 $\boldsymbol{x}$ 上的预测，有$$<br>H^{oob}(\boldsymbol{x})=\arg\max_{y\in\mathcal{Y}}\sum_{t=1}^{T}\Pi\big(h_{t}(\boldsymbol{x})=y\big)\centerdot\Pi(\boldsymbol{x}\notin D_{t})<br>$$则 Bagging 泛化误差的包外估计为$$<br>\epsilon^{oob}=\frac{1}{\vert D\vert}\sum_{(\boldsymbol{x},y)\in D}\Pi\big(H^{oob}(\boldsymbol{x})\neq y\big)<br>$$当基学习器是决策树时，包外样本还可以用于辅助剪枝，或用于估计决策树中各结点的后验概率以辅助对零训练样本结点的处理；当基学习器是神经网络时，可使用包外样本辅助早期停止以减小过拟合风险。从偏差-方差分解角度看， Bagging 主要关注<strong><em>降低方差</em></strong>，因此它在不剪枝决策树、神经网络等易受样本扰动的学习器中效用更为明显。</p><h2 id="随机森林"><a href="#随机森林" class="headerlink" title="随机森林"></a>随机森林</h2><p>随机森林 (Random Forset，RF) 是 Bagging 的一个扩展。RF 在以决策树为基学习器构建 Bagging 集成的基础上，在决策树的训练过程中引入随机属性选择。传统决策树在选择划分属性时是在当前节点的属性集合 (假定有 $d$ 个属性) 中选择一个最优属性；而在 RF 中，对基决策树的每个结点，先从该结点的属性集合中随机选择一个包含 $k$ 个属性的子集，然后再从这个子集中选择一个最优属性用于划分。参数 $k$ 控制了随机性的引入程度：</p><ul><li>若令 $k=d$，则基决策树的构建与传统决策树相同；</li><li>若令 $k=1$，则是随机选择一个属性划分；</li><li>一般情况下，推荐 $k=\log_{2}d$。</li></ul><p>与 Bagging 中基学习器的“多样性”仅通过样本扰动而来不同，随机森林中基学习器的多样性不仅来自样本扰动，还来自属性扰动，这使得最终集成的泛化性能可通过个体学习器之间异度的增加而进一步提升。此外，随机森林的收敛性与 Bagging 相似，随机森林的起始性能往往相对较差，特别是在集成中只包含一个基学习器时，因为通过引入属性扰动，随机森林中个体学习器的性能往往有所降低。然而，随着个体学习器数目的增加，随机森林通常会收敛到更低的泛化误差。值得一提的是，随机森林的训练效率常优于 Bagging，因为在个体决策树的构建过程中，Bagging 使用的是“确定型“决策树，在选择划分属性时要对结点的所有属性进行考察，而随机森林shiyongde”随机型“决策树则只需考察一个属性子集。</p><h1 id="结合策略"><a href="#结合策略" class="headerlink" title="结合策略"></a>结合策略</h1><p>学习器结合可能会从三个方面带来好处：</p><ul><li>从统计方面，由于学习任务的假设空间往往很大，可能有多个假设在训练集上达到同等性能，此时若使用单学习器可能因误选而导致泛化性能不佳，结合多个学习器则会减小这一风险；</li><li>从计算方面，学习算法往往会陷入局部极小，有的局部极小点对应的泛化性能可能很糟糕，而通过多次运行之后进行结合，可降低陷入糟糕局部极小点的风险；</li><li>从表示方面，某些学习任务的真实假设可能不在当前学习算法所考虑的假设空间中，此时若使用单学习器则肯定无效，而通过结合多个学习器，由于假设空间有所扩大，有可能学得更好的近似。</li></ul><p><img src="/images/machinelearning/ensemble/3.png" alt="3.png"><br>假定集成包含 $T$ 个基学习器 $\{h_{1},h_{2},\dots,h_{T}\}$，其中 $h_{i}$ 在示例 $\boldsymbol{x}$ 上的输出为 $h_{i}(\boldsymbol{x})$。</p><h2 id="平均法"><a href="#平均法" class="headerlink" title="平均法"></a>平均法</h2><ul><li><strong><em>简单平均法</em></strong> (simple averaging)：$$H(\boldsymbol{x})=\frac{1}{T}\sum_{i=1}^{T}h_{i}(\boldsymbol{x})$$</li><li><strong><em>加权平均法</em></strong> (weighted averaging)：$$H(\boldsymbol{x})=\sum_{i=1}^{T}w_{i}h_{i}(\boldsymbol{x})$$其中 $w_{i}$ 是个体学习器 $h_{i}$ 的权重，通常要求 $w_{i}\geq 0$，$\sum_{i=1}^{T}w_{i}=1$。</li></ul><p>加权平均法的权重一般从训练数据中学习而得，但由于样本不充分或噪声，通常学习的权重并不完全可靠，对于规模较大的集成，由于权重较多，甚至可能导致过拟合。一般而言，<strong><em>在个体学习器性能相差较大时宜使用加权平均法，而在个体学习器性能相近时宜使用简单平均法。</em></strong></p><h2 id="投票法"><a href="#投票法" class="headerlink" title="投票法"></a>投票法</h2><p>对分类任务，学习器 $h_{i}$ 将从类别标记集合 $\{c_{1},\dots,c_{N}\}$ 中预测出一个标记。将 $h_{i}$ 的预测输出表示为一个 $N$ 维向量 $\big(h_{i}^{1}(\boldsymbol{x}),\dots,h_{i}^{N}(\boldsymbol{x})\big)$，其中 $h_{i}^{j}(\boldsymbol{x})$ 表示 $h_{i}$ 在类别标记 $c_{j}$ 上的输出。</p><ul><li><strong><em>绝对多数投票法</em></strong> (majority voting)：$$H(\boldsymbol{x})=\begin{cases}<br>c_{j}, &amp; \sum_{i=1}^{T}h_{i}^{j}(\boldsymbol{x})&gt;0.5\sum_{k=1}^{N}\sum_{i=1}^{T}h_{i}^{k}(\boldsymbol{x});\\<br>reject, &amp; otherwise.<br>\end{cases}$$即若某标记得票过半数，则预测为该标记；否则拒绝预测。</li><li><strong><em>相对多数投票法</em></strong> (plurality voting)：$$H(\boldsymbol{x})=c_{\arg\max_{j}\sum_{i=1}^{T}h_{i}^{j}(\boldsymbol{x})}$$即预测为的票最多的标记，若同时有多个标记获最高票，则从中随机选择一个。</li><li><strong><em>加权投票法</em></strong> (weighted voting)：$$H(\boldsymbol{x})=c_{\arg\max_{j}\sum_{i=1}^{T}w_{i}h_{i}^{j}(\boldsymbol{x})}$$与加权平均法类似，$w_{i}$ 是 $h_{i}$ 的权重，通常 $w_{i}\geq 0$，$\sum_{i=1}^{T}w_{i}=1$。</li></ul><p>以上没有限制个体学习器输出值的类型，实际中，不同类型的个体学习器可能产生不同类型的输出值，常见的有：</p><ul><li>类标记：$h_{i}^{j}(\boldsymbol{x})\in\{0,1\}$，若 $h_{i}$ 将样本 $\boldsymbol{x}$ 预测为类别 $c_{j}$ 则取值为1，否则为0.使用类标记的投票亦称“硬投票” (hard voting)。</li><li>类概率：$h_{i}^{j}(\boldsymbol{x})\in[0,1]$，相当于对后验概率 $P(c_{j}|\boldsymbol{x})$ 的一个估计。使用概率的投票亦称“软投票” (soft voting)。</li></ul><p>注意：不同类型的 $h_{i}^{j}(\boldsymbol{x})$ 值不能混用。</p><h2 id="学习法"><a href="#学习法" class="headerlink" title="学习法"></a>学习法</h2><p>当训练数据很多时，一种更为强大的结合策略是使用“学习法”，即通过另一个学习器来进行结合。Stacking是学习法的典型代表，这里称个体学习器为初级学习器，用于结合的学习器称为次级学习器或元学习器 (meta-learner)。<br>Stacking先从初始数据集训练出初级学习器，然后生成一个新数据集用于训练次级学习器。在这个新数据集中，初级学习器的输出被当作样例输入特征，而初始样本的标记仍被当作样例标记。Stacking算法描述如下，这里假定初级学习器使用不同学习算法产生，即初级集成是异质的。</p><blockquote><p>输入：训练集 $D=\{(\boldsymbol{x}_{1},y_{1}),\dots,(\boldsymbol{x}_{m},y_{m})\}$，初级学习算法 $\mathfrak{L}_{1},\mathfrak{L}_{2},\dots,\mathfrak{L}_{T}$，次级学习算法 $\mathfrak{L}$。<br>过程：<br>$\qquad$<strong>for</strong> $t=1,2,\dots,T$ <strong>do</strong><br>$\qquad\qquad h_{t}=\mathfrak{L}_{t}(D)$<br>$\qquad$<strong>end for</strong><br>$\qquad D’=\emptyset$<br>$\qquad$<strong>for</strong> $i=1,2,\dots,m$ <strong>do</strong><br>$\qquad\qquad$<strong>for</strong> $t=1,2,\dots,T$ <strong>do</strong><br>$\qquad\qquad\qquad z_{it}=h_{t}(\boldsymbol{x}_{i})$<br>$\qquad\qquad$<strong>end for</strong><br>$\qquad\qquad D’=D’\bigcup\big((z_{i1},\dots,z_{iT}),y_{i}\big)$<br>$\qquad$<strong>end for</strong><br>$\qquad h’=\mathfrak{L}(D’)$<br>输出：$H(\boldsymbol{x})=h’\big(h_{1}(\boldsymbol{x}),\dots,h_{T}(\boldsymbol{x})\big)$</p></blockquote><p>在训练阶段，次级训练集释利用初级学习器产生的，若直接用初级学习器的训练集来产生次级训练集，则过拟合风险比较大。因此一般通过交叉验证或留一法这样的方式，用训练初级学习器未使用的样本来产生次级学习器的训练样本。次级学习器的输入属性表示和次级学习算法对Stacking集成的泛化性能优很大影响。将初级学习器的输出类概率作为次级学习器的输入属性，用多响应线性回归 (Multi-response Linear Regression，MLR) 作为次级学习算法效果较好，在MLR中使用不同的属性集效果更佳。<br>此外，贝叶斯模型平均 (Bayes Model Averaging，BMA) 基于后验概率来为不同模型赋予权重，可视为加权平均法的一种特殊实现，理论上来说，若数据生成模型恰在当前考虑的模型中，切数据噪声很少，则BMA不差于Stacking，然而实际中很难确保这一要求。因此 Stacking 通常优于 BMA，因为其鲁棒性比 BMA好，且 BMA 对模型近似误差非常敏感。</p><h1 id="多样性"><a href="#多样性" class="headerlink" title="多样性"></a>多样性</h1><h2 id="误差-分歧分解"><a href="#误差-分歧分解" class="headerlink" title="误差-分歧分解"></a>误差-分歧分解</h2><p>设集成泛化误差为 $E$，令 $\bar{E}=\sum_{i=1}^{T}w_{i}E_{i}$ 表示个体学习器泛化误差的加权均值，$\bar{A}=\sum_{i=1}^{T}w_{i}A_{i}$ 表示个体学习器的加权分歧值，有$$<br>E=\bar{E}-\bar{A}<br>$$这个式子明确提示出：<strong><em>个体学习器准确性越高、多样性越大，则集成越好</em></strong>。(推导此处省略，书中推导过程只适用于回归学习，难以直接推广到分类学习任务。)</p><h2 id="多样性度量"><a href="#多样性度量" class="headerlink" title="多样性度量"></a>多样性度量</h2><p>多样性度量 (diversity measure) 是用于度量集成中个体分类器的多样性，即估算个体学习器的多样化程度。典型做法是考虑个体分类器两两相似/不相似性。对二分类任务，分类器 $h_{i}$ 与 $h_{j}$ 的预测结果列联表 (contingency table) 为</p><table><thead><tr><th style="text-align:center"></th><th style="text-align:center">$h_{i}=+1$</th><th style="text-align:center">$h_{i}=-1$ </th></tr></thead><tbody><tr><td style="text-align:center">$h_{j}=+1$</td><td style="text-align:center">a</td><td style="text-align:center">c </td></tr><tr><td style="text-align:center">$h_{j}=-1$</td><td style="text-align:center">b</td><td style="text-align:center">d</td></tr></tbody></table><p>其中，a表示 $h_{i}$ 与 $h_{j}$ 均预测为正类的样本数目；b、c、d 含义类推；$a+b+c+d=m$。基于此列联遍，给出以下常见多样性度量。</p><ul><li><strong><em>不合度量</em></strong> (disafreement measure)：$$dis_{ij}=\frac{b+c}{m}$$$dis_{ij}\in[0,1]$，值越大多样性越大。</li><li><strong><em>相关系数</em></strong> (correlation coefficient)：$$\rho_{ij}=\frac{ad-bc}{\sqrt{(a+b)(a+c)(c+d)(b+d)}}$$$\rho_{ij}\in[-1,1]$，若 $h_{i}$ 与 $h_{j}$ 无关，则值为0；若 $h_{i}$ 与 $h_{j}$ 正相关则值为正，否则为负。</li><li><strong><em>$Q$-统计量</em></strong> ($Q$-statistic)：$$Q_{ij}=\frac{ad-bc}{ad+bc}$$$Q_{ij}$ 与相关系数 $\rho_{ij}$的符号相同，且 $\vert Q_{ij}\vert\leq\vert\rho_{ij}\vert$。</li><li><strong><em>$\kappa$-统计量</em></strong> ($\kappa$-statistic)：$$\kappa=\frac{p_{1}-p_{2}}{1-p_{2}}$$其中，$p_{1}$ 是两个分类器取得一致的概率；$p_{2}$ 是两个分类器偶然达成一致的概率，它们可由数据集 $D$ 估算：$$p_{1}=\frac{a+d}{m}$$$$p_{2}=\frac{(a+b)(a+c)+(c+d)(b+d)}{m^{2}}$$若分类器 $h_{i}$ 与 $h_{j}$ 在 $D$ 上完全一致，则 $\kappa=1$；若它们仅是偶然达成一致，则 $\kappa=0$。$\kappa$ 通常非负，仅在 $h_{i}$ 与 $h_{j}$ 达成一致的概率甚至低于偶然性的情况下取负值。</li></ul><p>以上都是“成对型” (pairwise) 多样性度量，可以通过二维图绘制出来，如著名的“$\kappa$-误差图”，如下面的示例<br><img src="/images/machinelearning/ensemble/4.png" alt="4.png"><br>其中横坐标是这对分类器的 $\kappa$ 值，纵坐标是它们的平均误差，显然，数据点云的位置越高，则个体分类器准确性越低；点云的位置越靠右，则个体学习器的多样性越小。</p><h2 id="多样性增强"><a href="#多样性增强" class="headerlink" title="多样性增强"></a>多样性增强</h2><p>集成学习中需有效地生成多样性大的个体学习器，一般方法是在学习过程中引入随机性，常见方法是对数据样本、输入属性、输出表示、算法参数进行扰动。</p><ul><li><strong><em>样本数据扰动</em></strong>：给定初始数据集，可从中残生不同数据子集，再利用不同的数据子集训练处不同的个体学习器。数据样本扰动通常基于采样法，简单高效。这种方法对“不稳定基学习器”，如决策树、神经网络等很有效，但对于对数据样本扰动不敏感的基学习器 (稳定基学习器)，如线性学习器、支持向量机、朴素贝叶斯、$k$-近邻学习器等效果不明显。</li><li><strong><em>输入属性扰动</em></strong>：训练样本通常由一组属性描述，不同的“子空间”提供了不同的数据观察视角。显然，从不同子空间训练出的个体学习器必然有所不同。随机子空间 (random subspace) 算法是一种代表性方法。</li><li><strong><em>输出表示扰动</em></strong>：通过对输出表示进行操纵以增强多样性。可对训练样本的类标记稍作变动，如“翻转法” (Flipping Output) 随机改变一些训练样本标记；也可对输出表示进行转化，如“输出调制法” (Output Smearing) 将分类输出转化为回归输出后构建个体学习器；还可将原任务拆解为多个可同时求解的子任务，如ECOC法利用纠错输出码将多分类任务拆解为一系列二分类任务来训练基学习器。</li><li><strong><em>算法参数扰动</em></strong>：基学习算法一般都有参数需要进行设置，因此可以通过随机设置不同的参数，往往可产生差别较大的个体学习器，如“负相关法” (Negative Correlation) 显式地通过正则化项来强制个体神经网络使用不同的参数。对参数较少的算法，可通过将其学习过程中某些环节用其它类似方式代替，从而达到扰动目的。</li></ul><h1 id="习题"><a href="#习题" class="headerlink" title="习题"></a>习题</h1><p><strong><em>8.1 假设抛硬币正面朝上的概率为 $p$，反面朝上的概率为 $1-p$，令 $H(n)$ 代表抛 $n$ 次硬币所得正面朝上的次数，则最多 $k$ 次正面朝上的概率为</em></strong>$$<br>P(H(n)\leq k)=\sum_{i=0}^{k}{n\choose i}p^{i}(1-p)^{n-i}<br>$$对 $\delta&gt;0$，$k=(p-\delta)n$，有 Hoeffding 不等式$$<br>P(H(n)\leq(p-\delta)n)\leq e^{-2\delta^{2}n}<br>$$<strong><em>试推导出式(3)</em></strong>。<br>Ans: 取$p-\delta=\frac{1}{2}$，则$\delta=p-\frac{1}{2}=\frac{1}{2}-\epsilon$，$$P(H(n)\leq\frac{n}{2})=\sum_{i=0}{\frac{n}{2}}{n \choose i}p^{i}(1-p)^{n-i}\leq e^{-2(\frac{1}{2}-\epsilon)^{2}n}=e^{-\frac{1}{2}(1-2\epsilon)^{2}n}$$</p><p><strong><em>8.2 对于 $0/1$ 损失函数来说，指数损失函数并非仅有的一致替代函数。考虑式(5)，试证明：任意损失函数 $\ell\big(-\boldsymbol{f}(\boldsymbol{x})H(\boldsymbol{x})\big)$，若对于 $H(\boldsymbol{x})$ 在区间 $[-\infty,\delta]$ $(\delta&gt;0)$ 上单调递减，则 $\ell$ 是 $0/1$ 损失函数的一致替代函数。</em></strong><br>Ans: 总损失$$<br>\mathcal{L}=\ell\big(-H(x)f(x)\big)P\big(f(x)|x\big)=\ell\big(-H(x)\big)\centerdot P\big(f(x)=1|x\big)+\ell\big(H(x)\big)\centerdot P\big(f(x)=0|x\big),\quad H(x)\in\{-1,1\}<br>$$要使$\mathcal{L}$最小，当$P(f(x)=1|x)&gt;P(f(x)=0|x)$时，会希望$\ell(-H(x))&lt;\ell(H(x))$，由于$\ell$是递减的，得$H(x)&gt;H(-x)$，的$H(x)=1$。同理当$P(f(x)=1|x)&lt;P(f(x)=0|x)$时，$H(x)=−1$。$\ell(−H(x)f(x))$是对$H(x)$的单调递减函数，那么可以认为$\ell(−H(x)f(x))$是对$−H(x)$的单调递增函数。此时$H(x)=\arg\max_{y\in0,1}P(f(x)=y|x)$,即达到了贝叶斯最优错误率，说明$\ell$是$0/1$损失函数的一致替代函数。</p><p><strong><em>8.3 从网上下载或自己编程实现AdaBoost，以不剪枝决策树为基学习器，在西瓜数据集3.0$\alpha$上训练一个AdaBoost集成。</em></strong><br>Ans: 由于西瓜数据集数据量太小，这里我们使用UCI数据集的Iris数据进行实验，并使用Sci-kit Learn机器学习包编程，具体细节见代码。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> AdaBoostClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split, cross_val_score</span><br><span class="line"><span class="keyword">from</span> sklearn.utils <span class="keyword">import</span> shuffle</span><br><span class="line">warnings.filterwarnings(<span class="string">'ignore'</span>)</span><br><span class="line"></span><br><span class="line">data = pd.read_csv(<span class="string">'iris.csv'</span>)</span><br><span class="line">labels = data[<span class="string">'class'</span>]  <span class="comment"># labels</span></span><br><span class="line">data.drop([<span class="string">'class'</span>], axis=<span class="number">1</span>, inplace=<span class="keyword">True</span>)  <span class="comment"># data</span></span><br><span class="line"><span class="comment"># shuffling data and labels</span></span><br><span class="line">data, labels = shuffle(data, labels, random_state=<span class="number">5</span>)</span><br><span class="line"><span class="comment"># split into train and test</span></span><br><span class="line">train_data, test_data, train_labels, test_labels = train_test_split(data, labels, test_size=<span class="number">0.1</span>, random_state=<span class="number">200</span>)</span><br><span class="line"><span class="comment"># create base learner</span></span><br><span class="line">base_learner = DecisionTreeClassifier(criterion=<span class="string">'gini'</span>, splitter=<span class="string">'best'</span>, max_depth=<span class="number">2</span>, random_state=<span class="number">100</span>,</span><br><span class="line">                                      max_leaf_nodes=<span class="number">5</span>)</span><br><span class="line"><span class="comment"># create AdaBoost classifier and fit train data</span></span><br><span class="line">ada_boost = AdaBoostClassifier(base_learner, n_estimators=<span class="number">10</span>, algorithm=<span class="string">'SAMME'</span>, learning_rate=<span class="number">0.5</span>, random_state=<span class="number">200</span>)</span><br><span class="line">print(ada_boost, <span class="string">'\n'</span>)  <span class="comment"># print AdaBoost configuration</span></span><br><span class="line"><span class="comment"># fit data</span></span><br><span class="line">ada_boost.fit(train_data, train_labels)</span><br><span class="line">print(ada_boost.base_estimator_, <span class="string">'\n'</span>)  <span class="comment"># print base learner</span></span><br><span class="line">print(<span class="string">'Base Learner error and weight:'</span>)</span><br><span class="line"><span class="keyword">for</span> idx, err, weight <span class="keyword">in</span> zip(range(<span class="number">1</span>, <span class="number">11</span>), ada_boost.estimator_errors_, ada_boost.estimator_weights_):</span><br><span class="line">    print(<span class="string">'Base Learner-%d\t'</span> % idx, err, <span class="string">'\t'</span>, weight)</span><br><span class="line">print(<span class="string">'\n'</span>)</span><br><span class="line">print(<span class="string">'Feature Importance:'</span>)</span><br><span class="line"><span class="keyword">for</span> feature, importance <span class="keyword">in</span> zip(list(train_data), ada_boost.feature_importances_):</span><br><span class="line">    print(feature, <span class="string">'\t'</span>, importance)</span><br><span class="line">print(<span class="string">'\n'</span>)</span><br><span class="line"><span class="comment"># Accuracy</span></span><br><span class="line">scores = cross_val_score(ada_boost, train_data, train_labels, cv=<span class="number">5</span>)</span><br><span class="line">print(<span class="string">"Accuracy: %0.2f (+/- %0.2f)\n"</span> % (scores.mean(), scores.std() * <span class="number">2</span>))</span><br><span class="line"><span class="comment"># predict</span></span><br><span class="line">predict_labels = ada_boost.predict(test_data)</span><br><span class="line"><span class="keyword">for</span> predict, test_label <span class="keyword">in</span> zip(predict_labels, test_labels):</span><br><span class="line">    print(predict, <span class="string">'\t'</span>, test_label)</span><br></pre></td></tr></table></figure></p><p>得到结果：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">AdaBoostClassifier(algorithm=<span class="string">'SAMME'</span>,</span><br><span class="line">          base_estimator=DecisionTreeClassifier(class_weight=None, criterion=<span class="string">'gini'</span>, max_depth=2,</span><br><span class="line">            max_features=None, max_leaf_nodes=5, min_impurity_split=1e-07,</span><br><span class="line">            min_samples_leaf=1, min_samples_split=2,</span><br><span class="line">            min_weight_fraction_leaf=0.0, presort=False, random_state=100,</span><br><span class="line">            splitter=<span class="string">'best'</span>),</span><br><span class="line">          learning_rate=0.5, n_estimators=10, random_state=200) </span><br><span class="line"></span><br><span class="line">DecisionTreeClassifier(class_weight=None, criterion=<span class="string">'gini'</span>, max_depth=2,</span><br><span class="line">            max_features=None, max_leaf_nodes=5, min_impurity_split=1e-07,</span><br><span class="line">            min_samples_leaf=1, min_samples_split=2,</span><br><span class="line">            min_weight_fraction_leaf=0.0, presort=False, random_state=100,</span><br><span class="line">            splitter=<span class="string">'best'</span>) </span><br><span class="line"></span><br><span class="line">Base Learner error and weight:</span><br><span class="line">Base Learner-1   0.0296296296296     2.09102507132</span><br><span class="line">Base Learner-2   0.104626988622      1.41999302436</span><br><span class="line">Base Learner-3   0.111889828638      1.38236413421</span><br><span class="line">Base Learner-4   0.116849602012      1.3578775189</span><br><span class="line">Base Learner-5   0.136963254896      1.26694588583</span><br><span class="line">Base Learner-6   0.180878117849      1.1017783246</span><br><span class="line">Base Learner-7   0.174611689277      1.12321827288</span><br><span class="line">Base Learner-8   0.169743533665      1.14029657839</span><br><span class="line">Base Learner-9   0.273137055613      0.835955706868</span><br><span class="line">Base Learner-10  0.127787913186      1.30690391613</span><br><span class="line"></span><br><span class="line">Feature Importance:</span><br><span class="line">sepalLength      0.134583407492</span><br><span class="line">sepalWidth   0.0584706598454</span><br><span class="line">petalLength      0.382968813518</span><br><span class="line">petalWidth   0.423977119144</span><br><span class="line"></span><br><span class="line">Accuracy: 0.94 (+/- 0.04)</span><br><span class="line"></span><br><span class="line">Predict              Actual</span><br><span class="line">Iris-setosa          Iris-setosa</span><br><span class="line">Iris-virginica       Iris-virginica</span><br><span class="line">Iris-versicolor      Iris-versicolor</span><br><span class="line">Iris-versicolor      Iris-versicolor</span><br><span class="line">Iris-versicolor      Iris-versicolor</span><br><span class="line">Iris-virginica       Iris-virginica</span><br><span class="line">Iris-setosa          Iris-setosa</span><br><span class="line">Iris-setosa          Iris-setosa</span><br><span class="line">Iris-setosa          Iris-setosa</span><br><span class="line">Iris-versicolor      Iris-versicolor</span><br><span class="line">Iris-virginica       Iris-virginica</span><br><span class="line">Iris-virginica       Iris-virginica</span><br><span class="line">Iris-versicolor      Iris-versicolor</span><br><span class="line">Iris-versicolor      Iris-versicolor</span><br><span class="line">Iris-versicolor      Iris-versicolor</span><br></pre></td></tr></table></figure></p><p><strong><em>8.4 <a href="https://en.wikipedia.org/wiki/Gradient_boosting" target="_blank" rel="noopener">GradientBoosting</a>是一种常用的Boosting算法，试分析其与AdaBoost的异同。</em></strong><br>Ans: GradientBoosting与AdaBoost相同的地方在于要生成多个分类器以及每个分类器都有一个权值，最后将所有分类器加权累加起来。不同在于：AdaBoost通过每个分类器的分类结果改变每个样本的权值用于新的分类器和生成权值，但不改变每个样本不会改变。GradientBoosting将每个分类器对样本的预测值与真实值的差值传入下一个分类器来生成新的分类器和权值(这个差值就是下降方向)，而每个样本的权值不变。</p><p><strong><em>8.5 试编程实现Bagging，以决策树桩为基学习器，在西瓜数据集3.0$\alpha$上训练一个Bagging集成。</em></strong><br>Ans: 这里同样适用UCI数据集的Iris数据进行实验，具体细节见代码。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> BaggingClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.utils <span class="keyword">import</span> shuffle</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split, cross_val_score</span><br><span class="line">warnings.filterwarnings(<span class="string">'ignore'</span>)</span><br><span class="line"></span><br><span class="line">data = pd.read_csv(<span class="string">'iris.csv'</span>)</span><br><span class="line">labels = data[<span class="string">'class'</span>]  <span class="comment"># labels</span></span><br><span class="line">data.drop([<span class="string">'class'</span>], axis=<span class="number">1</span>, inplace=<span class="keyword">True</span>)  <span class="comment"># data</span></span><br><span class="line"><span class="comment"># shuffling data and labels</span></span><br><span class="line">data, labels = shuffle(data, labels, random_state=<span class="number">5</span>)</span><br><span class="line"><span class="comment"># split into train and test</span></span><br><span class="line">train_data, test_data, train_labels, test_labels = train_test_split(data, labels, test_size=<span class="number">0.1</span>, random_state=<span class="number">200</span>)</span><br><span class="line"><span class="comment"># create base learner</span></span><br><span class="line">base_learner = DecisionTreeClassifier(criterion=<span class="string">'gini'</span>, splitter=<span class="string">'best'</span>, max_depth=<span class="number">2</span>, random_state=<span class="number">100</span>,</span><br><span class="line">                                      max_leaf_nodes=<span class="number">5</span>)</span><br><span class="line"><span class="comment"># create bagging classifier</span></span><br><span class="line">bagging = BaggingClassifier(base_learner, n_estimators=<span class="number">10</span>, bootstrap=<span class="keyword">True</span>, bootstrap_features=<span class="keyword">True</span>, oob_score=<span class="keyword">True</span>,</span><br><span class="line">                            max_samples=<span class="number">0.5</span>, random_state=<span class="number">200</span>, n_jobs=<span class="number">-1</span>)</span><br><span class="line">print(bagging, <span class="string">'\n'</span>)  <span class="comment"># print bagging configuration</span></span><br><span class="line"><span class="comment"># fit data</span></span><br><span class="line">bagging.fit(train_data, train_labels)</span><br><span class="line">print(bagging.base_estimator_, <span class="string">'\n'</span>)  <span class="comment"># print base learner</span></span><br><span class="line">print(<span class="string">'Base Learner Features:'</span>)</span><br><span class="line"><span class="keyword">for</span> idx, feature <span class="keyword">in</span> zip(range(<span class="number">1</span>, <span class="number">11</span>), bagging.estimators_features_):</span><br><span class="line">    print(<span class="string">'Base Learner-%d\t'</span> % idx, feature)</span><br><span class="line">print(<span class="string">'\n'</span>)</span><br><span class="line">scores = cross_val_score(bagging, train_data, train_labels, cv=<span class="number">5</span>)</span><br><span class="line">print(<span class="string">"Accuracy: %0.2f (+/- %0.2f)\n"</span> % (scores.mean(), scores.std() * <span class="number">2</span>))</span><br><span class="line"><span class="comment"># predict</span></span><br><span class="line">predict_labels = bagging.predict(test_data)</span><br><span class="line"><span class="keyword">for</span> predict, test_label <span class="keyword">in</span> zip(predict_labels, test_labels):</span><br><span class="line">    print(predict, <span class="string">'\t'</span>, test_label)</span><br></pre></td></tr></table></figure></p><p>输出结果为：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">BaggingClassifier(base_estimator=DecisionTreeClassifier(class_weight=None, criterion=<span class="string">'gini'</span>, max_depth=2,</span><br><span class="line">            max_features=None, max_leaf_nodes=5, min_impurity_split=1e-07,</span><br><span class="line">            min_samples_leaf=1, min_samples_split=2,</span><br><span class="line">            min_weight_fraction_leaf=0.0, presort=False, random_state=100,</span><br><span class="line">            splitter=<span class="string">'best'</span>),</span><br><span class="line">         bootstrap=True, bootstrap_features=True, max_features=1.0,</span><br><span class="line">         max_samples=0.5, n_estimators=10, n_jobs=-1, oob_score=True,</span><br><span class="line">         random_state=200, verbose=0, warm_start=False) </span><br><span class="line"></span><br><span class="line">DecisionTreeClassifier(class_weight=None, criterion=<span class="string">'gini'</span>, max_depth=2,</span><br><span class="line">            max_features=None, max_leaf_nodes=5, min_impurity_split=1e-07,</span><br><span class="line">            min_samples_leaf=1, min_samples_split=2,</span><br><span class="line">            min_weight_fraction_leaf=0.0, presort=False, random_state=100,</span><br><span class="line">            splitter=<span class="string">'best'</span>) </span><br><span class="line"></span><br><span class="line">Base Learner Features:</span><br><span class="line">Base Learner-1   [1 1 1 0]</span><br><span class="line">Base Learner-2   [1 2 3 0]</span><br><span class="line">Base Learner-3   [3 3 1 1]</span><br><span class="line">Base Learner-4   [2 2 1 2]</span><br><span class="line">Base Learner-5   [3 1 0 2]</span><br><span class="line">Base Learner-6   [2 2 1 2]</span><br><span class="line">Base Learner-7   [3 2 0 2]</span><br><span class="line">Base Learner-8   [3 0 2 0]</span><br><span class="line">Base Learner-9   [1 1 3 3]</span><br><span class="line">Base Learner-10  [1 0 1 3]</span><br><span class="line"></span><br><span class="line">Accuracy: 0.93 (+/- 0.03)</span><br><span class="line"></span><br><span class="line">Iris-setosa      Iris-setosa</span><br><span class="line">Iris-virginica   Iris-virginica</span><br><span class="line">Iris-versicolor      Iris-versicolor</span><br><span class="line">Iris-versicolor      Iris-versicolor</span><br><span class="line">Iris-versicolor      Iris-versicolor</span><br><span class="line">Iris-virginica   Iris-virginica</span><br><span class="line">Iris-setosa      Iris-setosa</span><br><span class="line">Iris-setosa      Iris-setosa</span><br><span class="line">Iris-setosa      Iris-setosa</span><br><span class="line">Iris-versicolor      Iris-versicolor</span><br><span class="line">Iris-virginica   Iris-virginica</span><br><span class="line">Iris-virginica   Iris-virginica</span><br><span class="line">Iris-versicolor      Iris-versicolor</span><br><span class="line">Iris-versicolor      Iris-versicolor</span><br><span class="line">Iris-versicolor      Iris-versicolor</span><br></pre></td></tr></table></figure></p><p><strong><em>8.6 试析Bagging通常为何难以提升朴素贝叶斯分类器的性能。</em></strong><br>Ans: Bagging主要是降低分类器的方差，而朴素贝叶斯分类器没有方差可以减小。对全训练样本生成的朴素贝叶斯分类器是最优的分类器，不能用随机抽样来提高泛化性能。</p><p><strong><em>8.7 试析随机森林为何比决策树Bagging集成的训练速度更快。</em></strong><br>Ans: 随机森林不仅会随机样本，还会在所有样本属性中随机几种出来计算。这样每次生成分类器时都是对部分属性计算最优，速度会比Bagging计算全属性要快。</p><p><strong><em>8.8 <a href="http://www.jmlr.org/papers/volume13/benbouzid12a/benbouzid12a.pdf" target="_blank" rel="noopener">MultiBoosting</a>算法将AdaBoost作为Bagging的基学习器，<a href="http://infochim.u-strasbg.fr/new/CS3_2010/Tutorial/Ensemble/EnsembleModeling.pdf" target="_blank" rel="noopener">Iterative Bagging</a>算法则是将Bagging作为AdaBoost的基学习器。试比较二者的优缺点。</em></strong><br>Ans: MultiBoosting由于集合了Bagging，Wagging，AdaBoost，可以有效的降低误差和方差，特别是误差。但是训练成本和预测成本都会显著增加。Iterative Bagging相比Bagging会降低误差，但是方差上升。由于Bagging本身就是一种降低方差的算法，所以Iterative Bagging相当于Bagging与单分类器的折中。</p><p><strong><em>8.9 试设计一种可视的多样性度量，对8.3和8.5中得到的集成进行评估，并与 $\kappa$-误差图比较。</em></strong><br>Ans: <strong><em>TODO</em></strong></p><p><strong><em>8.10 试设计一种能提升$k$近邻分类器性能的集成学习方法。</em></strong><br>Ans: 可以使用Bagging来提升k近邻分类器的性能，每次随机抽样出一个子样本，并训练一个k近邻分类器，对测试样本进行分类。最终取最多的一种分类。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;第八章-集成学习 (Ensemble Learning)
    
    </summary>
    
      <category term="Machine Learning" scheme="https://isaacchanghau.github.io/categories/Machine-Learning/"/>
    
    
      <category term="machine learning" scheme="https://isaacchanghau.github.io/tags/machine-learning/"/>
    
      <category term="python" scheme="https://isaacchanghau.github.io/tags/python/"/>
    
      <category term="boosting" scheme="https://isaacchanghau.github.io/tags/boosting/"/>
    
      <category term="random forest" scheme="https://isaacchanghau.github.io/tags/random-forest/"/>
    
      <category term="bagging" scheme="https://isaacchanghau.github.io/tags/bagging/"/>
    
      <category term="decision tree" scheme="https://isaacchanghau.github.io/tags/decision-tree/"/>
    
  </entry>
  
  <entry>
    <title>机器学习(周志华)--学习笔记(三) 支持向量机(Support Vector Machine)</title>
    <link href="https://isaacchanghau.github.io/2017/08/04/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%91%A8%E5%BF%97%E5%8D%8E-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-3/"/>
    <id>https://isaacchanghau.github.io/2017/08/04/机器学习-周志华-学习笔记-3/</id>
    <published>2017-08-04T07:15:24.000Z</published>
    <updated>2017-08-05T16:41:55.000Z</updated>
    
    <content type="html"><![CDATA[<p>第六章–支持向量机 (Support Vector Machine)<a id="more"></a></p><h1 id="间隔与支持向量"><a href="#间隔与支持向量" class="headerlink" title="间隔与支持向量"></a>间隔与支持向量</h1><p>给定数据集$D=\{(\boldsymbol{x}_{1},y_{1}),\dots,(\boldsymbol{x}_{m},y_{m})\}$，$y_{i}\in\{-1,+1\}$，分类学习最基本的思想是基于训练集$D$在样本空间中找到一个划分超平面，将不同类别的样本分开。但能将训练样本粉来的划分超平面可能有很多，直观上，应该去找两类训练样本“正中间”的划分超平面，即下图中粗线表示的划分超平面，因为该划分对训练样本<strong><em>局部扰动</em></strong>的<em>“容忍性”</em>最好。<br><img src="/images/machinelearning/svm/1.png" alt="1.png"><br>在样本空间中，划分超平面由下述线性方程表示：$$<br>\boldsymbol{w}^{T}\boldsymbol{x}+b=0\tag{1}<br>$$其中$\boldsymbol{w}=(w_{1},\dots,w_{d})$为<strong><em>法向量</em></strong>，决定了超平面的方向，$b$为<strong><em>位移项</em></strong>，决定了超平面与原点之间的距离。记超平面为$(\boldsymbol{w},b)$，样本空间中任意点$\boldsymbol{x}$到超平面$(\boldsymbol{w},b)$的距离可写为$$<br>r=\frac{\vert\boldsymbol{w}^{T}\boldsymbol{x}+b\vert}{\Vert\boldsymbol{w}\Vert}\tag{2}<br>$$假设$(\boldsymbol{w},b)$能将训练样本正确分类，即对于$(\boldsymbol{x}_{i},y_{i})\in D$，若$y_{i}=+1$，则有$\boldsymbol{w}^{T}\boldsymbol{x}_{i}+b&gt;0$；若$y_{i}=-1$，则$\boldsymbol{w}^{T}\boldsymbol{x}_{i}+b&lt;0$。令$$<br>\begin{cases} \boldsymbol{w}^{T}\boldsymbol{x}_{i}+b\geq+1,&amp;y_{i}=+1;\\<br>\boldsymbol{w}^{T}\boldsymbol{x}_{i}+b\leq-1,&amp;y_{i}=-1; \end{cases}\tag{3}$$如下图，距离超平面最近的几个训练样本点使公式(3)成立，它们被称为<strong><em>“支持向量”</em></strong> (support vector)，两个异类支持向量到超平面的距离之和为$$<br>\gamma=\frac{2}{\Vert\boldsymbol{w}\Vert}\tag{4}$$它被称为<strong><em>“间隔”</em></strong> (margin)。<br><img src="/images/machinelearning/svm/2.png" alt="2.png"><br>求解<strong><em>“最大间隔”</em></strong> (maximum margin)的划分超平面，即找到满足公式(3)中的约束参数$\boldsymbol{w}$和$b$，使得$\gamma$最大：$$\begin{aligned}<br>&amp; \max_{\boldsymbol{w},b}\frac{2}{\Vert\boldsymbol{w}\Vert}\\<br>&amp; s.t.\quad y_{i}(\boldsymbol{w}^{T}\boldsymbol{x}_{i}+b)\geq 1,\quad i=1,2,\dots,m<br>\end{aligned}\tag{5}<br>$$最大化间隔，仅需要最大化$\Vert\boldsymbol{w}\Vert^{-1}$，等价于最小化$\Vert\boldsymbol{w}\Vert^{2}$，于是有$$\begin{aligned}<br>&amp; \min_{\boldsymbol{w},b}\frac{1}{2}\Vert\boldsymbol{w}\Vert^{2}\\<br>&amp; s.t.\quad y_{i}(\boldsymbol{w}^{T}\boldsymbol{x}_{i}+b)\geq 1,\quad i=1,2,\dots,m<br>\end{aligned}\tag{6}<br>$$上式为支持向量机(Support Vector Machine，SVM)的基本型。</p><h1 id="对偶问题"><a href="#对偶问题" class="headerlink" title="对偶问题"></a>对偶问题</h1><p>我们希望求解式(6)来得到最大间隔划分超平面所对应的模型$$<br>f(\boldsymbol{x})=\boldsymbol{w}^{T}\boldsymbol{x}+b\tag{7}<br>$$公式(6)本身为一个<strong><em>凸二次规划</em></strong> (convex quadratic programming)问题，为求解(6)式，对其使用拉格朗日乘子法可得其<strong><em>“对偶问题”</em></strong> (dual problem)：$$<br>L(\boldsymbol{w},b,\boldsymbol{\alpha})=\frac{1}{2}\Vert\boldsymbol{w}\Vert^{2}+<br>\sum_{i=1}^{m}\alpha_{i}\big(1-y_{i}(\boldsymbol{w}^{T}\boldsymbol{x}_{i}+b)\big)\tag{8}<br>$$其中$\boldsymbol{\alpha}=(\alpha_{1};\dots;\alpha_{m})$。令$L(\boldsymbol{w},b,\boldsymbol{\alpha})$对$\boldsymbol{w}$和$b$的偏导为零可得$$<br>\boldsymbol{w}=\sum_{i=1}^{m}\alpha_{i}y_{i}\boldsymbol{x}_{i}\tag{9}$$$$<br>0=\sum_{i=1}^{m}\alpha_{i}y_{i}\tag{10}<br>$$将(9)代入(8)，即可将$L(\boldsymbol{w},b,\boldsymbol{\alpha})$中的$\boldsymbol{w}$和$b$消去，再考虑式(10)的约束，可得到式(6)的对偶问题$$<br>\begin{aligned}<br>&amp; \max_{\boldsymbol{\alpha}}\sum_{i=1}^{m}\alpha_{i}-\frac{1}{2}\sum_{i=1}^{m}\sum_{j=1}^{m}\alpha_{i}\alpha_{j}y_{i}y_{j}\boldsymbol{x}_{i}^{T}\boldsymbol{x}_{j}\\<br>&amp; s.t.\quad\sum_{i=1}^{m}\alpha_{i}y_{i}=0,\quad\alpha_{i}\geq 0, i=1,2,\dots,m.<br>\end{aligned}\tag{11}<br>$$解出$\boldsymbol{\alpha}$后，求出$\boldsymbol{w}$和$b$即可得到模型$$<br>f(\boldsymbol{x})=\boldsymbol{w}^{T}\boldsymbol{x}+b=\sum_{i=1}^{m}\alpha_{i}y_{i}\boldsymbol{x}_{i}^{T}\boldsymbol{x}+b\tag{12}<br>$$从对偶问题(11)解出的$\alpha_{i}$是式(8)中的拉格朗日乘子，它恰好对应训练样本$(\boldsymbol{x}_{i},y_{i})$。考虑到式(6)中有不等式约束，因此上述过程需要满足KKT (<a href="https://en.wikipedia.org/wiki/Karush%E2%80%93Kuhn%E2%80%93Tucker_conditions" target="_blank" rel="noopener">Karush-Kuhn-Tucker</a>)条件，即$$<br>\begin{cases} \alpha_{i}\geq 0;\\<br>y_{i}f(\boldsymbol{x}_{i})-1\geq 0;\\<br>\alpha_{i}(y_{i}f(\boldsymbol{x}_{i})-1)=0.<br>\end{cases}\tag{13}$$于是，对于任意训练样本$(\boldsymbol{x}_{i},y_{i})$，总有$\alpha_{i}=0$或$y_{i}f(\boldsymbol{x}_{i})=1$。若$\alpha_{i}=0$，则该样本不会在式(12)的求和中出现。若$\alpha_{i}&gt;0$，则必有$y_{i}f(\boldsymbol{x}_{i})=1$，所对应的样本点位于最大间隔边界上，是一个支持向量。这显示出支持向量机的一个重要性质：<strong><em>训练完成后，大部分的训练样本都不需要保留，最终模型仅与支持向量有关</em></strong>。<br>对于式(11)的求解，这是一个二次规划问题，常用SMO (<a href="https://en.wikipedia.org/wiki/Sequential_minimal_optimization" target="_blank" rel="noopener">Sequential Minimal Optimization</a>)等高效的方法来求解。SMO的基本思路是先固定$\alpha_{i}$以外的所有参数，然后求解$\alpha_{i}$上的极值。由于存在约束$\sum_{i=1}^{m}\alpha_{i}y_{i}=0$，若固定$\alpha_{i}$之外的其他变量，则$\alpha_{i}$可由其他变量导出。于是，SMO每次选择两个变量$\alpha_{i}$和$\alpha_{j}$，并固定其他参数。这样在参数初始化后，SMO不断执行如下两个操作直到收敛：</p><ul><li>选取一对需要更新的变量$\alpha_{i}$和$\alpha_{j}$。</li><li>固定$\alpha_{i}$和$\alpha_{j}$以外的参数，求解(11)获得更新后的$\alpha_{i}$和$\alpha_{j}$。</li></ul><p>注意到只需要选取的$\alpha_{i}$和$\alpha_{j}$中有一个不满足KKT条件，目标函数就会在迭代后见效，且KKT条件违背程度越大，则变量更新后可能导致的目标函数值减幅越大。因此SMO先选取违背KKT条件程度最大的变量。第二个变量应选择一个使目标函数值减小最快的变量，但比较各变量所对应的目标函数数值减幅的复杂度过高，SMO采用了一个启发式：使选取的两个变量所对应样本之间的间隔最大。这样的两个变量有很大的差别，与对两个相似变量进行更新相比，对它们进行更新会带给目标函数值更大的变化。具体来说，仅考虑$\alpha_{i}$和$\alpha_{j}$时，(11)中的约束可重写为$$<br>\alpha_{i}y_{i}+\alpha_{j}y_{j}=c,\quad\alpha_{i}\geq 0,\quad\alpha_{j}\geq 0\tag{14}$$其中$$<br>c=-\sum_{k\neq i,j}\alpha_{k}y_{k}\tag{15}$$是使$\sum_{i=1}^{m}\alpha_{i}y_{i}=0$成立的常数。用$$<br>\alpha_{i}y_{i}+\alpha_{j}y_{j}=c\tag{16}$$消去(11)中的变量$\alpha_{j}$，则得到一个关于$\alpha_{i}$的单变量二次规划问题，仅有约束$\alpha_{i}\geq 0$。这样的二次规划具有封闭解，能高效地计算出更新后的$\alpha_{i}$和$\alpha_{j}$。对于偏移项$b$，对任意支持向量$(\boldsymbol{x}_{s},y_{s})$都有$y_{s}f(\boldsymbol{x}_{s})=1$，即$$<br>y_{s}\bigg(\sum_{i\in S}\alpha_{i}y_{i}\boldsymbol{x}_{i}^{T}\boldsymbol{x}+b\bigg)=1\tag{17}<br>$$其中$S=\{i|\alpha_{i}&gt;0,i=1,2,\dots,m\}$为所有支持向量的下标集。理论上，可选取任意支持向量，通过求解(17)获得$b$，但实际中采用一个更加鲁棒的做法：使用所有支持向量求解的平均值$$<br>b=\frac{1}{\vert S\vert}\sum_{s\in S}\bigg(y_{s}-\sum_{i\in S}\alpha_{i}y_{i}\boldsymbol{x}_{i}^{T}\boldsymbol{x}_{s}\bigg)\tag{18}$$</p><h1 id="核函数"><a href="#核函数" class="headerlink" title="核函数"></a>核函数</h1><p>对于原始样本空间不存在一个能正确划分两类样本的超平面的问题，可将样本从原始空间映射到一个更高维德特征空间，使得样本在这个特征空间内线性可分。令$\phi(\boldsymbol{x})$表示将$\boldsymbol{x}$映射后的特征向量，在特征空间中划分超平面所对应的模型可表示为$$<br>f(\boldsymbol{x})=\boldsymbol{w}^{T}\phi(\boldsymbol{x})+b\tag{19}<br>$$类似(6)有$$\begin{aligned}<br>&amp;\min_{\boldsymbol{w},b}\frac{1}{2}\Vert\boldsymbol{w}\Vert^{2}\\<br>&amp;s.t.\quad y_{i}(\boldsymbol{w}^{T}\phi(\boldsymbol{x}_{i})+b)\geq 1,i=1,2,\dots,m<br>\end{aligned}\tag{20}$$其对偶问题是$$<br>\begin{aligned}<br>&amp; \max_{\boldsymbol{\alpha}}\sum_{i=1}^{m}\alpha_{i}-\frac{1}{2}\sum_{i=1}^{m}\sum_{j=1}^{m}\alpha_{i}\alpha_{j}y_{i}y_{j}\phi(\boldsymbol{x}_{i})^{T}\phi(\boldsymbol{x}_{j})\\<br>&amp; s.t.\quad\sum_{i=1}^{m}\alpha_{i}y_{i}=0,\quad\alpha_{i}\geq 0, i=1,2,\dots,m.<br>\end{aligned}\tag{21}<br>$$上式需要计算$\phi(\boldsymbol{x}_{i})^{T}\phi(\boldsymbol{x}_{j})$，由于特征空间维数较高，直接计算比较困难，通常设计一个函数：$$<br>\kappa(\boldsymbol{x}_{i},\boldsymbol{x}_{j})=\langle\phi(\boldsymbol{x}_{i}),\phi(\boldsymbol{x}_{j})\rangle=\phi(\boldsymbol{x}_{i})^{T}\phi(\boldsymbol{x}_{j})\tag{22}<br>$$即$\boldsymbol{x}_{i}$与$\boldsymbol{x}_{j}$在特征空间的内积等于它们在原始样本空间中通过函数$\kappa(\centerdot,\centerdot)$计算的结果，于是，(21)可重写为$$<br>\begin{aligned}<br>&amp; \max_{\boldsymbol{\alpha}}\sum_{i=1}^{m}\alpha_{i}-\frac{1}{2}\sum_{i=1}^{m}\sum_{j=1}^{m}\alpha_{i}\alpha_{j}y_{i}y_{j}\kappa(\boldsymbol{x}_{i},\boldsymbol{x}_{j})\\<br>&amp; s.t.\quad\sum_{i=1}^{m}\alpha_{i}y_{i}=0,\quad\alpha_{i}\geq 0, i=1,2,\dots,m.<br>\end{aligned}\tag{23}<br>$$求解后得到$$<br>f(\boldsymbol{x})=\boldsymbol{w}^{T}\phi(\boldsymbol{x})+b=\sum_{i=1}^{m}\alpha_{i}y_{i}\kappa(\boldsymbol{x},\boldsymbol{x}_{i})+b\tag{24}<br>$$这里的函数$\kappa(\centerdot,\centerdot)$就是<strong><em>“核函数”</em></strong> (kernel function)。式(24)显示出模型最优解可以通过训练样本的核函数展开，这一展式亦称“支持向量展式” (support vector expansion)。<br><strong><em>定理1 (核函数)</em></strong>：令$\chi$为输入空间，$\kappa(\centerdot,\centerdot)$是定义在 $\chi\times\chi$ 上的对称函数，则$\kappa$是核函数当且仅当对于任意数据$D={\boldsymbol{x}_{1},\dots,\boldsymbol{x}_{m}}$，“核矩阵” (kernel matrix) $\boldsymbol{K}$总是半正定的：$$\boldsymbol{K}=\begin{bmatrix}<br>\kappa(\boldsymbol{x}_{1},\boldsymbol{x}_{1}) &amp; \dots &amp; \kappa(\boldsymbol{x}_{1},\boldsymbol{x}_{j}) &amp; \dots &amp; \kappa(\boldsymbol{x}_{1},\boldsymbol{x}_{m})\\<br>\vdots &amp; \ddots &amp; \vdots &amp; \ddots &amp; \vdots\\<br>\kappa(\boldsymbol{x}_{i},\boldsymbol{x}_{1}) &amp; \dots &amp; \kappa(\boldsymbol{x}_{i},\boldsymbol{x}_{j}) &amp; \dots &amp; \kappa(\boldsymbol{x}_{i},\boldsymbol{x}_{m})\\<br>\vdots &amp; \ddots &amp; \vdots &amp; \ddots &amp; \vdots\\<br>\kappa(\boldsymbol{x}_{m},\boldsymbol{x}_{1}) &amp; \dots &amp; \kappa(\boldsymbol{x}_{m},\boldsymbol{x}_{j}) &amp; \dots &amp; \kappa(\boldsymbol{x}_{m},\boldsymbol{x}_{m})\\<br>\end{bmatrix}$$<strong><em>定理1</em></strong>表明只要一个对称函数所对应的核矩阵半正定，它就能作为核函数使用。事实上，对于一个半正定核矩阵，总能找到一个与之对应的映射$\phi$。即任何一个核函数都隐式地定义了一个称为<strong><em>“再生核希尔伯特空间”</em></strong> (<a href="https://en.wikipedia.org/wiki/Reproducing_kernel_Hilbert_space" target="_blank" rel="noopener">Reproducing Kernel Hilbert Space</a>，RKHS)的特征空间。我们希望样本在特征空间内线性可分，因此特征空间的好坏对支持向量机的性能至关重要。于是，“核函数选择”称为支持向量机的最大变数，若选择不合适，意味着将样本映射到了一个不合适的特征空间，很可能导致性能不佳。以下给出常用的核函数</p><table><thead><tr><th style="text-align:left">名称</th><th style="text-align:left">表达式</th><th style="text-align:left">参数 </th></tr></thead><tbody><tr><td style="text-align:left">线性核</td><td style="text-align:left">$\kappa(\boldsymbol{x}_{i},\boldsymbol{x}_{j})=\boldsymbol{x}_{i}^{T}\boldsymbol{x}_{j}$</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">多项式核</td><td style="text-align:left">$\kappa(\boldsymbol{x}_{i},\boldsymbol{x}_{j})=\big(\boldsymbol{x}_{i}^{T}\boldsymbol{x}_{j}\big)^{d}$</td><td style="text-align:left">$d\geq 1$为多项式的次数 </td></tr><tr><td style="text-align:left">高斯核</td><td style="text-align:left">$\kappa(\boldsymbol{x}_{i},\boldsymbol{x}_{j})=\exp\big(-\frac{\Vert\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\Vert^{2}}{2\sigma^{2}}\big)$</td><td style="text-align:left">$\sigma&gt;0$为高斯核的带宽(width) </td></tr><tr><td style="text-align:left">拉普拉斯核</td><td style="text-align:left">$\kappa(\boldsymbol{x}_{i},\boldsymbol{x}_{j})=\exp\big(-\frac{\Vert\boldsymbol{x}_{i}-\boldsymbol{x}_{j}\Vert}{\sigma}\big)$</td><td style="text-align:left">$\sigma&gt;0$ </td></tr><tr><td style="text-align:left">Sigmoid核</td><td style="text-align:left">$\kappa(\boldsymbol{x}_{i},\boldsymbol{x}_{j})=\tanh(\beta\boldsymbol{x}_{i}^{T}\boldsymbol{x}_{j}+\theta)$</td><td style="text-align:left">$\tanh$为双曲正切函数，$\beta&gt;0$，$\theta&lt;0$ </td></tr></tbody></table><p>此外，核函数还可通过函数组合得到，例如：</p><ul><li>若$\kappa_{1}$和$\kappa_{2}$为核函数，则对于任意正数$\gamma_{1}$、$\gamma_{2}$，其线性组合$$<br>\gamma_{1}\kappa_{1}+\gamma_{2}\kappa_{2}\tag{25}<br>$$也是核函数。</li><li>若$\kappa_{1}$和$\kappa_{2}$为核函数，则核函数的直积$$<br>\kappa_{1}\otimes\kappa_{2}(\boldsymbol{x},\boldsymbol{z})=\kappa_{1}(\boldsymbol{x},\boldsymbol{z})\kappa_{2}(\boldsymbol{x},\boldsymbol{z})\tag{26}<br>$$也是核函数。</li><li>若$\kappa_{1}$为核函数，则对于任意函数$g(\boldsymbol{x})$$$<br>\kappa(\boldsymbol{x},\boldsymbol{z})=g(\boldsymbol{x})\kappa_{1}(\boldsymbol{x},\boldsymbol{z})g(\boldsymbol{z})\tag{27}<br>$$也是核函数。</li></ul><h1 id="软间隔与正则化"><a href="#软间隔与正则化" class="headerlink" title="软间隔与正则化"></a>软间隔与正则化</h1><p>现实任务中往往很难确定合适的核函数使得训练样本在特征空间中线性可分，因此可以允许支持向量机载一些样本上出错，这里引入<strong><em>“软间隔”</em></strong> (soft margin)的概念，如下图所示<br><img src="/images/machinelearning/svm/3.png" alt="3.png"><br>之前假设支持向量机形式是要求所有样本均满足约束(3)，这称为<strong><em>“硬间隔”</em></strong> (hard margin)，而软间隔则是允许某些样本不满足约束$$<br>y_{i}(\boldsymbol{w}^{T}\boldsymbol{x}_{i}+b)\geq 1\tag{28}<br>$$当然，在最大化间隔的同时，不满足约束的样本尽可能少，于是优化目标写为$$<br>\min_{\boldsymbol{w},b}\frac{1}{2}\Vert\boldsymbol{w}\Vert^{2}+C\sum_{i=1}^{m}\ell_{0/1}\big(y_{i}(\boldsymbol{w}^{T}\boldsymbol{x}_{i}+b)-1\big)\tag{29}<br>$$其中$C&gt;0$是一个常数，$\ell_{0/1}$是“$0/1$损失函数”$$<br>\ell_{0/1}=\begin{cases}<br>1,\quad z&lt;0\\<br>0,\quad otherwise<br>\end{cases}\tag{30}$$显然，当$C$无穷大时，(29)迫使所有样本满足约束(28)，于是(29)等价于(6)，当$C$取有限值时，(29)允许一些样本不满足约束。然而$\ell_{0/1}$非凸、非连续，使得(29)不易直接求解。于是常采用一些函数来替代它，称为“替代损失” (surrogate loss)，这些函数通常侍凸的连续函数且是$\ell_{0/1}$的上界。下图给出了常用的三种替代损失函数：<br><img src="/images/machinelearning/svm/4.png" alt="4.png"></p><ul><li>hinge损失：$\ell_{hinge}(z)=\max(0,1-z)\tag{31}$</li><li>指数损失(exponential loss)：$\ell_{exp}(z)=\exp(-z)\tag{32}$</li><li>对率损失(logistic loss)：$\ell_{log}(z)=\log(1+\exp(-z))\tag{33}$</li></ul><p>若采用hinge损失，则(29)变为$$<br>\min_{\boldsymbol{w},b}\frac{1}{2}\Vert\boldsymbol{w}\Vert^{2}+C\sum_{i=1}^{m}\max\big(0,1-y_{i}(\boldsymbol{w}^{T}\boldsymbol{x}_{i}+b)\big)\tag{34}<br>$$引入<strong><em>“松弛变量”</em></strong> (slack variables) $\xi\geq 0$，则可将上式重写为$$<br>\min_{\boldsymbol{w},b,\xi_{i}}\frac{1}{2}\Vert\boldsymbol{w}\Vert^{2}+C\sum_{i=1}^{m}\xi_{i}\tag{35}<br>$$这就是常用的<strong><em>“软间隔支持向量机”</em></strong>。(35)中每个样本都有一个对应的松弛向量，用以表征该样本不满足约束(28)的程度，但是，与(6)相似，这是一个二次规划问题。于是，类似(8)，通过拉格朗日乘子法和得到式(35)的拉格朗日函数$$<br>L(\boldsymbol{w},b,\boldsymbol{\alpha},\boldsymbol{\xi},\boldsymbol{\mu})=<br>\frac{1}{2}\Vert\boldsymbol{w}\Vert^{2}+C\sum_{i=1}^{m}\xi_{i}+<br>\sum_{i=1}^{m}\alpha_{i}\big(1-\xi_{i}-y_{i}(\boldsymbol{w}^{T}\boldsymbol{x}_{i}+b)\big)-<br>\sum_{i=1}^{m}\mu_{i}\xi_{i}\tag{36}<br>$$其中$\alpha_{i}\geq 0$，$\mu_{i}\geq 0$是拉格朗日乘子。令$L(\boldsymbol{w},b,\boldsymbol{\alpha},\boldsymbol{\xi},\boldsymbol{\mu})$对$\boldsymbol{w}$，$b$，$\xi_{i}$的偏导为零可得$$<br>\boldsymbol{w}=\sum_{i}^{m}\alpha_{i}y_{i}\boldsymbol{x}_{i}\tag{37}$$$$<br>0=\sum_{i=1}^{m}\alpha_{i}y_{i}\tag{38}$$$$<br>C=\alpha_{i}+\mu_{i}\tag{39}<br>$$将上面三个式子代入(36)可得到(35)的对偶问题$$\begin{aligned}<br>&amp; \max_{\boldsymbol{\alpha}}\sum_{i=1}^{m}\alpha_{i}-\frac{1}{2}\sum_{i}^{m}\sum_{j=1}^{m}\alpha_{i}\alpha_{j}y_{i}y_{j}\boldsymbol{x}_{i}^{T}\boldsymbol{x}_{j}\\<br>&amp; s.t.\quad\sum_{i=1}^{m}\alpha_{i}y_{i}=0,\quad 0\leq\alpha_{i}\leq C, i=1,2,\dots,m<br>\end{aligned}\tag{40}$$上式与(11)对比可得，两者唯一的差别就在于对偶变量的约束不同：前者是$0\leq\alpha_{i}\leq C$，后者是$0\leq\alpha_{i}$。于是，可采用同样的方法(SMO)求解(40)。在引入核函数后能得到与式(24)同样的支持向量展式。类似(13)，对软间隔支持向量机，KKT条件要求$$\begin{cases}<br>\alpha_{i}\geq 0,\quad\mu_{i}\geq 0,\\<br>y_{i}f(\boldsymbol{x}_{i})-1+\xi_{i}\geq 0,\\<br>\alpha_{i}\big(y_{i}f(\boldsymbol{x}_{i})-1+\xi_{i}\big)=0,\\<br>\xi_{i}\geq 0,\quad\mu_{i}\xi_{i}=0,<br>\end{cases}\tag{41}$$于是，对于任意训练样本$(\boldsymbol{x}_{i},y_{i})$，总有$\alpha_{i}=0$或$y_{i}f(\boldsymbol{x}_{i})=1-\xi_{i}$。若$\alpha_{i}=0$，则该样本不会对$f(\boldsymbol{x})$有任何影响；若$\alpha_{i}&gt;0$，则必有$y_{i}f(\boldsymbol{x}_{i})=1-\xi_{i}$，即该样本是支持向量：由(39)可知，若$\alpha_{i}&lt; C$，则$\mu_{i}&gt;0$，进而有$\xi_{i}=0$，即该样本恰在最大间隔边界上；若$\alpha_{i}=C$，则有$\mu_{i}=0$，此时若$\xi_{i}\leq 1$则该样本落在最大间隔内部，若$\xi_{i}&gt;1$，则该样本被错误分类。因此，软间隔支持向量机的最终模型仅与支持向量有关，即通过采用hinge损失函数仍保持了<strong><em>稀疏性</em></strong>。<br>除了hinge损失函数，也可以使用其他的替代损失函数，若使用对率损失函数替代(29)中的$0/1$损失函数，则几乎得到对率回归模型(参考：<a href="https://isaacchanghau.github.io/2017/05/04/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%91%A8%E5%BF%97%E5%8D%8E-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-2/">link</a>)。实际上，<strong><em>支持向量机与对率回归的优化目标相近，通常情况下它们的性能也相当</em></strong>。对率回归的优势主要在于其输出具有自然的概率意义，即在给出预测标记的同时也给出了概率，而支持向量机的输出不具有概率意义，欲得到概率输出需要特殊处理。此外，对率回归能直接用于多任务分类，支持向量机则需进行推广。<br>而从上图能看出，<strong><em>hinge损失有一块“平坦”的零区域，这使得支持向量机的解具有稀疏性</em></strong>，而对率损失是光滑的单调递减函数，不能导出类似的支持向量概念，因此对率回归的解依赖于更多的训练样本，其预测开销更大。将$0/1$损失函数换成别的替代函数可以得到不同的学习模型，这些模型的性质与所用的替代函数直接相关，但它们具有一个共性：优化目标中的第一项用来描述划分超平面的“间隔”大小，另一项$\sum_{i=1}^{m}\ell\big(f(\boldsymbol{x}_{i},y_{i})\big)$涌来表述训练集上的误差，可写为更一般的形式$$<br>\min_{f}\Omega(f)+C\sum_{i=1}^{m}\ell\big(f(\boldsymbol{x}_{i},y_{i})\big)\tag{42}<br>$$其中$\Omega(f)$称为<strong><em>“结构风险”</em></strong> (structural risk)，用于描述模型$f$的某些性质；第二项$\sum_{i=1}^{m}\ell\big(f(\boldsymbol{x}_{i},y_{i})\big)$称为<strong><em>“经验风险”</em></strong> (empirical risk)，用于描述模型与训练数据的契合程度。$C$用于对二者进行折中。从经验风险最小化的角度来看，$\Omega(f)$表述了我们希望获得具有何种性质的模型(例如希望获得复杂度较小的模型)，这为引入领域知识和用户意图提供了途径；另一方面，该信息有助于消减假设空间，从而降低了最小化训练误差的过拟合风险。从这个角度来说，(42)称为“正则化” (regularization)问题，$\Omega(f)$称为正则化项，$C$则称为正则化常数。$\boldsymbol{L}_{p}$范数(norm)是常用的正则化项，其中$\boldsymbol{L}_{2}$范数$\Vert\boldsymbol{w}\Vert_{2}$倾向于$\boldsymbol{w}$的分量取值尽量均衡，即非零分量个数尽量稠密，而$\boldsymbol{L}_{0}$范数$\Vert\boldsymbol{w}\Vert_{0}$和$\boldsymbol{L}_{1}$范数$\Vert\boldsymbol{w}\Vert_{1}$则倾向于$boldsymbol{w}$的分量尽量稀疏，即非零分量个数尽量少。</p><h1 id="支持向量回归"><a href="#支持向量回归" class="headerlink" title="支持向量回归"></a>支持向量回归</h1><p>给定训练样本 $D=\{(\boldsymbol{x}_{1},y_{1}),\dots,(\boldsymbol{x}_{m},y_{m})\}$，$y_{i}\in\mathbb{R}$，我们希望学得一个形如式(7)的回归模型，使得$f(\boldsymbol{x})$与$y$尽可能的接近，$\boldsymbol{w}$和$b$是待定的模型参数。对于此问题，支持向量回归(Support Vector Regression，SVR)假设能容忍$f(\boldsymbol{x})$与$y$之间最多有$\epsilon$的偏差，即仅当$f(\boldsymbol{x})$与$y$之间的差别绝对值大于$\epsilon$时才计算损失。如下图所示，相当于以$f(\boldsymbol{x})$为中心，构建一个宽度为$2\epsilon$的间隔带，若样本落入此间隔带，则认为是被预测正确的。<br><img src="/images/machinelearning/svm/5.png" alt="5.png"><br>于是，SVR问题可形式化为$$<br>\min_{\boldsymbol{w},b}\frac{1}{2}\Vert\boldsymbol{w}\Vert^{2}+C\sum_{i=1}^{m}\ell_{\epsilon}\big(f(\boldsymbol{x}_{i})-y_{i}\big)\tag{43}<br>$$其中$C$为正则化常数，$\ell_{\epsilon}$是如下图所示的$\epsilon$-不敏感损失($\epsilon$-insensitive loss)函数$$<br>\ell_{\epsilon}(z)=\begin{cases}<br>0, &amp; ivertz\vert\leq\epsilon;<br>\vert z\vert-\epsilon, &amp; otherwise.<br>\end{cases}\tag{44}$$<br><img src="/images/machinelearning/svm/6.png" alt="6.png"><br>引入松弛变量$\xi_{i}$和$\hat{\xi}_{i}$，可将(43)重写为$$<br>\min_{\boldsymbol{w},b,\xi_{i},\hat{\xi}_{i}}\frac{1}{2}\Vert\boldsymbol{w}\Vert^{2}+C\sum_{i=1}^{m}(\xi_{i}+\hat{\xi}_{i})\tag{45}$$$$\begin{aligned}<br>s.t.\quad &amp; f(\boldsymbol{x}_{i})-y_{i}\leq\epsilon+\xi_{i},\\<br>&amp; y_{i}-f(\boldsymbol{x}_{i})\leq\epsilon+\hat{\xi}_{i},\\<br>&amp; \xi_{i}\geq 0,\quad\hat{\xi}_{i}\geq 0,\quad i=1,2,\dots,m<br>\end{aligned}$$类似(36)，通过引入拉格朗日乘子 $\mu_{i}\geq 0$，$\hat{\mu}_{i}\geq 0$，$\alpha_{i}\geq 0$，$\hat{\alpha}_{i}\geq 0$，由拉格朗日乘子法可得到(45)的拉格朗日函数$$\begin{aligned}<br>&amp; L(\boldsymbol{w},b,\boldsymbol{\alpha},\hat{\boldsymbol{\alpha}},\boldsymbol{\xi},\hat{\boldsymbol{\xi}},\boldsymbol{\mu},\hat{\boldsymbol{\mu}})\\<br>&amp; =\frac{1}{2}\Vert\boldsymbol{w}\Vert^{2}+C\sum_{i=1}^{m}(\xi_{i}+\hat{\xi}_{i})-\sum_{i=1}^{m}\mu_{i}\xi_{i}-\sum_{i=1}^{m}\hat{\mu}_{i}\hat{\xi}_{i}\\<br>&amp; +\sum_{i=1}^{m}\alpha_{i}\big(f(\boldsymbol{x}_{i})-y_{i}-\epsilon-\xi_{i}\big)+\sum_{i=1}^{m}\hat{\alpha}_{i}\big(y_{i}-f(\boldsymbol{x}_{i})-\epsilon-\hat{\xi}_{i}\big)<br>\end{aligned}\tag{46}$$将(7)代入，再令$L(\boldsymbol{w},b,\boldsymbol{\alpha},\hat{\boldsymbol{\alpha}},\boldsymbol{\xi},\hat{\boldsymbol{\xi}},\boldsymbol{\mu},\hat{\boldsymbol{\mu}})$对$\boldsymbol{w}$，$b$，$\xi_{i}$ 和 $\hat{\xi}_{i}$ 的偏导为零可得$$<br>\boldsymbol{w}=\sum_{i=1}^{m}(\hat{\alpha}_{i}-\alpha_{i})\boldsymbol{x}_{i}\tag{47}<br>$$$$<br>0=\sum_{i=1}^{m}(\hat{\alpha}_{i}-\alpha_{i})\tag{48}<br>$$$$<br>C=\alpha_{i}+\mu_{i}\tag{49}<br>$$$$<br>C=\hat{\alpha}_{i}+\hat{\mu}_{i}\tag{50}<br>$$将上述四个式子代入(46)可得到SVR的对偶问题$$<br>\begin{aligned}<br>&amp; \max_{\boldsymbol{\alpha},\hat{\boldsymbol{\alpha}}}\sum_{i=1}^{m}y_{i}(\hat{\alpha}_{i}-\alpha_{i})-\epsilon(\hat{\alpha}_{i}+\alpha_{i})-\frac{1}{2}\sum_{i=1}^{m}\sum_{j=1}^{m}(\hat{\alpha}_{i}-\alpha_{i})(\hat{\alpha}_{j}-\alpha_{j})\boldsymbol{x}_{i}^{T}\boldsymbol{x}_{j}\\<br>&amp; s.t.\quad\sum_{i=1}^{m}(\hat{\alpha}_{i}-\alpha_{i})=0,\quad 0\leq\alpha_{i},\hat{\alpha}_{i}\leq C.<br>\end{aligned}<br>\tag{51}<br>$$上述过程需满足KKT条件，即$$<br>\begin{cases}<br>\alpha_{i}\big(f(\boldsymbol{x}_{i})-y_{i}-\epsilon-\xi_{i}\big)=0,\\<br>\hat{\alpha}_{i}\big(y_{i}-f(\boldsymbol{x}_{i})-\epsilon-\hat{\xi}_{i}\big)=0,\\<br>\alpha_{i}\hat{\alpha}_{i}=0,\quad\xi_{i}\hat{\xi}_{i}=0,\\<br>(C-\alpha_{i})\xi_{i}=0,\quad(C-\hat{\alpha}_{i})\hat{\xi}_{i}=0.<br>\end{cases}\tag{52}<br>$$可以看出，当且仅当$f(\boldsymbol{x}_{i})-y_{i}-\epsilon-\xi_{i}=0$时$\alpha_{i}$能取非零值，当且仅当$y_{i}-f(\boldsymbol{x}_{i})-\epsilon-\hat{\xi}_{i}=0$时$\hat{\alpha}_{i}$能取非零值。换言之仅当样本$(\boldsymbol{x}_{i},y_{i})$不落入$\epsilon$-间隔带中，相应的$\alpha_{i}$和$\hat{\alpha}_{i}$能去非零值。此外，这两个约束不能同时成立，即$\alpha_{i}$和$\hat{\alpha}_{i}$中至少有一个为零。将(47)代入(7)，则SVR的解形如$$<br>f(\boldsymbol{x})=\sum_{i=1}^{m}(\hat{\alpha}_{i}-\alpha_{i})\boldsymbol{x}_{i}^{T}\boldsymbol{x}+b\tag{53}<br>$$使(53)中的$(\hat{\alpha}_{i}-\alpha_{i})\neq 0$的样本即为SVR的支持向量，它们必须落在$\epsilon$-间隔带之外。显然，SVR只是的支持向量只是训练样本的一部分，即其解仍具有稀疏性。而由KKT条件(52)可看出，对于每个样本$(\boldsymbol{x}_{i},y_{i})$都有$(C-\alpha_{i})\xi_{i}=0$且$\alpha_{i}\big(f(\boldsymbol{x}_{i})-y_{i}-\epsilon-\xi_{i}\big)=0$。于是，在得到$\alpha_{i}$后，若$0&lt;\alpha_{i}&lt; C$，则必有$\xi_{i}=0$，进而有$$<br>b=y_{i}+\epsilon-\sum_{i=1}^{m}(\hat{\alpha}_{i}-\alpha_{i})\boldsymbol{x}_{i}^{T}\boldsymbol{x}\tag{54}<br>$$因此，在求解(51)得到$\alpha_{i}$后，理论上来说，可任意选取满足$0&lt;\alpha_{i}&lt; C$的样本通过(54)求得$b$。实际中常采用更加鲁棒的方法：选取多个(或所有)满足该条件的样本求解$b$后取平均值。若考虑特征映射形式(19)，则相应的(47)将形如$$<br>\boldsymbol{w}=\sum_{i=1}^{m}(\hat{\alpha}_{i}-\alpha_{i})\phi(\boldsymbol{x}_{i})\tag{55}<br>$$将(55)代入(19)，则SVR可表示为$$<br>f(\boldsymbol{x})=\sum_{i=1}^{m}(\hat{\alpha}_{i}-\alpha_{i})\kappa(\boldsymbol{x},\boldsymbol{x}_{i})+b\tag{56}<br>$$其中，$\kappa(\boldsymbol{x}_{i},\boldsymbol{x}_{j})=\phi(\boldsymbol{x}_{i})^{T}\phi(\boldsymbol{x}_{j}))$为核函数。</p><h1 id="核方法"><a href="#核方法" class="headerlink" title="核方法"></a>核方法</h1><p>根据(24)和(56)，给定训练样本$\{(\boldsymbol{x}_{1},y_{1}),\dots,(\boldsymbol{x}_{m},y_{m})\}$，若不考虑偏移项$b$，无论SVM还是SVR，学得的模型总能表述成$\kappa(\boldsymbol{x},\boldsymbol{x}_{i})$的线性组合。<br><strong><em>定理2 表示定理(representer theorem)</em></strong>：令 $\mathbb{H}$ 为核函数 $\kappa$ 对应的再生核希尔伯特空间，$\Vert h\Vert_{\mathbb{H}}$ 表示 $\mathbb{H}$ 空间中关于 $h$ 的范数，对于任意单调递增函数 $\Omega:[0,\infty]\mapsto\mathbb{R}$ 和任意非负损失函数 $\ell:\mathbb{R}^{m}\mapsto[0,\infty]$，优化问题$$<br>\min_{h\in\mathbb{H}}F(h)=\Omega\big(\Vert h\Vert_{\mathbb{H}}\big)+\ell\big(h(\boldsymbol{x}_{1}),\dots,h(\boldsymbol{x}_{m})\big)\tag{57}<br>$$的解总可写为$$<br>h^{*}(\boldsymbol{x})=\sum_{i=1}^{m}\alpha_{i}\kappa(\boldsymbol{x},h(\boldsymbol{x}_{i}))\tag{58}<br>$$表示定理对损失函数没有限制，对正则化项 $\Omega$ 仅要求单调递增，甚至不要求 $\Omega$ 是凸函数，意味着对于一般的损失函数正则项，优化问题(57)的最优解 $h^{*}(\boldsymbol{x})$ 都可表示为核函数 $\kappa(\boldsymbol{x},\boldsymbol{x}_{i})$ 的线性组合。<br>通常将基于核函数的学习方法统称为<strong><em>“核方法”</em></strong> (kernel methods)。如通过“核化” (即引入核函数)来将线性学习器拓展为非线性学习器。以<strong><em>“核线性判别分析”</em></strong> (<a href="https://en.wikipedia.org/wiki/Kernel_Fisher_discriminant_analysis#Kernel_trick_with_LDA" target="_blank" rel="noopener">Kernelized Linear Discriminant Analysis</a>，KLDA) 为例，假设可通过某种映射 $\phi:\boldsymbol{\chi}\mapsto\mathbb{F}$ 将样本映射到一个特征空间 $\mathbb{F}$，然后在 $\mathbb{F}$ 中执行线性判别分析，以求得$$<br>h(\boldsymbol{x})=\boldsymbol{w}^{T}\phi(\boldsymbol{x})\tag{59}<br>$$类似<a href="https://en.wikipedia.org/wiki/Linear_discriminant_analysis" target="_blank" rel="noopener">LDA</a>，KLDA的学习目标是$$<br>\max_{\boldsymbol{w}}J(\boldsymbol{w})=\frac{\boldsymbol{w}^{T}\boldsymbol{S}_{b}^{\phi}\boldsymbol{w}}{\boldsymbol{w}^{T}\boldsymbol{S}_{\boldsymbol{w}}^{\phi}\boldsymbol{w}}\tag{60}<br>$$其中 $\boldsymbol{S}_{b}^{\phi}$ 和 $\boldsymbol{S}_{\boldsymbol{w}}^{\phi}$ 分别为训练样本在特征空间 $\mathbb{F}$ 中的类间散度矩阵 (<a href="https://stats.stackexchange.com/questions/123490/what-is-the-correct-formula-for-between-class-scatter-matrix-in-lda" target="_blank" rel="noopener">between-class scatter matrix</a>) 和类内散度矩阵 (<a href="https://stats.stackexchange.com/questions/123490/what-is-the-correct-formula-for-between-class-scatter-matrix-in-lda" target="_blank" rel="noopener">within-class scatter matrix</a>)。令 $X_{i}$ 表示第 $i\in\{0,1\}$ 类样本的集合，其样本数为 $m_{i}$；总样本数 $m=m_{0}+m_{1}$。第 $i$ 类样本在特征空间 $\mathbb{F}$ 中的均值为$$<br>\boldsymbol{\mu}_{i}^{\phi}=\frac{1}{m_{i}}\sum_{\boldsymbol{x}\in X_{i}}\phi(\boldsymbol{x})\tag{61}<br>$$两个散度矩阵分别为$$<br>\boldsymbol{S}_{b}^{\phi}=(\boldsymbol{\mu}_{1}^{\phi}-\boldsymbol{\mu}_{0}^{\phi})(\boldsymbol{\mu}_{1}^{\phi}-\boldsymbol{\mu}_{0}^{\phi})^{T}\tag{62}<br>$$$$<br>\boldsymbol{S}_{\boldsymbol{w}}^{\phi}=\sum_{i=0}^{1}\sum_{\boldsymbol{x}\in X_{i}}\big(\phi(\boldsymbol{x})-\boldsymbol{\mu}_{i}^{\phi}\big)\big(\phi(\boldsymbol{x})-\boldsymbol{\mu}_{i}^{\phi}\big)^{T}\tag{63}<br>$$通常映射 $\phi$ 的具体形式很难得到，因此使用核函数 $\kappa(\boldsymbol{x},\boldsymbol{x}_{i})=\phi(\boldsymbol{x}_{i})^{T}\phi(\boldsymbol{x})$ 来隐式地表达这个映射和特征空间 $\mathbb{F}$。把 $J(\boldsymbol{w})$ 作为(57) 中的损失函数 $\ell$，再令 $\Omega=0$，由表示定理，函数 $h(\boldsymbol{x})$ 可写为$$<br>h(\boldsymbol{x})=\sum_{i=1}^{m}\alpha_{i}\kappa(\boldsymbol{x},\boldsymbol{x}_{i})\tag{64}<br>$$于是由(59)可得$$<br>\boldsymbol{w}=\sum_{i=1}^{m}\alpha_{i}\phi(\boldsymbol{x}_{i})\tag{65}<br>$$令 $\boldsymbol{K}\in\mathbb{R}^{m\times m}$ 为核函数 $\kappa$ 所对应的核矩阵，$(\boldsymbol{K})_{ij}=\kappa(\boldsymbol{x}_{i},\boldsymbol{x}_{j})$。令 $\boldsymbol{1}_{i}\in\{1,0\}^{m\times 1}$ 为第 $i$ 类样本的指示向量，即 $\boldsymbol{1}_{i}$ 的第 $j$ 个分量为1当且仅当 $\boldsymbol{x}_{j}\in X_{i}$，否则 $\boldsymbol{1}_{i}$ 的第 $j$ 个分量为0.再令$$<br>\hat{\boldsymbol{\mu}}_{0}=\frac{1}{m_{0}}\boldsymbol{K}\boldsymbol{1}_{0}\tag{66}<br>$$$$<br>\hat{\boldsymbol{\mu}}_{1}=\frac{1}{m_{1}}\boldsymbol{K}\boldsymbol{1}_{1}\tag{67}<br>$$$$<br>\boldsymbol{M}=(\hat{\boldsymbol{\mu}}_{0}-\hat{\boldsymbol{\mu}}_{1})(\hat{\boldsymbol{\mu}}_{0}-\hat{\boldsymbol{\mu}}_{1})^{T}\tag{68}<br>$$$$<br>\boldsymbol{N}=\boldsymbol{K}\boldsymbol{K}^{T}-\sum_{i=0}^{1}m_{i}\hat{\boldsymbol{\mu}}_{i}\hat{\boldsymbol{\mu}}_{i}^{T}\tag{69}<br>$$于是(60)等价为$$<br>\max_{\boldsymbol{\alpha}}J(\boldsymbol{\alpha})=\frac{\boldsymbol{\alpha}^{T}\boldsymbol{M}\boldsymbol{\alpha}}{\boldsymbol{\alpha}^{T}\boldsymbol{N}\boldsymbol{\alpha}}\tag{70}<br>$$显然，使用线性判别分析求解方法即可得到 $\boldsymbol{\alpha}$，进而可由(64)得到投影函数 $h(\boldsymbol{x})$。</p><h1 id="习题"><a href="#习题" class="headerlink" title="习题"></a>习题</h1><p><strong>6.1 试证明样本空间中任意点 $\boldsymbol{x}$ 到超平面 $(\boldsymbol{w},b)$的距离为公式(2)。</strong><br>Ans: 超平面 $(\boldsymbol{w},b)$ 的平面法向量为 $\boldsymbol{w}$ ，任取平面上一点 $\boldsymbol{x}_{0}$，有 $\boldsymbol{w}^{T}\boldsymbol{x}_{0}+b=0$。$\boldsymbol{x}$ 到平面的距离就是 $\boldsymbol{x}$ 到 $\boldsymbol{x}_{0}$ 的距离往 $\boldsymbol{w}$ 方向的投影，就是 $\frac{\vert\boldsymbol{w}^{T}(\boldsymbol{x}-\boldsymbol{x}_{0})\vert}{\vert\boldsymbol{w}\vert}=\frac{\vert\boldsymbol{w}^{T}\boldsymbol{x}+b\vert}{\vert\boldsymbol{w}\vert}$。</p><p><strong>6.2 试使用LIBSVM，在<a href="https://isaacchanghau.github.io/2017/05/04/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%91%A8%E5%BF%97%E5%8D%8E-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-2/">西瓜数据集3.0$\alpha$</a>上分别用线性核和高斯核训练一个SVM，并比较其支持向量的差别。</strong><br>Ans: 由于西瓜数据集3.0$\alpha$数据量太小，我们这里直接采用<a href="https://archive.ics.uci.edu/ml/datasets.html" target="_blank" rel="noopener">UCI数据集</a>的<a href="https://archive.ics.uci.edu/ml/datasets/iris" target="_blank" rel="noopener">Iris数据集</a>，并使用<a href="http://www.cs.waikato.ac.nz/ml/weka/" target="_blank" rel="noopener">Weka机器学习包</a>进行试验。首先下载数据并转换为ARFF格式，如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">@relation Iris</span><br><span class="line">@attribute sepallength NUMERIC</span><br><span class="line">@attribute sepalwidth NUMERIC</span><br><span class="line">@attribute petallength NUMERIC </span><br><span class="line">@attribute petalwidth NUMERIC</span><br><span class="line">@attribute class &#123;setosa, versicolor, virginica&#125;</span><br><span class="line">@data</span><br><span class="line">5.1,3.5,1.4,0.2,setosa</span><br><span class="line">4.9,3.0,1.4,0.2,setosa</span><br><span class="line">4.7,3.2,1.3,0.2,setosa</span><br><span class="line">4.6,3.1,1.5,0.2,setosa</span><br><span class="line">5.0,3.6,1.4,0.2,setosa</span><br><span class="line">...</span><br></pre></td></tr></table></figure></p><p>之后读入数据：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Instances data = <span class="keyword">new</span> Instances(<span class="keyword">new</span> FileReader(<span class="string">"src/main/resources/iris.data"</span>));</span><br><span class="line">data.setClassIndex(data.numAttributes() - <span class="number">1</span>); <span class="comment">// set label</span></span><br><span class="line">data.randomize(<span class="keyword">new</span> Random(<span class="number">123</span>)); <span class="comment">// shuffling data</span></span><br></pre></td></tr></table></figure></p><p>现在我们使用Weka机器学习包的<a href="https://weka.wikispaces.com/LibSVM" target="_blank" rel="noopener">LibSVM</a>分别构建线性核和高斯核的SVM并训练数据：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// build linear SVM</span></span><br><span class="line">LibSVM linearSVM = <span class="keyword">new</span> LibSVM();</span><br><span class="line">linearSVM.setSVMType(<span class="keyword">new</span> SelectedTag(LibSVM.SVMTYPE_C_SVC, LibSVM.TAGS_SVMTYPE));</span><br><span class="line">linearSVM.setKernelType(<span class="keyword">new</span> SelectedTag(LibSVM.KERNELTYPE_LINEAR, LibSVM.TAGS_KERNELTYPE));</span><br><span class="line">linearSVM.buildClassifier(data); <span class="comment">// build classifier and train</span></span><br><span class="line"><span class="comment">// build gaussian SVM</span></span><br><span class="line">LibSVM gaussSVM = <span class="keyword">new</span> LibSVM();</span><br><span class="line">gaussSVM.setSVMType(<span class="keyword">new</span> SelectedTag(LibSVM.SVMTYPE_C_SVC, LibSVM.TAGS_SVMTYPE));</span><br><span class="line">gaussSVM.setKernelType(<span class="keyword">new</span> SelectedTag(LibSVM.KERNELTYPE_RBF, LibSVM.TAGS_KERNELTYPE));</span><br><span class="line">gaussSVM.buildClassifier(data); <span class="comment">// build classifier and train</span></span><br></pre></td></tr></table></figure></p><p>由于Weka封装了LibSVM，无法直接使用Weka得到训练后模型的支持向量，这里我们使用<a href="https://gist.github.com/DavidWiesner/2de8a6b2b89ffeaa6d0f" target="_blank" rel="noopener">如下方法</a>得到训练后的<a href="https://github.com/cjlin1/libsvm/blob/master/java/libsvm/svm_model.java" target="_blank" rel="noopener">模型</a>参数：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> svm_model <span class="title">getModel</span><span class="params">(LibSVM svm)</span> <span class="keyword">throws</span> IllegalAccessException, NoSuchFieldException </span>&#123;</span><br><span class="line">    Field modelField = svm.getClass().getDeclaredField(<span class="string">"m_Model"</span>);</span><br><span class="line">    modelField.setAccessible(<span class="keyword">true</span>);</span><br><span class="line">    <span class="keyword">return</span> (svm_model) modelField.get(svm);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>我们使用上面的函数来获得训练后的模型，并打印支持向量<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">svm_model linearModel = getModel(linearSVM);</span><br><span class="line">svm_model gaussModel = getModel(gaussSVM);</span><br><span class="line"><span class="keyword">int</span>[] indices = linearModel.sv_indices;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i : indices) System.out.println(i + <span class="string">": "</span> + data.instance(i - <span class="number">1</span>));</span><br><span class="line">System.out.println();</span><br><span class="line">indices = gaussModel.sv_indices;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i : indices) System.out.println(i + <span class="string">": "</span> + data.instance(i - <span class="number">1</span>));</span><br></pre></td></tr></table></figure></p><p>于是我们得到线性核和高斯核模型的支持向量：</p><table><thead><tr><th style="text-align:left">线性核</th><th style="text-align:left">高斯核 </th></tr></thead><tbody><tr><td style="text-align:left">4: 5.4,3,4.5,1.5,versicolor</td><td style="text-align:left">4: 5.4,3,4.5,1.5,versicolor</td></tr><tr><td style="text-align:left">5: 6.3,2.5,4.9,1.5,versicolor</td><td style="text-align:left">5: 6.3,2.5,4.9,1.5,versicolor</td></tr><tr><td style="text-align:left">37: 6.8,2.8,4.8,1.4,versicolor</td><td style="text-align:left">13: 4.9,2.4,3.3,1,versicolor</td></tr><tr><td style="text-align:left">42: 6.7,3,5,1.7,versicolor</td><td style="text-align:left">37: 6.8,2.8,4.8,1.4,versicolor</td></tr><tr><td style="text-align:left">45: 6.2,2.2,4.5,1.5,versicolor</td><td style="text-align:left">41: 6.7,3.1,4.7,1.5,versicolor</td></tr><tr><td style="text-align:left">61: 6.3,3.3,4.7,1.6,versicolor</td><td style="text-align:left">42: 6.7,3,5,1.7,versicolor</td></tr><tr><td style="text-align:left">63: 5.6,3,4.5,1.5,versicolor</td><td style="text-align:left">45: 6.2,2.2,4.5,1.5,versicolor</td></tr><tr><td style="text-align:left">99: 5.9,3.2,4.8,1.8,versicolor</td><td style="text-align:left">61: 6.3,3.3,4.7,1.6,versicolor</td></tr><tr><td style="text-align:left">104: 6,2.7,5.1,1.6,versicolor</td><td style="text-align:left">63: 5.6,3,4.5,1.5,versicolor</td></tr><tr><td style="text-align:left">108: 6.1,2.9,4.7,1.4,versicolor</td><td style="text-align:left">77: 6.5,2.8,4.6,1.5,versicolor</td></tr><tr><td style="text-align:left">124: 6.9,3.1,4.9,1.5,versicolor</td><td style="text-align:left">82: 6,2.9,4.5,1.5,versicolor</td></tr><tr><td style="text-align:left">126: 5.1,2.5,3,1.1,versicolor</td><td style="text-align:left">99: 5.9,3.2,4.8,1.8,versicolor</td></tr><tr><td style="text-align:left">7: 6.3,2.5,5,1.9,virginica</td><td style="text-align:left">104: 6,2.7,5.1,1.6,versicolor</td></tr><tr><td style="text-align:left">15: 4.9,2.5,4.5,1.7,virginica</td><td style="text-align:left">108: 6.1,2.9,4.7,1.4,versicolor</td></tr><tr><td style="text-align:left">21: 7.2,3,5.8,1.6,virginica</td><td style="text-align:left">113: 7,3.2,4.7,1.4,versicolor</td></tr><tr><td style="text-align:left">33: 6,3,4.8,1.8,virginica</td><td style="text-align:left">124: 6.9,3.1,4.9,1.5,versicolor</td></tr><tr><td style="text-align:left">39: 6.3,2.8,5.1,1.5,virginica</td><td style="text-align:left">126: 5.1,2.5,3,1.1,versicolor</td></tr><tr><td style="text-align:left">57: 6.3,2.7,4.9,1.8,virginica</td><td style="text-align:left">146: 6,3.4,4.5,1.6,versicolor</td></tr><tr><td style="text-align:left">64: 6.5,3,5.2,2,virginica</td><td style="text-align:left">148: 5,2,3.5,1,versicolor</td></tr><tr><td style="text-align:left">72: 6.5,3.2,5.1,2,virginica</td><td style="text-align:left">7: 6.3,2.5,5,1.9,virginica</td></tr><tr><td style="text-align:left">90: 6.1,3,4.9,1.8,virginica</td><td style="text-align:left">15: 4.9,2.5,4.5,1.7,virginica</td></tr><tr><td style="text-align:left">98: 5.9,3,5.1,1.8,virginica</td><td style="text-align:left">21: 7.2,3,5.8,1.6,virginica</td></tr><tr><td style="text-align:left">139: 6.2,2.8,4.8,1.8,virginica</td><td style="text-align:left">25: 6.1,2.6,5.6,1.4,virginica</td></tr><tr><td style="text-align:left">147: 6,2.2,5,1.5,virginica</td><td style="text-align:left">33: 6,3,4.8,1.8,virginica</td></tr><tr><td style="text-align:left">56: 4.5,2.3,1.3,0.3,setosa</td><td style="text-align:left">39: 6.3,2.8,5.1,1.5,virginica</td></tr><tr><td style="text-align:left">89: 5.1,3.3,1.7,0.5,setosa</td><td style="text-align:left">48: 6.3,3.3,6,2.5,virginica</td></tr><tr><td style="text-align:left">117: 4.8,3.4,1.9,0.2,setosa</td><td style="text-align:left">57: 6.3,2.7,4.9,1.8,virginica</td></tr><tr><td style="text-align:left">done</td><td style="text-align:left">…</td></tr><tr><td style="text-align:left">Total: 27</td><td style="text-align:left">Total: 45 </td></tr></tbody></table><p><strong>6.3 选择两个<a href="http://archive.ics.uci.edu/ml/" target="_blank" rel="noopener">UCI</a>数据集，分别用线性核和高斯核训练一个SVM，并与BP神经网络核C4.5决策树进行试验比较。</strong><br>Ans: 这里也选用UCI的Iris数据集进行测试。首先我们将数据拆分为训练集(80%)和测试集(20%)：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// load train dataset</span></span><br><span class="line">Instances trainData = <span class="keyword">new</span> Instances(<span class="keyword">new</span> FileReader(<span class="string">"src/main/resources/iris-train.data"</span>));</span><br><span class="line">trainData.setClassIndex(trainData.numAttributes() - <span class="number">1</span>);</span><br><span class="line">trainData.randomize(<span class="keyword">new</span> Random(<span class="number">123</span>));</span><br><span class="line"><span class="comment">// load test dataset</span></span><br><span class="line">Instances testData = <span class="keyword">new</span> Instances(<span class="keyword">new</span> FileReader(<span class="string">"src/main/resources/iris-test.data"</span>));</span><br><span class="line">testData.setClassIndex(testData.numAttributes() - <span class="number">1</span>);</span><br></pre></td></tr></table></figure></p><p>之后我们用Weka机器学习包分别构建线性核和高斯核的SVM、C4.5决策树和BP神经网络，如下(SVM的构建与6.2相同，这里省略)：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// build C4.5 decision tree</span></span><br><span class="line">J48 tree = <span class="keyword">new</span> J48();</span><br><span class="line">tree.setUnpruned(<span class="keyword">false</span>);</span><br><span class="line">tree.setReducedErrorPruning(<span class="keyword">true</span>);</span><br><span class="line">tree.buildClassifier(trainData);</span><br><span class="line"><span class="comment">// build BP neural networks</span></span><br><span class="line">MultilayerPerceptron mlp = <span class="keyword">new</span> MultilayerPerceptron();</span><br><span class="line">mlp.setLearningRate(<span class="number">0.3</span>);</span><br><span class="line">mlp.setMomentum(<span class="number">0.2</span>);</span><br><span class="line">mlp.setSeed(<span class="number">123</span>);</span><br><span class="line">mlp.setHiddenLayers(<span class="string">"3"</span>);</span><br><span class="line">mlp.setTrainingTime(<span class="number">10000</span>);</span><br><span class="line">mlp.buildClassifier(trainData);</span><br></pre></td></tr></table></figure></p><p>之后我们使用<code>Evaluation</code>对构建的四个模型分别进行评价：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Evaluate</span></span><br><span class="line">Evaluation evalLinear = <span class="keyword">new</span> Evaluation(testData);</span><br><span class="line">evalLinear.evaluateModel(linearSVM, testData);</span><br><span class="line">System.out.println(<span class="string">"Linear SVM"</span> + <span class="string">"\n"</span> + evalLinear.toSummaryString() + <span class="string">"\n"</span> + evalLinear.toMatrixString() + <span class="string">"\n"</span>);</span><br><span class="line">Evaluation evalGauss = <span class="keyword">new</span> Evaluation(testData);</span><br><span class="line">evalGauss.evaluateModel(gaussSVM, testData);</span><br><span class="line">System.out.println(<span class="string">"Gaussian SVM"</span> + <span class="string">"\n"</span> + evalGauss.toSummaryString() + <span class="string">"\n"</span> + evalGauss.toMatrixString() + <span class="string">"\n"</span>);</span><br><span class="line">Evaluation evalTree = <span class="keyword">new</span> Evaluation(testData);</span><br><span class="line">evalTree.evaluateModel(tree, testData);</span><br><span class="line">System.out.println(<span class="string">"C4.5 Decision Tree"</span> + <span class="string">"\n"</span> + evalTree.toSummaryString() + <span class="string">"\n"</span> + evalTree.toMatrixString() + <span class="string">"\n"</span>);</span><br><span class="line">Evaluation evalMlp = <span class="keyword">new</span> Evaluation(testData);</span><br><span class="line">evalMlp.evaluateModel(mlp, testData);</span><br><span class="line">System.out.println();</span><br><span class="line">System.out.println(<span class="string">"BP Neural Network"</span> + <span class="string">"\n"</span> + evalMlp.toSummaryString() + <span class="string">"\n"</span> + evalMlp.toMatrixString());</span><br></pre></td></tr></table></figure></p><p>得到如下结果：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line">Linear SVM</span><br><span class="line">Correctly Classified Instances          30              100      %</span><br><span class="line">Incorrectly Classified Instances         0                0      %</span><br><span class="line">Kappa statistic                          1     </span><br><span class="line">Mean absolute error                      0     </span><br><span class="line">Root mean squared error                  0     </span><br><span class="line">Relative absolute error                  0      %</span><br><span class="line">Root relative squared error              0      %</span><br><span class="line">Total Number of Instances               30     </span><br><span class="line">=== Confusion Matrix ===</span><br><span class="line">  a  b  c   &lt;-- classified as</span><br><span class="line"> 10  0  0 |  a = setosa</span><br><span class="line">  0 10  0 |  b = versicolor</span><br><span class="line">  0  0 10 |  c = virginica</span><br><span class="line"></span><br><span class="line">Gaussian SVM</span><br><span class="line">Correctly Classified Instances          30              100      %</span><br><span class="line">Incorrectly Classified Instances         0                0      %</span><br><span class="line">Kappa statistic                          1     </span><br><span class="line">Mean absolute error                      0     </span><br><span class="line">Root mean squared error                  0     </span><br><span class="line">Relative absolute error                  0      %</span><br><span class="line">Root relative squared error              0      %</span><br><span class="line">Total Number of Instances               30     </span><br><span class="line">=== Confusion Matrix ===</span><br><span class="line">  a  b  c   &lt;-- classified as</span><br><span class="line"> 10  0  0 |  a = setosa</span><br><span class="line">  0 10  0 |  b = versicolor</span><br><span class="line">  0  0 10 |  c = virginica</span><br><span class="line"></span><br><span class="line">C4.5 Decision Tree</span><br><span class="line">Correctly Classified Instances          29               96.6667 %</span><br><span class="line">Incorrectly Classified Instances         1                3.3333 %</span><br><span class="line">Kappa statistic                          0.95  </span><br><span class="line">Mean absolute error                      0.0466</span><br><span class="line">Root mean squared error                  0.149 </span><br><span class="line">Relative absolute error                 10.4945 %</span><br><span class="line">Root relative squared error             31.6147 %</span><br><span class="line">Total Number of Instances               30     </span><br><span class="line">=== Confusion Matrix ===</span><br><span class="line">  a  b  c   &lt;-- classified as</span><br><span class="line">  9  1  0 |  a = setosa</span><br><span class="line">  0 10  0 |  b = versicolor</span><br><span class="line">  0  0 10 |  c = virginica</span><br><span class="line"></span><br><span class="line">BP Neural Network</span><br><span class="line">Correctly Classified Instances          30              100      %</span><br><span class="line">Incorrectly Classified Instances         0                0      %</span><br><span class="line">Kappa statistic                          1     </span><br><span class="line">Mean absolute error                      0.0062</span><br><span class="line">Root mean squared error                  0.0121</span><br><span class="line">Relative absolute error                  1.3956 %</span><br><span class="line">Root relative squared error              2.5607 %</span><br><span class="line">Total Number of Instances               30     </span><br><span class="line">=== Confusion Matrix ===</span><br><span class="line">  a  b  c   &lt;-- classified as</span><br><span class="line"> 10  0  0 |  a = setosa</span><br><span class="line">  0 10  0 |  b = versicolor</span><br><span class="line">  0  0 10 |  c = virginica</span><br></pre></td></tr></table></figure></p><p><strong>6.4 试讨论线性判别分析与线性核支持向量机在何种条件下等价。</strong><br>Ans: 当线性SVM和LDA求出的 $\boldsymbol{w}$ 互相垂直时，两者是等价的，SVM此时也就比LDA多了个偏移项$b$。因为首先，如果可以使用软间隔的线性SVM，其实线性可分这个条件是不必要的，如果是硬间隔线性SVM，那么线性可分是必要条件。这个题只说了是线性SVM，就没必要关心数据是不是可分，毕竟LDA是都可以处理的。其次假如当前样本线性可分，且SVM与LDA求出的结果相互垂直。当SVM的支持向量固定时，再加入新的样本，并不会改变求出的w，但是新加入的样本会改变原类型数据的协方差和均值，从而导致LDA求出的结果发生改变。这个时候两者的 $\boldsymbol{w}$ 就不垂直了，但是数据依然是可分的。 </p><p><strong>6.5 试述高斯核SVM与RBF神经网络之间的联系。</strong><br>Ans: RBF网络的径向基函数与SVM都可以采用高斯核，也就分别得到了高斯核RBF网络与高斯核SVM。神经网络是最小化累计误差，将参数作为惩罚项，而SVM相反，主要是最小化参数，将误差作为惩罚项。在二分类问题中，如果将RBF中隐层数为样本个数，且每个样本中心就是样本参数，得出的RBF网络与核SVM基本等价，非支持向量将得到很小的$\boldsymbol{w}$。</p><p><strong>6.6 试分析SVM对噪声敏感的原因。</strong><br>Ans: SVM的目的是求出与支持向量有最大化距离的直线，以每个样本为圆心，该距离为半径做圆，可以近似认为圆内的点与该样本属于相同分类。如果出现了噪声，那么这个噪声所带来的错误分类也将最大化，所以SVM对噪声是很敏感的。</p><p><strong>6.7 试给出(52)的完整KKT条件。</strong><br>Ans: 非等式约束写成拉格朗日乘子式，取最优解要满足两个条件</p><ul><li>拉格朗日乘子式对所有非拉格朗日参数的一阶偏导为0。</li><li>非等式约束对应的拉格朗日项，要么非等式的等号成立，要么对应的拉格朗日参数为0。</li></ul><p>因此完整的KKT条件为$$\begin{cases}<br>\boldsymbol{w}=\sum_{i=1}^{m}(\hat{\alpha}_{i}-\alpha_{i})\boldsymbol{x}_{i},\\<br>0=\sum_{i=1}^{m}(\hat{\alpha}_{i}-\alpha_{i}),\\<br>C=\alpha_{i}+\mu_{i},\\<br>C=\hat{\alpha}_{i}+\hat{\mu}_{i},\\<br>\alpha_{i}\big(f(\boldsymbol{x}_{i})-y_{i}-\epsilon-\xi_{i}\big)=0,\\<br>\hat{\alpha}_{i}\big(y_{i}-f(\boldsymbol{x}_{i})-\epsilon-\hat{\xi}_{i}\big)=0,\\<br>(C-\alpha_{i})\xi_{i}=0,\\<br>(C-\hat{\alpha}_{i})\hat{\xi}_{i}=0.<br>\end{cases}$$</p><p><strong>6.8 以西瓜数据集3.0$\alpha$的“密度”为输入，“含糖率”为输出，试使用LIBSVM训练一个SVR。</strong><br>Ans: 首先将西瓜数据集转换为ARFF格式，然后搭建SVR进行试验。<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Instances data = <span class="keyword">new</span> Instances(<span class="keyword">new</span> FileReader(<span class="string">"src/main/resources/watermelon.data"</span>));</span><br><span class="line">data.setClassIndex(data.numAttributes() - <span class="number">1</span>);</span><br><span class="line">LibSVM svr = <span class="keyword">new</span> LibSVM();</span><br><span class="line">svr.setSVMType(<span class="keyword">new</span> SelectedTag(LibSVM.SVMTYPE_EPSILON_SVR, LibSVM.TAGS_SVMTYPE));</span><br><span class="line">svr.setKernelType(<span class="keyword">new</span> SelectedTag(LibSVM.KERNELTYPE_RBF, LibSVM.TAGS_KERNELTYPE));</span><br><span class="line">svr.buildClassifier(data);</span><br><span class="line"><span class="keyword">for</span> (Instance inst : data)  System.out.println(svr.classifyInstance(inst) + <span class="string">"\t"</span> + inst.classValue());</span><br><span class="line">Evaluation eval = <span class="keyword">new</span> Evaluation(data);</span><br><span class="line">eval.evaluateModel(svr, data);</span><br><span class="line">System.out.println(eval.toSummaryString());</span><br></pre></td></tr></table></figure></p><p>得到结果：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">0.23163745020672713 0.46</span><br><span class="line">0.24295566116372483 0.376</span><br><span class="line">0.22201597399900289 0.264</span><br><span class="line">0.21799998153899916 0.318</span><br><span class="line">0.2099638655436369  0.215</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">Correlation coefficient                  0.1952</span><br><span class="line">Mean absolute error                      0.0963</span><br><span class="line">Root mean squared error                  0.1142</span><br><span class="line">Relative absolute error                101.723  %</span><br><span class="line">Root relative squared error             98.1963 %</span><br><span class="line">Total Number of Instances               17</span><br></pre></td></tr></table></figure></p><p><strong>6.9 试使用核技巧推广对率回归，产生“核对率回归”。</strong><br>Ans: 由表示定理可知，一般优化问题的解可 $h(\boldsymbol{x})$ 以写成核函数的线性组合。即$$<br>h(\boldsymbol{x})=\sum_{i=1}^{m}\boldsymbol{w}\kappa(\boldsymbol{x},\boldsymbol{x}_{i})<br>$$可推出$$<br>\boldsymbol{w}=\sum_{i=1}^{m}\alpha_{i}\phi(\boldsymbol{x}_{i})<br>$$其中 $\phi(\boldsymbol{x})$ 是 $\boldsymbol{x}$ 在更高维的映射，于是 $\boldsymbol{w}^{T}\boldsymbol{x}+b$ 可以改写为$$<br>\sum_{i=1}^{m}\alpha_{i}\phi(\boldsymbol{x}_{i})\phi(\boldsymbol{x})=\sum_{i=1}^{m}\alpha_{i}\kappa(\boldsymbol{x},\boldsymbol{x}_{i})+b<br>$$令$\boldsymbol{\beta}=(\boldsymbol{\alpha};b)$，$\hat{\boldsymbol{t}}_{i}=(\kappa_{i};1)$，其中 $\kappa_{i}$ 表示核矩阵 $\kappa$ 的第 $i$ 列，可得$$<br>\mathcal{l}(\boldsymbol{\beta})=\sum_{i}\big(-y_{i}\boldsymbol{\beta}^{T}\boldsymbol{t}_{i}+\ln(1+e^{\boldsymbol{\beta}^{T}\boldsymbol{t}_{i}})\big)<br>$$之后的推导于线性回归中的<a href="https://isaacchanghau.github.io/2017/05/04/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%91%A8%E5%BF%97%E5%8D%8E-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-2/">对数几率回归</a>相同。</p><p><strong>6.10 试设计一个能显著减少SVM中支持向量的数目儿不显著降低泛化性能的方法。</strong><br>Ans: 对于线性的SVM，三个属性不完全一样的支持向量就能确定这个SVM，而其他的落在边缘上的点都可以舍弃。</p><p>注：以上习题的代码试验并没有考虑模型的优化问题，本人本身对Weka并不熟练，只是知道一些用法，所以直接使用Weka进行模型搭建和输出的演示。本人更习惯使用Spark的MLlib，但是由于MLlib对SVM的实现很少，只实现了线性二分类。没有非线性(核函数)，也没有多分类和回归，这也是无奈之举。(其实使用Python的Scikit-Learn机器学习包更方便)。</p><h1 id="Spark-MLlib实现二分类SVM"><a href="#Spark-MLlib实现二分类SVM" class="headerlink" title="Spark MLlib实现二分类SVM"></a>Spark MLlib实现二分类SVM</h1><p>这里我们同样使用Iris数据集，由于Spark MLlib 中的SVM模型只支持线性二分类，所以我们去掉<code>Iris-virginica</code>类型数据，只保留<code>Iris-versicolor</code>(记为1)和<code>Iris-setosa</code>(记为0)进行实验。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SVM</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">parsingIris</span> </span>(str: <span class="type">String</span>): <span class="type">LabeledPoint</span> = &#123; <span class="comment">// create Iris parser: CSV Data =&gt; LabeledPoint</span></span><br><span class="line">    <span class="keyword">val</span> fields = str.split(<span class="string">","</span>)</span><br><span class="line">    assert(fields.size == <span class="number">5</span>)</span><br><span class="line">    <span class="keyword">val</span> label = fields(<span class="number">4</span>) <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> <span class="string">"Iris-setosa"</span> =&gt; <span class="number">0.0</span></span><br><span class="line">      <span class="keyword">case</span> <span class="string">"Iris-versicolor"</span> =&gt; <span class="number">1.0</span></span><br><span class="line">      <span class="keyword">case</span> <span class="string">"Iris-virginica"</span> =&gt; <span class="number">2.0</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="type">LabeledPoint</span>.apply(label, <span class="type">Vectors</span>.dense(fields(<span class="number">0</span>).toDouble, fields(<span class="number">1</span>).toDouble, fields(<span class="number">2</span>).toDouble, fields(<span class="number">3</span>).toDouble))</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span> </span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="type">Logger</span>.getLogger(<span class="string">"org"</span>).setLevel(<span class="type">Level</span>.<span class="type">OFF</span>) <span class="comment">// close logger</span></span><br><span class="line">    <span class="comment">/** create Session */</span></span><br><span class="line">    <span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder().master(<span class="string">"local"</span>).appName(<span class="string">"SVM"</span>).getOrCreate()</span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line">    <span class="comment">/** load data and transform to dataframe */</span></span><br><span class="line">    <span class="keyword">val</span> iris : <span class="type">RDD</span>[<span class="type">LabeledPoint</span>] = spark.read.textFile(<span class="keyword">new</span> <span class="type">ClassPathResource</span>(<span class="string">"iris.txt"</span>).getFile.getAbsolutePath)</span><br><span class="line">      .map(parsingIris) <span class="comment">// convert Iris data to LabeledPoint</span></span><br><span class="line">      .filter(e =&gt; e.label != <span class="number">2.0</span>) <span class="comment">// drop third class to build a two classes data</span></span><br><span class="line">      .rdd <span class="comment">// convert to RDD</span></span><br><span class="line">    <span class="keyword">val</span> splits = iris.randomSplit(<span class="type">Array</span>(<span class="number">0.8</span>, <span class="number">0.2</span>), seed=<span class="number">123</span>L) <span class="comment">// 80% for training, 20% for testing</span></span><br><span class="line">    <span class="keyword">val</span> training = splits(<span class="number">0</span>).cache()</span><br><span class="line">    <span class="keyword">val</span> test = splits(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">val</span> model: <span class="type">SVMModel</span> = <span class="type">SVMWithSGD</span>.train(training, <span class="number">1000</span>) <span class="comment">// build model</span></span><br><span class="line">    model.clearThreshold() <span class="comment">// to get the score, not classified result</span></span><br><span class="line">    println(<span class="string">"Weight: "</span>.concat(model.weights.toString)) <span class="comment">// print trained weight</span></span><br><span class="line">    println(<span class="string">"Test Result: "</span>)</span><br><span class="line">    test.map(point =&gt; (model.predict(point.features), <span class="keyword">if</span>(model.predict(point.features) &gt; <span class="number">0</span>) <span class="number">1.0</span> <span class="keyword">else</span> <span class="number">0.0</span>, point.label)).foreach(println) <span class="comment">// print score, classified result and label</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>以下是分类结果：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">Weight: [-0.5139153688335307,-1.5350451605908375,2.1700813440361717,0.8963829165055544]</span><br><span class="line">Test Result: </span><br><span class="line">(-3.9059303241050616,0.0,0.0)</span><br><span class="line">(-4.327164416860141,0.0,0.0)</span><br><span class="line">(-3.688252095110469,0.0,0.0)</span><br><span class="line">(-3.495468123629213,0.0,0.0)</span><br><span class="line">(-5.020411486531796,0.0,0.0)</span><br><span class="line">(-5.582160204156088,0.0,0.0)</span><br><span class="line">(-3.3835161794399573,0.0,0.0)</span><br><span class="line">(-4.610619380646923,0.0,0.0)</span><br><span class="line">(-3.676140013649289,0.0,0.0)</span><br><span class="line">(-5.6212847686519805,0.0,0.0)</span><br><span class="line">(-4.172989806210081,0.0,0.0)</span><br><span class="line">(-4.969019949648443,0.0,0.0)</span><br><span class="line">(2.908737548495826,1.0,1.0)</span><br><span class="line">(3.3041995740088774,1.0,1.0)</span><br><span class="line">(2.8216798618199084,1.0,1.0)</span><br><span class="line">(1.6480335988062373,1.0,1.0)</span><br><span class="line">(4.08563451107511,1.0,1.0)</span><br><span class="line">(4.325889225283133,1.0,1.0)</span><br><span class="line">(3.57481724420649,1.0,1.0)</span><br><span class="line">(2.7216874014911334,1.0,1.0)</span><br><span class="line">(2.415040975436961,1.0,1.0)</span><br><span class="line">(2.8969329555616268,1.0,1.0)</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;第六章–支持向量机 (Support Vector Machine)
    
    </summary>
    
      <category term="Machine Learning" scheme="https://isaacchanghau.github.io/categories/Machine-Learning/"/>
    
    
      <category term="java" scheme="https://isaacchanghau.github.io/tags/java/"/>
    
      <category term="machine learning" scheme="https://isaacchanghau.github.io/tags/machine-learning/"/>
    
      <category term="spark" scheme="https://isaacchanghau.github.io/tags/spark/"/>
    
      <category term="scala" scheme="https://isaacchanghau.github.io/tags/scala/"/>
    
      <category term="support vector machine" scheme="https://isaacchanghau.github.io/tags/support-vector-machine/"/>
    
      <category term="weka" scheme="https://isaacchanghau.github.io/tags/weka/"/>
    
  </entry>
  
  <entry>
    <title>Seq2Seq Learning and Neural Conversational Model</title>
    <link href="https://isaacchanghau.github.io/2017/08/02/Seq2Seq-Learning-and-Neural-Conversational-Model/"/>
    <id>https://isaacchanghau.github.io/2017/08/02/Seq2Seq-Learning-and-Neural-Conversational-Model/</id>
    <published>2017-08-02T06:34:47.000Z</published>
    <updated>2018-01-30T08:33:56.000Z</updated>
    
    <content type="html"><![CDATA[<p>It is a summary of two papers: <a href="https://arxiv.org/abs/1409.3215" target="_blank" rel="noopener">Sequence to Sequence Learning with Neural Networks</a> and <a href="https://arxiv.org/abs/1506.05869" target="_blank" rel="noopener">A Neural Conversational Model</a>, as well as the implementation of Neural Conversation Model via Java with <a href="https://github.com/deeplearning4j/deeplearning4j" target="_blank" rel="noopener">deeplearning4j</a> package.<a id="more"></a></p><h1 id="Sequence-to-Sequence-Model"><a href="#Sequence-to-Sequence-Model" class="headerlink" title="Sequence to Sequence Model"></a>Sequence to Sequence Model</h1><p>In this paper, the author presents a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. The method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. As an example shown below:<br><img src="/images/nlp/seq2seq-neuralconver/seq2seq.png" alt=""></p><p><strong>The Model</strong>:<br>The author chooses <a href="https://www.cs.toronto.edu/~graves/" target="_blank" rel="noopener">GravesLSTM</a> as the network layer to avoid long term dependencies issue (vanishing gradient), where the goal of LSTM is to estimate the conditional probability $p(y_{1},\dots,y_{T’}|x_{1},\dots,x_{T})$, here $(x_{1},\dots,x_{T})$ is an input sequence and $(y_{1},\dots,y_{T’})$ is its corresponding output sequence whose length $T’$ may differ from $T$. The LSTM computes this conditional probability by first obtaining the fixed-dimensional representation $v$ of the input sequence $(x_{1},\dots,x_{T})$ given by the last hidden state of the LSTM, and then computing the probability of $(y_{1},\dots,y_{T’})$ with a standard LSTM-LM formulation whose initial hidden state is set to the representation $v$ of $(x_{1},\dots,x_{T})$:$$<br>p(y_{1},\dots,y_{T’}|x_{1},\dots,x_{T})=\prod_{t=1}^{T’}p(y_{t}|v,y_{1},\dots,y_{t-1})$$In this equation, each $p(y_{t}|v,y_{1},\dots,y_{t-1})$ distribution is represented with a softmax over all the words in the vocabulary. It is important to note that each sentence ends with a special end-of-sentence symbol <code>&lt;EOS&gt;</code>, which enables the model to define a distribution over sequences of all possible lengths. As the example shown below<br><img src="/images/nlp/seq2seq-neuralconver/model-example.png" alt=""><br>The LSTM computes the representation of <code>A</code>, <code>B</code>, <code>C</code>, <code>&lt;EOS&gt;</code> and then uses this representation to compute the probability of <code>W</code>, <code>X</code>, <code>Y</code>, <code>Z</code>, <code>&lt;EOS&gt;</code>. Thus, the model reads an input sentence <code>ABC</code> and produces <code>WXYZ</code> as the output sentence. The model stops making predictions after outputting the end-of-sentence token <code>&lt;EOS&gt;</code>.</p><p>The actual model that the author built has three important differences to make the model more sophisticated and robust:</p><ul><li>The author used <strong>two different LSTMs</strong>: one for the input sequence and another for the output sequence, because doing so increases the number model parameters at negligible computational cost and makes it natural to train the LSTM on multiple language pairs simultaneously.</li><li>The author chose an <strong>LSTM with four layers</strong>, since <strong>deep LSTMs significantly outperformed shallow LSTMs</strong>.</li><li>The author found <strong>it extremely valuable to reverse the order of the words of the input sentence</strong>. For instance, instead of mapping the sentence ($a$, $b$, $c$) to the sentence ($\alpha$, $\beta$, $\gamma$), the LSTM is asked to map ($c$, $b$, $a$) to ($\alpha$, $\beta$, $\gamma$), where ($\alpha$, $\beta$, $\gamma$) is the translation of ($a$, $b$, $c$). This way, $a$ is in close proximity to $\alpha$, $b$ is fairly close to $\beta$, and so on, a fact that makes it easy for SGD to <em>“establish communication”</em> between the input and the output.</li></ul><p>The author used the <a href="https://github.com/bicici/ParFDAWMT14" target="_blank" rel="noopener">WMT’14</a> English to French dataset, and trained the model on a subset of 12M sen- tences consisting of 348M French words and 304M English words, which is a clean “selected” subset from <a href="http://www-lium.univ-lemans.fr/~schwenk/cslm_joint_paper/" target="_blank" rel="noopener">here</a>. As typical neural language models rely on a vector representation for each word, the author used a fixed vocabulary for both languages, <code>160K</code> of the most frequent words for the source language and <code>80K</code> of the most frequent words for the target language. Every out-of-vocabulary word was replaced with a special <code>UNK</code> token. With the model and the dataset, the author achieved some good results (<em>the best result achieved by direct translation with large neural networks</em> at that time).</p><p>More details about <code>Decoding and Rescoring</code>, <code>Reversing the Source Sentences</code>, <code>Training</code> and so forth are available in the paper.</p><h1 id="Neural-Conversational-Model"><a href="#Neural-Conversational-Model" class="headerlink" title="Neural Conversational Model"></a>Neural Conversational Model</h1><p>Actually, this paper did not propose any novel idea, however, it did something interesting that the author applied the Seq2Seq model, described in <a href="https://arxiv.org/abs/1409.3215" target="_blank" rel="noopener">Sequence to Sequence Learning with Neural Networks</a>, to not translation tasks but the conversation generation tasks. Thus, this paper named as <em>A Neural Conversational Model</em>.</p><p>Taking a brief look at the Seq2Seq model, which is based on a recurrent neural network, it reads the input sequence <strong>one token at a time</strong>, and predicts the output sequence, also <strong>one token at a time</strong>. During training, the true output sequence is given to the model, so learning can be done by backpropagation. And the model is trained to maximize the cross entropy of the correct sequence given its context. Since given that the true output sequence is not observed during inference, so the model simply feed the predicted output token as input to predict the next output. This is a “greedy” inference approach. A less greedy approach would be to use <a href="https://en.wikipedia.org/wiki/Beam_search" target="_blank" rel="noopener">beam search</a>, and feed several candidates at the previous step to the next step. The predicted sequence can be selected based on the probability of the sequence.</p><p>The author indicated that Seq2Seq model is simple and general, and the way to use Seq2Seq to build conversation modeling is straight: the input sequence can be the concatenation of what has been conversed so far (the context), and the output sequence is the reply. However, the Seq2Seq model will not be able to successfully <em>“solve”</em> the problem of modeling dialogue due to several obvious simplifications:</p><ul><li>The objective function being optimized does not capture the actual objective achieved through human communication, which is typically longer term and based on exchange of information rather than next step prediction.</li><li>The lack of a model to ensure consistency and general world knowledge is another obvious limitation of a purely unsupervised model.</li></ul><p>In the paper, the author use this Seq2Seq model to handle two datasets and show the results: one is a closed-domain <a href="https://github.com/rkadlec/ubuntu-ranking-dataset-creator" target="_blank" rel="noopener">IT helpdesk troubleshooting dataset</a> and another is an open-domain <a href="https://github.com/Conchylicultor/DeepQA/tree/master/data/opensubs" target="_blank" rel="noopener">movie transcript dataset</a>.</p><h1 id="DL4J-Implementation"><a href="#DL4J-Implementation" class="headerlink" title="DL4J Implementation"></a>DL4J Implementation</h1><p>Note that it is an implementation of <em>A Neural Conversation Model</em> and the corpus used here is <a href="https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html" target="_blank" rel="noopener">Cornell Movie Dialogs Corpus</a>, which contains a large metadata-rich collection of fictional conversations extracted from raw movie scripts:</p><ul><li>220,579 conversational exchanges between 10,292 pairs of movie characters.</li><li>involves 9,035 characters from 617 movies.</li><li>in total 304,713 utterances and so forth.</li></ul><p>To implement the Sequence to Sequence model, we need to process the following steps: corpus pre-process, dataset iterator creation, dictionary construction, building neural networks model and so forth. Here I only focus on the model construction, for other parts, you can get information directly from codes: <a href="https://github.com/IsaacChanghau/NeuralNetworksLite/blob/master/src/main/java/com/isaac/dl4j/encdeclstm/CorpusProcessor.java" target="_blank" rel="noopener">CorpusProcessor</a>, <a href="https://github.com/IsaacChanghau/NeuralNetworksLite/blob/master/src/main/java/com/isaac/dl4j/encdeclstm/CorpusIterator.java" target="_blank" rel="noopener">CorpusIterator</a> and <a href="https://github.com/IsaacChanghau/NeuralNetworksLite/blob/master/src/main/java/com/isaac/dl4j/encdeclstm/Dictionaries.java" target="_blank" rel="noopener">Dictionaries</a>.<br>Below are some special tokens used in corpus process and training need to be introduced in advance:</p><ul><li><code>&lt;unk&gt;</code>: replaces any word or other token that’s not in the dictionary (too rare to be included or completely unknown)</li><li><code>&lt;eos&gt;</code>: end of sentence, used only in the output to stop the processing; the model input and output length is limited by the <code>ROW_SIZE</code> constant.</li><li><code>&lt;go&gt;</code>: used only in the decoder input as the first token before the model produced anything</li></ul><p>Generally, the model architecture looks like as follow:<br><strong><code>Input Layer =&gt; Embedding Layer =&gt; Encoder (LSTM Layer) =&gt; Decoder (LSTM Layer) =&gt; Output Layer(Softmax)</code></strong><br>The encoder layer produces a so called <strong>“thought vector”</strong> that contains a neurally-compressed representation of the input. Depending on that vector the model produces different sentences even if they start with the same token. There is one more input, connected directly to the decoder layer, it is used to provide the previous token of the output. For the very first output token, we send a special <code>&lt;go&gt;</code> token there, on the next iteration we use the token that the model produced the last time. On the training stage everything is simple, we apriori know the desired output so the decoder input would be the same token set prepended with the <code>&lt;go&gt;</code> token and without the last <code>&lt;eos&gt;</code> token. For instance:</p><blockquote><p>Input: “how” “do” “you” “do” “?”<br>Output: “I’m” “fine” “,” “thanks” “!” <code>&quot;&lt;eos&gt;&quot;</code><br>Decoder: <code>&quot;&lt;go&gt;&quot;</code> “I’m” “fine” “,” “thanks” “!”</p></blockquote><p>It is worth to mention that the input is reversed as per <a href="https://arxiv.org/abs/1409.3215" target="_blank" rel="noopener">Sequence to Sequence Learning with Neural Networks</a>, since the most important words are usually in the beginning of the phrase and they would get more weight if supplied last (the model “forgets” tokens that were supplied “long ago”, i.e. they have lesser weight than the recent ones). The output and decoder input sequence lengths are always equal.</p><p>The encoder and decoder layers work sequentially. First the encoder creates the thought vector, that is the last activations of the layer. Those activations are then duplicated for as many time steps as there are elements in the output so that every output element can have its own copy of the thought vector. Then the decoder starts working. It receives two inputs, the thought vector made by the encoder and the token that it <em>should have produced</em> (but usually it outputs something else so we have our loss metric and can compute gradients for the backward pass) on the previous step (or <code>&lt;go&gt;</code> for the very first step). These two vectors are simply concatenated by the merge vertex. The decoder’s output goes to the softmax layer and that’s it (More details are available <a href="https://github.com/IsaacChanghau/NeuralNetworksLite/blob/master/src/main/resources/encdec/readme.txt" target="_blank" rel="noopener">here</a>).</p><p>Implementing the Seq2Seq model via deeplearning4j, we need to create a <a href="https://deeplearning4j.org/compgraph" target="_blank" rel="noopener">ComputationGraph</a>, which is used to build complex network architectures and allows for greater freedom in network architectures in deeplearning4j. First of all, we need to configure a neural network by setting the <code>iterations</code>, <code>learning rate</code>, <code>optimizations</code>, <code>updater</code>, <code>parameters initialization</code> and etc., as shown below:<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">final</span> NeuralNetConfiguration.Builder builder = <span class="keyword">new</span> NeuralNetConfiguration.Builder()</span><br><span class="line">        .iterations(<span class="number">1</span>)</span><br><span class="line">        .learningRate(LEARNING_RATE)</span><br><span class="line">        .rmsDecay(RMS_DECAY)</span><br><span class="line">        .optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT)</span><br><span class="line">        .miniBatch(<span class="keyword">true</span>)</span><br><span class="line">        .updater(Updater.RMSPROP)</span><br><span class="line">        .weightInit(WeightInit.XAVIER)</span><br><span class="line">        .gradientNormalization(GradientNormalization.RenormalizeL2PerLayer);</span><br></pre></td></tr></table></figure></p><p>Note that those variables are custom defined, you can modify them as you want in order to achieve better results, and if you are not familiar with those variables or settings in deeplearning4j, you can visit the <a href="https://deeplearning4j.org/documentation" target="_blank" rel="noopener">official website</a> to get more knowledge.</p><p>Next step is to build the ComputationGraph<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">final</span> ComputationGraphConfiguration.GraphBuilder graphBuilder = builder.graphBuilder()</span><br><span class="line">        .addInputs(<span class="string">"inputLine"</span>, <span class="string">"decoderInput"</span>)</span><br><span class="line">        .setInputTypes(InputType.recurrent(dict.size()), InputType.recurrent(dict.size()))</span><br></pre></td></tr></table></figure></p><p><code>.addInputs</code> <a href="https://deeplearning4j.org/doc/org/deeplearning4j/nn/conf/ComputationGraphConfiguration.GraphBuilder.html#addInputs-java.lang.String...-" target="_blank" rel="noopener">specify</a> the inputs to the network, and their associated labels, while the names of the inputs also defines their order. Here we have two inputs for the computation graph, <code>inputLine</code> is feed to next layer (embedding layer) directly, while <code>decoderInput</code> will be used later.<br><code>.setInputTypes</code> <a href="https://deeplearning4j.org/doc/org/deeplearning4j/nn/conf/ComputationGraphConfiguration.GraphBuilder.html#setInputTypes-org.deeplearning4j.nn.conf.inputs.InputType...-" target="_blank" rel="noopener">specify</a> the types of inputs to the network, so that <strong>preprocessors can be automatically added</strong>, and <strong>the nIns (input size) for each layer can be automatically calculated and set</strong>. Meanwhile <code>InputType</code> is used to <a href="https://deeplearning4j.org/doc/org/deeplearning4j/nn/conf/inputs/InputType.html" target="_blank" rel="noopener">define</a> the types of activations etc used in a ComputationGraph, <code>.recurrent(...)</code> indicates that it is for recurrent neural network (time series) data.</p><p><strong>Add Embedding Layer</strong>:<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// following previous codes</span></span><br><span class="line">.addLayer(<span class="string">"embeddingEncoder"</span>,</span><br><span class="line">        <span class="keyword">new</span> EmbeddingLayer.Builder()</span><br><span class="line">                .nIn(dict.size())</span><br><span class="line">                .nOut(EMBEDDING_WIDTH)</span><br><span class="line">                .build(),</span><br><span class="line">        <span class="string">"inputLine"</span>)</span><br></pre></td></tr></table></figure></p><p>Here we use <code>.addLayer</code> to <a href="https://deeplearning4j.org/doc/org/deeplearning4j/nn/conf/ComputationGraphConfiguration.GraphBuilder.html#addLayer-java.lang.String-org.deeplearning4j.nn.conf.layers.Layer-java.lang.String...-" target="_blank" rel="noopener">add</a> an <a href="https://deeplearning4j.org/doc/org/deeplearning4j/nn/conf/layers/EmbeddingLayer.html" target="_blank" rel="noopener">Embedding Layer</a> named as <code>embeddingEncoder</code>, where its input is the <code>inputLine</code>, the <code>nIn</code> (input size) is set as the size of dictionary, while <code>nOut</code> (output size) is the pre-defined <code>EMBEDDING_WIDTH</code>. Note that Embedding Layer can only be used as the first layer for a network.</p><p><strong>Add Encoder (LSTM) Layer</strong>:<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// following previous codes</span></span><br><span class="line">.addLayer(<span class="string">"encoder"</span>,</span><br><span class="line">        <span class="keyword">new</span> GravesLSTM.Builder()</span><br><span class="line">                .nIn(EMBEDDING_WIDTH)</span><br><span class="line">                .nOut(HIDDEN_LAYER_WIDTH)</span><br><span class="line">                .activation(Activation.TANH)</span><br><span class="line">                .gateActivationFunction(Activation.HARDSIGMOID)</span><br><span class="line">                .build(),</span><br><span class="line">        <span class="string">"embeddingEncoder"</span>)</span><br></pre></td></tr></table></figure></p><p>After Embedding Layer, we add a <a href="https://deeplearning4j.org/doc/org/deeplearning4j/nn/conf/layers/GravesLSTM.html" target="_blank" rel="noopener">LSTM</a> as Encoder Layer, which named as <code>encoder</code> and the Embedding Layer act as its input, the activation function of LSTM gates are set as <code>HARDSIGMOID</code>, while the layer activation function is set as <code>TANH</code>.</p><p><strong>Add Vertex</strong>:<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// following previous codes</span></span><br><span class="line">.addVertex(<span class="string">"thoughtVector"</span>, <span class="keyword">new</span> LastTimeStepVertex(<span class="string">"inputLine"</span>), <span class="string">"encoder"</span>)</span><br><span class="line">.addVertex(<span class="string">"dup"</span>, <span class="keyword">new</span> DuplicateToTimeSeriesVertex(<span class="string">"decoderInput"</span>), <span class="string">"thoughtVector"</span>)</span><br><span class="line">.addVertex(<span class="string">"merge"</span>, <span class="keyword">new</span> MergeVertex(), <span class="string">"decoderInput"</span>, <span class="string">"dup"</span>)</span><br></pre></td></tr></table></figure></p><p>This part performs a role of concatenation between Encoder LSTM Layer and Decoder LSTM Layer, it addresses the output from previous layer and does some processes to generate the desired input for next layer. Below is the explanation of three different <a href="https://deeplearning4j.org/doc/org/deeplearning4j/nn/conf/graph/GraphVertex.html" target="_blank" rel="noopener">GraphVertexs</a> used in the neural network construction:</p><ul><li><a href="https://deeplearning4j.org/doc/org/deeplearning4j/nn/conf/graph/rnn/LastTimeStepVertex.html" target="_blank" rel="noopener">LastTimeStepVertex</a> is used in the context of recurrent neural network activations, to go from 3d (time series) activations to 2d activations, by <strong>extracting out the last time step of activations for each example</strong>. This can be used for example in sequence to sequence architectures, and potentially for sequence classification. <strong>Note that</strong> since RNNs may have masking arrays (to allow for examples/time series of different lengths in the same minibatch), it is necessary to provide the same of the network input that has the corresponding mask array. If this input does not have a mask array, the last time step of the input will be used for all examples; otherwise, the time step of the last non-zero entry in the mask array (for each example separately) will be used. <code>&quot;inputLine&quot;</code> here is the name of the input to look at when determining the last time step. Specifically, the mask array of this time series input is used when determining which time step to extract and return. Meanwhile <code>&quot;thoughtVector&quot;</code> is the Vertex Layer name, <code>&quot;encoder&quot;</code> is the Vertex inputs.</li><li><a href="https://deeplearning4j.org/doc/org/deeplearning4j/nn/conf/graph/rnn/DuplicateToTimeSeriesVertex.html" target="_blank" rel="noopener">DuplicateToTimeSeriesVertex</a> is a vertex that goes from 2d activations to a 3d time series activations, by means of duplication. That is, given a 2d input with shape <code>[numExamples,nIn]</code> duplicate each row to give output of <code>[numExamples,nIn,timeSeriesLength]</code>, where the activations are the same for all time steps. This method is used for example in sequence to sequence models. <strong>Note that</strong> the length of the output time series (number of time steps) is determined by means of referencing one of the inputs in the ComputationGraph, that is, since the length of the time series may differ at runtime, we generally want the number of time steps to match some other input; here, we are specifying the length of the output time series to be the same as one of the input time series. <code>&quot;decoderInput&quot;</code> is the name of the input in the ComputationGraph network to use, to determine how long the output time series should be. This input should exist, and be a time series input.</li><li><a href="https://deeplearning4j.org/doc/org/deeplearning4j/nn/conf/graph/MergeVertex.html" target="_blank" rel="noopener">MergeVertex</a> is used to combine the activations of two or more layers/GraphVertex by means of concatenation/merging. Here <code>&quot;decoderInput&quot;</code> and <code>&quot;dup&quot;</code> is the two layers to be merged, <code>&quot;merge&quot;</code> is the name of this Vertex Layer. Exactly how this is done depends on the type of input:<ul><li>For 2d (feed forward layer) inputs: <strong>MergeVertex([numExamples, layerSize1], [numExamples, layerSize2])-&gt;[numExamples, layerSize1+layerSize2]</strong>.</li><li>For 3d (time series) inputs: <strong>MergeVertex([numExamples, layerSize1, timeSeriesLength], [numExamples, layerSize2, timeSeriesLength])-&gt;[numExamples, layerSize1+layerSize2, timeSeriesLength]</strong>.</li><li>For 4d (convolutional) inputs: <strong>MergeVertex([numExamples, depth1, width, height], [numExamples, depth2, width, height])-&gt;[numExamples, depth1+depth2, width, height]</strong>.</li></ul></li></ul><p>So, generally, the LastTimeStepVertex extract out the last time step of activations from Encoder Layer as tge <code>&quot;thoughtVector&quot;</code>, then DuplicateToTimeSeriesVertex duplicates the <code>&quot;thoughtVector&quot;</code> according to the time series length of <code>&quot;decoderInput&quot;</code>, finally, MergeVertex concatenate the duplicated <code>&quot;thoughtVector&quot;</code> and <code>&quot;decoderInput&quot;</code> through the method for 3d (time series) inputs. Then using this <code>&quot;merge&quot;</code> result as the input of Decoder Layer.</p><p><strong>Add Decoder (LSTM) Layer</strong>:<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// following previous codes</span></span><br><span class="line">.addLayer(<span class="string">"decoder"</span>,</span><br><span class="line">        <span class="keyword">new</span> GravesLSTM.Builder()</span><br><span class="line">                .nIn(dict.size() + HIDDEN_LAYER_WIDTH)</span><br><span class="line">                .nOut(HIDDEN_LAYER_WIDTH)</span><br><span class="line">                .activation(Activation.TANH)</span><br><span class="line">                .gateActivationFunction(Activation.HARDSIGMOID) <span class="comment">// always be a (hard) sigmoid function</span></span><br><span class="line">                .build(),</span><br><span class="line">        <span class="string">"merge"</span>)</span><br></pre></td></tr></table></figure></p><p>The structure of Decoder Layer is similar to the Encoder Layer. One thing needs to mention here is that the <code>nIn</code> (input size) is <code>dict.size()+HIDDEN_LAYER_WIDTH</code>, since its input is from the <code>&quot;merge&quot;</code> Vertex Layer, which concatenates the <code>&quot;thoughtVector&quot;</code> and <code>&quot;decoderInput&quot;</code>.</p><p><strong>Add Output Layer and Further Settings</strong>:<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// following previous codes</span></span><br><span class="line">.addLayer(<span class="string">"output"</span>,</span><br><span class="line">        <span class="keyword">new</span> RnnOutputLayer.Builder()</span><br><span class="line">                .nIn(HIDDEN_LAYER_WIDTH)</span><br><span class="line">                .nOut(dict.size())</span><br><span class="line">                .activation(Activation.SOFTMAX)</span><br><span class="line">                .lossFunction(LossFunctions.LossFunction.MCXENT) <span class="comment">// multi-class cross entropy</span></span><br><span class="line">                .build(),</span><br><span class="line">        <span class="string">"decoder"</span>)</span><br><span class="line">.setOutputs(<span class="string">"output"</span>)</span><br><span class="line">.backpropType(BackpropType.Standard)</span><br><span class="line">.tBPTTForwardLength(TBPTT_SIZE)</span><br><span class="line">.tBPTTBackwardLength(TBPTT_SIZE)</span><br><span class="line">.pretrain(<span class="keyword">false</span>)</span><br><span class="line">.backprop(<span class="keyword">true</span>);</span><br></pre></td></tr></table></figure></p><p>The last layer is <a href="https://deeplearning4j.org/doc/org/deeplearning4j/nn/conf/layers/RnnOutputLayer.html" target="_blank" rel="noopener">RnnOutputLayer</a>, which is a type of layer used as the final layer with many recurrent neural network systems (for both regression and classification tasks). RnnOutputLayer <a href="https://deeplearning4j.org/usingrnns#rnnoutputlayer" target="_blank" rel="noopener">handles</a> things like score calculation, and error calculation (of prediction vs. actual) given a loss function etc. Functionally, it is very similar to the ‘standard’ OutputLayer class (which is used with feed-forward networks); however it both outputs (and expects as labels/targets) 3d time series data sets. After configuring the last layer, we still need to deal with several proper settings to make the whole computation graph works, like setting backpropagation as <code>.backpropType(BackpropType.Standard)</code>, setting BPTT forward and backward length as <code>.tBPTTForwardLength(TBPTT_SIZE)</code> and <code>.tBPTTBackwardLength(TBPTT_SIZE)</code> and etc.</p><p><strong>Build and Initialize ComputetionGraph</strong>:<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ComputationGraph net = <span class="keyword">new</span> ComputationGraph(graphBuilder.build());</span><br><span class="line">net.init();</span><br></pre></td></tr></table></figure></p><p>Finally, after configuring all the required settings, we can build the Sequence to Sequence Computation Graph and initialize it. Above is the general graph of building a simple version of Sequence to Sequence model for consersation tasks. More related information are available in <a href="https://deeplearning4j.org/usingrnns" target="_blank" rel="noopener">DL4J Guidance</a> and <a href="https://github.com/deeplearning4j/dl4j-examples" target="_blank" rel="noopener">DL4J Examples</a>.</p><p>Full Codes are available in my GitHub repository: <a href="https://github.com/IsaacChanghau/NeuralNetworksLite/tree/master/src/main/java/com/isaac/dl4j/encdeclstm" target="_blank" rel="noopener">EncDecLSTM</a>, and you can also get the original DL4J implementation of <code>encdec</code> based on Seq2Seq as well as <em>Python</em> and <em>Lua</em> implementation codes in the Resources at the end of this article.</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul><li><a href="https://arxiv.org/abs/1409.3215" target="_blank" rel="noopener">Sequence to Sequence Learning with Neural Networks</a></li><li><a href="https://arxiv.org/abs/1506.05869" target="_blank" rel="noopener">A Neural Conversational Model</a></li><li><a href="https://karpathy.github.io/2015/05/21/rnn-effectiveness/" target="_blank" rel="noopener">The Unreasonable Effectiveness of Recurrent Neural Networks</a></li><li><a href="https://deeplearning4j.org/usingrnns#recurrent-neural-networks-in-dl4j" target="_blank" rel="noopener">Recurrent Neural Networks in DL4J</a></li><li><a href="https://deeplearning4j.org/doc/org/deeplearning4j/nn/conf/ComputationGraphConfiguration.GraphBuilder.html" target="_blank" rel="noopener">Class ComputationGraphConfiguration.GraphBuilder – DL4J API</a></li><li><a href="https://en.wikibooks.org/wiki/Artificial_Intelligence/Search/Heuristic_search/Beam_search" target="_blank" rel="noopener">Artificial Intelligence/Search/Heuristic search/Beam search</a></li><li><a href="https://en.wikipedia.org/wiki/Beam_search" target="_blank" rel="noopener">Beam search</a></li><li><a href="https://stackoverflow.com/questions/22273119/beam-search-algorithm-how-does-it-work" target="_blank" rel="noopener">Beam Search Algorithm, how does it work?</a></li></ul><h1 id="Resources"><a href="#Resources" class="headerlink" title="Resources"></a>Resources</h1><p><strong>Python</strong>:<br><a href="https://github.com/JayParks/tf-seq2seq" target="_blank" rel="noopener">[JayParks/tf-seq2seq]</a>, <a href="https://github.com/farizrahman4u/seq2seq" target="_blank" rel="noopener">[farizrahman4u/seq2seq]</a><br><strong>Java</strong>:<br><a href="https://github.com/deeplearning4j/dl4j-examples/tree/master/dl4j-examples/src/main/java/org/deeplearning4j/examples/recurrent/encdec" target="_blank" rel="noopener">[dl4j-examples-encdec]</a>, <a href="https://github.com/deeplearning4j/dl4j-examples/tree/master/dl4j-examples/src/main/java/org/deeplearning4j/examples/recurrent/character" target="_blank" rel="noopener">[dl4j-examples-character]</a><br><strong>Lua</strong>:<br><a href="https://github.com/harvardnlp/seq2seq-attn" target="_blank" rel="noopener">[harvardnlp/seq2seq-attn]</a>, <a href="http://nlp.seas.harvard.edu/code/" target="_blank" rel="noopener">[harvardnlp]</a><br><strong>DataSets</strong>:<br><a href="https://github.com/rkadlec/ubuntu-ranking-dataset-creator" target="_blank" rel="noopener">[IT helpdesk troubleshooting dataset]</a>, <a href="https://github.com/Conchylicultor/DeepQA/tree/master/data/opensubs" target="_blank" rel="noopener">[movie transcript dataset]</a>, <a href="https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html" target="_blank" rel="noopener">[Cornell Movie Dialogs Corpus]</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;It is a summary of two papers: &lt;a href=&quot;https://arxiv.org/abs/1409.3215&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Sequence to Sequence Learning with Neural Networks&lt;/a&gt; and &lt;a href=&quot;https://arxiv.org/abs/1506.05869&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;A Neural Conversational Model&lt;/a&gt;, as well as the implementation of Neural Conversation Model via Java with &lt;a href=&quot;https://github.com/deeplearning4j/deeplearning4j&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;deeplearning4j&lt;/a&gt; package.
    
    </summary>
    
      <category term="Natural Language Processing" scheme="https://isaacchanghau.github.io/categories/Natural-Language-Processing/"/>
    
    
      <category term="java" scheme="https://isaacchanghau.github.io/tags/java/"/>
    
      <category term="deep learning" scheme="https://isaacchanghau.github.io/tags/deep-learning/"/>
    
      <category term="lstm" scheme="https://isaacchanghau.github.io/tags/lstm/"/>
    
      <category term="gru" scheme="https://isaacchanghau.github.io/tags/gru/"/>
    
      <category term="deeplearning4j" scheme="https://isaacchanghau.github.io/tags/deeplearning4j/"/>
    
      <category term="natural language processing" scheme="https://isaacchanghau.github.io/tags/natural-language-processing/"/>
    
      <category term="seq2seq" scheme="https://isaacchanghau.github.io/tags/seq2seq/"/>
    
  </entry>
  
  <entry>
    <title>Plain Stock Close-Price Prediction via LSTM</title>
    <link href="https://isaacchanghau.github.io/2017/07/26/Plain-Stock-Close-Price-Prediction-via-LSTM-Initial-Exploration/"/>
    <id>https://isaacchanghau.github.io/2017/07/26/Plain-Stock-Close-Price-Prediction-via-LSTM-Initial-Exploration/</id>
    <published>2017-07-26T05:58:38.000Z</published>
    <updated>2018-01-30T08:33:21.000Z</updated>
    
    <content type="html"><![CDATA[<p>This is a practice of using <a href="https://en.wikipedia.org/wiki/Long_short-term_memory" target="_blank" rel="noopener">LSTM</a> to do the one day ahead prediction of the stock close price. The dataset I used here is the <a href="https://www.kaggle.com/dgawlik/nyse" target="_blank" rel="noopener">New York Stock Exchange</a> from <a href="https://www.kaggle.com/" target="_blank" rel="noopener">Kaggle</a><a id="more"></a>, which consists of following files:</p><ul><li><code>prices.csv</code>: raw, as-is daily prices. Most of data spans from 2010 to the end 2016, for companies new on stock market date range is shorter. There have been approx. 140 stock splits in that time, this set doesn’t account for that.</li><li><code>prices-split-adjusted.csv</code>: same as prices, but there have been added adjustments for splits.</li><li><code>securities.csv</code>: general description of each company with division on sectors</li><li><code>fundamentals.csv</code>: metrics extracted from annual SEC 10K fillings (2012-2016), should be enough to derive most of popular fundamental indicators.</li></ul><p>For those four <code>.csv</code> files, I only use the <code>prices-split-adjusted.csv</code> to train the LSTM-based neural networks model. Since the data in <code>prices-split-adjusted.csv</code> does not have any missing values, thus, data preprocessing tasks, like data clean, transforming, can be eliminated. Meanwhile, <code>prices-split-adjusted.csv</code> contains the information of more than 500 different stocks, here I only select one of them and extract all the information of such stock to analyze it.<br>To implement the LSTM-based neural networks for learning, I use <a href="https://github.com/deeplearning4j/deeplearning4j" target="_blank" rel="noopener">Deeplearning4J (DL4J)</a> to process the data and build the model. For more information about how to play with DL4J, you can access their offical website <a href="https://deeplearning4j.org/" target="_blank" rel="noopener">here</a>. Typically, if you wonder how to implement a LSTMs, we may want to visit DL4J’s introduction of <a href="https://deeplearning4j.org/usingrnns" target="_blank" rel="noopener">“Using Recurrent Neural Networks in DL4J”</a> as well as some examples of DL4J: <a href="https://github.com/deeplearning4j/dl4j-examples" target="_blank" rel="noopener">[link]</a>.</p><h1 id="Data-Preview"><a href="#Data-Preview" class="headerlink" title="Data Preview"></a>Data Preview</h1><p>Here I use <a href="https://spark.apache.org/docs/latest/sql-programming-guide.html" target="_blank" rel="noopener">Spark Dataframe</a> to analyze the data and show some statistical information. First, we need to configure spark and load the data from file to build the dataframe.<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">Logger.getLogger(<span class="string">"org"</span>).setLevel(Level.OFF); <span class="comment">// shut down log info.</span></span><br><span class="line">SparkSession spark = SparkSession.builder().master(<span class="string">"local"</span>).appName(<span class="string">"DataProcess"</span>).getOrCreate();</span><br><span class="line">String filename = <span class="string">"prices-split-adjusted.csv"</span>;</span><br><span class="line"><span class="comment">// load data from csv file</span></span><br><span class="line">Dataset&lt;Row&gt; data = spark.read().format(<span class="string">"csv"</span>).option(<span class="string">"header"</span>, <span class="keyword">true</span>)</span><br><span class="line">        .load(<span class="keyword">new</span> ClassPathResource(filename).getFile().getAbsolutePath())</span><br><span class="line">        .withColumn(<span class="string">"openPrice"</span>, functions.col(<span class="string">"open"</span>).cast(<span class="string">"double"</span>)).drop(<span class="string">"open"</span>)</span><br><span class="line">        .withColumn(<span class="string">"lowPrice"</span>, functions.col(<span class="string">"low"</span>).cast(<span class="string">"double"</span>)).drop(<span class="string">"low"</span>)</span><br><span class="line">        .withColumn(<span class="string">"highPrice"</span>, functions.col(<span class="string">"high"</span>).cast(<span class="string">"double"</span>)).drop(<span class="string">"high"</span>)</span><br><span class="line">        .withColumn(<span class="string">"volumeTmp"</span>, functions.col(<span class="string">"volume"</span>).cast(<span class="string">"double"</span>)).drop(<span class="string">"volume"</span>)</span><br><span class="line">        .withColumn(<span class="string">"closePrice"</span>, functions.col(<span class="string">"close"</span>).cast(<span class="string">"double"</span>)).drop(<span class="string">"close"</span>)</span><br><span class="line">        .toDF(<span class="string">"date"</span>, <span class="string">"symbol"</span>, <span class="string">"open"</span>, <span class="string">"low"</span>, <span class="string">"high"</span>, <span class="string">"volume"</span>, <span class="string">"close"</span>);</span><br><span class="line">data.show();</span><br></pre></td></tr></table></figure></p><p>Below gives first 20 lines of the data, inside the <code>prices-split-adjusted.csv</code>, there are 7 fields, <em>date</em>, <em>symbol</em>, <em>open</em>, <em>close</em>, <em>low</em>, <em>high</em> and <em>volume</em>, where <em>symbol</em> denotes the name of stock.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">+----------+------+----------+----------+----------+----------+---------+</span><br><span class="line">|      date|symbol|      open|     close|       low|      high|   volume|</span><br><span class="line">+----------+------+----------+----------+----------+----------+---------+</span><br><span class="line">|2016-01-05|  WLTW|    123.43|125.839996|122.309998|    126.25|2163600.0|</span><br><span class="line">|2016-01-06|  WLTW|125.239998|119.980003|119.940002|125.540001|2386400.0|</span><br><span class="line">|2016-01-07|  WLTW|116.379997|114.949997|    114.93|119.739998|2489500.0|</span><br><span class="line">|2016-01-08|  WLTW|115.480003|116.620003|     113.5|117.440002|2006300.0|</span><br><span class="line">|2016-01-11|  WLTW|117.010002|114.970001|114.089996|117.330002|1408600.0|</span><br><span class="line">|2016-01-12|  WLTW|115.510002|115.550003|     114.5|116.059998|1098000.0|</span><br><span class="line">|2016-01-13|  WLTW|116.459999|112.849998|112.589996|    117.07| 949600.0|</span><br><span class="line">|2016-01-14|  WLTW|113.510002|114.379997|110.050003|115.029999| 785300.0|</span><br><span class="line">|2016-01-15|  WLTW|113.330002|112.529999|111.919998|114.879997|1093700.0|</span><br><span class="line">|2016-01-19|  WLTW|113.660004|110.379997|109.870003|115.870003|1523500.0|</span><br><span class="line">|2016-01-20|  WLTW|109.059998|109.300003|    108.32|111.599998|1653900.0|</span><br><span class="line">|2016-01-21|  WLTW|109.730003|     110.0|    108.32|110.580002| 944300.0|</span><br><span class="line">|2016-01-22|  WLTW|111.879997|111.949997|110.190002|112.949997| 744900.0|</span><br><span class="line">|2016-01-25|  WLTW|    111.32|110.120003|     110.0|114.629997| 703800.0|</span><br><span class="line">|2016-01-26|  WLTW|110.419998|     111.0|107.300003|111.400002| 563100.0|</span><br><span class="line">|2016-01-27|  WLTW|110.769997|110.709999|109.019997|    112.57| 896100.0|</span><br><span class="line">|2016-01-28|  WLTW|110.900002|112.580002|109.900002|112.970001| 680400.0|</span><br><span class="line">|2016-01-29|  WLTW|113.349998|114.470001|111.669998|114.589996| 749900.0|</span><br><span class="line">|2016-02-01|  WLTW|     114.0|     114.5|112.900002|114.849998| 574200.0|</span><br><span class="line">|2016-02-02|  WLTW|    113.25|110.559998|    109.75|113.860001| 694800.0|</span><br><span class="line">+----------+------+----------+----------+----------+----------+---------+</span><br><span class="line">only showing top 20 rows</span><br></pre></td></tr></table></figure></p><p>Then we extract all symbols from the dataset and display some of them:<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Dataset&lt;Row&gt; symbols = data.select(<span class="string">"date"</span>, <span class="string">"symbol"</span>).groupBy(<span class="string">"symbol"</span>).agg(functions.count(<span class="string">"date"</span>).as(<span class="string">"count"</span>));</span><br><span class="line">System.out.println(symbols.count());</span><br><span class="line">symbols.show();</span><br></pre></td></tr></table></figure></p><p>Here we got the result, <code>count</code> means the number of data available for each <code>symbol</code>:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Number of Symbols: 501</span><br><span class="line">+------+-----+</span><br><span class="line">|symbol|count|</span><br><span class="line">+------+-----+</span><br><span class="line">|  ALXN| 1762|</span><br><span class="line">|   GIS| 1762|</span><br><span class="line">|     K| 1762|</span><br><span class="line">|   LEN| 1762|</span><br><span class="line">|  SPGI| 1762|</span><br><span class="line">|   AVY| 1762|</span><br><span class="line">...</span><br></pre></td></tr></table></figure></p><p>Actually, with Spark Dataframe, you can easily handle the data and deal with a lot of different tasks, here I only show the general information above and do not go deep in this part, since my own task is to build a lstm networks to train the data and make a prediction.<br>In order to transform the data to the format suitable for training, here I define a <code>StockData</code> class to store the stock information, this class will be used in the “Customize DataSet Iterator” part.<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Created by zhanghao on 25/7/17.</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> ZHANG HAO</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">StockData</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> String date; <span class="comment">// date</span></span><br><span class="line">    <span class="keyword">private</span> String symbol; <span class="comment">// stock name</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">double</span> open; <span class="comment">// open price</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">double</span> close; <span class="comment">// close price</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">double</span> low; <span class="comment">// low price</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">double</span> high; <span class="comment">// high price</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">double</span> volume; <span class="comment">// volume</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">StockData</span> <span class="params">()</span> </span>&#123;&#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">StockData</span> <span class="params">(String date, String symbol, <span class="keyword">double</span> open, <span class="keyword">double</span> close, <span class="keyword">double</span> low, <span class="keyword">double</span> high, <span class="keyword">double</span> volume)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.date = date;</span><br><span class="line">        <span class="keyword">this</span>.symbol = symbol;</span><br><span class="line">        <span class="keyword">this</span>.open = open;</span><br><span class="line">        <span class="keyword">this</span>.close = close;</span><br><span class="line">        <span class="keyword">this</span>.low = low;</span><br><span class="line">        <span class="keyword">this</span>.high = high;</span><br><span class="line">        <span class="keyword">this</span>.volume = volume;</span><br><span class="line">    &#125;</span><br><span class="line">    ... Getter and Setter</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h1 id="Customize-DataSet-Iterator"><a href="#Customize-DataSet-Iterator" class="headerlink" title="Customize DataSet Iterator"></a>Customize DataSet Iterator</h1><p>For a stock, we have 5 features, say, <code>open</code>, <code>close</code>, <code>low</code>, <code>high</code> and <code>volume</code>, and our task is to predict “future” <code>close</code> price of such stock. So here we set <code>VECTOR_SIZE=5</code>.<br>Since LSTMs is one kind of <a href="https://en.wikipedia.org/wiki/Recurrent_neural_network" target="_blank" rel="noopener">Recurrent Neural Networks</a>, unlike standard feed-forward network, who expect input and output data that is two-dimensional, that is, data with “shape” <code>[numExamples,inputSize]</code>. It means that the data into a feed-forward network has <code>numExamples</code> rows/examples, where each row consists of <code>inputSize</code> columns. Similarly, output data for a standard feed-forward network is also two dimensional, with shape <code>[numExamples,outputSize]</code>.<br>Conversely, data for RNNs are time series. Thus, they have 3 dimensions: one additional dimension for time. Input data has shape <code>[numExamples,inputSize,timeSeriesLength]</code>, and output data has shape <code>[numExamples,outputSize,timeSeriesLength]</code>. It means that the data in <a href="http://nd4j.org/doc/org/nd4j/linalg/api/ndarray/INDArray.html" target="_blank" rel="noopener">INDArray</a> is laid out such that the value at position (i,j,k) is the jth value at the kth time step of the ith example in the minibatch. This data layout is shown below.<br><img src="/images/deeplearning/stockpredict/rnndata.png" alt="RNN and FNN Data"><br>In this case, we need to indicate the time series length of the data, so I define the <code>exampleLength</code> to represent the time series length for RNNs. Another thing is the <code>miniBatchSize</code>, which is defined to create the mini-batch for training.<br>Given the <code>miniBatchSize</code>, <code>VECTOR_SIZE</code> and <code>exampleLength</code>, we need to create the mini-batch <a href="http://nd4j.org/doc/org/nd4j/linalg/dataset/DataSet.html" target="_blank" rel="noopener">DataSet</a> for training, in the DataSet, we have two things, one is 3-dimensional input data, and another is labels. Since we need to predict one-day ahead close price of a stock, and my strategy is to <strong>using the previous one-month information to construct the input time series data</strong>, then <strong>predict the close price of next day</strong>. Suppose that there are 22 working days per month, so the time series length should be 22. For instance, with <code>miniBatchSize=64</code>, assume time start from $t=0$, our input data has shape <code>[numExamples,inputSize,timeSeriesLength]</code>, where <code>numExamples=64</code>, <code>inputSize=5</code> and <code>timeSeriesLength=0~22</code>, while label (output) data has shape <code>[numExamples,outputSize,timeSeriesLength]</code>, where <code>numExamples=64</code>, <code>outputSize=1</code> (only predict close price) and <code>timeSeriesLength=1~23</code> (one-day ahead).<br>More implementation details are shown in the codes below, the <code>StockDataSetIterator</code> implements the standard <a href="http://nd4j.org/doc/org/nd4j/linalg/dataset/api/iterator/DataSetIterator.html" target="_blank" rel="noopener">DataSetIterator</a>, which is pre-defined interface by Deeplearning4J to help user to construct a proper custom dataset available for Deeplearning4J neural networks and for iterating over DataSet objects - objects that encapsulate the input and target INDArrays, plus (optionally) the input and labels mask arrays.<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Created by zhanghao on 26/7/17.</span></span><br><span class="line"><span class="comment"> * Modified by zhanghao on 28/9/17.</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> ZHANG HAO</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">StockDataSetIterator</span> <span class="keyword">implements</span> <span class="title">DataSetIterator</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">int</span> VECTOR_SIZE = <span class="number">5</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> miniBatchSize;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> exampleLength = <span class="number">22</span>; <span class="comment">// default 22, say, 22 working days per month</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">double</span>[] minNum = <span class="keyword">new</span> <span class="keyword">double</span>[VECTOR_SIZE];</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">double</span>[] maxNum = <span class="keyword">new</span> <span class="keyword">double</span>[VECTOR_SIZE];</span><br><span class="line">    <span class="keyword">private</span> PriceCategory category = PriceCategory.CLOSE; <span class="comment">// default to train to predict the CLOSE price model</span></span><br><span class="line">    <span class="keyword">private</span> LinkedList&lt;Integer&gt; exampleStartOffsets = <span class="keyword">new</span> LinkedList&lt;&gt;();</span><br><span class="line">    <span class="keyword">private</span> List&lt;StockData&gt; train;</span><br><span class="line">    <span class="keyword">private</span> List&lt;Pair&lt;INDArray, INDArray&gt;&gt; test;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">StockDataSetIterator</span> <span class="params">(String filename, String symbol, <span class="keyword">int</span> miniBatchSize, <span class="keyword">int</span> exampleLength, <span class="keyword">double</span> splitRatio, PriceCategory category)</span> </span>&#123;</span><br><span class="line">        List&lt;StockData&gt; stockDataList = readStockDataFromFile(filename, symbol);</span><br><span class="line">        <span class="keyword">this</span>.miniBatchSize = miniBatchSize;</span><br><span class="line">        <span class="keyword">this</span>.exampleLength = exampleLength;</span><br><span class="line">        <span class="keyword">this</span>.category = category;</span><br><span class="line">        <span class="keyword">int</span> split = (<span class="keyword">int</span>) Math.round(stockDataList.size() * splitRatio);</span><br><span class="line">        train = stockDataList.subList(<span class="number">0</span>, split);</span><br><span class="line">        test = generateTestDataSet(stockDataList.subList(split, stockDataList.size()));</span><br><span class="line">        initializeOffsets();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">initializeOffsets</span> <span class="params">()</span> </span>&#123;</span><br><span class="line">        exampleStartOffsets.clear();</span><br><span class="line">        <span class="keyword">int</span> window = exampleLength + <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; train.size() - window; i++) &#123; exampleStartOffsets.add(i); &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">public</span> List&lt;Pair&lt;INDArray, INDArray&gt;&gt; getTestDataSet() &#123; <span class="keyword">return</span> test; &#125;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">double</span>[] getMaxNum() &#123; <span class="keyword">return</span> maxNum; &#125;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">double</span>[] getMinNum() &#123; <span class="keyword">return</span> minNum; &#125;</span><br><span class="line">    <span class="meta">@Override</span> <span class="function"><span class="keyword">public</span> DataSet <span class="title">next</span><span class="params">(<span class="keyword">int</span> num)</span> </span>&#123; ... &#125;</span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">double</span> <span class="title">feedLabel</span><span class="params">(StockData data)</span> </span>&#123; ... &#125;</span><br><span class="line">    <span class="meta">@Override</span> <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">totalExamples</span><span class="params">()</span> </span>&#123; <span class="keyword">return</span> train.size() - exampleLength - <span class="number">1</span>; &#125;</span><br><span class="line">    <span class="meta">@Override</span> <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">inputColumns</span><span class="params">()</span> </span>&#123; <span class="keyword">return</span> VECTOR_SIZE; &#125;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">totalOutcomes</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (<span class="keyword">this</span>.category.equals(PriceCategory.ALL)) <span class="keyword">return</span> VECTOR_SIZE;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="meta">@Override</span> <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">resetSupported</span><span class="params">()</span> </span>&#123; <span class="keyword">return</span> <span class="keyword">false</span>; &#125;</span><br><span class="line">    <span class="meta">@Override</span> <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">asyncSupported</span><span class="params">()</span> </span>&#123; <span class="keyword">return</span> <span class="keyword">false</span>; &#125;</span><br><span class="line">    <span class="meta">@Override</span> <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">reset</span><span class="params">()</span> </span>&#123; initializeOffsets(); &#125;</span><br><span class="line">    <span class="meta">@Override</span> <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">batch</span><span class="params">()</span> </span>&#123; <span class="keyword">return</span> miniBatchSize; &#125;</span><br><span class="line">    <span class="meta">@Override</span> <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">cursor</span><span class="params">()</span> </span>&#123; <span class="keyword">return</span> totalExamples() - exampleStartOffsets.size(); &#125;</span><br><span class="line">    <span class="meta">@Override</span> <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">numExamples</span><span class="params">()</span> </span>&#123; <span class="keyword">return</span> totalExamples(); &#125;</span><br><span class="line">    <span class="meta">@Override</span> <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setPreProcessor</span><span class="params">(DataSetPreProcessor dataSetPreProcessor)</span> </span>&#123; <span class="keyword">throw</span> <span class="keyword">new</span> UnsupportedOperationException(<span class="string">"Not Implemented"</span>); &#125;</span><br><span class="line">    <span class="meta">@Override</span> <span class="function"><span class="keyword">public</span> DataSetPreProcessor <span class="title">getPreProcessor</span><span class="params">()</span> </span>&#123; <span class="keyword">throw</span> <span class="keyword">new</span> UnsupportedOperationException(<span class="string">"Not Implemented"</span>); &#125;</span><br><span class="line">    <span class="meta">@Override</span> <span class="function"><span class="keyword">public</span> List&lt;String&gt; <span class="title">getLabels</span><span class="params">()</span> </span>&#123; <span class="keyword">throw</span> <span class="keyword">new</span> UnsupportedOperationException(<span class="string">"Not Implemented"</span>); &#125;</span><br><span class="line">    <span class="meta">@Override</span> <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">hasNext</span><span class="params">()</span> </span>&#123; <span class="keyword">return</span> exampleStartOffsets.size() &gt; <span class="number">0</span>; &#125;</span><br><span class="line">    <span class="meta">@Override</span> <span class="function"><span class="keyword">public</span> DataSet <span class="title">next</span><span class="params">()</span> </span>&#123; <span class="keyword">return</span> next(miniBatchSize); &#125;</span><br><span class="line">    <span class="keyword">private</span> List&lt;Pair&lt;INDArray, INDArray&gt;&gt; generateTestDataSet (List&lt;StockData&gt; stockDataList) &#123; ... &#125;</span><br><span class="line">    <span class="function"><span class="keyword">private</span> List&lt;StockData&gt; <span class="title">readStockDataFromFile</span> <span class="params">(String filename, String symbol)</span> </span>&#123; ... &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h1 id="Build-LSTM-RNNs"><a href="#Build-LSTM-RNNs" class="headerlink" title="Build LSTM RNNs"></a>Build LSTM RNNs</h1><p>After customizing the <code>StockDataSetIterator</code>, we need to build the recurrent neural networks model to train. To handle this task, I construct a four layer neural network, which contains two LSTM layers and two dense layers, <code>GravesLSTM</code> -&gt;  <code>GraveLSTM</code> -&gt; <code>DenseLayer</code> -&gt; <code>RNNOutputLayer</code>, and the stucture of the model and its parameters are shown in the codes.<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Created by zhanghao on 25/7/17.</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> ZHANG HAO</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">LSTMNetwork</span> </span>&#123;</span><br><span class="line">    ... (Parameters definition)</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> MultiLayerNetwork <span class="title">buildLstmNetworks</span> <span class="params">(<span class="keyword">int</span> nIn, <span class="keyword">int</span> nOut)</span> </span>&#123;</span><br><span class="line">        MultiLayerConfiguration conf = <span class="keyword">new</span> NeuralNetConfiguration.Builder()</span><br><span class="line">                .seed(seed)</span><br><span class="line">                .iterations(iterations)</span><br><span class="line">                .learningRate(learningRate)</span><br><span class="line">                .optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT)</span><br><span class="line">                .weightInit(WeightInit.XAVIER)</span><br><span class="line">                .updater(Updater.RMSPROP)</span><br><span class="line">                .regularization(<span class="keyword">true</span>)</span><br><span class="line">                .l2(<span class="number">1e-4</span>)</span><br><span class="line">                .list()</span><br><span class="line">                .layer(<span class="number">0</span>, <span class="keyword">new</span> GravesLSTM.Builder().nIn(nIn).nOut(lstmLayer1Size).activation(Activation.TANH).gateActivationFunction(Activation.HARDSIGMOID).dropOut(dropoutRatio).build())</span><br><span class="line">                .layer(<span class="number">1</span>, <span class="keyword">new</span> GravesLSTM.Builder().nIn(lstmLayer1Size).nOut(lstmLayer2Size).activation(Activation.TANH).gateActivationFunction(Activation.HARDSIGMOID).dropOut(dropoutRatio).build())</span><br><span class="line">                .layer(<span class="number">2</span>, <span class="keyword">new</span> DenseLayer.Builder().nIn(lstmLayer2Size).nOut(denseLayerSize).activation(Activation.RELU).build())</span><br><span class="line">                .layer(<span class="number">3</span>, <span class="keyword">new</span> RnnOutputLayer.Builder().nIn(denseLayerSize).nOut(nOut).activation(Activation.IDENTITY).lossFunction(LossFunctions.LossFunction.MSE).build())</span><br><span class="line">                .backpropType(BackpropType.TruncatedBPTT)</span><br><span class="line">                .tBPTTForwardLength(exampleLength)</span><br><span class="line">                .tBPTTBackwardLength(exampleLength)</span><br><span class="line">                .pretrain(<span class="keyword">false</span>)</span><br><span class="line">                .backprop(<span class="keyword">true</span>)</span><br><span class="line">                .build();</span><br><span class="line">        MultiLayerNetwork net = <span class="keyword">new</span> MultiLayerNetwork(conf);</span><br><span class="line">        net.init();</span><br><span class="line">        net.setListeners(<span class="keyword">new</span> ScoreIterationListener(<span class="number">100</span>));</span><br><span class="line">        <span class="keyword">return</span> net;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h1 id="Train-and-Predict"><a href="#Train-and-Predict" class="headerlink" title="Train and Predict"></a>Train and Predict</h1><p>For training and prediction process, we use the stock <code>GOOG</code> as an example. Firstly, we load the file, create the <code>StockDataSetIterator</code> and split the dataset to training dataset and test dataset. Then, construct the LSTM RNN model, and do the training process. The codes are shown below.<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Created by zhanghao on 26/7/17.</span></span><br><span class="line"><span class="comment"> * Modified by zhanghao on 28/9/17.</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> ZHANG HAO</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">StockPricePrediction</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Logger log = LoggerFactory.getLogger(StockPricePrediction.class);</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span> <span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        String file = <span class="keyword">new</span> ClassPathResource(<span class="string">"prices-split-adjusted.csv"</span>).getFile().getAbsolutePath();</span><br><span class="line">        String symbol = <span class="string">"GOOG"</span>; <span class="comment">// stock name</span></span><br><span class="line">        <span class="keyword">int</span> batchSize = <span class="number">64</span>; <span class="comment">// mini-batch size</span></span><br><span class="line">        <span class="keyword">int</span> exampleLength = <span class="number">22</span>; <span class="comment">// time series length, assume 22 working days per month</span></span><br><span class="line">        <span class="keyword">double</span> splitRatio = <span class="number">0.9</span>; <span class="comment">// 90% for training, 10% for testing</span></span><br><span class="line">        <span class="keyword">int</span> epochs = <span class="number">100</span>; <span class="comment">// training epochs</span></span><br><span class="line">        log.info(<span class="string">"Create dataSet iterator..."</span>);</span><br><span class="line">        PriceCategory category = PriceCategory.CLOSE; <span class="comment">// CLOSE: predict close price</span></span><br><span class="line">        StockDataSetIterator iterator = <span class="keyword">new</span> StockDataSetIterator(file, symbol, batchSize, exampleLength, splitRatio, category);</span><br><span class="line">        log.info(<span class="string">"Load test dataset..."</span>);</span><br><span class="line">        List&lt;Pair&lt;INDArray, INDArray&gt;&gt; test = iterator.getTestDataSet();</span><br><span class="line">        log.info(<span class="string">"Build lstm networks..."</span>);</span><br><span class="line">        MultiLayerNetwork net = RecurrentNets.buildLstmNetworks(iterator.inputColumns(), iterator.totalOutcomes());</span><br><span class="line">        log.info(<span class="string">"Training..."</span>);</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; epochs; i++) &#123;</span><br><span class="line">            <span class="keyword">while</span> (iterator.hasNext()) net.fit(iterator.next()); <span class="comment">// fit model using mini-batch data</span></span><br><span class="line">            iterator.reset(); <span class="comment">// reset iterator</span></span><br><span class="line">            net.rnnClearPreviousState(); <span class="comment">// clear previous state</span></span><br><span class="line">        &#125;</span><br><span class="line">        log.info(<span class="string">"Saving model..."</span>);</span><br><span class="line">        File locationToSave = <span class="keyword">new</span> File(<span class="string">"src/main/resources/StockPriceLSTM_"</span>.concat(String.valueOf(category)).concat(<span class="string">".zip"</span>));</span><br><span class="line">        <span class="comment">// saveUpdater: i.e., the state for Momentum, RMSProp, Adagrad etc. Save this to train your network more in the future</span></span><br><span class="line">        ModelSerializer.writeModel(net, locationToSave, <span class="keyword">true</span>);</span><br><span class="line">        log.info(<span class="string">"Load model..."</span>);</span><br><span class="line">        net = ModelSerializer.restoreMultiLayerNetwork(locationToSave);</span><br><span class="line">        log.info(<span class="string">"Testing..."</span>);</span><br><span class="line">        ...</span><br><span class="line">        log.info(<span class="string">"Done..."</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>Here is some <code>log</code> information while training, and the final prediction graph is shown below.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">[main] INFO com.isaac.stock.StockClosePricePrediction - create stock dataSet iterator...</span><br><span class="line">[main] INFO org.nd4j.linalg.factory.Nd4jBackend - Loaded [CpuBackend] backend</span><br><span class="line">...</span><br><span class="line">[main] INFO com.isaac.stock.StockClosePricePrediction - load <span class="built_in">test</span> dataset...</span><br><span class="line">[main] INFO com.isaac.stock.StockClosePricePrediction - build lstm networks...</span><br><span class="line">...</span><br><span class="line">[main] INFO com.isaac.stock.StockClosePricePrediction - training...</span><br><span class="line">[main] INFO org.deeplearning4j.optimize.listeners.ScoreIterationListener - Score at iteration 0 is 0.23434746144095608</span><br><span class="line">[main] INFO org.deeplearning4j.optimize.listeners.ScoreIterationListener - Score at iteration 100 is 0.20113769402560447</span><br><span class="line">[main] INFO org.deeplearning4j.optimize.listeners.ScoreIterationListener - Score at iteration 200 is 0.06535478890549833</span><br><span class="line">[main] INFO org.deeplearning4j.optimize.listeners.ScoreIterationListener - Score at iteration 300 is 0.007232614184257477</span><br><span class="line">[main] INFO org.deeplearning4j.optimize.listeners.ScoreIterationListener - Score at iteration 400 is 0.005167405140985004</span><br><span class="line">[main] INFO org.deeplearning4j.optimize.listeners.ScoreIterationListener - Score at iteration 500 is 0.00527686810765035</span><br><span class="line">...</span><br><span class="line">[main] INFO org.deeplearning4j.optimize.listeners.ScoreIterationListener - Score at iteration 2100 is 0.005341530172120316</span><br><span class="line">[main] INFO org.deeplearning4j.optimize.listeners.ScoreIterationListener - Score at iteration 2200 is 0.008277905296288288</span><br><span class="line">[main] INFO org.deeplearning4j.optimize.listeners.ScoreIterationListener - Score at iteration 2300 is 0.0065657371367870785</span><br><span class="line">[main] INFO org.deeplearning4j.optimize.listeners.ScoreIterationListener - Score at iteration 2400 is 0.006721078631611399</span><br><span class="line">...</span><br></pre></td></tr></table></figure></p><p><img src="/images/deeplearning/stockpredict/predict.png" alt="Prediction"><br>The graph above shows that the prediction result is not bad. Actually, the dataset I used here is quiet small, if the dataset is larger, the result should be more accurate, and another things is that the parameters setting also can be modified to achieve more convinced prediction. Anyway, it is just my first attempt to deal with stock price prediction tasks usring LSTMs. Plus, <a href="https://www.quandl.com/product/WIKIP/WIKI/PRICES-Quandl-End-Of-Day-Stocks-Info" target="_blank" rel="noopener">Quandl Financial and Economic Data</a> provides up to 40 years stock prices information for more than 3000 tickers, you can get more related data here.<br>Full Java Codes are available on my GitHub repository: <a href="https://github.com/IsaacChanghau/StockPrediction" target="_blank" rel="noopener">StockPrediction</a></p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul><li><a href="https://www.kaggle.com/dgawlik/nyse" target="_blank" rel="noopener">New York Stock Exchange – Kaggle</a></li><li><a href="https://deeplearning4j.org/usingrnns" target="_blank" rel="noopener">Using Recurrent Neural Networks in DL4J</a></li><li><a href="https://github.com/deeplearning4j/dl4j-examples/tree/master/dl4j-examples/src/main/java/org/deeplearning4j/examples/recurrent/character" target="_blank" rel="noopener">DL4J examples – GravesLSTMCharModellingExample</a></li><li><a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="noopener">Understanding LSTM Networks</a></li><li><a href="http://www.jakob-aungiers.com/articles/a/LSTM-Neural-Network-for-Time-Series-Prediction" target="_blank" rel="noopener">LSTM Neural Network for Time Series Prediction</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;This is a practice of using &lt;a href=&quot;https://en.wikipedia.org/wiki/Long_short-term_memory&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;LSTM&lt;/a&gt; to do the one day ahead prediction of the stock close price. The dataset I used here is the &lt;a href=&quot;https://www.kaggle.com/dgawlik/nyse&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;New York Stock Exchange&lt;/a&gt; from &lt;a href=&quot;https://www.kaggle.com/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Kaggle&lt;/a&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="https://isaacchanghau.github.io/categories/Machine-Learning/"/>
    
    
      <category term="java" scheme="https://isaacchanghau.github.io/tags/java/"/>
    
      <category term="deep learning" scheme="https://isaacchanghau.github.io/tags/deep-learning/"/>
    
      <category term="machine learning" scheme="https://isaacchanghau.github.io/tags/machine-learning/"/>
    
      <category term="lstm" scheme="https://isaacchanghau.github.io/tags/lstm/"/>
    
      <category term="deeplearning4j" scheme="https://isaacchanghau.github.io/tags/deeplearning4j/"/>
    
      <category term="spark" scheme="https://isaacchanghau.github.io/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>LSTM and GRU -- Formula Summary</title>
    <link href="https://isaacchanghau.github.io/2017/07/22/LSTM-and-GRU-Formula-Summary/"/>
    <id>https://isaacchanghau.github.io/2017/07/22/LSTM-and-GRU-Formula-Summary/</id>
    <published>2017-07-22T08:04:41.000Z</published>
    <updated>2018-01-30T08:32:28.000Z</updated>
    
    <content type="html"><![CDATA[<p>Long Short-Term Memory (LSTM) unit and Gated Recurrent Unit (GRU) RNNs are among the most widely used models in Deep Learning for NLP today. Both LSTM (<a href="http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf" target="_blank" rel="noopener">1997</a>) and GRU (<a href="http://arxiv.org/pdf/1406.1078v3.pdf" target="_blank" rel="noopener">2014</a>) are designed to combat the vanishing gradient problem prevents<a id="more"></a> standard RNNs from learning long-term dependencies through gating mechanism.<br>Note that, this article heavily rely on the following to articles, <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="noopener">Understanding LSTM Networks</a> and <a href="http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/" target="_blank" rel="noopener">Recurrent Neural Network Tutorial</a>, I summary the formula definition and explanation from them to enhance my understanding of LSTM and GRU as well as their similarity and difference.<br>GRU is a simpler variant of LSTMs that share many of the same properties, it combines the forget and input gates into a single “update gate”. And it also merges the cell state and hidden state, and makes some other changes. The resulting model is simpler than standard LSTM models, but its performance comparable to LSTM on sequence modeling, but less parameters and easier to train.<br>Below gives the general graph (screenshot from <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="noopener">Understanding LSTM Networks</a> article, LSTM is redrawn) of LSTM and GRU:<br><img src="/images/deeplearning/lstmgru/lstmandgru.png" alt="LSTM and GRU"><br>Denote $\ast$ as elementwise multiplication and <em>ignore bias term</em>, LSTM calculates a hidden state $h_{t}$ as$$\begin{aligned}<br>i_{t} &amp; =\sigma\big(x_{t}U^{i}+h_{t-1}W^{i}\big)\\<br>f_{t} &amp; =\sigma\big(x_{t}U^{f}+h_{t-1}W^{f}\big)\\<br>o_{t} &amp; =\sigma\big(x_{t}U^{o}+h_{t-1}W^{o}\big)\\<br>\tilde{C}_{t} &amp; =\tanh\big(x_{t}U^{g}+h_{t-1}W^{g}\big)\\<br>C_{t} &amp; =\sigma\big(f_{t}\ast C_{t-1}+i_{t}\ast\tilde{C}_{t}\big)\\<br>h_{t} &amp; =\tanh(C_{t})\ast o_{t}<br>\end{aligned}$$Here, $i$, $f$, $o$ are called the input, forget and output gates, respectively. Note that they have the exact same equations, just with different parameter matrices ($W$ is the <strong>recurrent connection at the previous hidden layer and current hidden layer</strong>, $U$ is the weight matrix <strong>connecting the inputs to the current hidden layer</strong>). They care called gates because the sigmoid function squashes the values of these vectors between 0 and 1, and by multiplying them elementwise with another vector you define how much of that other vector you want to “let through”. The input gate defines how much of the newly computed state for the current input you want to let through. The forget gate defines how much of the previous state you want to let through. Finally, The output gate defines how much of the internal state you want to expose to the external network (higher layers and the next time step). All the gates have the same dimensions $d_h$, the size of your hidden state.<br>$\tilde{C}$ is a “candidate” hidden state that is computed based on the current input and the previous hidden state.<br>$C$ is the internal memory of the unit. It is a combination of the previous memory, multiplied by the forget gate, and the newly computed hidden state, multiplied by the input gate. Thus, intuitively it is a combination of how we want to combine previous memory and the new input. We could choose to ignore the old memory completely (forget gate all 0’s) or ignore the newly computed state completely (input gate all 0’s), but most likely we want something in between these two extremes.<br>$h_{t}$ is output hidden state, computed by multiplying the memory with the output gate. Not all of the internal memory may be relevant to the hidden state used by other units in the network.<br>Intuitively, plain RNNs could be considered a special case of LSTMs. If fix the input gate all 1’s, the forget gate to all 0’s (say, always forget the previous memory) and the output gate to all 1’s (say, expose the whole memory), it will almost get a standard RNN.<br>For GRU, the hidden state $h_{t}$ is computed as$$\begin{aligned}<br>z_{t} &amp; =\sigma\big(x_{t}U^{z}+h_{t-1}W^{z}\big)\\<br>r_{t} &amp; =\sigma\big(x_{t}U^{r}+h_{t-1}W^{r}\big)\\<br>\tilde{h}_{t} &amp; =\tanh\big(x_{t}U^{h}+(r_{t}\ast h_{t-1})W^{h}\big)\\<br>h_{t} &amp; =(1-z_{t})\ast h_{t-1}+z_{t}\ast\tilde{h}_{t}<br>\end{aligned}$$Here $r$ is a reset gate, and $z$ is an update gate. Intuitively, the reset gate determines how to combine the new input with the previous memory, and the update gate defines how much of the previous memory to keep around. If set the reset to all 1’s and update gate to all 0’s, it will arrive at the vanilla RNN model. </p><p><strong>Reference</strong></p><ul><li><a href="https://arxiv.org/pdf/1412.3555.pdf" target="_blank" rel="noopener">Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling</a></li><li><a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="noopener">Understanding LSTM Networks</a></li><li><a href="http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/" target="_blank" rel="noopener">Recurrent Neural Network Tutorial, Part 4 – Implementing a GRU/LSTM RNN with Python and Theano</a></li><li><a href="https://arxiv.org/pdf/1308.0850.pdf" target="_blank" rel="noopener">Generating Sequences With Recurrent Neural Networks</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Long Short-Term Memory (LSTM) unit and Gated Recurrent Unit (GRU) RNNs are among the most widely used models in Deep Learning for NLP today. Both LSTM (&lt;a href=&quot;http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;1997&lt;/a&gt;) and GRU (&lt;a href=&quot;http://arxiv.org/pdf/1406.1078v3.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;2014&lt;/a&gt;) are designed to combat the vanishing gradient problem prevents
    
    </summary>
    
      <category term="Machine Learning" scheme="https://isaacchanghau.github.io/categories/Machine-Learning/"/>
    
    
      <category term="deep learning" scheme="https://isaacchanghau.github.io/tags/deep-learning/"/>
    
      <category term="machine learning" scheme="https://isaacchanghau.github.io/tags/machine-learning/"/>
    
      <category term="lstm" scheme="https://isaacchanghau.github.io/tags/lstm/"/>
    
      <category term="gru" scheme="https://isaacchanghau.github.io/tags/gru/"/>
    
  </entry>
  
  <entry>
    <title>Neural Responding Machine for Short-Text Conversation (STC) -- Summary</title>
    <link href="https://isaacchanghau.github.io/2017/07/19/Neural-Responding-Machine-for-Short-Text-Conversation/"/>
    <id>https://isaacchanghau.github.io/2017/07/19/Neural-Responding-Machine-for-Short-Text-Conversation/</id>
    <published>2017-07-19T08:09:37.000Z</published>
    <updated>2018-01-30T08:36:29.000Z</updated>
    
    <content type="html"><![CDATA[<p>The paper, <a href="https://arxiv.org/pdf/1503.02364.pdf" target="_blank" rel="noopener">Neural Responding Machine for Short-Text Conversation</a>, published by L Shang et. al. in 2015, introduces Neural Responding Machine (NRM), a neural network-based response generator for short-text conversation.<a id="more"></a> The NRM takes the general encoder-decoder framework, which formalizes the generation of response as a decoding process based on the latent representation of the input text, while both encoding and decoding are realized with recurrent neural networks (RNN). In this paper, the author demonstrates that the proposed encoder-decoder-based neural network overperform the traditional <strong>Retrivial-based methods</strong> and <strong>Statistical Machine Translation (SMT) based methods</strong>.</p><h1 id="Dataset-for-STC"><a href="#Dataset-for-STC" class="headerlink" title="Dataset for STC"></a>Dataset for STC</h1><p>The author prepares around <strong>4.4 million pairs</strong> of conversations from <a href="http://www.weibo.com/" target="_blank" rel="noopener">Weibo</a> to train the model. And the dataset is derived from hundreds of millions of post-response pairs by cleaning this raw data including:</p><ul><li>removing trivial responses like “wow”</li><li>filtering out potential advertisements</li><li>removing the response after first 30 ones for topic consistency.</li></ul><p>Below is some statistics of the dataset used:<br><img src="/images/nlp/NRM/dataset.png" alt="Dataset Overview"></p><h1 id="Neural-Responding-Machines"><a href="#Neural-Responding-Machines" class="headerlink" title="Neural Responding Machines"></a>Neural Responding Machines</h1><p>The neural responding machine generally contains three parts, <em>encoder</em>, <em>context generator</em> and <em>decoder</em>, as shown in the graph below. Here, both encoder and decoder are <strong>Recurrent Neural Networks</strong>, beasue of its natural ability to summarize and generate word sequence of arbitrary lengths.<br><img src="/images/nlp/NRM/framework.png" alt="Framework"><br>where $\mathbf{x}=(x_{1},\dots,x_{T})$ is the input sequence ($x_{i}$ denotes the $i$-th word), $\mathbf{h}=(h_{1},\dots,h_{T})$ is a set of high-dimensional hidden representations, $\alpha_{t}$ is the attention signal at time $t$, while $c_{t}$ is the context at time $t$, $y_{t}$ is the generated $t$-th word of response. And the general process is:</p><ol><li>The encoder converts the input sequence $\mathbf{x}$ into the hidden representations $\mathbf{h}$;</li><li>$\mathbf{h}$, along with attention signal $\alpha_{t}$, are fed to the context-generator to build the context $c_{t}$ input to decoder at time $t$.</li><li>The $c_{t}$ is linearly transformed by the transformation matrix $\mathbf{L}$ into a stimulus of generating RNN to produce the $t$-th word $y_{t}$ of response.</li></ol><p>Here $\mathbf{L}$ performs the role to transform the representation of post (or some part of it) to the rich representation of many plausible responses, while $\alpha_{t}$ is to determine which part of the hidden representation $\mathbf{h}$ should be emphasized during the generation process, normally, $\alpha_{t}$ is function of historically generated subsequence $(y_{1},\dots,y_{t-1})$, $\mathbf{x}$ or their latent representations.</p><h2 id="Encoder-and-Context-Generator"><a href="#Encoder-and-Context-Generator" class="headerlink" title="Encoder and Context Generator"></a>Encoder and Context Generator</h2><p>The author introduces three types of encoding schemes: global scheme, local scheme and hybrid scheme (combination of global and local schemes). And the context generator of each scheme is different.</p><h3 id="Global-Scheme"><a href="#Global-Scheme" class="headerlink" title="Global Scheme"></a>Global Scheme</h3><p>Graph below visualizes the global scheme of the RNN-encoder and related context generator.<br><img src="/images/nlp/NRM/global.png" alt="Global Scheme"><br>The hidden state at time $t$ is computed by $h_{t}=\mathcal{f}(x_{t},h_{t-1})$, while the context generator simply uses the last hidden state as the context vector, i.e, $c_{t}=h_{T}$, it means that final hidden state $h_{T}$ is used as the global representation of the sentence. However, this scheme has drawbacks that <strong>a vectorial summarization of the entire post is often hard to obtain and may lose important details for response generation, especially when the dimension of the hidden state is not big enough</strong>.<br>It is worth to mentione that $\mathcal{f}(\centerdot)$ can be a logistic function, sophisticated <a href="https://arxiv.org/pdf/1503.04069.pdf" target="_blank" rel="noopener">Long Short-Term Memory (LSTM) unit</a>, or <a href="https://arxiv.org/pdf/1412.3555.pdf" target="_blank" rel="noopener">Gated Recurrent Unit (GRU)</a>. Compared to “Ungated” or vanilla logistic function, LSTM and GRU are specially designed for its long term memory, which is able to store information over extended time steps without too much decay. The author chooses GRU for this task, due to its performance comparable to LSTM on sequence modeling, but less parameters and easier to train.</p><h3 id="Local-Scheme"><a href="#Local-Scheme" class="headerlink" title="Local Scheme"></a>Local Scheme</h3><p>Local scheme introduces an attention mechanism, which allows the encoder to dynamically select and linearly combine different parts of the input sequence, say, the contect vector here is computed by$$c_{t}=\sum_{j=1}^{T}\alpha_{tj}h_{j}$$where weighting factor $\alpha_{tj}$ is the attention signal to determine which part should be selected to generate the rew word $y_{t}$, which in turn is a function of hidden states $\alpha_{tj}=q(h_{j},s_{t-1})$, $s_{t-1}$ is the hidden state of decoder at time $t-1$, will be introduced later, graph of local scheme is shown below:<br><img src="/images/nlp/NRM/local.png" alt="Local Scheme"><br>Basically, the attention mechanism $\alpha_{tj}$ models the alignment between the inputs around position $j$ and the output at position $t$, so it can be viewed as a local matching model. This scheme enjoys <strong>the advantage of adaptively focusing on some important words of the input text according to the generated words of response</strong>, i.e., it cover the deficit that global scheme lost some important information.</p><h3 id="Hybrid-Scheme"><a href="#Hybrid-Scheme" class="headerlink" title="Hybrid Scheme"></a>Hybrid Scheme</h3><p>As mentioned before, global scheme has the summarization of the entire post, while local scheme can adaptively select the important words in post for various suitable responses. Since post-response pairs in STC are not strictly parallel and a word in different context can have different meanings, the author assumes that the global representation in may provide useful context for extracting the local context, therefore complementary to the local scheme. Thus, <strong>hybrid scheme is therefore to combine the global and local schemes by concatenating their encoded hidden states to form an extended hidden representation for each time stamp</strong>, as shown below:<br><img src="/images/nlp/NRM/hybrid.png" alt="Hybrid Scheme"><br>Here the summarization $h_{T}^{g}$ is incorporated into $c_{t}$ and $\alpha_{tj}$ to provide a global context for local matching, thus the context generator function is$$c_{t}=\sum_{j=1}^{T}\alpha_{tj}\big[h_{j}^{l};h_{T}^{g}\big]$$where $\big[h_{j}^{l};h_{T}^{g}\big]$ denotes the concatenation of vectors $h_{j}^{l}$ and $h_{T}^{g}$. The author also mention that the context generator in hybrid scheme will evoke different encoding mechanisms in the global encoder and the local encoder, although they will be combined later in forming a unified representation. More specifically, the last hidden state of global scheme plays a role different from that of the last state of local scheme, since it has the responsibility to encode the entire input sentence. This role of global scheme, however, tends to be not adequately emphasized in training the hybrid encoder when the parameters of the two encoding RNNs are learned jointly from scratch. For this the author first initialize hybrid scheme with the parameters of local scheme and global scheme trained separately, then fine ture the parameters in encoder along with training the parameters of decoder.</p><h2 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h2><p>The graph of decoder is shown below, which is essentially a standard RNN language model except conditioned on the context input $\mathbf{c}$.<br><img src="/images/nlp/NRM/decoder.png" alt="Decoder Scheme"><br>The generation probability of the $t$-th word is computed by$$p(y_{t}|y_{t-1},\dots,y_{1},\mathbf{x})=\mathcal{g}(y_{t-1},s_{t},c_{t})$$where $y_{t}$ is a one-hot word representation, $\mathcal{g}(\centerdot)$ is a softmax activation function, and $s_{t}$ is the hidden state of decoder at time $t$ computed by$$s_{t}=\mathcal{f}(y_{t-1},s_{t-1},c_{t})$$and $\mathcal{f}(\centerdot)$ is a non-linear activation function (i.e., still GRU unit), and the transformation $\mathbf{L}$ is often assigned as parameters of $\mathcal{f}(\centerdot)$.</p><p>To learn the parameters of the model, the target is to maximize the likelihood of observing the original response conditioned on the post in the training set. While for a new post, NRMs generate their responses by using a left-to-right <a href="https://en.wikipedia.org/wiki/Beam_search" target="_blank" rel="noopener">beam search</a> with beam size = 10.</p><h1 id="Implementation-Details-and-Experiments"><a href="#Implementation-Details-and-Experiments" class="headerlink" title="Implementation Details and Experiments"></a>Implementation Details and Experiments</h1><p>The author uses <a href="https://nlp.stanford.edu/software/segmenter.shtml" target="_blank" rel="noopener">Stanford Chinese word segmenter</a> to split the posts and responses into sequences of words. Since the number of unique words in post text is 125,237, and that of response text is 679,958. The author therefore construct two separate vocabularies for posts and responses by using <strong>40,000</strong> most frequent words on each side, covering 97.8% usage of words for post and 96.2% for response respectively. All the remaining words are replaced by a special token “UNK”. The dimensions of the hidden states of encoder and decoder are both <strong>1000</strong>, and the dimensions of the word-embedding for post and response are both <strong>620</strong>. Model parameters are initialized by randomly sampling from a uniform distribution between -0.1 and 0.1. Models are trained by stochastic gradient descent algorithm with mini-batch.<br>Below shows the responses generated by different models, where <em>NRM-glo</em> denotes global scheme, <em>NRM-loc</em> represents local scheme and <em>NRM-hyb</em> is the hybrid scheme, while <em>Rtr.-based</em> represents the Retrieval-based method used as a competitor.<br><img src="/images/nlp/NRM/result.png" alt="Result"><br>Above is a general summary of the paper: “Neural Responding Machine for Short-Text Conversation”, I only describe the algorithm introduced in this paper, and I am trying to build this model by myself in the furture.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;The paper, &lt;a href=&quot;https://arxiv.org/pdf/1503.02364.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Neural Responding Machine for Short-Text Conversation&lt;/a&gt;, published by L Shang et. al. in 2015, introduces Neural Responding Machine (NRM), a neural network-based response generator for short-text conversation.
    
    </summary>
    
      <category term="Natural Language Processing" scheme="https://isaacchanghau.github.io/categories/Natural-Language-Processing/"/>
    
    
      <category term="deep learning" scheme="https://isaacchanghau.github.io/tags/deep-learning/"/>
    
      <category term="lstm" scheme="https://isaacchanghau.github.io/tags/lstm/"/>
    
      <category term="gru" scheme="https://isaacchanghau.github.io/tags/gru/"/>
    
      <category term="natural language processing" scheme="https://isaacchanghau.github.io/tags/natural-language-processing/"/>
    
      <category term="seq2seq" scheme="https://isaacchanghau.github.io/tags/seq2seq/"/>
    
  </entry>
  
  <entry>
    <title>Singapore, Two Years</title>
    <link href="https://isaacchanghau.github.io/2017/07/14/%E6%96%B0%E5%8A%A0%E5%9D%A1%E4%B8%A4%E5%B9%B4/"/>
    <id>https://isaacchanghau.github.io/2017/07/14/新加坡两年/</id>
    <published>2017-07-14T14:22:09.000Z</published>
    <updated>2017-07-23T07:54:41.000Z</updated>
    
    <content type="html"><![CDATA[<p>以此纪念在异国漂泊的两年……<a id="more"></a><br>14岁…离开家去另一座城市上高中…<br>17岁…高中毕业，在家短暂的待了一个月，便启程去了遥远的北方城市，大连…<br>21岁…大学毕业，匆匆回家，十四天后，登上了飞往新加坡的航班…<br>这个故事，便从21岁开始。</p><p>2015年7月14日一大早，带上前一晚打包好的行李出发…<br>不舍，因为离别。</p><p>新加坡真的是很干净，从机场出来，坐上的士赶往NTU，一路上的风景也算是让我小小的惊呆了。到NTU已经是晚上，学校很安静，但那会儿的我好像没有心思去感受，因为从踏上这片土地时，心里一直有种空虚感，很少像这样，想家……</p><p>似乎攻略没做好，不认路，没有晚餐… 在新朋友的帮助下，好不容易找到一间7-11，吃了来新加坡的第一顿饭：泡面。而回到寝室发现没有被子，没有床单，没有枕头。那晚，我就简单的擦了擦床垫，蜷在上面挨过了异国他乡的第一个夜晚。</p><p>那晚，我不停问自己，这个选择，好吗？明知道只有今后漫长的时光能慢慢告诉我答案，可还是忍不住沉思…</p><p>假如，我没有选择出国，不用花费大量的时间学英语，也许我会继续大学时的研究方向，接着走下去；不用沉重的签下放弃保研承诺书，这样的话，我最终会去到哪个学校呢？</p><p>很快，我便不再思考这些没用的问题，因为我没有重来一次的机会，也因为新的生活，比我想象中，更有趣。</p><p>这里的生活没有国内丰富，但这里的人很nice。</p><p>在学校的一年里，主旋律是学习，伴奏是旅行和探索。第一次面对全英文授课时，内心无遗是崩溃的，而论文也是一头雾水。一路跌跌撞撞，参杂着喜和忧，走到了毕业。</p><p>一年很短，很快又是离别…</p><p>毕业，我送给自己一首诗：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">小小的屋里弥漫着</span><br><span class="line">每个夜晚</span><br><span class="line">安抚着伤感焦虑的灵魂</span><br><span class="line">伴着芬芳</span><br><span class="line">静静飘散在孤单单的梦里</span><br><span class="line"></span><br><span class="line">启程、离别</span><br><span class="line">充满着勇气</span><br><span class="line">却没有魄力去忘记</span><br><span class="line">几日几年后</span><br><span class="line">回首看看</span><br><span class="line">也算是无悔的经历</span><br><span class="line"></span><br><span class="line">若从来没有无谓的坚持</span><br><span class="line">又何为无奈的放弃</span><br><span class="line">凡事未必结果</span><br><span class="line">却并非没有意义</span><br><span class="line">我只是努力和憧憬着小小的愿望和私心罢了</span><br><span class="line"></span><br><span class="line">坚持，为爱的人，为想要的生活</span><br><span class="line">坚持下去</span><br><span class="line">什么样的方式</span><br><span class="line">什么样的存在</span><br><span class="line">什么样的结局</span><br><span class="line">都无所谓</span><br><span class="line">我有强大的内心</span><br><span class="line"></span><br><span class="line">暂别，Singapore</span><br><span class="line">再回来，就不再是可以不管不顾的傻傻学生了...</span><br></pre></td></tr></table></figure></p><p>离开学校，很快就开始工作，连毕业旅行的时间都没留下。而工作，正如很多人所说的那样，教会了我许许多多在学校里学不到和从来不曾想过的事情。这一年里，我最大的体悟便是那曾经最为触动我内心的一句话 – 成熟，不是学会表达，而是学会咽下，当你一点一点学会克制住很多东西，才能驾驭好人生。</p><p>本想好好的，写一写，曾经的点滴，但我发现，似乎不太有意义。曾经老师教过，说过程是重要的，但后来，我发现，没人在乎过程，大家想要的似乎都只是结果而已。而慢慢的，我还发现，经历过什么其实也不是那么的重要，重要的是，从这些经历中得到了什么。努力过，拼搏过，感动过，伤感过，哭泣过，跌倒过，战胜过，成功过，又有何用，那些都只是过去，都将被淹没在时间的长河里，被人遗忘，被自己遗忘。而只有刻在内心深处的那些因为经历而留下的印记和感悟或许才会终身受用，这些印记又如何能表达，如何能让人感同身受？我觉得似乎不能。。。</p><p>生活从来都不会太美好，但它还会继续，无论是否愿意。倒不是悲观，我很享受现在，即使迷惘，即使彷徨，好奇和期待着接下来的发生的一切。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;以此纪念在异国漂泊的两年……
    
    </summary>
    
      <category term="Life" scheme="https://isaacchanghau.github.io/categories/Life/"/>
    
    
      <category term="life" scheme="https://isaacchanghau.github.io/tags/life/"/>
    
  </entry>
  
  <entry>
    <title>House Prices Advanced Regression Techniques -- Training and Prediction</title>
    <link href="https://isaacchanghau.github.io/2017/07/10/House-Price-Advanced-Regression-Techniques-2/"/>
    <id>https://isaacchanghau.github.io/2017/07/10/House-Price-Advanced-Regression-Techniques-2/</id>
    <published>2017-07-10T04:45:46.000Z</published>
    <updated>2017-09-21T09:19:29.000Z</updated>
    
    <content type="html"><![CDATA[<p>After cleaning and transforming processes in <a href="https://isaacchanghau.github.io/2017/07/08/House-Price-Advanced-Regression-Techniques-1/">House Prices Advanced Regression Techniques – Data Analysis Exploration</a>.<a id="more"></a> Here we continue to build machine learning model to train the dataset and make a prediction based on the trained model. We use <strong>Elastic Net</strong> and <strong>Gradient Boosting</strong> models to train the dataset and make predictions separately, then average the results of the two models to generate the final output.</p><h1 id="Elastic-Net-Regression"><a href="#Elastic-Net-Regression" class="headerlink" title="Elastic Net Regression"></a>Elastic Net Regression</h1><p>In statistics and, in particular, in the fitting of linear or logistic regression models, the elastic net is a regularized regression method that linearly combines the L1 and L2 penalties of the <a href="https://en.wikipedia.org/wiki/Lasso_%28statistics%29" target="_blank" rel="noopener">lasso</a> and <a href="https://en.wikipedia.org/wiki/Tikhonov_regularization" target="_blank" rel="noopener">ridge</a> methods.<br>The target function of Lasso is given as:$$\min_{\mathbf{w}}\frac{1}{2n}\Vert\mathbf{X}^{T}\mathbf{w}-\mathbf{y}\Vert_{2}^{2}+\alpha\Vert\mathbf{w}\Vert_{1}$$where $n$ is number of samples, $\mathbf{w}$ is the coefficient parameters to learn, $\Vert\mathbf{w}\Vert_{1}$ term is L1 penality, which promotes sparsity, reduces the redundancy and improves the accurancy and robustness of regression (it alleviates the overfitting in some degree). Also if there is a group of highly correlated variables, then the Lasso tends to select one variable from a group and ignore the others. However, the L1 norm in Lass has some limitations that, for example, in the “large $p$ and small $n$” case (high-dimensional data with few examples), the Lasso selects at most n variables before it saturates.<br>While the target function of Ridge is computed by:$$\min_{\mathbf{w}}\frac{1}{2}\Vert\mathbf{X}^{T}\mathbf{w}-\mathbf{y}\Vert_{2}^{2}+\frac{\alpha}{2}\Vert\mathbf{w}\Vert_{2}$$where $\Vert\mathbf{w}\Vert_{2}^{2}$ term is L2 penality, which constrains the module of $\mathbf{w}$ in to a L2 ball to alleviate the overfitting problem. However, L2 norm shrinkages the value of approximated parameters, but does not make it zero, which means it does not perform the function of parameter selection.<br>For Elastic Net, it can be treated as a compromise between Lasso and Ridge, since it integrates the L1 and L2 regularizations. Its target function is described as:$$\min_{\mathbf{w}}\frac{1}{2n}\Vert\mathbf{X}^{T}\mathbf{w}-\mathbf{y}\Vert_{2}^{2}+\alpha\rho\Vert\mathbf{w}\Vert_{1}+\frac{\alpha(1-\rho)}{2}\Vert\mathbf{w}\Vert_{2}^{2}$$where $\rho$ is an adaptive hyper-parameter to control the contributions of L1 norm and L2 norm. With this compromise, Elastic Net remains part of the parameter selection function in Lasso and part of rotary stability property in Ridge. Meanwhile, compare to Lasso, Elastic Net not only randomly select one of the correlated variables, but also tends to obtain all of them and enahnces their group effect.</p><h1 id="Gradient-Boosting-Regression"><a href="#Gradient-Boosting-Regression" class="headerlink" title="Gradient Boosting Regression"></a>Gradient Boosting Regression</h1><p>Gradient boosting is a machine learning technique for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees. It builds the model in a stage-wise fashion like other boosting methods do, and it generalizes them by allowing optimization of an arbitrary differentiable loss function.<br>Like other boosting methods, gradient boosting combines weak “learners” into a single strong learner in an iterative fashion. It is easiest to explain in the least-squares regression setting, where the goal is to “teach” a model $F$ to predict values in the form $\hat{y} = F(\mathbf{x})$ by minimizing the mean squared error $(\hat{y} - y)^{2}$, averaged over some training set of actual values of the output variable $y$.<br>Given a training dataset $\{(\mathbf{x}_{1},y_{1}),(\mathbf{x}_{2},y_{2}),\dots,(\mathbf{x}_{n},y_{n})\}$, the goal is to find an approximation $\hat{F}(\mathbf{x})$ to a function $F(\mathbf{x})$ that minimizes the expected value of some specified loss function $\mathcal{L}(y, F(x))$:$$\hat{F}=\arg\min_{F}\mathbb{E}_{\mathbf{x},y}\big[\mathcal{L}(y, F(\mathbf{x}))\big]$$The gradient boosting method assumes a real-valued $y$ and seeks an approximation $\hat{F}(\mathbf{x})$ in the form of a weighted sum of functions $h_{i}(\mathbf{x})$ from some class $\mathcal{H}$, called base (or weak) learners:$$F(\mathbf{x})=\sum_{i=1}^{M}\gamma_{i}h_{i}(\mathbf{x})+const.$$In accordance with the empirical risk minimization principle, the method tries to find an approximation $\hat{F}(\mathbf{x})$ that minimizes the average value of the loss function on the training set. It does so by starting with a model, consisting of a constant function $F_{0}(\mathbf{x})$, and incrementally expanding it in a greedy fashion:$$F_{0}(\mathbf{x})=\arg\min_{\gamma}\sum_{i=1}^{n}\mathcal{L}(y_{i},\gamma)\\F_{m}(\mathbf{x})=F_{m-1}(\mathbf{x})+\arg\min_{h\in\mathcal{H}}\sum_{i=1}^{n}\mathcal{L}(y_{i},F_{m-1}(\mathbf{x}_{i})+h(\mathbf{x}_{i}))$$where $h\in\mathcal{H}$ is a base learner function.<br>Unfortunately, choosing the best function $h$ at each step for an arbitrary loss function $\mathcal{L}$ is a computationally infeasible optimization problem in general. Therefore, we will restrict to a simplification.<br>The idea is to apply a steepest descent step to this minimization problem. If we considered the continuous case, i.e. $\mathcal{H}$ the set of arbitrary differentiable functions on $\mathbb{R}$, we would update the model in accordance with the following equations:$$F_{m}(\mathbf{x})=F_{m-1}(\mathbf{x})-\gamma_{m}\sum_{i=1}^{n}\nabla_{F_{m-1}}\mathcal{L}(y_{i},F_{m-1}(\mathbf{x}_{i}))\\\gamma_{m}=\arg\min_{\gamma}\sum_{i=1}^{n}\mathcal{L}\bigg(y_{i},F_{m-1}(\mathbf{x}_{i})-\gamma\frac{\partial\mathcal{L}(y_{i},F_{m-1}(\mathbf{x}_{i}))}{\partial F_{m-1}(\mathbf{x}_{i})}\bigg)$$where the derivatives are taken with respect to the functions $F_{i}$ for $i\in\{1,\dots,m\}$. In the discrete case however, i.e. the set $\mathcal{H}$ is finite, we will choose the candidate function $h$ closest to the gradient of $\mathcal{L}$ for which the coefficient $\gamma$ may then be calculated with the aid of line search the above equations. Note that this approach is a heuristic and will therefore not yield an exact solution to the given problem, yet a satisfactory approximation.<br>In pseudocode, the generic gradient boosting method is:</p><ol><li>Input: training dataset $\{(\mathbf{x}_{i},y_{i})\}_{i=1}^{n}$, a differentiable loss function $\mathcal{L}(y,F(\mathbf{x}))$, number of iterations $M$.</li><li>Initialize model with a constant value: $F_{0}(\mathbf{x})=\arg\min_{\gamma}\sum_{i=1}^{n}\mathcal{L}(y_{i},\gamma)$.</li><li>For $m=1$ to $M$:<br>3.1. Compute so-called pseudo-residuals: $r_{im}=-\big[\frac{\partial\mathcal{L}(y_{i},F(\mathbf{x}_{i}))}{\partial F(\mathbf{x}_{i})}\big]_{F(\mathbf{x})=F_{m-1}(\mathbf{x})}$, for $i=1,\dots,n$.<br>3.2. Fit a base learner (e.g. tree) $h_{m}(\mathbf{x})$ to pseudo-residuals, i.e., train it using the training dataset $\{\mathbf{x}_{i},r_{im}\}_{i=1}^{n}$.<br>3.3. Compute multiplier $\gamma_{m}$ by solving <a href="https://en.wikipedia.org/wiki/Line_search" target="_blank" rel="noopener">one-dimensional optimization</a> problem: $\gamma_{m}=\arg\min_{\gamma}\sum_{i=1}^{n}\mathcal{L}(y_{i},F_{m-1}(\mathbf{x}_{i})+\gamma h_{m}(\mathbf{x}_{i}))$.<br>3.4. Update the model: $F_{m}(\mathbf{x})=F_{m-1}(\mathbf{x})+\gamma_{m}h_{m}(\mathbf{x})$.</li><li>Output $F_{M}(\mathbf{x})$.</li></ol><h1 id="Summary-of-Data-Analysis-Process"><a href="#Summary-of-Data-Analysis-Process" class="headerlink" title="Summary of Data Analysis Process"></a>Summary of Data Analysis Process</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> ensemble, linear_model</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split, cross_val_score</span><br><span class="line"><span class="keyword">from</span> sklearn.utils <span class="keyword">import</span> shuffle</span><br><span class="line"><span class="keyword">from</span> method2.functions <span class="keyword">import</span> train_test</span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line">warnings.filterwarnings(<span class="string">'ignore'</span>)</span><br><span class="line"><span class="comment"># load data</span></span><br><span class="line">train = pd.read_csv(<span class="string">'train.csv'</span>)</span><br><span class="line">test = pd.read_csv(<span class="string">'test.csv'</span>)</span><br><span class="line">train_labels = train.pop(<span class="string">'SalePrice'</span>)</span><br><span class="line">data = pd.concat([train, test], keys=[<span class="string">'train'</span>, <span class="string">'test'</span>])</span><br><span class="line"><span class="comment"># drop the data with high missing percentage</span></span><br><span class="line">data.drop([<span class="string">'Id'</span>, <span class="string">'MiscFeature'</span>, <span class="string">'Fence'</span>, <span class="string">'PoolQC'</span>, <span class="string">'FireplaceQu'</span>, <span class="string">'Alley'</span>], axis=<span class="number">1</span>, inplace=<span class="keyword">True</span>)</span><br><span class="line"><span class="comment"># drop the Bsmt feature group, Garage feature group as well as some trivial features</span></span><br><span class="line">data.drop([<span class="string">'Utilities'</span>, <span class="string">'RoofMatl'</span>, <span class="string">'MasVnrArea'</span>, <span class="string">'MasVnrType'</span>, <span class="string">'Heating'</span>, <span class="string">'LowQualFinSF'</span>, <span class="string">'BsmtFullBath'</span>,</span><br><span class="line">           <span class="string">'BsmtHalfBath'</span>, <span class="string">'BsmtQual'</span>, <span class="string">'BsmtCond'</span>, <span class="string">'BsmtExposure'</span>, <span class="string">'BsmtFinType1'</span>, <span class="string">'BsmtFinSF1'</span>, <span class="string">'BsmtFinSF2'</span>,</span><br><span class="line">           <span class="string">'BsmtUnfSF'</span>, <span class="string">'BsmtFinType2'</span>, <span class="string">'Functional'</span>, <span class="string">'WoodDeckSF'</span>, <span class="string">'OpenPorchSF'</span>,</span><br><span class="line">           <span class="string">'GarageYrBlt'</span>, <span class="string">'GarageCond'</span>, <span class="string">'GarageType'</span>, <span class="string">'GarageFinish'</span>, <span class="string">'GarageQual'</span>, <span class="string">'GarageArea'</span>,</span><br><span class="line">           <span class="string">'EnclosedPorch'</span>, <span class="string">'3SsnPorch'</span>, <span class="string">'ScreenPorch'</span>, <span class="string">'PoolArea'</span>, <span class="string">'MiscVal'</span>], axis=<span class="number">1</span>, inplace=<span class="keyword">True</span>)</span><br><span class="line"><span class="comment"># merge TotalBsmtSF, 1stFlrSF, 2ndFlrSF to TotalSF</span></span><br><span class="line">data[<span class="string">'TotalBsmtSF'</span>] = data[<span class="string">'TotalBsmtSF'</span>].fillna(<span class="number">0</span>)</span><br><span class="line">data[<span class="string">'1stFlrSF'</span>] = data[<span class="string">'1stFlrSF'</span>].fillna(<span class="number">0</span>)</span><br><span class="line">data[<span class="string">'2ndFlrSF'</span>] = data[<span class="string">'2ndFlrSF'</span>].fillna(<span class="number">0</span>)</span><br><span class="line">data[<span class="string">'TotalSF'</span>] = data[<span class="string">'TotalBsmtSF'</span>] + data[<span class="string">'1stFlrSF'</span>] + data[<span class="string">'2ndFlrSF'</span>]</span><br><span class="line">data.drop([<span class="string">'TotalBsmtSF'</span>, <span class="string">'1stFlrSF'</span>, <span class="string">'2ndFlrSF'</span>], axis=<span class="number">1</span>, inplace=<span class="keyword">True</span>)</span><br><span class="line"><span class="comment"># MSSubClass as categorical</span></span><br><span class="line">data[<span class="string">'MSSubClass'</span>] = data[<span class="string">'MSSubClass'</span>].astype(str)</span><br><span class="line"><span class="comment"># MSZoning: filling NA with most popular values</span></span><br><span class="line">data[<span class="string">'MSZoning'</span>] = data[<span class="string">'MSZoning'</span>].fillna(data[<span class="string">'MSZoning'</span>].mode()[<span class="number">0</span>])</span><br><span class="line"><span class="comment"># LotFrontage: fill NA with mean value</span></span><br><span class="line">data[<span class="string">'LotFrontage'</span>] = data[<span class="string">'LotFrontage'</span>].fillna(data[<span class="string">'LotFrontage'</span>].mean())</span><br><span class="line"><span class="comment"># OverallCond as categorical</span></span><br><span class="line">data[<span class="string">'OverallCond'</span>] = data[<span class="string">'OverallCond'</span>].astype(str)</span><br><span class="line"><span class="comment"># Electrical: fill NA with most popular values</span></span><br><span class="line">data[<span class="string">'Electrical'</span>] = data[<span class="string">'Electrical'</span>].fillna(data[<span class="string">'Electrical'</span>].mode()[<span class="number">0</span>])</span><br><span class="line"><span class="comment"># KitchenAbvGr as categorical</span></span><br><span class="line">data[<span class="string">'KitchenAbvGr'</span>] = data[<span class="string">'KitchenAbvGr'</span>].astype(str)</span><br><span class="line"><span class="comment"># KitchenQual: fill NA with most popular values</span></span><br><span class="line">data[<span class="string">'KitchenQual'</span>] = data[<span class="string">'KitchenQual'</span>].fillna(data[<span class="string">'KitchenQual'</span>].mode()[<span class="number">0</span>])</span><br><span class="line"><span class="comment"># GarageCars: fill NA with 0</span></span><br><span class="line">data[<span class="string">'GarageCars'</span>] = data[<span class="string">'GarageCars'</span>].fillna(<span class="number">0.0</span>)</span><br><span class="line"><span class="comment"># SaleType: fill NA with most popular values</span></span><br><span class="line">data[<span class="string">'SaleType'</span>] = data[<span class="string">'SaleType'</span>].fillna(data[<span class="string">'SaleType'</span>].mode()[<span class="number">0</span>])</span><br><span class="line"><span class="comment"># Year and Month as categorical</span></span><br><span class="line">data[<span class="string">'YrSold'</span>] = data[<span class="string">'YrSold'</span>].astype(str)</span><br><span class="line">data[<span class="string">'MoSold'</span>] = data[<span class="string">'MoSold'</span>].astype(str)</span><br><span class="line"><span class="comment"># Exterior1st and Exterior2nd: fill NA with most popular values</span></span><br><span class="line">data[<span class="string">'Exterior1st'</span>] = data[<span class="string">'Exterior1st'</span>].fillna(data[<span class="string">'Exterior2nd'</span>].mode()[<span class="number">0</span>])</span><br><span class="line">data[<span class="string">'Exterior2nd'</span>] = data[<span class="string">'Exterior2nd'</span>].fillna(data[<span class="string">'Exterior2nd'</span>].mode()[<span class="number">0</span>])</span><br><span class="line"><span class="comment"># Standardizing LotFrontage and LotArea</span></span><br><span class="line">numeric_data = data.loc[:, [<span class="string">'LotFrontage'</span>, <span class="string">'LotArea'</span>, <span class="string">'GrLivArea'</span>, <span class="string">'TotalSF'</span>]]</span><br><span class="line">numeric_data_standardized = (numeric_data - numeric_data.mean())/numeric_data.std()</span><br><span class="line"><span class="comment"># Log transformation of labels, GrLivArea and TotalSF</span></span><br><span class="line">train_labels = np.log(train_labels)</span><br><span class="line"><span class="comment"># data['GrLivArea'] = np.log(data['GrLivArea'])</span></span><br><span class="line"><span class="comment"># data['TotalSF'] = np.log(data['TotalSF'])</span></span><br><span class="line"><span class="comment"># Getting Dummies from Condition1 and Condition2</span></span><br><span class="line">conditions = set([x <span class="keyword">for</span> x <span class="keyword">in</span> data[<span class="string">'Condition1'</span>]] + [x <span class="keyword">for</span> x <span class="keyword">in</span> data[<span class="string">'Condition2'</span>]])</span><br><span class="line">dummies = pd.DataFrame(data=np.zeros((len(data.index), len(conditions))), index=data.index, columns=conditions)</span><br><span class="line"><span class="keyword">for</span> i, cond <span class="keyword">in</span> enumerate(zip(data[<span class="string">'Condition1'</span>], data[<span class="string">'Condition2'</span>])):</span><br><span class="line">    dummies.ix[i, cond] = <span class="number">1</span></span><br><span class="line">data = pd.concat([data, dummies.add_prefix(<span class="string">'Condition_'</span>)], axis=<span class="number">1</span>)</span><br><span class="line">data.drop([<span class="string">'Condition1'</span>, <span class="string">'Condition2'</span>], axis=<span class="number">1</span>, inplace=<span class="keyword">True</span>)</span><br><span class="line"><span class="comment"># Getting Dummies from Exterior1st and Exterior2nd</span></span><br><span class="line">exteriors = set([x <span class="keyword">for</span> x <span class="keyword">in</span> data[<span class="string">'Exterior1st'</span>]] + [x <span class="keyword">for</span> x <span class="keyword">in</span> data[<span class="string">'Exterior2nd'</span>]])</span><br><span class="line">dummies = pd.DataFrame(data=np.zeros((len(data.index), len(exteriors))), index=data.index, columns=exteriors)</span><br><span class="line"><span class="keyword">for</span> i, ext <span class="keyword">in</span> enumerate(zip(data[<span class="string">'Exterior1st'</span>], data[<span class="string">'Exterior2nd'</span>])):</span><br><span class="line">    dummies.ix[i, ext] = <span class="number">1</span></span><br><span class="line">data = pd.concat([data, dummies.add_prefix(<span class="string">'Exterior_'</span>)], axis=<span class="number">1</span>)</span><br><span class="line">data.drop([<span class="string">'Exterior1st'</span>, <span class="string">'Exterior2nd'</span>], axis=<span class="number">1</span>, inplace=<span class="keyword">True</span>)</span><br><span class="line"><span class="comment"># Getting Dummies from all other categorical vars</span></span><br><span class="line"><span class="keyword">for</span> col <span class="keyword">in</span> data.dtypes[data.dtypes == <span class="string">'object'</span>].index:</span><br><span class="line">    for_dummy = data.pop(col)</span><br><span class="line">    data = pd.concat([data, pd.get_dummies(for_dummy, prefix=col)], axis=<span class="number">1</span>)</span><br><span class="line"><span class="comment"># copy data</span></span><br><span class="line">data_standardized = data.copy()</span><br><span class="line"><span class="comment"># Replacing numeric feature by standardized values</span></span><br><span class="line">data_standardized.update(numeric_data_standardized)</span><br><span class="line"><span class="comment"># Splitting dataset to train and test</span></span><br><span class="line">train_data = data.loc[<span class="string">'train'</span>].select_dtypes(include=[np.number]).values</span><br><span class="line">test_data = data.loc[<span class="string">'test'</span>].select_dtypes(include=[np.number]).values</span><br><span class="line"><span class="comment"># Splitting standardized features</span></span><br><span class="line">train_data_st = data_standardized.loc[<span class="string">'train'</span>].select_dtypes(include=[np.number]).values</span><br><span class="line">test_data_st = data_standardized.loc[<span class="string">'test'</span>].select_dtypes(include=[np.number]).values</span><br></pre></td></tr></table></figure><h1 id="Model-Construction-Training-and-Prediction"><a href="#Model-Construction-Training-and-Prediction" class="headerlink" title="Model Construction, Training and Prediction"></a>Model Construction, Training and Prediction</h1><h2 id="Shuffling-and-Splitting-Data"><a href="#Shuffling-and-Splitting-Data" class="headerlink" title="Shuffling and Splitting Data"></a>Shuffling and Splitting Data</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Shuffling train sets</span></span><br><span class="line">train_features_st, train_features, train_labels = shuffle(train_features_st, train_features, train_labels, random_state=<span class="number">5</span>)</span><br><span class="line"><span class="comment"># Splitting</span></span><br><span class="line">x_train, x_test, y_train, y_test = train_test_split(train_features, train_labels, test_size=<span class="number">0.1</span>, random_state=<span class="number">200</span>)</span><br><span class="line">x_train_st, x_test_st, y_train_st, y_test_st = train_test_split(train_features_st, train_labels, test_size=<span class="number">0.1</span>, random_state=<span class="number">200</span>)</span><br></pre></td></tr></table></figure><p>where <code>**_features_st</code> dataset is used for training Elastic Net, while <code>**_features</code> dataset is used for training Gradient Boosting Regressor.<br><strong>Define two functions to show R2 and RMSE scores for train and validation sets</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> r2_score, mean_squared_error</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment"># Prints R2 and RMSE scores</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_score</span><span class="params">(prediction, labels)</span>:</span></span><br><span class="line">    print(<span class="string">'R2: &#123;&#125;'</span>.format(r2_score(prediction, labels)))</span><br><span class="line">    print(<span class="string">'RMSE: &#123;&#125;'</span>.format(np.sqrt(mean_squared_error(prediction, labels))))</span><br><span class="line"><span class="comment"># Shows scores for train and validation sets</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_test</span><span class="params">(estimator, x_trn, x_tst, y_trn, y_tst)</span>:</span></span><br><span class="line">    prediction_train = estimator.predict(x_trn)</span><br><span class="line">    <span class="comment"># Printing estimator</span></span><br><span class="line">    print(estimator)</span><br><span class="line">    <span class="comment"># Printing train scores</span></span><br><span class="line">    get_score(prediction_train, y_trn)</span><br><span class="line">    prediction_test = estimator.predict(x_tst)</span><br><span class="line">    <span class="comment"># Printing test scores</span></span><br><span class="line">    print(<span class="string">"Test"</span>)</span><br><span class="line">    get_score(prediction_test, y_tst)</span><br></pre></td></tr></table></figure></p><h2 id="Model-Construction"><a href="#Model-Construction" class="headerlink" title="Model Construction"></a>Model Construction</h2><p>For Elastic Net, we use cross validation method to select the best parameters group for a given parameters map.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ens_test = linear_model.ElasticNetCV(alphas=[<span class="number">0.0001</span>, <span class="number">0.0005</span>, <span class="number">0.001</span>, <span class="number">0.01</span>, <span class="number">0.1</span>, <span class="number">1</span>, <span class="number">10</span>], l1_ratio=[<span class="number">0.01</span>, <span class="number">0.1</span>, <span class="number">0.5</span>, <span class="number">0.9</span>, <span class="number">0.99</span>], max_iter=<span class="number">5000</span>).fit(x_train_st, y_train_st)</span><br><span class="line">train_test(ens_test, x_train_st, x_test_st, y_train_st, y_test_st)</span><br><span class="line"><span class="comment"># Average R2 score and standard deviation of 5-fold cross-validation</span></span><br><span class="line">scores = cross_val_score(ens_test, train_features_st, train_labels, cv=<span class="number">5</span>)</span><br><span class="line">print(<span class="string">"Accuracy: %0.2f (+/- %0.2f)"</span> % (scores.mean(), scores.std() * <span class="number">2</span>))</span><br></pre></td></tr></table></figure></p><p>Here the $\alpha$ is given as <code>alphas=[0.0001, 0.0005, 0.001, 0.01, 0.1, 1, 10]</code>, $\rho$ is given as <code>l1_ratio=[0.01, 0.1, 0.5, 0.9, 0.99]</code>, and the maximal iterations is set as $5000$, $K$ of cross validation is set as $5$.<br>For Gradient Boosting Regressor, the cross validation technique is also used, parameters here are fixed and $K$ of cross validation is set as $5$.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">g_best = ensemble.GradientBoostingRegressor(n_estimators=<span class="number">3000</span>, learning_rate=<span class="number">0.05</span>, max_depth=<span class="number">3</span>, max_features=<span class="string">'sqrt'</span>, min_samples_leaf=<span class="number">15</span>, min_samples_split=<span class="number">10</span>, loss=<span class="string">'huber'</span>).fit(x_train, y_train)</span><br><span class="line">train_test(g_best, x_train, x_test, y_train, y_test)</span><br><span class="line"><span class="comment"># Average R2 score and standard deviation of 5-fold cross-validation</span></span><br><span class="line">scores = cross_val_score(g_best, train_features_st, train_labels, cv=<span class="number">5</span>)</span><br><span class="line">print(<span class="string">"Accuracy: %0.2f (+/- %0.2f)"</span> % (scores.mean(), scores.std() * <span class="number">2</span>))</span><br></pre></td></tr></table></figure></p><p>After cross validation and obtaining the best model of Elastic Net and Gradient Boosting Regressor, we need to retraining the model:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Retraining models</span></span><br><span class="line">gb_model = g_best.fit(train_features, train_labels)</span><br><span class="line">enst_model = ens_test.fit(train_features_st, train_labels)</span><br></pre></td></tr></table></figure></p><p>Then get the predictions and save to file<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Getting our SalePrice estimation</span></span><br><span class="line">final_labels = (np.exp(gb_model.predict(test_features)) + np.exp(enst_model.predict(test_features_st))) / <span class="number">2</span></span><br><span class="line"><span class="comment"># print result</span></span><br><span class="line">output = pd.DataFrame(&#123;<span class="string">'Id'</span>: test.Id, <span class="string">'SalePrice'</span>: final_labels&#125;)</span><br><span class="line">print(output)</span><br><span class="line"><span class="comment"># Saving to CSV</span></span><br><span class="line">output.to_csv(<span class="string">'submission.csv'</span>, index=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure></p><p>Below shows some information in training process and the prediction results:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">ElasticNetCV(alphas=[0.0001, 0.0005, 0.001, 0.01, 0.1, 1, 10], copy_X=True,</span><br><span class="line">       cv=None, eps=0.001, fit_intercept=True,</span><br><span class="line">       l1_ratio=[0.01, 0.1, 0.5, 0.9, 0.99], max_iter=5000, n_alphas=100,</span><br><span class="line">       n_jobs=1, normalize=False, positive=False, precompute=<span class="string">'auto'</span>,</span><br><span class="line">       random_state=None, selection=<span class="string">'cyclic'</span>, tol=0.0001, verbose=0)</span><br><span class="line">R2: 0.9009282706669409</span><br><span class="line">RMSE: 0.1192142029440696</span><br><span class="line">Test</span><br><span class="line">R2: 0.8967299864999421</span><br><span class="line">RMSE: 0.11097041288345283</span><br><span class="line">Accuracy: 0.88 (+/- 0.10)</span><br><span class="line">GradientBoostingRegressor(alpha=0.9, criterion=<span class="string">'friedman_mse'</span>, init=None,</span><br><span class="line">             learning_rate=0.05, loss=<span class="string">'huber'</span>, max_depth=3,</span><br><span class="line">             max_features=<span class="string">'sqrt'</span>, max_leaf_nodes=None,</span><br><span class="line">             min_impurity_split=1e-07, min_samples_leaf=15,</span><br><span class="line">             min_samples_split=10, min_weight_fraction_leaf=0.0,</span><br><span class="line">             n_estimators=3000, presort=<span class="string">'auto'</span>, random_state=None,</span><br><span class="line">             subsample=1.0, verbose=0, warm_start=False)</span><br><span class="line">R2: 0.9617959062813555</span><br><span class="line">RMSE: 0.07593410094831428</span><br><span class="line">Test</span><br><span class="line">R2: 0.9062977017781593</span><br><span class="line">RMSE: 0.10586499921275429</span><br><span class="line">Accuracy: 0.90 (+/- 0.04)</span><br><span class="line"></span><br><span class="line">Predictions:</span><br><span class="line">        Id      SalePrice</span><br><span class="line">0     1461  119290.186865</span><br><span class="line">1     1462  152342.289928</span><br><span class="line">2     1463  180487.561653</span><br><span class="line">3     1464  201057.891220</span><br><span class="line">4     1465  191830.128695</span><br><span class="line">5     1466  170253.195886</span><br><span class="line">6     1467  170236.219405</span><br><span class="line">7     1468  167112.682814</span><br><span class="line">8     1469  190918.239557</span><br><span class="line">9     1470  123530.454116</span><br><span class="line">...    ...            ...</span><br><span class="line">1450  2911   86655.651123</span><br><span class="line">1451  2912  149131.128117</span><br><span class="line">1452  2913   80815.070698</span><br><span class="line">1453  2914   76847.953224</span><br><span class="line">1454  2915   83052.396061</span><br><span class="line">1455  2916   82826.406679</span><br><span class="line">1456  2917  157280.476185</span><br><span class="line">1457  2918  119939.102469</span><br><span class="line">1458  2919  221954.591126</span><br><span class="line"></span><br><span class="line">[1459 rows x 2 columns]</span><br></pre></td></tr></table></figure></p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul><li><a href="https://en.wikipedia.org/wiki/Elastic_net_regularization" target="_blank" rel="noopener">Elastic Net Regularization</a></li><li><a href="http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html" target="_blank" rel="noopener">Elastic Net in sklearn</a></li><li><a href="http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNetCV.html" target="_blank" rel="noopener">Cross Validation of Elastic Net in sklearn</a></li><li><a href="https://en.wikipedia.org/wiki/Gradient_boosting" target="_blank" rel="noopener">Gradient Boosting</a></li><li><a href="http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html" target="_blank" rel="noopener">Gradient Boosting Regressor in sklearn</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;After cleaning and transforming processes in &lt;a href=&quot;https://isaacchanghau.github.io/2017/07/08/House-Price-Advanced-Regression-Techniques-1/&quot;&gt;House Prices Advanced Regression Techniques – Data Analysis Exploration&lt;/a&gt;.
    
    </summary>
    
      <category term="Machine Learning" scheme="https://isaacchanghau.github.io/categories/Machine-Learning/"/>
    
    
      <category term="machine learning" scheme="https://isaacchanghau.github.io/tags/machine-learning/"/>
    
      <category term="python" scheme="https://isaacchanghau.github.io/tags/python/"/>
    
      <category term="elastic net" scheme="https://isaacchanghau.github.io/tags/elastic-net/"/>
    
      <category term="boosting" scheme="https://isaacchanghau.github.io/tags/boosting/"/>
    
  </entry>
  
  <entry>
    <title>House Prices Advanced Regression Techniques -- Data Analysis Exploration</title>
    <link href="https://isaacchanghau.github.io/2017/07/08/House-Price-Advanced-Regression-Techniques-1/"/>
    <id>https://isaacchanghau.github.io/2017/07/08/House-Price-Advanced-Regression-Techniques-1/</id>
    <published>2017-07-08T14:08:02.000Z</published>
    <updated>2017-09-21T08:51:07.000Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://www.kaggle.com/c/house-prices-advanced-regression-techniques" target="_blank" rel="noopener">House Prices: Advanced Regression Techniques</a> is a <a href="https://www.kaggle.com/" target="_blank" rel="noopener">kaggle</a> competition to predict the house prices, which aims to practice feature engineering, RFs, and gradient boosting.<a id="more"></a> After exploring and referring others’ methods, I decide to do it by myself to improve my python skill in data science and data analysis ability.</p><h1 id="Competition-Description"><a href="#Competition-Description" class="headerlink" title="Competition Description"></a>Competition Description</h1><p>Ask a home buyer to describe their dream house, and they probably won’t begin with the height of the basement ceiling or the proximity to an east-west railroad. But this playground competition’s dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence. With 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home.</p><p>The data contains three parts, one is <code>data_description.txt</code>, which gives brief summary of the data information, another two, <code>train.csv</code> and <code>test.csv</code>, are the datasets used to train and test. More details of the data fields’ description are available here: <a href="https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data" target="_blank" rel="noopener">[link]</a>.</p><h1 id="Data-Analysis-and-Feature-Extraction"><a href="#Data-Analysis-and-Feature-Extraction" class="headerlink" title="Data Analysis and Feature Extraction"></a>Data Analysis and Feature Extraction</h1><h2 id="Preliminary-Analysis"><a href="#Preliminary-Analysis" class="headerlink" title="Preliminary Analysis"></a>Preliminary Analysis</h2><h3 id="Data-Preview"><a href="#Data-Preview" class="headerlink" title="Data Preview"></a>Data Preview</h3><p>First of all, we need to load the data and have a glance of it.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">train = pd.read_csv(<span class="string">'train.csv'</span>)</span><br><span class="line">test = pd.read_csv(<span class="string">'test.csv'</span>)</span><br><span class="line">train_labels = train.pop(<span class="string">'SalePrice'</span>) <span class="comment"># separate labels from train dataset</span></span><br><span class="line">data = pd.concat([train, test], keys=[<span class="string">'train'</span>, <span class="string">'test'</span>])</span><br><span class="line">print(data.columns) <span class="comment"># check column decorations</span></span><br><span class="line">print(<span class="string">'rows:'</span>, data.shape[<span class="number">0</span>], <span class="string">', columns:'</span>, data.shape[<span class="number">1</span>]) <span class="comment"># count rows of total dataset</span></span><br><span class="line">print(<span class="string">'rows in train dataset:'</span>, train.shape[<span class="number">0</span>])</span><br><span class="line">print(<span class="string">'rows in test dataset:'</span>, test.shape[<span class="number">0</span>])</span><br></pre></td></tr></table></figure></p><p>the output is shown below:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">Index([<span class="string">'Id'</span>, <span class="string">'MSSubClass'</span>, <span class="string">'MSZoning'</span>, <span class="string">'LotFrontage'</span>, <span class="string">'LotArea'</span>, <span class="string">'Street'</span>,</span><br><span class="line">       <span class="string">'Alley'</span>, <span class="string">'LotShape'</span>, <span class="string">'LandContour'</span>, <span class="string">'Utilities'</span>, <span class="string">'LotConfig'</span>,</span><br><span class="line">       <span class="string">'LandSlope'</span>, <span class="string">'Neighborhood'</span>, <span class="string">'Condition1'</span>, <span class="string">'Condition2'</span>, <span class="string">'BldgType'</span>,</span><br><span class="line">       <span class="string">'HouseStyle'</span>, <span class="string">'OverallQual'</span>, <span class="string">'OverallCond'</span>, <span class="string">'YearBuilt'</span>, <span class="string">'YearRemodAdd'</span>,</span><br><span class="line">       <span class="string">'RoofStyle'</span>, <span class="string">'RoofMatl'</span>, <span class="string">'Exterior1st'</span>, <span class="string">'Exterior2nd'</span>, <span class="string">'MasVnrType'</span>,</span><br><span class="line">       <span class="string">'MasVnrArea'</span>, <span class="string">'ExterQual'</span>, <span class="string">'ExterCond'</span>, <span class="string">'Foundation'</span>, <span class="string">'BsmtQual'</span>,</span><br><span class="line">       <span class="string">'BsmtCond'</span>, <span class="string">'BsmtExposure'</span>, <span class="string">'BsmtFinType1'</span>, <span class="string">'BsmtFinSF1'</span>,</span><br><span class="line">       <span class="string">'BsmtFinType2'</span>, <span class="string">'BsmtFinSF2'</span>, <span class="string">'BsmtUnfSF'</span>, <span class="string">'TotalBsmtSF'</span>, <span class="string">'Heating'</span>,</span><br><span class="line">       <span class="string">'HeatingQC'</span>, <span class="string">'CentralAir'</span>, <span class="string">'Electrical'</span>, <span class="string">'1stFlrSF'</span>, <span class="string">'2ndFlrSF'</span>,</span><br><span class="line">       <span class="string">'LowQualFinSF'</span>, <span class="string">'GrLivArea'</span>, <span class="string">'BsmtFullBath'</span>, <span class="string">'BsmtHalfBath'</span>, <span class="string">'FullBath'</span>,</span><br><span class="line">       <span class="string">'HalfBath'</span>, <span class="string">'BedroomAbvGr'</span>, <span class="string">'KitchenAbvGr'</span>, <span class="string">'KitchenQual'</span>,</span><br><span class="line">       <span class="string">'TotRmsAbvGrd'</span>, <span class="string">'Functional'</span>, <span class="string">'Fireplaces'</span>, <span class="string">'FireplaceQu'</span>, <span class="string">'GarageType'</span>,</span><br><span class="line">       <span class="string">'GarageYrBlt'</span>, <span class="string">'GarageFinish'</span>, <span class="string">'GarageCars'</span>, <span class="string">'GarageArea'</span>, <span class="string">'GarageQual'</span>,</span><br><span class="line">       <span class="string">'GarageCond'</span>, <span class="string">'PavedDrive'</span>, <span class="string">'WoodDeckSF'</span>, <span class="string">'OpenPorchSF'</span>,</span><br><span class="line">       <span class="string">'EnclosedPorch'</span>, <span class="string">'3SsnPorch'</span>, <span class="string">'ScreenPorch'</span>, <span class="string">'PoolArea'</span>, <span class="string">'PoolQC'</span>,</span><br><span class="line">       <span class="string">'Fence'</span>, <span class="string">'MiscFeature'</span>, <span class="string">'MiscVal'</span>, <span class="string">'MoSold'</span>, <span class="string">'YrSold'</span>, <span class="string">'SaleType'</span>,</span><br><span class="line">       <span class="string">'SaleCondition'</span>],</span><br><span class="line">      dtype=<span class="string">'object'</span>)</span><br><span class="line">rows: 2919, columns: 80</span><br><span class="line">rows <span class="keyword">in</span> train dataset: 1460</span><br><span class="line">rows <span class="keyword">in</span> <span class="built_in">test</span> dataset: 1459</span><br></pre></td></tr></table></figure></p><p>Here we have 2919 lines records in total, 1460 lines for training and 1459 lines for testing. Each line has 79 properties (<code>Id</code> is eliminated). Then we take a look at the situation of data missing:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">nans = pd.concat([train.isnull().sum(), train.isnull().sum() / train.shape[<span class="number">0</span>], test.isnull().sum(), test.isnull().sum() / test.shape[<span class="number">0</span>]], axis=<span class="number">1</span>, keys=[<span class="string">'Train'</span>, <span class="string">'Percentage'</span>, <span class="string">'Test'</span>, <span class="string">'Percentage'</span>])</span><br><span class="line">print(nans[nans.sum(axis=<span class="number">1</span>) &gt; <span class="number">0</span>])</span><br></pre></td></tr></table></figure></p><p>we find that <code>Alley</code>, <code>FireplaceQu</code>, <code>PoolQC</code>, <code>Fence</code> and <code>MiscFeature</code> suffer almost or more than 50% data missing. Normally, if the property has more 15% information loss, we should delete it.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">              Train  Percentage  Test  Percentage</span><br><span class="line">MSZoning          0    0.000000     4    0.002742</span><br><span class="line">LotFrontage     259    0.177397   227    0.155586</span><br><span class="line">Alley          1369    0.937671  1352    0.926662</span><br><span class="line">Utilities         0    0.000000     2    0.001371</span><br><span class="line">Exterior1st       0    0.000000     1    0.000685</span><br><span class="line">Exterior2nd       0    0.000000     1    0.000685</span><br><span class="line">MasVnrType        8    0.005479    16    0.010966</span><br><span class="line">MasVnrArea        8    0.005479    15    0.010281</span><br><span class="line">BsmtQual         37    0.025342    44    0.030158</span><br><span class="line">BsmtCond         37    0.025342    45    0.030843</span><br><span class="line">BsmtExposure     38    0.026027    44    0.030158</span><br><span class="line">BsmtFinType1     37    0.025342    42    0.028787</span><br><span class="line">BsmtFinSF1        0    0.000000     1    0.000685</span><br><span class="line">BsmtFinType2     38    0.026027    42    0.028787</span><br><span class="line">BsmtFinSF2        0    0.000000     1    0.000685</span><br><span class="line">BsmtUnfSF         0    0.000000     1    0.000685</span><br><span class="line">TotalBsmtSF       0    0.000000     1    0.000685</span><br><span class="line">Electrical        1    0.000685     0    0.000000</span><br><span class="line">BsmtFullBath      0    0.000000     2    0.001371</span><br><span class="line">BsmtHalfBath      0    0.000000     2    0.001371</span><br><span class="line">KitchenQual       0    0.000000     1    0.000685</span><br><span class="line">Functional        0    0.000000     2    0.001371</span><br><span class="line">FireplaceQu     690    0.472603   730    0.500343</span><br><span class="line">GarageType       81    0.055479    76    0.052090</span><br><span class="line">GarageYrBlt      81    0.055479    78    0.053461</span><br><span class="line">GarageFinish     81    0.055479    78    0.053461</span><br><span class="line">GarageCars        0    0.000000     1    0.000685</span><br><span class="line">GarageArea        0    0.000000     1    0.000685</span><br><span class="line">GarageQual       81    0.055479    78    0.053461</span><br><span class="line">GarageCond       81    0.055479    78    0.053461</span><br><span class="line">PoolQC         1453    0.995205  1456    0.997944</span><br><span class="line">Fence          1179    0.807534  1169    0.801234</span><br><span class="line">MiscFeature    1406    0.963014  1408    0.965045</span><br><span class="line">SaleType          0    0.000000     1    0.000685</span><br></pre></td></tr></table></figure></p><h3 id="Labels-“SalePrice”"><a href="#Labels-“SalePrice”" class="headerlink" title="Labels (“SalePrice”)"></a>Labels (“SalePrice”)</h3><p>After that, let’s extract some information from the <code>SalePrice</code>, which is also the most important thing, since our task is to predict it. From the information shown below, we can derive the data summary of <code>SalePrice</code>, which provides <code>mean</code>, <code>standard deviation</code>, <code>minimum</code>, <code>maximum</code> and so forth.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">print(train_labels.describe())</span><br><span class="line"></span><br><span class="line">count      <span class="number">1460.000000</span></span><br><span class="line">mean     <span class="number">180921.195890</span></span><br><span class="line">std       <span class="number">79442.502883</span></span><br><span class="line">min       <span class="number">34900.000000</span></span><br><span class="line"><span class="number">25</span>%      <span class="number">129975.000000</span></span><br><span class="line"><span class="number">50</span>%      <span class="number">163000.000000</span></span><br><span class="line"><span class="number">75</span>%      <span class="number">214000.000000</span></span><br><span class="line">max      <span class="number">755000.000000</span></span><br><span class="line">Name: SalePrice, dtype: float64</span><br></pre></td></tr></table></figure></p><p>Then we plot its distribution histogram and normal probalility graph.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> stats</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line">sns.plt.figure()</span><br><span class="line">sns.plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">sns.plt.title(<span class="string">"Sale Prices Dist"</span>)</span><br><span class="line">sns.distplot(train_labels, fit=stats.norm)</span><br><span class="line">sns.plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">stats.probplot(train_labels, plot=sns.plt)</span><br><span class="line">sns.plt.show()</span><br><span class="line">print(<span class="string">"Skewness: %f"</span> % train_labels.skew())</span><br><span class="line">print(<span class="string">"Kurtosis: %f"</span> % train_labels.kurt())</span><br><span class="line"><span class="comment"># Skewness and Kurtosis</span></span><br><span class="line">Skewness: <span class="number">1.882876</span></span><br><span class="line">Kurtosis: <span class="number">6.536282</span></span><br></pre></td></tr></table></figure></p><p>The graph shows that the distribution of <code>SalePrice</code> deviates from the normal distribution, it has peak value and it is positive biased, but it does not follow the diagonal line (right side), the diagonal line represents the normal distribution in normal probability graph, and a good data distribution should follow this line closely.<br><img src="/images/machinelearning/houseprices/salepricedist.png" alt="Sale Prices Distribution"><br>In order to solve such problems, we can explore to use the <strong>logarithmic transformation</strong> to see if it is able to handle these problems.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">sns.plt.figure()</span><br><span class="line">sns.plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">sns.plt.title(<span class="string">"Sale Prices Dist"</span>)</span><br><span class="line">sns.distplot(np.log(train_labels), fit=stats.norm)</span><br><span class="line">sns.plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">stats.probplot(np.log(train_labels), plot=sns.plt)</span><br><span class="line">sns.plt.show()</span><br><span class="line">print(<span class="string">"Skewness: %f"</span> % np.log(train_labels).skew())</span><br><span class="line">print(<span class="string">"Kurtosis: %f"</span> % np.log(train_labels).kurt())</span><br><span class="line"><span class="comment"># Skewness and Kurtosis</span></span><br><span class="line">Skewness: <span class="number">0.121335</span></span><br><span class="line">Kurtosis: <span class="number">0.809532</span></span><br></pre></td></tr></table></figure></p><p>See the graph below, it is obvious that the data distribution follows diagonal line better than the previous one, although some outliers exist. thus, it seems that the log transformation is a good choice to transform the data.<br><img src="/images/machinelearning/houseprices/salepricedistlog.png" alt="Sale Prices Distribution after Log"></p><h2 id="Systematic-Analysis"><a href="#Systematic-Analysis" class="headerlink" title="Systematic Analysis"></a>Systematic Analysis</h2><h3 id="Data-Clean-Integration-and-Correlation-Analysis"><a href="#Data-Clean-Integration-and-Correlation-Analysis" class="headerlink" title="Data Clean, Integration and Correlation Analysis"></a>Data Clean, Integration and Correlation Analysis</h3><p>After having some preliminary understanding of the data, next step, we need to explore the relationship between those properties and labels, as well as the internal relationship among those properties and so forth. In the preliminary analysis, we did the analysis on the overall dataset (training dataset + testing dataset). However, in this part, we should only consider the training part (To make the analysis more fair).<br>There are (generally) two types of properties, one is numeric and another is categorical. First of all, we need to drop some unused properties according to the data missing information. As mentioned before, <code>Alley</code>, <code>FireplaceQu</code>, <code>PoolQC</code>, <code>Fence</code> and <code>MiscFeature</code> suffer almost or more than 50% data loss:</p><ul><li><strong>Alley</strong>: Type of alley access</li><li><strong>FireplaceQu</strong>: Fireplace quality</li><li><strong>PoolQC</strong>: Pool quality</li><li><strong>Fence</strong>: Fence quality</li><li><strong>MiscFeature</strong>: Miscellaneous feature not covered in other categories</li></ul><p>Under normal conditions, those features are not within the scope of our consideration when we buy a house, so these features are not important and should be eliminated from the dataset.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train.drop([<span class="string">'Id'</span>, <span class="string">'MiscFeature'</span>, <span class="string">'Fence'</span>, <span class="string">'PoolQC'</span>, <span class="string">'FireplaceQu'</span>, <span class="string">'Alley'</span>], axis=<span class="number">1</span>, inplace=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure></p><p>Here we draw the correlation coefficient matrix to explore the coorelation among different properties:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># draw correlation coefficient matrix</span></span><br><span class="line">corrmat = train.corr()</span><br><span class="line">f, ax = sns.plt.subplots(figsize=(<span class="number">12</span>, <span class="number">9</span>))</span><br><span class="line">sns.heatmap(corrmat, vmax=<span class="number">.8</span>, square=<span class="keyword">True</span>)</span><br><span class="line">sns.plt.yticks(rotation=<span class="number">0</span>)</span><br><span class="line">sns.plt.xticks(rotation=<span class="number">90</span>)</span><br><span class="line">sns.plt.show()</span><br></pre></td></tr></table></figure></p><p>From the graph below, we can clearly see that there are two group of features show high correlation (the two big red blocks), one is the correlation coefficient between <code>TotalBsmtSF</code> and <code>1stFlrSF</code>, another is <code>Garage...</code> feature group.<br><img src="/images/machinelearning/houseprices/correlation1.png" alt="correlation coefficient matrix"><br>The two examples indicate that these features have strong correlation with each other, actually, the degree of correlation reaches the situation of multicollinearity, thus, we can treat that those features contains almost the same information. If you see the <code>SalePrice</code>, you will find that the <code>SalePrice</code> has strong correlations not only <code>GrLivArea</code>, <code>TotalBsmtSF</code> and <code>OverallQual</code>, but also some other features, here we draw the correlation coefficient matrix of <code>SalePrice</code> to show the top 10 features:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">cols = corrmat.nlargest(<span class="number">10</span>, <span class="string">'SalePrice'</span>)[<span class="string">'SalePrice'</span>].index</span><br><span class="line">cm = np.corrcoef(train[cols].values.T)</span><br><span class="line">sns.set(font_scale=<span class="number">1.25</span>)</span><br><span class="line">hm = sns.heatmap(cm, cbar=<span class="keyword">True</span>, annot=<span class="keyword">True</span>, square=<span class="keyword">True</span>, fmt=<span class="string">'.2f'</span>, annot_kws=&#123;<span class="string">'size'</span>: <span class="number">10</span>&#125;, yticklabels=cols.values,</span><br><span class="line">                 xticklabels=cols.values)</span><br><span class="line">sns.plt.yticks(rotation=<span class="number">0</span>)</span><br><span class="line">sns.plt.xticks(rotation=<span class="number">90</span>)</span><br><span class="line">sns.plt.show()</span><br></pre></td></tr></table></figure></p><p>The graph below gives top 10 features which have strong correlation with each other.<br><img src="/images/machinelearning/houseprices/correlation2.png" alt="correlation coefficient matrix of SalePrice"><br>We can derive here that</p><ul><li><code>OverallQual</code>, <code>GrLivArea</code> and <code>TotalBsmtSF</code> have strong correlation with <code>SalePrice</code>.</li><li><code>GarageCars</code> and <code>GarageArea</code> not only have strong correlation with <code>SalePrice</code>, but also have strong correlation with each other. It is easy to imagine that the number of cars store in garage is strong depends on the area of garage, which means that one of them is enough to represent the relationship between <code>SalePrice</code> and <code>Garage...</code>. Here, we choose <code>GarageCars</code>, since it has slightly higher score (Same for other <code>Garage...</code> features).</li><li>The relationship between <code>TotalBsmtSF</code> and <code>1stFlrSF</code> is almost same as <code>GarageCars</code> and <code>GarageArea</code>, we can also choose one of them, but here we will use another strategy, and will be discussed later.</li><li>According to the data description and the graph here, we find that <code>GrLivArea</code> and <code>TotRmsAbvGrd</code> are the similar features too. Their correlation is <strong>0.83</strong>, and the <code>GrLivArea</code> represents the “Above grade (ground) living area square feet”, while <code>TotRmsAbvGrd</code> indicates “Total rooms above grade (does not include bathrooms)”.</li><li>Another thing is that the <code>FullBath</code> and <code>YearBuilt</code> do not have siginificantly strong correlation with <code>SalePrice</code>.</li></ul><p>Here we also show the scatter graph between <code>SalePrice</code> and the most important features:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sns.set()</span><br><span class="line">cols = [<span class="string">'SalePrice'</span>, <span class="string">'OverallQual'</span>, <span class="string">'GrLivArea'</span>, <span class="string">'GarageCars'</span>, <span class="string">'TotalBsmtSF'</span>, <span class="string">'FullBath'</span>, <span class="string">'YearBuilt'</span>]</span><br><span class="line">sns.pairplot(train[cols], size=<span class="number">2.5</span>)</span><br><span class="line">sns.plt.show()</span><br></pre></td></tr></table></figure></p><p><img src="/images/machinelearning/houseprices/scatters.png" alt="Scatters"><br>This scatters graph gives much more affluent information of the relationships among those features.<br>As mentioned just now, we will drop the feature <code>GarageArea</code>. For <code>Bsmt...</code> features group, if you have studied the data in Excel carefully, you will find that <code>TotalBsmtSF</code> equals to the sum of <code>BsmtFinSF1</code>, <code>BsmtFinSF2</code> and <code>BsmtUnfSF</code>, which means that by keeping the <code>TotalBsmtSF</code>, <code>BsmtFinSF1</code>, <code>BsmtFinSF2</code> and <code>BsmtUnfSF</code> are able to be eliminated. Since we also need to handle <code>TotalBsmtSF</code> and <code>1stFlrSF</code>, here we visually show their relationship with <code>SalePrice</code>:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">f, (ax1, ax2, ax3) = sns.plt.subplots(<span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line">data_total = pd.concat([train[<span class="string">'SalePrice'</span>], train[<span class="string">'TotalBsmtSF'</span>]], axis=<span class="number">1</span>)</span><br><span class="line">data_total.plot.scatter(x=<span class="string">'TotalBsmtSF'</span>, y=<span class="string">'SalePrice'</span>, ylim=(<span class="number">0</span>, <span class="number">800000</span>), ax=ax1)</span><br><span class="line">data1 = pd.concat([train[<span class="string">'SalePrice'</span>], train[<span class="string">'1stFlrSF'</span>]], axis=<span class="number">1</span>)</span><br><span class="line">data1.plot.scatter(x=<span class="string">'1stFlrSF'</span>, y=<span class="string">'SalePrice'</span>, ylim=(<span class="number">0</span>, <span class="number">800000</span>), ax=ax2)</span><br><span class="line">data2 = pd.concat([train[<span class="string">'SalePrice'</span>], train[<span class="string">'2ndFlrSF'</span>]], axis=<span class="number">1</span>)</span><br><span class="line">data2.plot.scatter(x=<span class="string">'2ndFlrSF'</span>, y=<span class="string">'SalePrice'</span>, ylim=(<span class="number">0</span>, <span class="number">800000</span>), ax=ax3)</span><br><span class="line">sns.plt.show()</span><br></pre></td></tr></table></figure></p><p>we can derive the fighre below, the first graph is <code>TotalBsmtSF</code>-<code>SalePrice</code>, the second is <code>1stFlrSF</code>-<code>SalePrice</code> and the last one is <code>2ndFlrSF</code>-<code>SalePrice</code>. Generally, all of these three features have closely relationship with <code>SalePrice</code>, and follow the exponential distribution. Only some specific situations that the <code>TotalBsmtSF</code> has no influence on <code>SalePrice</code> (those zero values), while <code>2ndFlrSF</code> does also (although there are much more zero values in <code>2ndFlrSF</code>, the trend is similar).<br><img src="/images/machinelearning/houseprices/bsmt.png" alt="Basement, 1st, 2nd"><br>The description of <code>TotalBsmtSF</code>, <code>1stFlrSF</code> and <code>2ndFlrSF</code>:</p><ul><li><strong>TotalBsmtSF</strong>: Total square feet of basement area.</li><li><strong>1stFlrSF</strong>: First Floor square feet.</li><li><strong>2ndFlrSF</strong>: Second floor square feet.</li></ul><p>In this case, since all of them are the area information of the house and their relationships with <code>SalePrice</code> are similar, so our strategy is creating a new feature named as <code>TotalSF</code> to add those three features, then drop them.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">train.drop([<span class="string">'BsmtFinSF1'</span>, <span class="string">'BsmtFinSF2'</span>, <span class="string">'BsmtUnfSF'</span>], axis=<span class="number">1</span>, inplace=<span class="keyword">True</span>)</span><br><span class="line">train[<span class="string">'TotalBsmtSF'</span>] = train[<span class="string">'TotalBsmtSF'</span>].fillna(<span class="number">0</span>)</span><br><span class="line">train[<span class="string">'1stFlrSF'</span>] = train[<span class="string">'1stFlrSF'</span>].fillna(<span class="number">0</span>)</span><br><span class="line">train[<span class="string">'2ndFlrSF'</span>] = train[<span class="string">'2ndFlrSF'</span>].fillna(<span class="number">0</span>)</span><br><span class="line">train[<span class="string">'TotalSF'</span>] = train[<span class="string">'TotalBsmtSF'</span>] + train[<span class="string">'1stFlrSF'</span>] + train[<span class="string">'2ndFlrSF'</span>]</span><br><span class="line">train.drop([<span class="string">'TotalBsmtSF'</span>, <span class="string">'1stFlrSF'</span>, <span class="string">'2ndFlrSF'</span>], axis=<span class="number">1</span>, inplace=<span class="keyword">True</span>)</span><br><span class="line">train.drop([<span class="string">'GarageArea'</span>], axis=<span class="number">1</span>, inplace=<span class="keyword">True</span>) <span class="comment"># as analysis before</span></span><br></pre></td></tr></table></figure></p><p>and the relationship between <code>TotalSF</code> and <code>SalePrice</code> is show below:<br><img src="/images/machinelearning/houseprices/totalsf.png" alt="TotalSF"><br>Intuitively, the merged feature has a better exponential relationship with <code>SalePrice</code>, compare to the previous three fatures. The two outliers appear in the graph can be considered as the abnormal values and delete them.<br>In the discuss above, we only anslysis and clear some of the most important numeric features, other numeric also needs to be analyzed as well as the categorical features. Becasue of the space limit, here we do not talk much on this process, and below gives some features that I choose to get rid of, since I think they do not have much correlation to <code>SalePrice</code> or some of these features have strong correlation with the features we already decide to retain (which means they are similar, so keep one of them is enough).<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">train.drop([<span class="string">'Utilities'</span>, <span class="string">'RoofMatl'</span>, <span class="string">'MasVnrArea'</span>, <span class="string">'MasVnrType'</span>, <span class="string">'Heating'</span>, <span class="string">'LowQualFinSF'</span>,</span><br><span class="line">            <span class="string">'BsmtFullBath'</span>, <span class="string">'BsmtHalfBath'</span>, <span class="string">'BsmtQual'</span>, <span class="string">'BsmtCond'</span>, <span class="string">'BsmtExposure'</span>, <span class="string">'BsmtFinType1'</span>, <span class="string">'BsmtFinType2'</span>,</span><br><span class="line">            <span class="string">'Functional'</span>, <span class="string">'GarageYrBlt'</span>, <span class="string">'GarageCond'</span>, <span class="string">'GarageType'</span>, <span class="string">'GarageFinish'</span>, <span class="string">'GarageQual'</span>, <span class="string">'WoodDeckSF'</span>,</span><br><span class="line">            <span class="string">'OpenPorchSF'</span>, <span class="string">'EnclosedPorch'</span>, <span class="string">'3SsnPorch'</span>, <span class="string">'ScreenPorch'</span>, <span class="string">'PoolArea'</span>,</span><br><span class="line">            <span class="string">'MiscVal'</span>], axis=<span class="number">1</span>, inplace=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure></p><p>After this process, the following features are remains:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">print(train.columns)</span><br><span class="line"></span><br><span class="line">Index([<span class="string">'MSSubClass'</span>, <span class="string">'MSZoning'</span>, <span class="string">'LotFrontage'</span>, <span class="string">'LotArea'</span>, <span class="string">'Street'</span>,</span><br><span class="line">       <span class="string">'LotShape'</span>, <span class="string">'LandContour'</span>, <span class="string">'LotConfig'</span>, <span class="string">'LandSlope'</span>, <span class="string">'Neighborhood'</span>,</span><br><span class="line">       <span class="string">'Condition1'</span>, <span class="string">'Condition2'</span>, <span class="string">'BldgType'</span>, <span class="string">'HouseStyle'</span>, <span class="string">'OverallQual'</span>,</span><br><span class="line">       <span class="string">'OverallCond'</span>, <span class="string">'YearBuilt'</span>, <span class="string">'YearRemodAdd'</span>, <span class="string">'RoofStyle'</span>, <span class="string">'Exterior1st'</span>,</span><br><span class="line">       <span class="string">'Exterior2nd'</span>, <span class="string">'ExterQual'</span>, <span class="string">'ExterCond'</span>, <span class="string">'Foundation'</span>, <span class="string">'HeatingQC'</span>,</span><br><span class="line">       <span class="string">'CentralAir'</span>, <span class="string">'Electrical'</span>, <span class="string">'GrLivArea'</span>, <span class="string">'FullBath'</span>, <span class="string">'HalfBath'</span>,</span><br><span class="line">       <span class="string">'BedroomAbvGr'</span>, <span class="string">'KitchenAbvGr'</span>, <span class="string">'KitchenQual'</span>, <span class="string">'TotRmsAbvGrd'</span>,</span><br><span class="line">       <span class="string">'Fireplaces'</span>, <span class="string">'GarageCars'</span>, <span class="string">'PavedDrive'</span>, <span class="string">'MoSold'</span>, <span class="string">'YrSold'</span>,</span><br><span class="line">       <span class="string">'SaleType'</span>, <span class="string">'SaleCondition'</span>, <span class="string">'SalePrice'</span>, <span class="string">'TotalSF'</span>],</span><br><span class="line">      dtype=<span class="string">'object'</span>)</span><br></pre></td></tr></table></figure></p><p>There is another important process is to fill the <code>NA</code> values within those features, normally, for categorical features, we can fill the <code>NA</code> with <strong>most frequent values</strong> or <strong>No(such feature)</strong>, while for the numeric features, we can fill <code>NA</code> with <strong>mean value</strong> or <strong>zero</strong>. And we do not talk too much about this here. Next step, we will analysis the relationship between each feature and <code>SalePrice</code>. </p><h3 id="Univariate-Analysis"><a href="#Univariate-Analysis" class="headerlink" title="Univariate Analysis"></a>Univariate Analysis</h3><p>Actually, there are four features which act as siginificantly important roles in this issue. Those four features are <code>OverallQual</code>, <code>YearBuilt</code>, <code>TotalBsmtSF</code> (after data integration process, it becomes <code>TotalSF</code>) and <code>GrLivArea</code>.</p><p><strong>OverallQual and SalePrice</strong><br>Here we explore the relationship between <code>OverallQual</code> and <code>SalePrice</code>:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">overall_qual = pd.concat([train[<span class="string">'SalePrice'</span>], train[<span class="string">'OverallQual'</span>]], axis=<span class="number">1</span>)</span><br><span class="line">f, ax = sns.plt.subplots(figsize=(<span class="number">8</span>, <span class="number">6</span>))</span><br><span class="line">fig = sns.boxplot(x=<span class="string">'OverallQual'</span>, y=<span class="string">"SalePrice"</span>, data=overall_qual)</span><br><span class="line">fig.axis(ymin=<span class="number">0</span>, ymax=<span class="number">800000</span>)</span><br><span class="line">sns.plt.show()</span><br></pre></td></tr></table></figure></p><p>It is obvious that the distribution trends of <code>OverallQual</code> and <code>SalePrice</code> are same. Generally, the <code>SalePrice</code> increase while <code>OverallQual</code> increase.<br><img src="/images/machinelearning/houseprices/overallqual.png" alt="OverallQual-SalePrice"></p><p><strong>YearBuilt and SalePrice</strong><br>Here is the relationship between <code>YearBuilt</code> and <code>SalePrice</code>:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">year_built = pd.concat([train[<span class="string">'SalePrice'</span>], train[<span class="string">'YearBuilt'</span>]], axis=<span class="number">1</span>)</span><br><span class="line">f, ax = sns.plt.subplots(figsize=(<span class="number">16</span>, <span class="number">8</span>))</span><br><span class="line">fig = sns.boxplot(x=<span class="string">'YearBuilt'</span>, y=<span class="string">"SalePrice"</span>, data=year_built)</span><br><span class="line">fig.axis(ymin=<span class="number">0</span>, ymax=<span class="number">800000</span>)</span><br><span class="line">sns.plt.xticks(rotation=<span class="number">90</span>)</span><br><span class="line">sns.plt.show()</span><br></pre></td></tr></table></figure></p><p>Although they do not show a strong following trends, this graph also indicates that, generally, the house price is higher while the built year is later.<br><img src="/images/machinelearning/houseprices/yearbuilt.png" alt="YearBuilt-SalePrice"></p><p><strong>TotalSF and SalePrice</strong><br>Here is the relationship between <code>TotalSF</code> and <code>SalePrice</code>:<br><img src="/images/machinelearning/houseprices/totalsf.png" alt="TotalSF"><br>As discussed before, this two features have strong correlation, and their trends are same. Since <code>TotalSF</code> is one of the most important feature, so let’s go deep. Here we show the distribution histogram and normal probalility graph of <code>TotalSF</code> to explore its normality:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">sns.plt.figure()</span><br><span class="line">sns.plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">sns.plt.title(<span class="string">"TotalSF Dist"</span>)</span><br><span class="line">sns.distplot(train[<span class="string">'TotalSF'</span>], fit=stats.norm)</span><br><span class="line">sns.plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">stats.probplot(train[<span class="string">'TotalSF'</span>], plot=sns.plt)</span><br><span class="line">sns.plt.show()</span><br><span class="line">print(<span class="string">"Skewness: %f"</span> % train[<span class="string">'TotalSF'</span>].skew())</span><br><span class="line">print(<span class="string">"Kurtosis: %f"</span> % train[<span class="string">'TotalSF'</span>].kurt())</span><br><span class="line"><span class="comment"># Skewness and Kurtosis</span></span><br><span class="line">Skewness: <span class="number">1.776700</span></span><br><span class="line">Kurtosis: <span class="number">12.621968</span></span><br></pre></td></tr></table></figure></p><p>Compare with the distribution histogram and normal probalility graph of <code>SalePrice</code> we draw before, you will find that their situation are exactly similar， <code>TotalSF</code> shows positive biased, deviates from the normal distribution, has peak value.<br><img src="/images/machinelearning/houseprices/totalsf-dist.png" alt="TotalSF Distribution"><br>Same as <code>SalePrice</code>, we process <strong>logarithmic transformation</strong> on <code>TotalSF</code> and draw the graph again:<br><img src="/images/machinelearning/houseprices/totalsf-dist-log.png" alt="TotalSF Distribution Log"><br>The graph above shows that after log transformation, the normality of <code>TotalSF</code> becomes better. And below is the homoscedasticity of variance graph of <code>TotalSF</code> and <code>SalePrice</code>:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sf = np.log(train[<span class="string">'TotalSF'</span>])</span><br><span class="line">sp = np.log(train[<span class="string">'SalePrice'</span>])</span><br><span class="line">sns.plt.scatter(sf[sf &gt; <span class="number">0</span>], sp[sf &gt; <span class="number">0</span>])</span><br><span class="line">sns.plt.show()</span><br></pre></td></tr></table></figure></p><p><img src="/images/machinelearning/houseprices/homoscedasticity-totalsf.png" alt="Homoscedasticity TotalSF"><br>This graph shows that <code>SalePrice</code> performs the same level of change within variable range of <code>TotalSF</code>. In this case, we may take the <strong>logarithmic transformation</strong> for <code>TotalSF</code> to fit <code>SalePrice</code> well.</p><p><strong>GrLivArea and SalePrice</strong><br>Here we do the same thing as <code>TotalSF</code> for <code>GrLivArea</code>. First, we draw the relationship between <code>GrLivArea</code> and <code>SalePrice</code>:<br><img src="/images/machinelearning/houseprices/grlivarea.png" alt="GrLivArea"><br>Then plot the distribution histogram and normal probalility graph of <code>GrLivArea</code> to show its normality.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Skewness and Kurtosis</span></span><br><span class="line">Skewness: <span class="number">1.366560</span></span><br><span class="line">Kurtosis: <span class="number">4.895121</span></span><br></pre></td></tr></table></figure></p><p><img src="/images/machinelearning/houseprices/grlivarea-dist.png" alt="GrLivArea Distribution"><br>And do the <strong>logarithmic transformation</strong> on <code>GrLivArea</code> and show the graph again:<br><img src="/images/machinelearning/houseprices/grlivarea-dist-log.png" alt="GrLivArea Distribution Log"><br>Samely, the normality of <code>GrLivArea</code> becomes better after log transformation. Next, we explore the homoscedasticity of variance graph of <code>GrLivArea</code> and <code>SalePrice</code>:<br><img src="/images/machinelearning/houseprices/homoscedasticity-grlivarea.png" alt="Homoscedasticity GrLivArea"><br>This graph shows that <code>SalePrice</code> performs the same level of change within variable range of <code>GrLivArea</code>. In this case, we may take the <strong>logarithmic transformation</strong> for <code>GrLivArea</code> to fit <code>SalePrice</code> well.</p><p><strong>Others Univariables analysis</strong><br>Below shows the relationships between <code>SalePrice</code> and <code>LotFrontage</code>, <code>LotArea</code>, <code>LotShape</code>, <code>LandContour</code>, <code>LandShape</code> and <code>LotConfig</code>. From graph, we can derive that the <code>LogFrontage</code> shows slightly similar trends as <code>SalePrice</code>, it tends to increase while <code>SalePrice</code> increase. However, others do not have such feature, while each value in each feature does not give a clear trends.<br><img src="/images/machinelearning/houseprices/1.png" alt="1"><br>And the following six features also do not have a clear relationship on <code>SalePrice</code>:<br><img src="/images/machinelearning/houseprices/2.png" alt="2"><br>However, although <code>YearRemodAdd</code> feature does not give a clear trends, compare to <code>YearBuilt</code> feature, it still indicates that the house price is higher while the <code>RemodAdd</code> year is later.<br><img src="/images/machinelearning/houseprices/yearremodadd.png" alt="YearRemodAdd"><br>There are still many features do not show here, but each of them are worth to analyze. For analysis, we stop here and make a short summary.</p><ul><li>For <code>Bsmt...</code> feature group, drop all of them, except <code>TotalBsmtSF</code>, then merge <code>TotalBsmtSF</code> with <code>1stFlrSF</code> and <code>2ndFlrSF</code> to create a new feature named as <code>TotalSF</code>.</li><li>For <code>Garage...</code> feature group, drop all of them, except <code>GarageCars</code>.</li><li>Drop the features whose missing percentage is large than 20%.</li><li>Drop the features that we treat them as the trivial features for the house prices predictation.</li><li>Fill <code>NA</code> with most frequent value or zero or mean.</li></ul><p>and so forth.</p><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>In the above, I describe a procedure to analysis the data and the machine learning model for training and predicting the data will be introduced in next article. Actually, with the data process procedures introduced above and the machine learning model (I choose a model with combination of gradient boosting regressor and elastic net), I achieve the score <strong>0.126</strong>. It is not a very good result, but, anyway, this is just a practice after learning the basic knowledge of data science and mechine learning. Below is the data clean, integration and transformation codes:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> ensemble, linear_model</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split, cross_val_score</span><br><span class="line"><span class="keyword">from</span> sklearn.utils <span class="keyword">import</span> shuffle</span><br><span class="line"><span class="keyword">from</span> method2.functions <span class="keyword">import</span> train_test</span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line">warnings.filterwarnings(<span class="string">'ignore'</span>)</span><br><span class="line"><span class="comment"># load data</span></span><br><span class="line">train = pd.read_csv(<span class="string">'train.csv'</span>)</span><br><span class="line">test = pd.read_csv(<span class="string">'test.csv'</span>)</span><br><span class="line">train_labels = train.pop(<span class="string">'SalePrice'</span>)</span><br><span class="line">data = pd.concat([train, test], keys=[<span class="string">'train'</span>, <span class="string">'test'</span>])</span><br><span class="line"><span class="comment"># drop the data with high missing percentage</span></span><br><span class="line">data.drop([<span class="string">'Id'</span>, <span class="string">'MiscFeature'</span>, <span class="string">'Fence'</span>, <span class="string">'PoolQC'</span>, <span class="string">'FireplaceQu'</span>, <span class="string">'Alley'</span>], axis=<span class="number">1</span>, inplace=<span class="keyword">True</span>)</span><br><span class="line"><span class="comment"># drop the Bsmt feature group, Garage feature group as well as some trivial features</span></span><br><span class="line">data.drop([<span class="string">'Utilities'</span>, <span class="string">'RoofMatl'</span>, <span class="string">'MasVnrArea'</span>, <span class="string">'MasVnrType'</span>, <span class="string">'Heating'</span>, <span class="string">'LowQualFinSF'</span>, <span class="string">'BsmtFullBath'</span>,</span><br><span class="line">           <span class="string">'BsmtHalfBath'</span>, <span class="string">'BsmtQual'</span>, <span class="string">'BsmtCond'</span>, <span class="string">'BsmtExposure'</span>, <span class="string">'BsmtFinType1'</span>, <span class="string">'BsmtFinSF1'</span>, <span class="string">'BsmtFinSF2'</span>,</span><br><span class="line">           <span class="string">'BsmtUnfSF'</span>, <span class="string">'BsmtFinType2'</span>, <span class="string">'Functional'</span>, <span class="string">'WoodDeckSF'</span>, <span class="string">'OpenPorchSF'</span>,</span><br><span class="line">           <span class="string">'GarageYrBlt'</span>, <span class="string">'GarageCond'</span>, <span class="string">'GarageType'</span>, <span class="string">'GarageFinish'</span>, <span class="string">'GarageQual'</span>, <span class="string">'GarageArea'</span>,</span><br><span class="line">           <span class="string">'EnclosedPorch'</span>, <span class="string">'3SsnPorch'</span>, <span class="string">'ScreenPorch'</span>, <span class="string">'PoolArea'</span>, <span class="string">'MiscVal'</span>], axis=<span class="number">1</span>, inplace=<span class="keyword">True</span>)</span><br><span class="line"><span class="comment"># merge TotalBsmtSF, 1stFlrSF, 2ndFlrSF to TotalSF</span></span><br><span class="line">data[<span class="string">'TotalBsmtSF'</span>] = data[<span class="string">'TotalBsmtSF'</span>].fillna(<span class="number">0</span>)</span><br><span class="line">data[<span class="string">'1stFlrSF'</span>] = data[<span class="string">'1stFlrSF'</span>].fillna(<span class="number">0</span>)</span><br><span class="line">data[<span class="string">'2ndFlrSF'</span>] = data[<span class="string">'2ndFlrSF'</span>].fillna(<span class="number">0</span>)</span><br><span class="line">data[<span class="string">'TotalSF'</span>] = data[<span class="string">'TotalBsmtSF'</span>] + data[<span class="string">'1stFlrSF'</span>] + data[<span class="string">'2ndFlrSF'</span>]</span><br><span class="line">data.drop([<span class="string">'TotalBsmtSF'</span>, <span class="string">'1stFlrSF'</span>, <span class="string">'2ndFlrSF'</span>], axis=<span class="number">1</span>, inplace=<span class="keyword">True</span>)</span><br><span class="line"><span class="comment"># MSSubClass as categorical</span></span><br><span class="line">data[<span class="string">'MSSubClass'</span>] = data[<span class="string">'MSSubClass'</span>].astype(str)</span><br><span class="line"><span class="comment"># MSZoning: filling NA with most popular values</span></span><br><span class="line">data[<span class="string">'MSZoning'</span>] = data[<span class="string">'MSZoning'</span>].fillna(data[<span class="string">'MSZoning'</span>].mode()[<span class="number">0</span>])</span><br><span class="line"><span class="comment"># LotFrontage: fill NA with mean value</span></span><br><span class="line">data[<span class="string">'LotFrontage'</span>] = data[<span class="string">'LotFrontage'</span>].fillna(data[<span class="string">'LotFrontage'</span>].mean())</span><br><span class="line"><span class="comment"># OverallCond as categorical</span></span><br><span class="line">data[<span class="string">'OverallCond'</span>] = data[<span class="string">'OverallCond'</span>].astype(str)</span><br><span class="line"><span class="comment"># Electrical: fill NA with most popular values</span></span><br><span class="line">data[<span class="string">'Electrical'</span>] = data[<span class="string">'Electrical'</span>].fillna(data[<span class="string">'Electrical'</span>].mode()[<span class="number">0</span>])</span><br><span class="line"><span class="comment"># KitchenAbvGr as categorical</span></span><br><span class="line">data[<span class="string">'KitchenAbvGr'</span>] = data[<span class="string">'KitchenAbvGr'</span>].astype(str)</span><br><span class="line"><span class="comment"># KitchenQual: fill NA with most popular values</span></span><br><span class="line">data[<span class="string">'KitchenQual'</span>] = data[<span class="string">'KitchenQual'</span>].fillna(data[<span class="string">'KitchenQual'</span>].mode()[<span class="number">0</span>])</span><br><span class="line"><span class="comment"># GarageCars: fill NA with 0</span></span><br><span class="line">data[<span class="string">'GarageCars'</span>] = data[<span class="string">'GarageCars'</span>].fillna(<span class="number">0.0</span>)</span><br><span class="line"><span class="comment"># SaleType: fill NA with most popular values</span></span><br><span class="line">data[<span class="string">'SaleType'</span>] = data[<span class="string">'SaleType'</span>].fillna(data[<span class="string">'SaleType'</span>].mode()[<span class="number">0</span>])</span><br><span class="line"><span class="comment"># Year and Month as categorical</span></span><br><span class="line">data[<span class="string">'YrSold'</span>] = data[<span class="string">'YrSold'</span>].astype(str)</span><br><span class="line">data[<span class="string">'MoSold'</span>] = data[<span class="string">'MoSold'</span>].astype(str)</span><br><span class="line"><span class="comment"># Exterior1st and Exterior2nd: fill NA with most popular values</span></span><br><span class="line">data[<span class="string">'Exterior1st'</span>] = data[<span class="string">'Exterior1st'</span>].fillna(data[<span class="string">'Exterior2nd'</span>].mode()[<span class="number">0</span>])</span><br><span class="line">data[<span class="string">'Exterior2nd'</span>] = data[<span class="string">'Exterior2nd'</span>].fillna(data[<span class="string">'Exterior2nd'</span>].mode()[<span class="number">0</span>])</span><br><span class="line"><span class="comment"># Standardizing LotFrontage and LotArea</span></span><br><span class="line">numeric_data = data.loc[:, [<span class="string">'LotFrontage'</span>, <span class="string">'LotArea'</span>, <span class="string">'GrLivArea'</span>, <span class="string">'TotalSF'</span>]]</span><br><span class="line">numeric_data_standardized = (numeric_data - numeric_data.mean())/numeric_data.std()</span><br><span class="line"><span class="comment"># Log transformation of labels, GrLivArea and TotalSF</span></span><br><span class="line">train_labels = np.log(train_labels)</span><br><span class="line"><span class="comment"># data['GrLivArea'] = np.log(data['GrLivArea'])</span></span><br><span class="line"><span class="comment"># data['TotalSF'] = np.log(data['TotalSF'])</span></span><br><span class="line"><span class="comment"># Getting Dummies from Condition1 and Condition2</span></span><br><span class="line">conditions = set([x <span class="keyword">for</span> x <span class="keyword">in</span> data[<span class="string">'Condition1'</span>]] + [x <span class="keyword">for</span> x <span class="keyword">in</span> data[<span class="string">'Condition2'</span>]])</span><br><span class="line">dummies = pd.DataFrame(data=np.zeros((len(data.index), len(conditions))), index=data.index, columns=conditions)</span><br><span class="line"><span class="keyword">for</span> i, cond <span class="keyword">in</span> enumerate(zip(data[<span class="string">'Condition1'</span>], data[<span class="string">'Condition2'</span>])):</span><br><span class="line">    dummies.ix[i, cond] = <span class="number">1</span></span><br><span class="line">data = pd.concat([data, dummies.add_prefix(<span class="string">'Condition_'</span>)], axis=<span class="number">1</span>)</span><br><span class="line">data.drop([<span class="string">'Condition1'</span>, <span class="string">'Condition2'</span>], axis=<span class="number">1</span>, inplace=<span class="keyword">True</span>)</span><br><span class="line"><span class="comment"># Getting Dummies from Exterior1st and Exterior2nd</span></span><br><span class="line">exteriors = set([x <span class="keyword">for</span> x <span class="keyword">in</span> data[<span class="string">'Exterior1st'</span>]] + [x <span class="keyword">for</span> x <span class="keyword">in</span> data[<span class="string">'Exterior2nd'</span>]])</span><br><span class="line">dummies = pd.DataFrame(data=np.zeros((len(data.index), len(exteriors))), index=data.index, columns=exteriors)</span><br><span class="line"><span class="keyword">for</span> i, ext <span class="keyword">in</span> enumerate(zip(data[<span class="string">'Exterior1st'</span>], data[<span class="string">'Exterior2nd'</span>])):</span><br><span class="line">    dummies.ix[i, ext] = <span class="number">1</span></span><br><span class="line">data = pd.concat([data, dummies.add_prefix(<span class="string">'Exterior_'</span>)], axis=<span class="number">1</span>)</span><br><span class="line">data.drop([<span class="string">'Exterior1st'</span>, <span class="string">'Exterior2nd'</span>], axis=<span class="number">1</span>, inplace=<span class="keyword">True</span>)</span><br><span class="line"><span class="comment"># Getting Dummies from all other categorical vars</span></span><br><span class="line"><span class="keyword">for</span> col <span class="keyword">in</span> data.dtypes[data.dtypes == <span class="string">'object'</span>].index:</span><br><span class="line">    for_dummy = data.pop(col)</span><br><span class="line">    data = pd.concat([data, pd.get_dummies(for_dummy, prefix=col)], axis=<span class="number">1</span>)</span><br><span class="line"><span class="comment"># copy data</span></span><br><span class="line">data_standardized = data.copy()</span><br><span class="line"><span class="comment"># Replacing numeric feature by standardized values</span></span><br><span class="line">data_standardized.update(numeric_data_standardized)</span><br><span class="line"><span class="comment"># Splitting dataset to train and test</span></span><br><span class="line">train_data = data.loc[<span class="string">'train'</span>].select_dtypes(include=[np.number]).values</span><br><span class="line">test_data = data.loc[<span class="string">'test'</span>].select_dtypes(include=[np.number]).values</span><br><span class="line"><span class="comment"># Splitting standardized features</span></span><br><span class="line">train_data_st = data_standardized.loc[<span class="string">'train'</span>].select_dtypes(include=[np.number]).values</span><br><span class="line">test_data_st = data_standardized.loc[<span class="string">'test'</span>].select_dtypes(include=[np.number]).values</span><br></pre></td></tr></table></figure></p><p>I also explored others methods and make a midification to improve the result to <strong>0.116</strong>. I will describe it in the future.</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul><li><a href="https://www.kaggle.com/c/house-prices-advanced-regression-techniques/kernels" target="_blank" rel="noopener">Kaggle Kernels of House Prices</a></li><li><a href="https://www.leiphone.com/news/201704/Py7Mu3TwRF97pWc7.html" target="_blank" rel="noopener">Comprehensive Data Exploration using Python</a></li><li><a href="https://www.kaggle.com/neviadomski/how-to-get-to-top-25-with-simple-model-sklearn" target="_blank" rel="noopener">How to get to TOP 25% with Simple Model (sklearn)</a></li><li><a href="https://www.kaggle.com/miguelangelnieto/pca-and-regression" target="_blank" rel="noopener">PCA and Regression</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://www.kaggle.com/c/house-prices-advanced-regression-techniques&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;House Prices: Advanced Regression Techniques&lt;/a&gt; is a &lt;a href=&quot;https://www.kaggle.com/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;kaggle&lt;/a&gt; competition to predict the house prices, which aims to practice feature engineering, RFs, and gradient boosting.
    
    </summary>
    
      <category term="Machine Learning" scheme="https://isaacchanghau.github.io/categories/Machine-Learning/"/>
    
    
      <category term="machine learning" scheme="https://isaacchanghau.github.io/tags/machine-learning/"/>
    
      <category term="python" scheme="https://isaacchanghau.github.io/tags/python/"/>
    
      <category term="data analysis" scheme="https://isaacchanghau.github.io/tags/data-analysis/"/>
    
  </entry>
  
  <entry>
    <title>Spark Installation on Mac OS X</title>
    <link href="https://isaacchanghau.github.io/2017/06/28/Spark-Installation-on-Mac-OS-X/"/>
    <id>https://isaacchanghau.github.io/2017/06/28/Spark-Installation-on-Mac-OS-X/</id>
    <published>2017-06-28T03:33:51.000Z</published>
    <updated>2017-09-21T08:56:58.000Z</updated>
    
    <content type="html"><![CDATA[<p>It is an introduction of Spark installation under localhost mode. Generally, Spark is available for<a id="more"></a> multiple models: <strong>local</strong>, <strong>clustered–Spark Standalone</strong>, <strong>clustered–Spark on Apache Mesos</strong> and so forth, more details here: <a href="https://jaceklaskowski.gitbooks.io/mastering-apache-spark/spark-deployment-environments.html" target="_blank" rel="noopener">link</a>.</p><h1 id="Install-Spark"><a href="#Install-Spark" class="headerlink" title="Install Spark"></a>Install Spark</h1><p>Since Spark requires Hadoop environment, so we need to install and configure Hadoop first. In this case, referring my another article: <a href="https://isaacchanghau.github.io/2017/07/02/Hadoop-Installation-on-Mac-OS-X/">Hadoop Installation on Max OS X</a> to see how to install Hadoop as well as Homebrew, Java installation and SSH configuration. After the Hadoop is installed and configured successfully, we need to install Scala (&gt;2.9.3 version).<br>First, go to Scala official website: <a href="http://www.scala-lang.org/download/" target="_blank" rel="noopener">link</a> and download the newest version of Scala, then unzip and move it to <code>/usr/local/</code>:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ sudo tar -zxvf ~/downloads/scala-2.12.2.tgz -C /usr/<span class="built_in">local</span>/</span><br><span class="line">$ <span class="built_in">cd</span> /usr/<span class="built_in">local</span>/</span><br><span class="line">$ sudo mv scala-2.12.2/ scala/</span><br></pre></td></tr></table></figure></p><p>Open <code>/etc/profile</code>, input and save:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> SCALA_HOME=/usr/<span class="built_in">local</span>/scala</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$SCALA_HOME</span>/bin</span><br></pre></td></tr></table></figure></p><p>Then <code>source profile</code> to activate the path declaration. (Simply, you can install Scala easily by Homebrew <code>brew install scala</code>, it will install Scala under <code>/usr/local/Cellar</code> directory).</p><p>After all things are done properly above, we can start to install Spark, go to Apach Spark official website: <a href="http://spark.apache.org/downloads.html" target="_blank" rel="noopener">link</a>, and download it (you need to select the spark release and package type to ensure the spark you download fits your environment). Then, unzip and move Spark to <code>/usr/local/</code>:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ sudo tar -zxvf ~/Dowmloads/spark-2.1.0-bin-hadoop2.7.tgz -C /usr/<span class="built_in">local</span>/</span><br><span class="line">$ <span class="built_in">cd</span> /usr/<span class="built_in">local</span>/</span><br><span class="line">$ sudo mv spark-2.1.0-bin-hadoop2.7/ spark/</span><br></pre></td></tr></table></figure></p><h1 id="Configure-Spark"><a href="#Configure-Spark" class="headerlink" title="Configure Spark"></a>Configure Spark</h1><p>Open <code>/etc/profile</code> and add:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> SPARK_HOME=/usr/<span class="built_in">local</span>/spark</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$SPARK_HOME</span>/bin</span><br></pre></td></tr></table></figure></p><p>and we also need to configure <code>spark-env.sh</code>:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /usr/<span class="built_in">local</span>/spark/conf</span><br><span class="line">$ sudo cp spark-env.sh.template spark-env.sh</span><br><span class="line">$ sudo vim spark-env.sh</span><br></pre></td></tr></table></figure></p><p>then add:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> SCALA_HOME=/usr/<span class="built_in">local</span>/scala</span><br><span class="line"><span class="built_in">export</span> SPARK_MASTER_IP=localhost</span><br><span class="line"><span class="built_in">export</span> SPARK_WORKER_MEMORY=4g</span><br></pre></td></tr></table></figure></p><p>Here we finish the configuration of Spark environment variables. Then we need to test the Spark by executing <code>spark-shell</code> in the terminal. If it returns the error like:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Mon Jul 03 11:19:17 SGT 2017 Thread[main,5,main] Cleanup action starting</span><br><span class="line">ERROR XBM0H: Directory /usr/<span class="built_in">local</span>/spark/conf/metastore_db cannot be created.</span><br><span class="line">    at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)</span><br><span class="line">    at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)</span><br><span class="line">    at org.apache.derby.impl.services.monitor.StorageFactoryService<span class="variable">$10</span>.run(Unknown Source)</span><br><span class="line">    at java.security.AccessController.doPrivileged(Native Method)</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure></p><p>It means that you do not have permissions to write in that directory, try <code>sudo spark-shell</code>. Normally, it will return:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">Spark context Web UI available at http://192.168.68.237:4040</span><br><span class="line">Spark context available as <span class="string">'sc'</span> (master = <span class="built_in">local</span>[*], app id = <span class="built_in">local</span>-1499055411427).</span><br><span class="line">Spark session available as <span class="string">'spark'</span>.</span><br><span class="line">Welcome to</span><br><span class="line">      ____              __</span><br><span class="line">     / __/__  ___ _____/ /__</span><br><span class="line">    _\ \/ _ \/ _ `/ __/  <span class="string">'_/</span></span><br><span class="line"><span class="string">   /___/ .__/\_,_/_/ /_/\_\   version 2.1.0</span></span><br><span class="line"><span class="string">      /_/</span></span><br><span class="line"><span class="string">         </span></span><br><span class="line"><span class="string">Using Scala version 2.11.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_131)</span></span><br><span class="line"><span class="string">Type in expressions to have them evaluated.</span></span><br><span class="line"><span class="string">Type :help for more information.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">scala&gt;</span></span><br></pre></td></tr></table></figure></p><p>Finally, Spark is installed successfully.</p><h1 id="Use-Spark-with-spark-shell"><a href="#Use-Spark-with-spark-shell" class="headerlink" title="Use Spark with spark-shell"></a>Use Spark with spark-shell</h1><p>After launching Spark (as above), we create a RDD from <code>/usr/local/spark/README.md</code>:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val textFile = sc.textFile(<span class="string">"file:///usr/local/spark/README.md"</span>)</span><br><span class="line">&gt;textFile: org.apache.spark.rdd.RDD[String] = file:///usr/<span class="built_in">local</span>/spark/README.md MapPartitionsRDD[1] at textFile at &lt;console&gt;:24</span><br></pre></td></tr></table></figure></p><p>here, <code>file://</code> prefix (or ignore it) indicates to load a local file. If you want to access a HDFS file, you should indicate <code>hdfs://remote host/Hadoop port/filename</code> and you should make sure that the file has beed updated to HDFS. RDD supports two operation types</p><ol><li>actions: manipulating on current dataset and return result.</li><li>transformations: transform and create a new dataset from current dataset.</li></ol><p>After the RDD is created, we can execute <code>count()</code> and <code>first()</code> method to get the following information:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; textFile.count()</span><br><span class="line">res0: Long = 104</span><br><span class="line"></span><br><span class="line">scala&gt; textFile.first()</span><br><span class="line">res1: String = <span class="comment"># Apache Spark</span></span><br></pre></td></tr></table></figure></p><p>where <code>.count()</code> operation returns the number of item in RDD, for text file, it denotes the total lines of this text file. While <code>.first()</code> operation returns the content of first line of such text file.<br>For transformations, we can use filter transformatin to return a new RDD:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; var linesWithSpark = textFile.filter(line =&gt; line.contains(<span class="string">"Spark"</span>))</span><br><span class="line">linesWithSpark: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[2] at filter at &lt;console&gt;:26</span><br><span class="line"></span><br><span class="line">scala&gt; linesWithSpark.count()</span><br><span class="line">res2: Long = 20</span><br></pre></td></tr></table></figure></p><p><code>.filter()</code> operation will filter the dataset with user-defined rules and return a new RDD (<code>linesWithSpark</code>). In practice, actions and transformations operations of RDD are able to handle various complex manipulations. For instance, we can find a line with highest number of words and return the number:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; import java.lang.Math</span><br><span class="line">import java.lang.Math</span><br><span class="line"></span><br><span class="line">scala&gt; textFile.map(line =&gt; line.split(<span class="string">" "</span>).size).reduce((a, b) =&gt; Math.max(a, b))</span><br><span class="line">res3: Int = 22</span><br></pre></td></tr></table></figure></p><p>It is a lambda expression, firstly, <code>.map()</code> operation map each line to a Integer, which will create a new RDD, then execute <code>.reduce()</code> operation on this RDD to obtain maximum. Besides, Spark is able to implement Hadoop MapReduce too:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val wordCounts = textFile.flatMap(line =&gt; line.split(<span class="string">" "</span>)).map(word =&gt; (word, 1)).reduceByKey((a, b) =&gt; a + b)</span><br><span class="line">wordCounts: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[6] at reduceByKey at &lt;console&gt;:27</span><br><span class="line"></span><br><span class="line">scala&gt; wordCounts.collect()</span><br><span class="line">res4: Array[(String, Int)] = Array((package,1), (this,1), (Version<span class="string">"](http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version),1), (Because,1), (Python,2), (page](http://spark.apache.org/documentation.html).,1), (cluster.,1), (its,1), ([run,1), (general,3), (have,1), (pre-built,1), (YARN,,1), ([http://spark.apache.org/developer-tools.html](the,1), (changed,1), (locally,2), (sc.parallelize(1,1), (only,1), (locally.,1), (several,1), (This,2), (basic,1), (Configuration,1), (learning,,1), (documentation,3), (first,1), (graph,1), (Hive,2), (info,1), (["</span>Specifying,1), (<span class="string">"yarn"</span>,1), ([params]`.,1), ([project,1), (prefer,1), (SparkPi,2), (&lt;http://spark.apache.org/&gt;,1), (engine,1), (version,1), (file,1), (documentation,,1), (MASTER,1), (example,3), ([<span class="string">"Parallel,1), (are...</span></span><br></pre></td></tr></table></figure></p><p>Above only give some simple examples of spark usage through Scala, you can get more examples and information through Google and related forum.</p><h1 id="Create-Spark-Project-in-IntelliJ"><a href="#Create-Spark-Project-in-IntelliJ" class="headerlink" title="Create Spark Project in IntelliJ"></a>Create Spark Project in IntelliJ</h1><p>First create a maven project in IntelliJ and add the Spark dependency:<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version="1.0" encoding="UTF-8"?&gt;</span><br><span class="line"><span class="tag">&lt;<span class="name">project</span> <span class="attr">xmlns</span>=<span class="string">"http://maven.apache.org/POM/4.0.0"</span></span></span><br><span class="line"><span class="tag">         <span class="attr">xmlns:xsi</span>=<span class="string">"http://www.w3.org/2001/XMLSchema-instance"</span></span></span><br><span class="line"><span class="tag">         <span class="attr">xsi:schemaLocation</span>=<span class="string">"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">modelVersion</span>&gt;</span>4.0.0<span class="tag">&lt;/<span class="name">modelVersion</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.isaac.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>SparkTest<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.0-SNAPSHOT<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">build</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugins</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-compiler-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">source</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">source</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">target</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">target</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugins</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">build</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">properties</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">project.build.sourceEncoding</span>&gt;</span>UTF-8<span class="tag">&lt;/<span class="name">project.build.sourceEncoding</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">spark.version</span>&gt;</span>2.1.0<span class="tag">&lt;/<span class="name">spark.version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">properties</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- https://mvnrepository.com/artifact/org.apache.spark/spark-core_2.10 --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-core_2.10<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;spark.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">project</span>&gt;</span></span><br></pre></td></tr></table></figure></p><p>Then create <code>WordCount.java</code> file:<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.isaac.spark;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.SparkConf;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaPairRDD;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaRDD;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.FlatMapFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.Function2;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.PairFunction;</span><br><span class="line"><span class="keyword">import</span> scala.Tuple2;</span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="keyword">import</span> java.util.Iterator;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"><span class="keyword">import</span> java.util.regex.Pattern;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Created by zhanghao on 3/7/17.</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> ZHANG HAO</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCount</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span> <span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// Create RDD object</span></span><br><span class="line">        SparkConf conf = <span class="keyword">new</span> SparkConf().setAppName(<span class="string">"Simple Test"</span>).setMaster(<span class="string">"local"</span>);</span><br><span class="line">        JavaSparkContext sc = <span class="keyword">new</span> JavaSparkContext(conf);</span><br><span class="line">        <span class="comment">// load data</span></span><br><span class="line">        JavaRDD&lt;String&gt; textFile = sc.textFile(<span class="string">"file:///Users/zhanghao/Desktop/test.txt"</span>);</span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         * User is able to handle different operations on the obtained DStream,</span></span><br><span class="line"><span class="comment">         * first we split the data, then use Map and ReduceByKey to calculation.</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        JavaRDD&lt;String&gt; words = textFile.flatMap(<span class="keyword">new</span> FlatMapFunction&lt;String, String&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> Iterator&lt;String&gt; <span class="title">call</span><span class="params">(String s)</span> </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> Arrays.asList(Pattern.compile(<span class="string">" "</span>).split(s)).iterator();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">        JavaPairRDD&lt;String, Integer&gt; ones = words.mapToPair(<span class="keyword">new</span> PairFunction&lt;String, String, Integer&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> Tuple2&lt;String, Integer&gt; <span class="title">call</span><span class="params">(String s)</span> </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> Tuple2&lt;&gt;(s, <span class="number">1</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">        JavaPairRDD&lt;String, Integer&gt; counts = ones.reduceByKey(<span class="keyword">new</span> Function2&lt;Integer, Integer, Integer&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> Integer <span class="title">call</span><span class="params">(Integer i1, Integer i2)</span> </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> i1 + i2;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">        List&lt;Tuple2&lt;String, Integer&gt;&gt; output = counts.collect();</span><br><span class="line">        <span class="keyword">for</span> (Tuple2&lt;?, ?&gt; tuple : output) &#123;</span><br><span class="line">            System.out.println(tuple._1() + <span class="string">": "</span> + tuple._2());</span><br><span class="line">        &#125;</span><br><span class="line">        sc.stop();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>The output is shown below:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">Using Spark<span class="string">'s default log4j profile: org/apache/spark/log4j-defaults.properties</span></span><br><span class="line"><span class="string">17/07/03 22:18:01 INFO SparkContext: Running Spark version 2.1.0</span></span><br><span class="line"><span class="string">17/07/03 22:18:01 WARN SparkContext: Support for Scala 2.10 is deprecated as of Spark 2.1.0</span></span><br><span class="line"><span class="string">17/07/03 22:18:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span></span><br><span class="line"><span class="string">...</span></span><br><span class="line"><span class="string">17/07/03 22:18:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 7 ms</span></span><br><span class="line"><span class="string">17/07/03 22:18:06 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1979 bytes result sent to driver</span></span><br><span class="line"><span class="string">17/07/03 22:18:06 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 55 ms on localhost (executor driver) (1/1)</span></span><br><span class="line"><span class="string">17/07/03 22:18:06 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool </span></span><br><span class="line"><span class="string">17/07/03 22:18:06 INFO DAGScheduler: ResultStage 1 (collect at WordCount.java:54) finished in 0.056 s</span></span><br><span class="line"><span class="string">Hello: 1</span></span><br><span class="line"><span class="string">love: 1</span></span><br><span class="line"><span class="string">Java: 1</span></span><br><span class="line"><span class="string">I: 1</span></span><br><span class="line"><span class="string">Spark.: 1</span></span><br><span class="line"><span class="string">and: 1</span></span><br><span class="line"><span class="string">World,: 1</span></span><br><span class="line"><span class="string">17/07/03 22:18:06 INFO DAGScheduler: Job 0 finished: collect at WordCount.java:54, took 0.561756 s</span></span><br><span class="line"><span class="string">17/07/03 22:18:06 INFO SparkUI: Stopped Spark web UI at http://192.168.1.4:4040</span></span><br><span class="line"><span class="string">17/07/03 22:18:07 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!</span></span><br><span class="line"><span class="string">...</span></span><br></pre></td></tr></table></figure></p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul><li><a href="http://codingxiaxw.cn/2016/12/07/60-mac-spark/" target="_blank" rel="noopener">Install and Use Spark under Mac</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;It is an introduction of Spark installation under localhost mode. Generally, Spark is available for
    
    </summary>
    
      <category term="Setting" scheme="https://isaacchanghau.github.io/categories/Setting/"/>
    
    
      <category term="mac os x" scheme="https://isaacchanghau.github.io/tags/mac-os-x/"/>
    
      <category term="hadoop" scheme="https://isaacchanghau.github.io/tags/hadoop/"/>
    
      <category term="spark" scheme="https://isaacchanghau.github.io/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop Installation on Mac OS X</title>
    <link href="https://isaacchanghau.github.io/2017/06/27/Hadoop-Installation-on-Mac-OS-X/"/>
    <id>https://isaacchanghau.github.io/2017/06/27/Hadoop-Installation-on-Mac-OS-X/</id>
    <published>2017-06-27T08:58:37.000Z</published>
    <updated>2017-09-21T08:32:32.000Z</updated>
    
    <content type="html"><![CDATA[<p>It is an introduction of Hadoop installation under pseudo-distributed model. The difference among<a id="more"></a> single node, pseudo-distributed and distributed is introduced here: <a href="https://stackoverflow.com/questions/23435333/what-is-the-difference-between-single-node-pseudo-distributed-mode-in-hadoop" target="_blank" rel="noopener">link</a>.</p><h1 id="Install-Homebrew-and-Cask"><a href="#Install-Homebrew-and-Cask" class="headerlink" title="Install Homebrew and Cask"></a>Install Homebrew and Cask</h1><p>Homebrew is a free and open-source software package management system that simplifies the installation of software on Apple’s macOS operating system. Originally written by Max Howell, the package manager has gained popularity in the Ruby on Rails community and earned praise for its extensibility. Homebrew has been recommended for its ease of use as well as its integration into the command line.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ ruby -e <span class="string">"<span class="variable">$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)</span>"</span></span><br><span class="line">$ brew install caskroom/cask/brew-cask</span><br></pre></td></tr></table></figure></p><h1 id="Install-Java"><a href="#Install-Java" class="headerlink" title="Install Java"></a>Install Java</h1><p>Use Homebrew:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ brew update</span><br><span class="line">$ brew cask install java</span><br></pre></td></tr></table></figure></p><p>Or, download Java from offical website and install it manually: <a href="http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html" target="_blank" rel="noopener">link</a>.</p><h1 id="Configure-SSH"><a href="#Configure-SSH" class="headerlink" title="Configure SSH"></a>Configure SSH</h1><p>In order to keep the safety of Hadoop remote administration as well as user sharing among Hadoop nodes, Hadoop requires SSH protocol. First, go to <code>System Preferences -&gt; Sharing</code>, change <code>Allow access for</code>: <strong>All Users</strong>. Then open Terminal, input <code>ssh localhost</code>, if terminal returns <code>Last login: Sun Jul  2 16:57:36 2017</code>, which means that you have configured SSH Keys successfully before.<br>If you suffer the problem of <code>ssh: connect to host localhost port 22: Connection refused</code>, it happens since the remote login is closed.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ sudo systemsetup -getremotelogin</span><br><span class="line">Remote Login: off</span><br></pre></td></tr></table></figure></p><p>You need to open port 22 in Mac OS X:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ sudo systemsetup -setremotelogin on</span><br><span class="line">$ ssh localhost</span><br><span class="line">Last login: ...</span><br></pre></td></tr></table></figure></p><p>And if you did not get the information of <code>Last login: ...</code>, then you need to create a new configuration:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ssh-keygen -t rsa</span><br></pre></td></tr></table></figure></p><p>Executing the command above will generate a <code>id_rsa</code> file in <code>.ssh</code> directory under the current user directory, after that, input the following command:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys</span><br></pre></td></tr></table></figure></p><p>which is used to authorize your public key to the local to prevent the passphrase request when you login, then, input <code>ssh localhost</code> again, if it will return <code>Last login: ...</code>.</p><h1 id="Install-Hadoop"><a href="#Install-Hadoop" class="headerlink" title="Install Hadoop"></a>Install Hadoop</h1><p>First, install Hadoop via Homebrew: <code>brew install hadoop</code>, it will install the hadoop under <code>/usr/local/Cellar/hadoop</code>.<br>Then, you need to modify the configuration files:<br>Go to <code>usr/local/Cellar/hadoop/2.8.0/libexec/etc/hadoop</code>, then open <code>hadoop-env.sh</code><br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export HADOOP_OPTS="$HADOOP_OPTS -Djava.net.preferIPv4Stack=true"</span><br></pre></td></tr></table></figure></p><p>change to<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export HADOOP_OPTS="$HADOOP_OPTS -Djava.net.preferIPv4Stack=true -Djava.security.krb5.realm= -Djava.security.krb5.kdc="</span><br><span class="line">export JAVA_HOME="/Library/Java/JavaVirtualMachines/jdk1.7.0_79.jdk/Contents/Home"</span><br></pre></td></tr></table></figure></p><p>Then configure HDFS address and port number, open <code>core-site.xml</code>, input following content in <code>&lt;configuration&gt;&lt;/configuration&gt;</code> tag<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- Put site-specific property overrides in this file. --&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/usr/local/Cellar/hadoop/hdfs/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>A base for other temporary directories.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.default.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://localhost:8020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure></p><p>Configure <code>jobtracker</code> address and port number in map-reduce, first <code>sudo cp mapred-site.xml.template mapred-site.xml</code> to make a copy of <code>mapred-site.xml</code>, and open <code>mapred-site.xml</code>, add<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapred.job.tracker<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>localhost:8021<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure></p><p>Set HDFS default backup, the default value is 3, we should change to 1, open <code>hdfs-site.xml</code>, add<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure></p><p>Before running background program, we should format the installed HDFS first, executing command <code>hdfs namenode -format</code>, when terminal returns a long inforamtion like:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">17/07/02 16:11:05 INFO namenode.NameNode: STARTUP_MSG: </span><br><span class="line">/************************************************************</span><br><span class="line">......</span><br><span class="line">17/07/02 16:11:07 INFO namenode.NameNode: SHUTDOWN_MSG: </span><br><span class="line">/************************************************************</span><br><span class="line">SHUTDOWN_MSG: Shutting down NameNode at haodemacbook-pro.local/192.168.1.4</span><br><span class="line">************************************************************/</span><br></pre></td></tr></table></figure></p><p>It means that we finish HDFS configuration, and Hadoop is ready to launch. Besides, maybe you will get a warning<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ... WARN util.NativeCodeLoader: Unable to load native-hadoop library <span class="keyword">for</span> your platform... using <span class="built_in">builtin</span>-java classes <span class="built_in">where</span> applicable</span><br></pre></td></tr></table></figure></p><p>It happens since you are running on 64-bit system but Hadoop native library is based on 32-bit. This is not a big issue. If it appears, you can fixed by refering this link: <a href="https://stackoverflow.com/questions/19943766/hadoop-unable-to-load-native-hadoop-library-for-your-platform-warning" target="_blank" rel="noopener">here</a>.</p><h1 id="Launch-Hadoop"><a href="#Launch-Hadoop" class="headerlink" title="Launch Hadoop"></a>Launch Hadoop</h1><p>Go to <code>/usr/local/Cellar/hadoop/2.8.0/sbin</code>, execute:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ ./start-dfs.sh <span class="comment"># start HDFS service</span></span><br><span class="line">$ ./stop-dfs.sh <span class="comment"># stop HDFS service</span></span><br></pre></td></tr></table></figure></p><p>Ternimal will return the following information:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Starting namenodes on [localhost]</span><br><span class="line">localhost: starting namenode, logging to /usr/<span class="built_in">local</span>/Cellar/hadoop/2.8.0/libexec/logs/hadoop-zhanghao-namenode-HaodeMacBook-Pro.local.out</span><br><span class="line">localhost: starting datanode, logging to /usr/<span class="built_in">local</span>/Cellar/hadoop/2.8.0/libexec/logs/hadoop-zhanghao-datanode-HaodeMacBook-Pro.local.out</span><br><span class="line">Starting secondary namenodes [0.0.0.0]</span><br></pre></td></tr></table></figure></p><p>It means the local service launched successfully, then open Resource Manager in browser through the link <code>http://localhost:50070</code>, you can see the following page<br><img src="/images/settings/hadoop/resource-manager.png" alt="Resource Manager"><br>Samely, under current diretory, you can start the JobTracker through the commands:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ ./start-yarn.sh <span class="comment"># start yarn, MapReduce framework</span></span><br><span class="line">$ ./stop-yarn.sh <span class="comment"># stop yarn</span></span><br></pre></td></tr></table></figure></p><p>Then open browser and go to the page <code>http://localhost:8088</code>, Specific Node Information <code>http://localhost:8042</code>, you will see<br><img src="/images/settings/hadoop/jobtracker.png" alt="JobTracker"><br><img src="/images/settings/hadoop/node-info.png" alt="Specific Node Information"><br>Simply, you can execute <code>./start-all.sh</code> and <code>./stop-all.sh</code> to start or close all the hadoop service.<br>Finally, open <code>/etc/profile</code> and add the configuration information of Hadoop environment variables.<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export HADOOP_HOME=/usr/local/Cellar/hadoop/2.8.0</span><br><span class="line">export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin</span><br></pre></td></tr></table></figure></p><p>Then you can start and close Hadoop under the user directory rather than go to <code>/usr/local/Cellar/hadoop/2.8.0/sbin</code> every time.</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul><li><a href="https://en.wikipedia.org/wiki/Homebrew_%28package_management_software%29" target="_blank" rel="noopener">Homebrew</a></li><li><a href="http://codingxiaxw.cn/2016/12/06/59-mac-hadoop/" target="_blank" rel="noopener">Installation and Usage of Hadoop</a></li><li><a href="http://dongfeiwww.com/hadoop/2014/09/05/hadoop-on-mac/" target="_blank" rel="noopener">Hadoop on Mac</a></li><li><a href="http://www.aboutyun.com/thread-6839-1-1.html" target="_blank" rel="noopener">Difference of single node, pseudo-distributed and distributed model of Hadoop</a></li><li><a href="https://stackoverflow.com/questions/23435333/what-is-the-difference-between-single-node-pseudo-distributed-mode-in-hadoop" target="_blank" rel="noopener">What is the difference between single node &amp; pseudo-distributed mode in Hadoop?</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;It is an introduction of Hadoop installation under pseudo-distributed model. The difference among
    
    </summary>
    
      <category term="Setting" scheme="https://isaacchanghau.github.io/categories/Setting/"/>
    
    
      <category term="mac os x" scheme="https://isaacchanghau.github.io/tags/mac-os-x/"/>
    
      <category term="hadoop" scheme="https://isaacchanghau.github.io/tags/hadoop/"/>
    
  </entry>
  
  <entry>
    <title>Install XGBoost on Mac OS X</title>
    <link href="https://isaacchanghau.github.io/2017/06/20/Install-XGBoost-on-Mac-OS-X/"/>
    <id>https://isaacchanghau.github.io/2017/06/20/Install-XGBoost-on-Mac-OS-X/</id>
    <published>2017-06-20T06:52:50.000Z</published>
    <updated>2017-09-21T09:19:16.000Z</updated>
    
    <content type="html"><![CDATA[<p>Generally, installing XGBoost on Mac is not a cumbersome task, but I still suffered some errors while dealing with it.<a id="more"></a> So I make a note here to record the overall process to make it works.</p><p>XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. Um… those words are directly copy from XGBoost GitHub page. Here is not a introduction of XGBoost, but a tutorial for how to install it on Mac.</p><h1 id="Prerequisite"><a href="#Prerequisite" class="headerlink" title="Prerequisite"></a>Prerequisite</h1><p>First of all, the Homebrew is required, if you do not install it yet, you can get it from here: <a href="https://brew.sh/" target="_blank" rel="noopener">[link]</a>. Installing it is straightforward, just copy the link provided in Homebrew home page and execute it.</p><p>Then you need install <code>gcc</code> with OpenMP:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ brew install gcc --without-multilib</span><br></pre></td></tr></table></figure></p><p>It will download and build <code>gcc</code> automatically. This process takes a while, around 20 minutes or more. After <code>gcc</code> is built successfully, we can start to install XGBoost.</p><h1 id="Install-XGBoost"><a href="#Install-XGBoost" class="headerlink" title="Install XGBoost"></a>Install XGBoost</h1><p>First, get the XGBoost:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> &lt;Directory&gt; <span class="comment"># a directory where you want to store it</span></span><br><span class="line">$ git <span class="built_in">clone</span> --recursive https://github.com/dmlc/xgboost</span><br></pre></td></tr></table></figure></p><p>After cloning XGBoost to you local directory, we need to build it. Here I name the <code>&lt;Directory&gt;</code> as <code>xgboost</code><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> xgboost</span><br><span class="line">$ vi make/config.mk</span><br></pre></td></tr></table></figure></p><p>Then uncomment the following two lines:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> CC = gcc</span><br><span class="line"><span class="built_in">export</span> CXX = g++</span><br></pre></td></tr></table></figure></p><p>This is not enough, if you just uncomment it and try to build the XGBoost, it will return the following error to you (system tried to build XGBoost with <code>clang</code>):<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">clang: error: unsupported option <span class="string">'-fopenmp'</span></span><br><span class="line">...</span><br></pre></td></tr></table></figure></p><p>Since it will use the self-contained modules of system to make and compile it, the system has the <code>clang</code> version accord to ~3.7, and openMP was default supported since 3.8. Here we need to use the installed <code>gcc</code> just now to build it. Here we should check which version of <code>gcc</code> we have installed first:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /usr/<span class="built_in">local</span>/bin</span><br></pre></td></tr></table></figure></p><p>Check which version of <code>gcc</code> was installed, mine is <code>gcc-7</code> and <code>g++-7</code>. So I change the two umcomment lines to:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> CC = gcc-7</span><br><span class="line"><span class="built_in">export</span> CXX = g++-7</span><br></pre></td></tr></table></figure></p><p>Then build XGBoost with following commands:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> ..</span><br><span class="line">$ cp make/config.mk .</span><br><span class="line">$ sudo make -j4 <span class="comment"># sudo is important, prevent permission denied issue</span></span><br></pre></td></tr></table></figure></p><h1 id="Python-Package-Configuration"><a href="#Python-Package-Configuration" class="headerlink" title="Python-Package Configuration"></a>Python-Package Configuration</h1><p>Once XGBoost is built, we can use it with its command line. Since I am using Python, so I also do the following step to set it to my python environment:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> python-package</span><br><span class="line">$ sudo python3 setup.py install</span><br></pre></td></tr></table></figure></p><p>Then you will get the information:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">Installed /usr/<span class="built_in">local</span>/lib/python3.6/site-packages/xgboost-0.6-py3.6.egg</span><br><span class="line">Processing dependencies <span class="keyword">for</span> xgboost==0.6</span><br><span class="line">Searching <span class="keyword">for</span> scipy==0.19.1</span><br><span class="line">Best match: scipy 0.19.1</span><br><span class="line">Adding scipy 0.19.1 to easy-install.pth file</span><br><span class="line"></span><br><span class="line">Using /usr/<span class="built_in">local</span>/lib/python3.6/site-packages</span><br><span class="line">Searching <span class="keyword">for</span> numpy==1.13.0</span><br><span class="line">Best match: numpy 1.13.0</span><br><span class="line">Adding numpy 1.13.0 to easy-install.pth file</span><br><span class="line"></span><br><span class="line">Using /usr/<span class="built_in">local</span>/lib/python3.6/site-packages</span><br><span class="line">Finished processing dependencies <span class="keyword">for</span> xgboost==0.6</span><br></pre></td></tr></table></figure></p><p>which means you have installed XGBoost successfully, then you can restart Python (or the IDE you used), and you will be able to <code>import xgboost</code>.</p><h1 id="JVM-Package-Configuration"><a href="#JVM-Package-Configuration" class="headerlink" title="JVM-Package Configuration"></a>JVM-Package Configuration</h1><p>Currently, XGBoost4J only support installation from source. Building XGBoost4J using Maven requires Maven 3 or newer, Java 7+ and CMake 3.2+ for compiling the JNI bindings. Before you install XGBoost4J, you need to define environment variable <code>JAVA_HOME</code> as your JDK directory to ensure that your compiler can find <code>jni.h</code> correctly, since XGBoost4J relies on JNI to implement the interaction between the JVM and native libraries.<br>After your <code>JAVA_HOME</code> is defined correctly, it is as simple as run <code>mvn package</code> under jvm-packages directory to install XGBoost4J. You can also skip the tests by running <code>mvn -DskipTests=true package</code>, if you are sure about the correctness of your local setup.<br>Go to <code>jvm-packages</code>, then use <code>mvn</code> command to compile it.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> jvm-packages</span><br><span class="line">$ sudo mvn package <span class="comment"># sudo is important, prevent permission denied issue</span></span><br></pre></td></tr></table></figure></p><p>If the setting is correct, we will get the following information (It will take some time to compile, be patient):<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">[INFO] </span><br><span class="line">[INFO] --- maven-jar-plugin:2.4:jar (default-jar) @ xgboost4j-example ---</span><br><span class="line">[INFO] Building jar: /usr/<span class="built_in">local</span>/xgboost/jvm-packages/xgboost4j-example/target/xgboost4j-example-0.7.jar</span><br><span class="line">[INFO] </span><br><span class="line">[INFO] --- maven-assembly-plugin:2.6:single (make-assembly) @ xgboost4j-example ---</span><br><span class="line">[INFO] Building jar: /usr/<span class="built_in">local</span>/xgboost/jvm-packages/xgboost4j-example/target/xgboost4j-example-0.7-jar-with-dependencies.jar</span><br><span class="line">[INFO] ------------------------------------------------------------------------</span><br><span class="line">[INFO] Reactor Summary:</span><br><span class="line">[INFO] </span><br><span class="line">[INFO] xgboost-jvm ........................................ SUCCESS [  5.185 s]</span><br><span class="line">[INFO] xgboost4j .......................................... SUCCESS [02:26 min]</span><br><span class="line">[INFO] xgboost4j-spark .................................... SUCCESS [02:55 min]</span><br><span class="line">[INFO] xgboost4j-flink .................................... SUCCESS [ 59.954 s]</span><br><span class="line">[INFO] xgboost4j-example .................................. SUCCESS [ 19.563 s]</span><br><span class="line">[INFO] ------------------------------------------------------------------------</span><br><span class="line">[INFO] BUILD SUCCESS</span><br><span class="line">[INFO] ------------------------------------------------------------------------</span><br><span class="line">[INFO] Total time: 06:46 min</span><br><span class="line">[INFO] Finished at: 2017-07-27T19:04:16+08:00</span><br><span class="line">[INFO] Final Memory: 434M/1551M</span><br><span class="line">[INFO] ------------------------------------------------------------------------</span><br><span class="line">IsaacChanghaus-MacBook-Pro:jvm-packages zhanghao$</span><br></pre></td></tr></table></figure></p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul><li><a href="https://github.com/dmlc/xgboost" target="_blank" rel="noopener">XGBoost GitHub Page</a></li><li><a href="https://stackoverflow.com/questions/39315156/how-to-install-xgboost-in-python-on-mac" target="_blank" rel="noopener">How to install xgboost in python on Mac?</a></li><li><a href="https://www.ibm.com/developerworks/community/blogs/jfp/entry/Installing_XGBoost_on_Mac_OSX?lang=en" target="_blank" rel="noopener">Installing XGBoost on Mac OSX</a></li><li><a href="https://github.com/Microsoft/LightGBM/issues/3" target="_blank" rel="noopener">unsupported option ‘-fopenmp’ on Mac OS X</a></li><li><a href="http://xgboost.readthedocs.io/en/latest/jvm/" target="_blank" rel="noopener">XGBoost JVM Package</a></li><li><a href="https://docs.databricks.com/user-guide/faq/xgboost.html" target="_blank" rel="noopener">databricks – Install and Use XGBoost</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Generally, installing XGBoost on Mac is not a cumbersome task, but I still suffered some errors while dealing with it.
    
    </summary>
    
      <category term="Setting" scheme="https://isaacchanghau.github.io/categories/Setting/"/>
    
    
      <category term="mac os x" scheme="https://isaacchanghau.github.io/tags/mac-os-x/"/>
    
  </entry>
  
</feed>
